
  
    
      
        
        Kofi Annan, the Secretary-General of the United Nations, recently called attention to
        the clear inequalities in science between developing and developed countries and to the
        challenges of building bridges across these gaps that should bring the United Nations and
        the world scientific community closer to each other (Annan 2003). Mr. Annan stressed the
        importance of reducing the inequalities in science between developed and developing
        countries, asserting that “This unbalanced distribution of scientific activity generates
        serious problems not only for the scientific community in the developing countries, but for
        development itself.” Indeed, Mr. Annan's sentiments have also been echoed recently by
        several scientists, who present overwhelming evidence for the disparity in scientific
        output between the developing and already developed countries (Gibbs 1995; May 1997;
        Goldemberg 1998; Riddoch 2000). For example, recent United Nations Educational, Scientific,
        and Cultural Organization (UNESCO) estimates (UNESCO 2001) indicate that, in 1997, the
        developed countries accounted for some 84% of the global investment in scientific research
        and development, had approximately 72% of the world researchers, and produced approximately
        88% of all scientific and technical publications registered by the Science Citation Index
        (SCI). North America and Europe clearly dominate the number of scientific publications
        produced annually, with 36.6% and 37.5%, respectively, worldwide (UNESCO 2001).
        
          
            North America and Europe clearly dominate the number of scientific
            publications produced annually.
          
        
        It is rather obvious that richer countries are able to invest more resources in science
        and therefore account for the largest number of publications. It is also likely that there
        is a statistical bias on the part of the SCI as a bibliometric database, since it
        represents North American and European publications far better than those of the rest of
        the world (Gibbs 1995; May 1997; Alonso and Fernández-Juricic 2001; Vohora and Vohora
        2001). But is the disparity in scientific contributions between the developed and
        developing worlds actually remaining unchanged or even increasing, as Mr. Annan has
        implied? A closer look at the trends over the last decade reveals important advances in
        developing countries. For example, Latin America and China, although representing,
        respectively, only 1.8% and 2% of scientific publications worldwide, have increased the
        number of their publications between 1990 and 1997 by 36% and 70%, respectively, which is a
        much higher percentage than the increments reached by Europe (10%) and industrial Asia
        (26%). The percentage of global scientific publications from North America actually
        decreased by 8% over the same period (UNESCO 2001).
      
      
        Publishing Trends in the Americas
        Using the SCI databases produced by the Institute for Scientific Information (ISI), as
        well as data compiled by the Red Iberoamericana de Indicadores de Ciencia y Tecnología
        (RICYT), we examined the differences in the number and proportion of scientific
        publications between the developed world and the developing world from 1990 until 2000,
        focusing on the Americas as a case study. Not surprisingly, there was a huge disparity in
        the number of publications from 1990 until 2000, with the United States contributing the
        lion's share (84.2%), followed by Canada (10.35%). Latin America as a whole contributed
        only 5.45% to the total number of scientific publications in these ten years (RICYT
        2002).
        The total number of publications, however, is not necessarily the best measure for
        assessing scientific productivity or technical advances (May 1997). More relevant
        measurements for these factors include the proportional change in the number of
        publications and the total number of publications when corrected for investment in research
        and development (May 1997). The proportional change in the number of publications, using
        1990 as a comparison, revealed that scientific publishing in Latin America increased the
        most rapidly in the Americas, far outpacing the United States and Canada (Figure 1).
        Further analyses, correcting the number of overall publications for the amount of money
        invested in research and development for each region, also show that, in contrast to both
        Canada and United States, the trend in Latin America has been an increase in relative
        output throughout the 1990s (Figure 2). Moreover, when taking into account the amount of
        research money available to researchers, Latin America actually out-published the United
        States and Canada by the year 2000 (Figure 2). Although the cost of research is undoubtedly
        cheaper in the developing world due to relatively low researcher salaries, overhead and
        other work standards, these factors do not explain the substantial increase in the number
        of publications per amount of money allocated to research and development in Latin America,
        particularly from 1995 until 2000 (Figure 2).
        Other relative indicators of scientific productivity, such as the number of publications
        picked up by the SCI in relation to the number of scientists in a particular country, also
        demonstrate that such developing regions as Latin America are making substantial
        contributions to science, despite the fact that the average proportion of gross domestic
        product (GDP) invested in science in Latin America throughout this 10-year period was only
        21% of the amount invested in United States (RICYT 2002). Indeed, this scientific
        productivity is remarkable when we compare it with the relatively low investment in science
        itself as compared with the GDP of Latin America as a whole. In fact, Albornoz (2001)
        concluded that, as a group, Latin America could afford to invest a much higher proportion
        of its resources in scientific research and development. Latin American investment in
        research and development represented only 0.59% of the regional GDP in 1998, a very weak
        effort compared with that of the United States (2.84%) and Canada (1.5%).
        Among Latin American countries, there is a high degree of variability in publication
        rate as well as in financial investment in science and technology. Some countries have
        performed particularly well. For example, Uruguay, Chile, Panama, and Cuba averaged,
        respectively, 6.8, 5.3, 5.2, and 3.4 publications per million dollars of research and
        development investment in the 10 years studied, which is notoriously high compared with
        United States (1.5) and even Canada (3.3) (RICYT 2002). Other countries, such as Costa
        Rica, Cuba, Brazil, and Chile, have invested a much greater proportion of their GDP in
        research and development than the other countries of this region (Albornoz 2001).
        
          
            Why has the number of publications per dollar invested in research and
            development been increasing in Latin America while decreasing in United States and
            Canada?
          
        
      
      
        Explaining the Increase in Publishing Productivity in Latin America
        One potential explanation for the increase in scientific productivity in Latin America
        is that scientific development during the 1990s was particularly strong for many countries
        of this region. Indeed, this would explain the rapid rise in the number of publications in
        Latin America compared with the relatively flat increases in the United States and Canada,
        which were publishing just as well at the beginning of the decade. A potentially more
        important question, however, is why the number of publications per dollar invested in
        research and development has been increasing in Latin America while decreasing in the
        United States and Canada. This pattern could be the result of a variety of factors, none of
        which are mutually exclusive. It is possible that publishing in international journals as a
        measure of scientific productivity is becoming more important in Latin America. Increased
        funding to the most productive scientists from the national science development programs
        might have been an important stimulus. International cooperation resulting in more
        scientific collaborations among scientists in Latin America, Europe, and the United States
        may also have increased the relative number of publications in Latin America. In contrast,
        the decreasing trends in the number of publications per investment dollar in Canada and
        United States could reflect a trend towards more costly research in larger scientific
        programs.
      
      
        Scientific Impact from Latin America
        What, exactly, is the relative impact of such developing regions as Latin America on the
        scientific community? We used SCI 2001 data to examine the proportion of publications in
        the area of ecology (including the fields of evolutionary biology, conservation biology,
        and global change biology) between 1990 and 2002 in both the two top general science
        journals (
        Nature and 
        Science ; with impact factors of 27.96 and 23.33, respectively) and in
        the 20 top ecological journals (with impact factors of 10.51–2.47) (ISI 2001a). We credited
        a region with a publication if any of the authors were affiliated with institutions from
        that region. Thus, more than one region would receive credit for a single publication if
        that publication had been written by multiple authors from institutions of different
        regions.
        For the top 20 ecological journals, the American subcontinents of South, Central, and
        North America accounted for 62% of the publications worldwide. Within the Americas,
        however, Latin America represented only 6%, while Canada and United States accounted,
        respectively, for 13% and 82% of the top 20 ecological publications. When we examined the
        data as contributions to the top 10 ecological journals (impact factors 10.51–3.31) versus
        the top 11–20 (impact factors 3.28–2.47), the Latin American countries contributed nearly
        twice as many publications to journals in the second category (8% in the top 11–20 compared
        with 4% in the top 10). These findings suggest that publications from such developing
        regions as Latin America are falling short of reaching the top journals. In contrast, the
        United States contributed somewhat more publications to the top 10 journals (84%) than the
        top 11–20 journals (79%). The difference in the proportion of publications contributed by
        the United States to the top 10 and top 20 journals was even more pronounced when we
        examined it in respect to worldwide publications. In this case, the United States
        contributed 60% of the publications to the top 10 journals and only 40% of the publications
        to the top 11–20 journals.
        Interestingly, the proportion of publications from Latin America, the United States, and
        Canada across all subject areas in 
        Science and 
        Nature were nearly identical to those of the top 20 ecological journals.
        In 
        Science and 
        Nature , Latin America had 7% of the publications within the Americas
        versus 6% in the top 20 ecological journals, whereas the United States and Canada had 81%
        versus 82% and 12% versus 13%, respectively. These similarities suggest that the Latin
        American researchers are not shying away from the two top-ranked general science journals.
        However, publishing in 
        Science and 
        Nature was not enough to gain prominence, as evidenced by the number of
        citations of these researchers. The latest list of the 247 most-cited researchers in
        ecology and environmental sciences emphasizes the overwhelming contributions of authors
        from North America (73%) and Europe (21%) (ISI 2001b). No researcher working in a Latin
        American institution was included in the remaining 6%. Overall, these data indicate that
        the scientific output in the field of ecology in Latin America is having a relatively low
        impact in the international scientific community and is underrepresented in the top
        international journals, despite its robust productivity as measured by the number of
        publications per researcher funding amount. Similar findings were also reported for Asia
        (Swinbanks et al. 1997) and thus could be a general phenomenon in the developing world.
        Although there are outstanding scientific researchers in the developing world who
        independently are making important contributions to the international scientific community,
        they are the exception. Why, in general, do Latin American scientists often fail to reach
        the top journals or become amongst the most cited researchers in their fields? One
        possibility is that the main research agendas between both regions are somewhat different
        and that the top journals, which are published in the developed world, respond more to the
        scientific mainstream of the developed regions. This is not to suggest any sort of
        conspiracy, but rather it implies that the perception of the most important science is
        linked to the region and that because the major funding agencies as well as most prominent
        journals share a similar economic region, they also share the same perception of what
        science is most interesting to them. Another consideration is that more local journals from
        developed regions are listed by the SCI than similar journals from developing regions
        (Gibbs 1995). Consequently, there are more high-profile regional publication opportunities
        available to scientists from the developed region, whereas much of the research published
        locally in the developing world is overlooked. But it takes more than publishing good
        papers to become a highly cited scientist. It requires attending international meetings and
        introducing novel research findings in multiple scientific forums. Funding these
        activities, however, requires a greater proportion of research money being spent on
        meetings for researchers in the developing compared with the developed world.
      
      
        A Long Road Yet to Travel
        The positive trends in scientific productivity in Latin America should not be
        misinterpreted as a reason to be unconcerned about the existing gap highlighted by Mr.
        Annan. There are many compelling reasons for the push to increase scientific input from the
        developing world (Goldemberg 1998; Annan 2003). One is that science, as a discipline, would
        benefit from the contributions of many disparate groups around the world, rather than being
        dominated by two geographic regions. Many scientific problems could be solved much more
        readily with the cooperation and scientific insight of scientists from developing regions.
        Climate change and biodiversity research, for example, urgently need the scientific input
        from those developing regions that are so important for these global processes. It is also
        critical for the developing world to promote, through research and publications, those
        areas of concern that are having a proportionally greater scientific and social impact upon
        them. There are now examples in which research on priority areas for the developing nations
        can actually become pioneering work in areas neglected by the research agenda of the
        industrialized world. This has been the case for research on renewable energy sources in
        Brazil (Goldemberg 1998) and biomedical sciences in Cuba (Castro Díaz-Balart 2002). These
        examples are important not only for those regions of the developing world, but are also in
        themselves scientific innovations that can greatly advance the knowledge of the rest of the
        world.
        
          
            Climate change and biodiversity research urgently need the scientific
            input from those developing regions that are so important for global processes.
          
        
        Although the evidence presented here demonstrates that there is a long way to go before
        developing countries contribute a more equitable share to the international scientific
        community, there are also reasons to be optimistic. The relative increase in the number of
        publications, especially when corrected for the amount of money available in research and
        development, demonstrates that many developing countries are heading in the right
        direction. The extremely high scientific productivity of many developing nations, corrected
        for and despite the rather limited availability of funds, suggests that increased funding
        to the sciences will be an excellent investment by developing nations in terms of
        publications as a measure of scientific output, particularly if these publications can
        target the journals that have the greatest impact. Although there may still be a long road
        to travel, we feel optimistic that the bridges mentioned by Mr. Annan are slowly being
        built.
      
    
  

  
    
      
        
        JSTOR is successful for reasons its founders did not intend. Bill Bowen's inspired
        vision was of a solution to libraries' ever-voracious demands for space to house paper
        volumes. The idea was that libraries could save space by removing volumes available in
        electronic format. Few libraries have discarded the volumes digitised in JSTOR, but many
        libraries without the paper volumes have been able to offer their users access to the
        important journal runs JSTOR has digitised. Paper holdings have not decreased dramatically,
        but electronic holdings have increased. So a space-saving service became an access
        service.
        As an access service, JSTOR is a creation of its time. Understandable though the
        decision to use page images may have been eight years ago, future user-friendly access
        requires searching capabilities across full-text, which page images cannot supply.
        Likewise, the decision to digitise the back-runs of around 100—now 218—paper journals was a
        bold decision at the time, but the future for access to journal literature lies in
        electronic versions of thousands rather than hundreds of titles, both current and
        retrospective. When we reach that point, JSTOR will still have a valued place in the
        content on offer, but it is difficult to see JSTOR providing thousands rather than a few
        hundred titles. Its technical solutions and financial models look dated as both
        subscription-based and open-access publishers improve their services to authors and to
        readers. As the number of journal articles accessible over the networks increases, JSTOR
        will be seen as a small-scale pioneer from which we learned valuable lessons.
        Roger Schonfeld ends his very detailed description of JSTOR with a chapter on ‘Lessons
        Learned’, many of which are relevant to current access initiatives. The need for grant
        funding to launch any such initiative has to be accompanied by a sound business plan to
        ensure long-term economic viability. JSTOR has achieved that transition, and its success
        provides a model for others. Much of the credit must go to JSTOR's enterprising president,
        Kevin Guthrie, who found the quickest way through the maze of conflicting advice—much of
        which could have resulted in JSTOR's reaching a deadend—and convinced the library and
        publishing communities to buy into a product that was only a promise. Meeting user needs
        for easy access to high-quality content was the key to the fulfilment of that promise.
        JSTOR's public image is of quality in depth—long runs of core journals—and that image has
        to become the hallmark of the new open-access initiatives as they develop.
        It is understandable that some mistakes were made on the way. The difficulty that JSTOR
        financial planning had in coming to terms with consortial purchases delayed its growth as
        an access service. Although selling to consortia of academic libraries may not have
        improved JSTOR's financial position in the short-term, consortia are a route to spreading
        access and therefore securing longer-term financial stability (as the major publishers have
        realised through their ‘Big Deals’ in selling hundreds of journals to hundreds of libraries
        in a consortium). Some opportunities were also delayed—not lost—through too slow an
        adaptation of the JSTOR purchasing model for selling outside the United States, the United
        Kingdom being the exception. The UK deal was with JISC, the Joint Information Systems
        Committee of the UK Higher Education Funding Councils, acting more as a negotiating agent
        than a consortium, and this model could have been applied in other countries. More
        countries would have valued access to JSTOR earlier, but the approach to non-US deals had
        to be imaginative. For all vendors, there has to be an understanding of the political,
        social, economic, and educational structure of the country into which the product is being
        sold, an understanding that takes time to acquire but that pays dividends. Open-access
        publishers do not have to sell their product to users of their journals, but local
        knowledge is essential in ‘selling’ their services to authors. The globalisation of
        publishing has combined with the globalisation of the networks and with the globalisation
        of research to provide opportunities for high-quality research conducted outside North
        America and Western Europe to be published in peer-reviewed open-access journals more
        readily than in the traditional subscription-based journals.
        Roger Schonfeld's book draws out many of the significant points about JSTOR's place in
        the history of electronic publication through a minute examination of the process leading
        to JSTOR as it is today. There is so much detail in the book that the reader may feel that
        its comprehensiveness cannot be questioned, but one small omission of which I have personal
        knowledge makes me question the value of so much detail. The omission concerns the interest
        by my institution, University College London, in joining JSTOR before the JISC deal was
        considered. Not a detail of world-shattering significance, but it does illustrate the fact
        that outside the United States, as well as within, the early interest in JSTOR came from
        individual institutions rather than from consortia. I sympathise with Roger Schonfeld in
        attempting to write such a comprehensive history, but what is the point of appearing to be
        comprehensive when comprehensiveness is an impossible goal? Would a briefer history have
        been just as valuable?
        Leaving aside quibbles and caveats about the book and about JSTOR, this remains a
        fascinating and instructive history of an important and ground-breaking initiative. Bill
        Bowen's vision may not have developed in quite the way he expected, but the ‘bottom-line’
        is that the vision did become a successful reality. The problem of ever-expanding libraries
        has not gone away in the ten years since JSTOR was conceived, but the ultimate solution—the
        availability of electronic content—has become closer, and JSTOR's success has encouraged
        others to develop services that are more in accord with 2003 than 1993. One lesson Roger
        Schonfeld does not draw out is the pace of change in electronic publishing, and if so much
        has been achieved since 1993, what promise is held out by the next ten years'!
      
      
        
      
    
  

  
    
      
        
        The pathologist makes do with red wine until an effective drug is available, the
        biochemist discards the bread from her sandwiches, and the mathematician indulges in
        designer chocolate with a clear conscience. The demographer sticks to vitamin supplements,
        and while the evolutionary biologist calculates the compensations of celibacy, the
        population biologist transplants gonads, but so far only those of his laboratory mice.
        Their common cause is to control and extend the healthy lifespan of humans. They want to
        cure ageing and the diseases that come with it.
        “I would take resveratrol if it were feasible,” notes David Sinclair, assistant
        professor of pathology at Harvard Medical School in Boston, Massachusetts. In the meantime,
        he adds, “I do enjoy a glass of red wine about once a day.” It was Sinclair's laboratory,
        in association with a commercial partner, that revealed last August how the team had
        identified for the first time a group of simple organic molecules capable of extending
        lifespan. The most proficient of the group is resveratrol, the plant polyphenol found in
        red wine, and its discovery as a potential elixir to combat ageing represents another
        extraordinary advance in a decade of discoveries that have revolutionised the field.
        
          
            “These molecules will be useful for treating diseases associated with
            ageing, like diabetes and Alzheimer's.”
          
        
      
      
        Extending Life
        Although the life-enhancing effects of Sinclair's polyphenols are so far confined to the
        baker's yeast 
        Saccharomyces cerevisiae , the work suggests that researchers
        are only one small step from making a giant leap for humankind. “People imagined that it
        might have been possible, but few people thought that it was going to be possible so
        quickly to find such things,” says Sinclair. The field of ageing research is buzzing.
        Resveratrol stimulated a known activator of increased longevity in yeast, the enzyme
        Sir-2, and thereby extended the organism's lifespan by 70% (Box 1). Sir-2 belongs to a
        family of proteins with members in higher organisms, including SIR-2.1, an enzyme that
        regulates lifespan in worms, and SIRT-1, the human enzyme that promotes cell survival
        (Figure 1). Though researchers still do not know whether SIRT-1, or “Sir-2 in humans,” as
        Sinclair puts it, has anything to do with longevity, there is a good chance that it does,
        judging by its pedigree. In any event, resveratrol proved to be a potent activator of the
        human enzyme. This might not be altogether surprising, at least not now, given that the
        polyphenol is already associated with health benefits in humans, notably the mitigation of
        such age-related defects as neurodegeneration, carcinogenesis, and atherosclerosis.
        “The study came out from a pretty big gamble,” recalls Sinclair, who used the human
        enzyme to screen and identify molecules that he expected would also stimulate those related
        enzymes in lower organisms. Unlike SIRT-1, these related enzymes are known to increase
        longevity when activated, usually by restricting the organism's calorie intake. Not only
        did they find “a whole collection of related polyphenols that activate ‘Sir-2 from humans,’
        … but we put them onto yeast, justbeing the simplest model, and amazingly [they] did what
        we were hoping [they] would do,” recalls Sinclair. “But it was a real long shot.”
        Now there's great eagerness in the Sinclair laboratory to complete and publish related
        research, notably by replicating the yeast work in higher organisms. “We have very
        promising results in 
        Drosophila , which is a huge jump from a yeast cell,” says
        Sinclair. “So we're very encouraged by that.” Publication of these results is imminent. The
        team has also quickly broadened its horizons and is already testing the polyphenols on
        mouse disease models. “We think we may have tapped into a cell survival and defence
        programme [and] that these molecules will be useful for treating diseases associated with
        ageing, like diabetes and Alzheimer's,” says Sinclair. He hopes to publish the diabetes
        results by mid-2004 and those for Alzheimer's by the end of the year. Harvard and BIOMOL
        Research Laboratories, its commercial partner based in Pennsylvania, have already filed a
        patent application for the use of “synthetic related molecules” to combat diseases of
        ageing—an application, Sinclair adds, “very much linked to the [polyphenols] paper.”
        There's been a radical shift in attitude towards ageing, says Sinclair. Before the
        1990s, “people thought that we were a lot like cars, that we would just rust and
        breakdown—nothing we could do about it. The new idea is that there are pathways that can
        boost our defences against ageing—the ‘ageing-can-be-regulated’ discovery … that genes can
        control ageing [and] that there are pathways that [we can use to] slow down the process,”
        he says. “If that's true—and it really seems to be true for a lot of organisms—if it's true
        for us, it really means that there is hope that we will be able, one day, to find small
        molecules that can alter these pathways.”
      
      
        How Long Could We Live?
        Sinclair expects to see such developments within his lifetime, but he ridicules the
        notion that humans will experience anything like the 70% extension to lifespan of his
        cultured yeast. “It'll be great if we can just give people an extra five years and have
        less disease in their old age and make it less painful,” he says. “We won't be seeing any
        Methuselahs around,” he insists.
        On his side are James Vaupel, one of Europe's leading demographers, and Marc Mangel, a
        mathematical modeller at the University of California at Santa Cruz. “Since 1840, life
        expectancy has been going up at 2.5 years per decade and will continue at this rate, maybe
        a little faster,” says Vaupel, head of the Laboratory of Survival and Longevity at the Max
        Planck Institute for Demographic Research in Rostock, Germany. Women in Japan currently
        have the highest average life expectancy of 85, he notes: “So the figure could be 100 in
        six decades, but not 500.” There's remarkably little people can do even if they want to
        live as long as possible, he says. “Give up smoking, lose weight, don't drive when drunk,
        install a smoke detector, take regular exercise,” suggests Vaupel, who insists he does them
        all, as well as taking vitamin supplements.
        
          
            “You look at these worms and think, ‘Oh my God, these worms should be
            dead.’ But they're not. They're moving around.”
          
        
        Mangel sees the problem of assessing the limitations of ageing research as fairly
        straightforward. Mathematical models, he says, could solve it by linking demographic
        properties and physiological developments. “We've had a separation of the biology of ageing
        and the demography of ageing, and they need to come together again,” notes Mangel, whose
        personal anti-ageing regime involves taking “a dose of anti-oxidant chocolate with a good
        feeling.”
        But Cythnia Kenyon, whose laboratory reported in October that it had generated a 6-fold
        increase in the lifespan of its nematodes, is not so sure about the limitations. “You look
        at these worms and think, ‘Oh my God, these worms should be dead.’ But they're not. They're
        moving around…. Once you get your brain wrapped around that … then you start thinking, oh
        my goodness, so lifespan is something you can change—it's plastic. Then who knows what the
        limit is?” (Cynthia Kenyon has recorded video clips of the superstars of her lab, 
        Caenorhabditis elegans , to show how long-lived mutant nematodes
        are as vigorous as normal young adults [Videos 1–4].)
        Warming to the theme, Kenyon hypothesises: “If you'd asked me many generations ago, when
        we were actually common precursors of worms and flies, ‘Cynthia, you have a two-week
        lifespan, do you think that you could [live longer]?’ And if I'd told you, ‘Well, I think
        our descendants will live 1,000 times longer,’ you'd have said, ‘Oh, come on!’ But we do.
        It happened,” she notes.
        “Who knows what you could do in people?” Kenyon muses. “I don't want to go on record
        saying that it's not possible in people because I don't see why it wouldn't be…. I'm
        certainly not imagining that my company in the next few years is going to come up with a
        compound that can make people live to be 500. That seems just preposterous.” So the
        timescale is millions of years? “No, not necessarily,” she insists, “because once we
        understand the mechanism, then we can intervene and see what we can accomplish.”
      
      
        Signalling Life and Sweet 16
        Kenyon, professor of biochemistry and biophysics at the University of California at San
        Francisco, is among the key contributors responsible for showing that a single gene, and
        subsequently many genes, can change an organism's lifespan.
        
          “It is inconceivable … that a life-extending therapy will ever be developed that is
          able to extend life independent of every other change.”
        
        In a seminal paper published a decade ago, Kenyon's laboratory showed that mutations in
        the 
        daf-2 gene doubled the lifespan of the nematode 
        C. elegans . 
        daf-2 encodes a receptor that is similar to those for insulin and
        insulin-like growth factor-1 (IGF-1) in humans; this hormone receptor normally speeds up
        ageing in worms, but the mutations inhibit its action and enable the organisms to live
        longer. Before the results appeared, there was a “very negative attitude” towards ageing
        research, recalls Kenyon. Since then, and especially over the past few years in response to
        later findings, graduate students have been scrambling for a chance to work in her
        laboratory. “You can't believe the difference—there was such resistance to it,” she says. “
        daf-2 made a huge difference.” But then so did her subsequent research in
        the field.
        Among her most significant findings is the identification of many more longevity genes;
        the results, published in July, derive directly from her early work on 
        daf-2 . “We discovered that in order for long-lived worms to live so
        long, they need another gene called 
        daf-16 ,” recalls Kenyon. “
        daf-16 is kind of the opposite of 
        daf-2 , in the sense that it promotes longevity and youthfulness … so we
        call it ‘sweet 16.’” 
        daf-16 encodes a transcription factor that controls the expression of
        more than 100 genes. “They don't do just one thing, they do many things,” says Kenyon. They
        can act as anti-oxidants (to prevent damage from oxygen radicals), as chaperones (to
        prevent misfolded proteins from forming aggregates), as antimicrobials (to protect against
        bacteria and fungi), and as metabolic agents.
        “So the picture that emerges is that the way the insulin/IGF-1 hormone system produces
        these enormous effects on lifespan is by coordinating the expression of many genes that do
        different things to affect lifespan, each of which on its own has only a small effect,”
        notes Kenyon. “It's as though 
        daf-2 and 
        daf-16 , the regulators, would be the conductors of an orchestra. They
        bring together the flutes and the violins and the French horns, each of which do different
        things, and they make them all work together in concert.”
        Kenyon is unequivocal about the bottom line: “Now we have a whole set of genes whose
        biochemical functions we can be working on to understand more about the actual mechanisms
        of ageing.” Complementary results in flies and mammals persuade her to be more explicit.
        “The common ancestor of worms, flies, and mice must have had an insulin/IGF-1-like hormone
        system that controlled ageing. And that ability has been maintained. So the question is,
        has [that ability] been lost in humans? I think it's quite likely that it will also
        function in humans, but there isn't a direct demonstration yet that that's the case.”
        Nevertheless, the discoveries about the role of the insulin/IGF-1 pathway in ageing have
        had a profound impact on her own lifestyle, which includes a tendency to discard the bread
        from sandwiches and eat only the toppings of pizzas (Box 2). “I'm on a low-carb diet. I
        gave my worms glucose, and it shortened their lifespan. [The diet] makes sense because it
        keeps your insulin levels down,” she says.
        “Caloric restriction extends lifespan of mice, and so does the insulin/IGF-1 pathway,”
        she notes. Indeed, starting a low-calorie diet at any point in adulthood appears to help
        fruit flies live longer, according to research in Britain published last September. “What
        we don't know for sure in mice,” Kenyon continues, “is whether the two pathways are
        different or the same.”
        While much ageing research focuses on these two influences, she says that there are
        another two areas of investigation. Her laboratory reported in December 2002 that
        inhibiting the respiration of mitochondria in developing worms increased longevity, but
        that it had no effect in adult worms, for reasons still unexplained, she says. Further
        microarray analysis is underway to pinpoint whether the cause simply lies downstream of the
        insulin/IGF-1 pathway or whether it is something different altogether.
      
      
        The Price of Life
        Then there's research looking at the effects on lifespan of changes to an organism's
        reproductive system. For Kenyon, such work often involves a battle to convince sceptics
        that longevity is not a trade-off with fertility. Four years ago, her laboratory reported
        that killing germ cells increases the lifespan of worms by 60%, but only because, she
        stresses, it affects endocrine signalling and not because it prevents reproduction. Further
        research, published last year, showed quite clearly, she says, that ageing and reproduction
        are controlled independently of one another. And as for her recent work on infertile worms,
        which lived six times as long as normal following the removal of their entire reproductive
        systems, she says: “If we could intervene in the hormone signalling pathways directly, we
        think the animals would still live six times as long as normal, but would be fertile as
        well.”
        Jim Carey is one of those “trade-off” sceptics. He is a population biologist at the
        University of California at Davis and his research, on the effect on life expectancy of
        replacing the ovaries of old mice with ovaries from younger mice, is intended to complement
        Kenyon's work. But he insists that “an honest discussion of lifespan extension must include
        consideration of tradeoffs.” Many manipulations that extend lifespan in model systems,
        whether genetic or dietary, for example, ignore or gloss over the side effects, such as
        permanent sterility, huge weight loss, distorted organ-to-body ratios, or major behavioural
        aberrations, he notes. “It is inconceivable to me that a life-extending therapy will ever
        be developed that is able to extend life independent of every other change,” he concludes.
        “All life systems are interlinked and hierarchically integrated at all levels, so to talk
        about life extension using analogies with a car warranty concept is wrong-headed.”
        Another “trade-off” sceptic takes a different tack. As Armand Leroi puts it: “During
        occasional periods of involuntary celibacy I have thought, well, I may not be getting laid,
        but at least I shall live to a miserable and solitary old age.” Leroi, an evolutionary
        biologist at Imperial College of Science, Technology, and Medicine in London, offers an
        optimistic appraisal of the chances of finding a cure for ageing in his new book about the
        effects of genetic variety on the human body. He sees ageing simply as a collection of
        curable diseases: “There is no obvious impediment to that advance, nothing to make us think
        that human beings have a fixed lifespan.”
      
    
  

  
    
      
        
        Inside eukaryotic cells there is a massive protein complex called the proteasome whose
        raison d'être is to remove unnecessary proteins by breaking them down into short peptides.
        The proteasome is thus responsible for an important aspect of cellular regulation because
        the timely and controlled proteolysis of key cellular factors regulates numerous biological
        processes such as cell cycle, differentiation, stress response, neuronal morphogenesis,
        cell surface receptor modulation, secretion, DNA repair, transcriptional regulation,
        long-term memory, circadian rhythms, immune response, and biogenesis of organelles
        (Glickman and Ciechanover 2002). With the multitude of substrates targeted and the myriad
        processes involved, it is not surprising that aberrations in the pathway are implicated in
        the pathogenesis of many diseases, including cancer.
        With so many proteins to target for degradation, the activity of the proteasome is
        subject to multiple levels of regulation. In the overwhelming majority of cases, selected
        proteins are first “labeled” by the addition of several copies of a small protein tag
        called ubiquitin and are thus targeted for degradation in the proteasome (Figure 1). The
        ubiquitination of proteins is regulated through precise selection of protein substrates by
        specific E3 ubiquitin ligases (Pickart 2001). These enzyme complexes each recognize a
        subset of substrates and tag them by linking the carboxyl terminus of ubiquitin with an
        amino group on the target protein via an amide bond (Figure 1).
        Interestingly, ubiquitination is a reversible process. Even when a protein has been
        tagged with ubiquitin, its fate is not sealed—specific hydrolytic enzymes called
        deubiquitinases can remove the ubiquitin label intact (Figure 1). By deubiquitinating their
        substrates, these enzymes compete with the proteasome, which acts on the polyubiquitined
        form. In the competition between proteolysis and deubiquitination, polyubiquitinated
        proteins rarely accumulate in the cytoplasm of “healthy” cells, as they are either
        irreversibly degraded or deubiquitinated and rescued. It is thought that this competition
        provides a certain level of stringency or quality control to the system. Based on sequence
        homology, deubiquitinating enzymes were traditionally classified into two families:
        ubiquitin-specific proteases (UBPs or USPs) and ubiquitin carboxy-terminal hydrolases
        (UCHs). Both enzyme families are classified as cysteine proteases that employ an active
        site thiol to cleave ubiquitin from its target (Kim et al. 2003; Wing 2003).
        The proteasome itself is made up of a multiprotein core particle (CP) where proteolysis
        occurs and a separate multiprotein regulatory particle (RP) that recognizes and prepares
        substrates for degradation by the CP. A base subcomplex of the RP is pivotal in anchoring
        polyubiquitin chains during this process, either directly or via auxiliary
        ubiquitin-binding proteins (Lam et al. 2002; Hartmann-Petersen et al. 2003). The base
        attaches to the outer surface of the CP and uses energy to unravel the substrate,
        simultaneously with preparing the channel that leads into the proteolytic chamber of the CP
        (Forster and Hill 2003). The lid subcomplex of the RP attaches to the base and is required
        for proteolysis of ubiquitin–protein conjugates, but not of unstructured polypeptides
        (Glickman et al. 1998; Guterman and Glickman 2003). The size and complexity of this
        protein-eating machine hints at the exquisite controls that must rgulate its function.
        An intriguing evolutionary and structural relationship between the proteasome lid and an
        independent complex, the COP9 signalosome (CSN), may shed light on their respective roles
        in regulated protein degradation. Both are made up of eight homologous protein subunits
        that contain similar structural and functional motifs. While a lot is still unknown, the
        CSN appears to mediate responses to signals (e.g., light, hormones, adhesion, nutrients,
        DNA damage) in a manner that is intimately linked to the ubiquitin–proteasome system. This
        is accomplished, for instance, by suppressing ubiquitin E3 ligase activity or interacting
        with various components of the pathway (Bech-Otschir et al. 2002; Cope and Deshaies 2003;
        Li and Deng 2003). In particular, one subunit—Csn5—moderates SCF (Skp1–cullin–F box) and
        other cullin-based E3 ubiquitin ligases by removal of the ubiquitin-like Rub1/Nedd8
        molecule from the cullin subunit of the ligase complex. Further analysis of the CSN will no
        doubt uncover additional mechanisms whereby ubiquitin-mediated protein degradation is
        controlled.
        Surprisingly, the proteasome itself harbors intrinsic deubiquitination activity (Eytan
        et al. 1993). Moreover, both the lid and the base contribute independently to RP
        deubiquitination activity. The source of this activity has been attributed to a number of
        different subunits. These include the associated cysteine proteases Ubp6/USP14 (Borodovsky
        et al. 2001; Legget et al. 2002), UCH37/p37 (Lam et al. 1997; Hoelzl et al. 2000), and
        Doa4/Ubp4 (Papa et al. 1999), as well as the intrinsic proteasome subunit Rpn11/POH1 (Verma
        et al. 2002; Yao and Cohen 2002). The importance of these components to proteasome function
        is apparent in their partially overlapping properties. In groundbreaking work, an intrinsic
        “cryptic” deubiquitinating activity that is sensitive to metal chelators has been reported
        for the proteasome, in addition to “classic” cysteine protease behavior (Verma et al. 2002;
        Yao and Cohen 2002). This metalloprotease-like activity maps to the putative catalytic
        MPN+/JAMM motif of the lid subunit Rpn11 and lies at the heart of proteasome mechanism by
        linking deubiquitination with protein degradation. Notably, Rpn11 shares close homology
        with Csn5, which is also responsible for proteolytic activities in its respective
        complex.
        By defining a new family of putative metalloproteases that includes a proteasomal
        subunit, a CSN subunit, and additional proteins from all domains of life, the MPN
        + /JAMM motif garnered great attention. The trademark of the MPN
        + /JAMM motif is a consensus sequence E—HxHx
        (7) Sx
        (2) D that bears some resemblance to the active site of zinc
        metalloproteases. Members of this family were predicted to be hydrolytic enzymes, some of
        which are specific for removal of ubiquitin or ubiquitin-like domains from their targets
        (Maytal-Kivity et al. 2002; Verma et al. 2002; Yao and Cohen 2002).
        In a further development, two independent groups determined the molecular structure of
        an MPN
        + /JAMM protein from an archaebacterium (Ambroggio et al. 2003; Tran et
        al. 2003). The structures identify a zinc ion chelated to the two histidines and the
        aspartic residue of the MPN
        + /JAMM sequence. The fourth ligand appears to be a water molecule
        activated through interactions with the conserved glutamate to serve as the active site
        nucleophile. Overall, this protein certainly has properties consistent with a
        metallohydrolase and can serve as the prototype for the deubiquitinating enzymes in its
        class. This revelation adds an all-new enzymatic activity and, with it, an additional layer
        of regulation to the ubiquitin–proteasome system.
        Now that it is evident that the proteasome contains a member of a novel metalloprotease
        family, a fundamental question can be raised: why does a proteolytic enzyme like the
        proteasome need auxiliary proteases for hydrolysis of ubiquitin domains? At first glance,
        the delegation of tasks between the proteolytic subunits of the proteasome (situated in the
        proteolytic core particle) and the auxiliary deubiquitinating enzymes (situated in the
        regulatory particle) is clear-cut: the latter cleave between ubiquitin domains, while the
        core proteolytic subunits process the target protein itself (Figure 1). However, this still
        does not explain the mechanistic rational for finding deubiquitination within the
        proteasome itself. In principle, deubiquitination could be used for (1) recycling of
        ubiquitin, (2) abetting degradation by removal of the tightly folded highly stable globular
        ubiquitin domain, or (3) mitigating degradation by removal of the ubiquitin anchor, without
        which the substrate is easily released and rescued. There is evidence that recycling of
        ubiquitin by the proteasome is indeed a crucial feature of deubiquitination in proper
        cellular maintenance (Legget et al. 2002). Distinguishing between options 2 and 3, however,
        depends to a large extent on the delicate balance between the two proteolytic activities
        associated with the proteasome: proteolysis and deubiquitination (Figure 2).
        Once bound to the proteasome, a polyubiquitinated substrate can be unfolded by the RP
        and irreversibly translocated into the CP. It has been proposed that long polyubiquitin
        chains commit a substrate to unfolding and degradation by the proteasome, whereas short
        chains are poor substrates because they are edited by deubiquitinating enzymes, resulting
        in premature substrate release (Eytan et al. 1993; Lam et al. 1997; Thrower et al. 2000;
        Guterman and Glickman 2003). Extended polyubiquitin chains could slow down chain
        disassembly, thereby allowing ample time for unfolding and proteolysis of the substrate
        (Figure 2). Interestingly, both “trimming” and “shaving” deubiquitinating activities are
        associated with the proteasome, though the exact contribution of the various
        proteasome-associated deubiquitinating enzymes to each of these distinct activities has yet
        to be elucidated. It is expected that in order to obtain efficient proteolysis of the
        target, shaving of chains at their proximal ubiquitin should be slower than the rate of
        trimming at the distal moiety. As an outcome of this requirement, longer polyubiquitin tags
        would be preferential substrates for degradation by the proteasome. Thus, the uniqueness of
        ubiquitin as a label for degradation may lie in its being a reversible tag.
        Deubiquitinases, such as Rpn11, serve as proofreading devices for reversal of fortune at
        various stages of the process, right up to the final step before irreversible degradation
        by the proteasome.
        Identifying Rpn11 and Csn5 as members of a novel class of metallohydrolases immediately
        elevates them into promising “drugable” candidates. Undoubtedly, the molecular structures
        deciphered by the groups of Deshaies (Ambroggio et al. 2003) and Bycroft (Tran et al. 2003)
        will focus efforts to design novel site-specific inhibitors of the ubiquitin–proteasome
        pathway. While Csn5 is thought to impede the action of ubiquitin ligases through shaving
        cullins from their Rub1/Nedd8 modification (and possibly also by deubiquitinating
        substrates bound to the cullins), the outcome of Rpn11 inhibition will depend largely on
        whether Rpn11 participates primarily in shaving substrates from their chains, promoting
        release and rescue, or in trimming the polyubiquitin tag, allowing for proteolysis quality
        control (Figure 2).
      
    
  

  
    
      
        
        Individuals within a wild population show remarkably little morphological variation,
        given the amount of environmental variation they encounter during development and the
        amount of genetic variation within the population. This phenotypic constancy led to the
        proposal that individuals were somehow buffered, or canalized, against genetic and
        environmental variation (Waddington 1942). Clearly, such a mechanism would have important
        evolutionary consequences; because natural selection acts upon phenotypic variation within
        a population, canalization first appears to reduce the evolvability of the trait upon which
        it is acting (Gibson and Wagner 2000). However, canalization also reduces the effects of
        new mutations (which may be deleterious), potentially allowing individuals to store this
        genetic variation without suffering the consequences. If canalization breaks down due to
        genetic or environmental circumstances, then the stored genetic variation will be released,
        providing an additional substrate for natural selection. In this way, individuals could
        potentially undergo large, rapid phenotypic changes.
        Experiments in both 
        Drosophila and 
        Arabidopsis have suggested that Hsp90 (heat shock protein 90), a
        member of a family of proteins expressed at high temperatures (heat shock), may be an
        excellent candidate for bringing about canalization (Rutherford and Lindquist 1998;
        Queitsch et al. 2002). Several features of Hsp90 suggest that it is an evolutionary buffer,
        capable of hiding and then releasing genetic variation: (1) individuals heterozygous for
        mutations in 
        Hsp83 (the gene encoding Hsp90) show increased levels of morphological
        abnormalities; (2) individuals treated with a pharmacological inhibitor of Hsp90 show
        severe morphological abnormalities; (3) the normal function of Hsp90 is to stabilise the
        tertiary structure of signal transduction molecules involved in developmental pathways; and
        (4) this function may be compromised by environmental factors, e.g., heat shock.
      
      
        Gene Networks Generate Canalization
        Hsp90 may not, however, be uniquely placed to act as an evolutionary buffer producing
        canalization. Recent theoretical work has suggested that canalization may be an emergent
        property of complex gene networks and may not require specific mechanisms of protein
        stabilisation and environmental coupling such as those provided by Hsp90 (Siegal and
        Bergman 2002). Siegal and Bergman (2002) proposed that when a network is compromised by
        ‘knocking out’ one of several genes, buffering may be lost or compromised, releasing
        variation that was hidden in the intact network. To test this, Bergman and Siegal (2003)
        used numerical simulations of a complex network of ten genes in which each gene is capable
        of influencing the expression of other genes as well as itself (Figure 1A). This network
        essentially defines the genotype of the individuals within the population, and the amount
        of gene expression at equilibrium defines the phenotype. Comparison of populations founded
        by either wild-type individuals or those with a single gene ‘knockout’ revealed much higher
        levels of phenotypic variation in populations derived from the ‘knockouts’.
        Thus, populations derived from ‘knockouts’ express phenotypic variation that was not
        expressed by the wild-type network, suggesting that any of the genes within the network may
        buffer genetic variation. This suggests that at least one aspect of generating evolutionary
        buffering is not unique to Hsp90. But can genes that, unlike Hsp90, are not conditional
        upon the environment act as evolutionary buffers? To test this, Bergman and Siegal (2003)
        simulated a gene network that incorporated a mutation process in which single genes may be
        ‘knocked out’ and then, at a later time, restored. The simulated populations were allowed
        to evolve whilst being selected for an optimum phenotype (i.e., the populations were
        exposed to an environment in which a particular phenotype was optimal). A new optimum
        phenotype was then specified in which the expression of three of the ten network genes
        changed from on to off or vice versa (i.e., there was a shift in the environmental
        conditions favouring a different phenotype). Populations evolving with the mutation process
        reached the new optimum before populations without the mutation process. Thus, the
        ‘knockout’ mutations were clearly beneficial because they sped up adaptation to a new
        phenotypic optimum by releasing hidden genetic variation, thereby providing a new substrate
        upon which natural selection may act. Yet these mutations were not coupled to the new
        environment, suggesting that the release of the hidden genetic variation does not have to
        be linked to an environmental change in order to be beneficial.
        The simulations described by Bergman and Siegal (2003) suggest that the key properties
        of an evolutionary buffer, the ability to store and then release genetic variation in
        response to environmental or genetic change, are not unique to Hsp90. Indeed, the
        simulations suggest that evolutionary buffering may be a widespread property of gene
        networks. They also suggest that the hidden genetic variation does not have to be revealed
        by an environmental change, but can be produced by a gene ‘knockout’. These results may go
        some way to explain the original observation by Waddington (1942) of phenotypic constancy,
        yet many questions remain (Stearns 2003). One of the major outstanding questions must be
        whether it is possible to verify these results experimentally. Bergman and Siegal (2003)
        used data from the yeast 
        Sacchromyces cerevisiae , in which each gene may be ‘knocked
        out’ in turn and the expression of the remaining genes determined, to demonstrate that
        their simulations also had application to biological gene networks. Using these data, they
        showed that ‘knockouts’ show greater variability in gene expression than wild-type yeast,
        suggesting that buffering has been disrupted.
      
      
        Ion Channels as Evolutionary Buffers
        Given the results of Bergman and Siegal (2003), it should be possible to find gene
        networks in which the elimination of single genes reveals variation in gene expression and
        hence in phenotype. One class of gene network that may conform to the structure outlined by
        Bergman and Siegal (2003) is that of the gene networks regulating ion channel expression in
        neurons. Neurons contain an array of voltage-dependent Na
        + and K
        + channels as well as numerous Cl
        − , Ca
        2+ , and voltage-independent leak channels. The electrical properties of
        a single neuron are dependent, though not exclusively, upon the suite of ion channels
        expressed within that neuron. The properties of a neural network, which generates
        behaviour, are determined both by the intrinsic expression patterns of ion channels within
        neurons and the connectivity between neurons. The nervous system develops as an interaction
        between experience and genetically programmed events. One mechanism by which this
        interaction is achieved is ion channel compensation (Turrigiano 1999); individual neurons
        can change their sensitivity to inputs by altering the relative proportion of ion channels,
        enabling them to maintain stable properties in the face of changing experience (Turrigiano
        et al. 1994; Brickley et al. 2001; Maclean et al. 2003; Niven et al. 2003a) (Figure
        1B).
        Many studies of ion channel ‘knockouts’ show relatively little change in overall
        neuronal activity, although predictions based upon pharmacological blockade of the ion
        channels suggest there should be a more severe phenotypic change (Marder and Prinz 2002).
        Subsequent work has shown that the loss of an ion channel may often be compensated by a
        change in the expression of other ion channels. For example, the neurons upon which I work
        are 
        Drosophila photoreceptors. In these neurons, loss of the one
        particular ion channel leads to compensatory changes in other ion channels linked to the
        activity of the neuron to restore the ability to process visual information (Niven et al.
        2003a, 2003b). However, these changes do not restore the original phenotype completely, and
        the compensated photoreceptors still show a reduced ability to process visual information.
        In many neurons, it appears that the intracellular Ca
        2+ concentration acts as an internal sensor of neural activity (Marder
        and Prinz 2002). Ca
        2+ , along with other second messengers, may influence the expression of
        genes encoding ion channels, allowing their expression to be coupled to neural activity
        (Berridge 1998) (Figure 1B and 1C). Additionally, activity-independent mechanisms of ion
        channel compensation have been described in which the expression of one ion channel is
        linked to the expression of other opposing ion channels within a neuron (Maclean et al.
        2003). These two systems of activity-dependent and activity-independent ion channel
        compensation bear a close resemblance to the gene network simulated by Bergman and Siegal
        (2003) in which each gene regulates its own expression and that of other network genes.
        It is possible, therefore, that the networks of genes regulating ion channel expression
        may act as evolutionary buffers. The relationship between neural activity and the network
        of ion channel encoding genes may stabilise the neural activity in relation to both the
        genetic and environmental variation. The stabilisation of neural activity may have
        consequences for the generation of adaptive behaviour, which is constructed from neural
        activity. It is possible that ion channels could canalize the evolution of the nervous
        system by reducing behavioural variation and therefore removing the substrate on which
        natural selection may act. For example, changes in voltage-dependent Na
        + channel properties (such as the activation voltage) may be compensated
        for by regulating the expression of other ion channels. ‘Knockout’ of one of these
        compensating ion channels may reveal the change in voltage-dependent Na
        + channel properties, resulting in a shift in the output of the neuron.
        This hypothesis has several testable predictions. For example, ‘knocking out’ an ion
        channel should increase the variation in the activity of particular neurons among
        individuals in a population. This variation in neural activity may produce an effect on the
        behaviour of the whole organism.
        Studying canalization in ion channel gene networks may have significant advantages over
        studying developmental gene networks because it is relatively straightforward to measure
        the amounts of ion channels expressed in single identified neurons, to alter the expression
        of individual ion channels, and to relate these alterations to behaviour. I am currently
        pursuing the impact of ion channel compensation in 
        Drosophila photoreceptors (Niven et al. 2003a, 2003b, 2003c). In
        this system, changes in ion channel expression produce changes in the coding of visual
        information, which may lead to behavioural differences. The possible role of ion channel
        compensation in canalizing the evolution of the nervous system may have important
        implications not just for understanding this system, but also for understanding the
        contribution of ion channel compensation to the function of the nervous system and its
        evolution.
      
    
  

  
    
      
        
        RNA interference (RNAi) has been called “one of the most has exciting discoveries in
        biology in the last couple decades,” and since it was first recognized by Andrew Fire et
        al. in 1998, it has quickly become one of the most powerful and indispensable tools in the
        molecular biologist's toolkit. Using short double-stranded RNA (dsRNA) molecules, RNAi can
        selectively silence essentially any gene in the genome. It is an ancient mechanism of gene
        regulation, found in eukaryotes as diverse as yeast and mammals, and probably plays a
        central role in controlling gene expression in all eukaryotes. In the lab, RNAi is
        routinely used to reveal the genetic secrets of development, intracellular signaling,
        cancer, infection, and a full range of other phenomena. But can the phenomenon hailed by
        the journal 
        Science as the “Breakthrough of the Year” in 2002 break out of the lab
        and lead to novel therapies as well? Pharmaceutical giants are hoping so, and several
        biotech companies have bet their futures on it, but not everyone is sanguine about the
        future of RNAi therapy.
        At the heart of its promise as a powerful therapeutic drug lies the exquisite
        selectivity of RNAi: like the fabled “magic bullet,” an RNAi sequence seeks out and
        destroys its target without affecting other genes. The clinical applications appear
        endless: any gene whose expression contributes to disease is a potential target, from viral
        genes to oncogenes to genes responsible for heart disease, Alzheimer's disease, diabetes,
        and more.
        But for all its promise, RNAi therapy is a long way from entering the clinic. While it
        is a proven wunderkind in the lab, to date no tests have been done in humans, and only the
        most modest and circumscribed successes have been demonstrated in animals. The road to
        clinical success is littered with great ideas that have come a cropper along the way,
        including two other RNA-based therapies, antisense and ribozymes, both of which showed
        promise at the bench but have largely stumbled before reaching the bedside. Is RNAi also
        likely to fall short? Or is it different enough to make this third try the charm?
      
      
        Clinical Naïveté, Mysterious Mechanisms
        To be a successful drug, a molecule must overcome a long set of hurdles. First, it must
        be able to be manufactured at reasonable cost and administered safely and conveniently.
        Then, and even more importantly, it must be stable enough to reach its target cells before
        it is degraded or excreted; it must get into those cells, link up with its intracellular
        target, and exert its effect; and it must exert enough of an effect to improve the health
        of the person taking it. And, finally, it must do all this without causing significant
        toxic effects in either target or nontarget tissues. No matter how good a compound looks in
        the lab, if it fails to clear any one of these hurdles, it is useless as a drug.
        For RNA-based therapies, manufacture has been seen as a soluble problem, while delivery,
        stability, and potency have been the most significant obstacles. “There was a lot of
        clinical naïveté” in the early days of antisense and ribozymes, according to Nassim Usman,
        Vice President for Research and Development at Sirna Therapeutics in Boulder, Colorado.
        “Compounds were pushed into the clinic prematurely.” Sirna began as the biotech startup
        Ribozyme Pharmaceuticals, which tried to develop ribozymes to treat several conditions,
        including hepatitis C. A ribozyme is an RNA molecule whose sequence and structure allow it
        to cleave specific target RNA molecules (see Figure 1). “The initial results with hepatitis
        C were not that inspiring,” says Usman, because the molecule they used had low potency and
        a short half-life once in the body. Despite “enormous doses,” the viral load was not
        significantly affected. “It just didn't have the characteristics to be a drug,” he says. No
        ribozyme has yet been approved for use by the United States Food and Drug Administration
        (FDA).
        Similarly, despite much initial enthusiasm, attempts to develop antisense drugs have
        been largely disappointing. Antisense is a single strand of RNA or DNA, complementary to a
        target messenger RNA (mRNA) sequence; by pairing up with it, the antisense strand prevents
        translation of the mRNA (see Figure 2). At least that was the theory, and early clinical
        results seemed to support the theory: antisense drugs effectively reduced tumor sizes in
        anticancer trials and viral loads in antiviral trials. But closer inspection revealed these
        results were largely due to an increase in production of interferons by the immune system
        in response to high doses of the foreign RNA, rather than to specific silencing of any
        target genes. (The relatively high proportion of C–G sequences in antisense mimics
        bacterial and viral genes, thus triggering the immune response.) When the antisense dose
        was lowered to prevent the interferon response, the clinical benefit largely disappeared as
        well. Thus, rather than being a highly specific therapy, antisense seemed to be a general
        immune system booster.
        But as long as patients were getting better, does it matter what the mechanism was? “It
        doesn't matter if you are a patient, but it does matter if you are trying to develop the
        next drug,” says Cy Stein, Associate Professor of Medicine and Pharmacology at Columbia
        University College of Physicians and Surgeons in New York City. Stein has researched
        antisense for more than a decade. “If you get the mechanism wrong, you're not going to be
        able to do it.”
        To date, only one antisense drug has received FDA approval. Vitravene, from Isis
        Pharmaceuticals in Carlsbad, California, is used to treat cytomegalovirus infections in the
        eye for patients with HIV. Vitravene is actually a DNA antisense drug, which binds to viral
        DNA, though, says Usman, “it's unclear whether it actually works by an antisense
        mechanism.” Stein expresses a similar skepticism about the mechanism of a second antisense
        drug, Genasense. Genasense is a DNA-based treatment that targets Bcl-2, a protein expressed
        in high levels in cancer cells, which is thought to protect them from standard
        chemotherapy. The FDA is currently reviewing an application for Genasense, based on
        promising results in the treatment of malignant melanoma.
      
      
        RNAi: A Natural Alternative
        Growing disillusionment with antisense and ribozymes coincided with the discovery of
        RNAi and the realization that it was a far more potent way to silence gene expression. RNAi
        uses short dsRNA molecules whose sequence matches that of the gene of interest. Once in a
        cell, a dsRNA molecule is cleaved into segments approximately 22 nucleotides long, called
        short interfering RNAs (siRNAs) (see Figure 3). siRNAs become bound to the RNA-induced
        silencing complex (RISC), which then also binds any matching mRNA sequence. Once this
        occurs, the mRNA is degraded, effectively silencing the gene from which it came. (Details
        of the structure and function of the RISC are still largely unknown, but it is thought to
        act as a true enzyme complex, requiring only one or several siRNA molecules to degrade many
        times that number of matching mRNAs.)
        The extraordinary selectivity of RNAi, combined with its potency—in theory, only a few
        dsRNAs are needed per cell—quickly made it the tool of choice for functional genomics
        (determining what a gene product does and with what other products it interacts) and for
        drug target discovery and validation. By “knocking down” a gene with RNAi and determining
        how a cell responds, a researcher can, in the course of only a few days, develop
        significant insight into the function of the gene and determine whether reducing its
        expression is likely to be therapeutically useful. But does RNAi have a better chance to
        succeed as a drug than antisense or ribozymes?
        “The fundamental difference favoring RNAi is that we're harnessing an endogenous,
        natural pathway,” says Nagesh Mahanthappa, Director of Corporate Development at Alnylam
        Pharmaceuticals in Cambridge, Massachusetts, the second of two major biotech company
        developing RNAi-based therapy. The exploitation of a pre-existing mechanism, he says, is
        the main reason RNAi is orders of magnitude more potent than either of the other two types
        of RNA strategies.
      
      
        Delivery, Delivery, Delivery
        More potent in the test tube, at least. But stability and delivery are also the major
        obstacles to successful RNAi therapy, obstacles that are intrinsic to the biochemical
        nature of RNA itself, as well as the body's defenses against infection with foreign
        nucleotides. “For the strongest reasons, you can't get away from this,” says Stein. “The
        problem is that a charged oligonucleotide will not pass through a lipid layer,” which it
        must do in order to enter a cell. John Rossi, Director of the Department of Molecular
        Biology at City of Hope Hospital in Duarte, California, who has worked on RNA-based
        therapies for 15 years, concurs. “The cell doesn't want to take up RNA,” he says, which
        makes evolutionary sense, since extracellular RNA usually signifies a viral infection.
        Injected into the bloodstream, unmodified RNA is rapidly excreted by the kidneys or
        degraded by enzymes.
        To solve the problem of cell penetration, most researchers have either complexed the RNA
        with a lipid or modified the RNA's phosphate backbone to minimize its charge. Mahanthappa
        thinks the complexing approach is unlikely to be a simple solution, since drug approval
        would require independent testing of the lipid delivery system as well. Instead, Alnylam is
        pursuing backbone modification. “Some minimal modification is going to be necessary” to
        increase cell uptake and to improve stability in the blood stream, Mahanthappa says. “What
        we have learned from the antisense field is that even without other delivery strategies,
        when you administer RNA at sufficient doses, if it's stable, it gets taken up by
        cells.”
        “Anything that can be done to increase half-life in circulation would improve delivery,”
        says Judy Lieberman, a Senior Investigator at the Center for Blood Research and Associate
        Professor of Pediatrics at Harvard Medical School in Cambridge, Massachusetts. But that may
        not be the only problem, she cautions. Lieberman's lab recently demonstrated the ability of
        RNAi to silence expression of the Fas gene in mice, protecting them from fulminant
        hepatitis. Fas triggers apoptosis, or programmed cell death, in response to a variety of
        cell insults. In her experiment, Lieberman delivered the RNA by high-pressure injection
        into the tail. The RNA got to the liver, silenced Fas, and protected the mice from
        hepatitis. However, a significant fraction of animals died of heart failure, brought on
        because the injection volume was about 20% of the mouse blood volume. Such a delivery
        scheme simply will not work in humans. “Delivery to the cell is still an obstacle,”
        Lieberman explains. “Unless you really focus on how to solve that problem, you're not going
        to get very far.”
      
      
        Unanswered Questions
        Even assuming delivery problems can be solved, other questions remain, including that of
        whether therapeutic levels of RNAi may be toxic. Mahanthappa says, “The conservative answer
        is we just don't know. The more aggressive answer is there's no reason to think so.” Rossi
        isn't so sure. “The target of interest may be in normal cells as well as cancer cells,” he
        says. “That's where you get toxicity.”
        But if small RNAs can be delivered to target cells efficiently and without significant
        toxicity, will they be effective medicines? Usman of Sirna is confident they will be. “If
        you can get it there, and if it's in one piece, there no doubt in our minds that it will
        work,” he says. To date, numerous experiments in animal models suggest RNAi can
        downregulate a variety of target genes effectively. However, there are still two unanswered
        questions about whether that will translate into effective therapy.
        The first is whether RNAi's exquisite specificity is really an advantage beyond the
        bench. “It's unclear whether highly specific drugs give you a big therapeutic effect,” says
        Cy Stein. For instance, he says, “most active antitumor medicines have multiple mechanisms
        of action. The more specific you make it, the less robust the therapeutic activity is
        likely to be.” Rossi agrees: “Overspecificity has never worked,” he says.
        The second question is what effect an excess of RNA from outside the cell will have on
        the normal function of the RISC, the complex at the heart of the RNAi mechanism. The number
        of RISCs in the cell is unknown, and one concern is that the amount of RNA needed to have a
        therapeutic effect may occupy all the available complexes. “We are usurping a natural cell
        system that is there for some other purpose, for knocking out endogenous gene function,”
        says Rossi. With the introduction of foreign RNA, will the system continue to perform its
        normal function as well, or will it become saturated? “That's the big black box in the
        field,” he says.
      
      
        Looking Ahead to the Clinic
        Despite the questions and unsolved problems, Sirna, Alnylam, and several other companies
        are moving ahead to develop RNAi therapy; indeed, some outstanding questions are probably
        only likely to be answered in the process of therapeutic development. The first
        applications are likely to be in cancer (targeting out-of-control oncogenes) or viral
        infection (targeting viral genes). To avoid some of the problems of delivery, initial
        trials may deliver the RNA by direct injection into the target tissue (for a tumor, for
        instance) or ex vivo, treating white blood cells infected with HIV, for example.
        Having spent a decade trying to develop ribozymes, says Usman, Sirna is prepared for the
        rough road it faces. “We haven't solved all the problems, but we know how to proceed to
        work through them. It's no surprise to us—we've seen this movie before.” Usman expects
        Sirna to file an Investigational New Drug Application with the FDA by the end of 2004 and
        to have a human clinical trial in progress in 2005. “Without a doubt, there will be an
        RNAi-based drug within ten years,” Usman predicts.
        Stein isn't so sure and thinks that too much is still to be learned about RNAi and the
        body's reaction to it to be confident that RNA-based therapies will ultimately be
        successful. “The whole field was founded on the belief it was rational, but there are huge
        gaps in our knowledge, and so you need a bit of luck to be successful,” he says. “I think
        people are surprised at how complicated it is, but why should it be any other way? It's an
        intellectual conceit to think that nature is simple.”
      
    
  

  
    
      
        
        For half a century, natural products from microorganisms have been the main source of
        medicines for treating infectious disease. The most important chemical class of these
        antibiotics, apart from the penicillins, is the polyketides. They are made by the stepwise
        building of long carbon chains, two atoms at a time, by multifunctional enzymes that
        determine the chain length, oxidation state, and pattern of branching, cyclisation, and
        stereochemistry of the molecules in a combinatorial fashion to produce an enormous variety
        of structures. Recent elucidation of the genetic ‘programming’ of the enzymes has opened a
        new field of drug discovery based on rationally engineering the enzymes to produce
        ‘unnatural natural products’ with novel properties.
        Following the development of penicillin for the treatment of septicemia in the early
        1940s, numerous antibiotics were discovered and introduced into medicine. While a fungus
        makes penicillin, semisynthetic derivatives of which have been a mainstay of antibacterial
        therapy for decades, most natural antibacterial antibiotics come from a group of
        soil-dwelling, filamentous bacteria called the actinomycetes, of which 
        Streptomyces is the best-known genus. These organisms make an
        amazing array of so-called secondary metabolites that have evolved to give their producers
        a competitive advantage in the complex soil environment, where they are exposed to stresses
        of all kinds (Challis and Hopwood 2003). The compounds have many functions, but those with
        antibiotic activity are the most important from the human perspective. Actinomycete
        antibiotics include such antibacterial compounds as tetracycline and erythromycin,
        antifungal agents like candicidin and amphotericin, anticancer drugs such as doxorubicin,
        and the antiparasitic avermectin (Walsh 2003).
        While many different chemical classes are represented amongst actinomycete antibiotics,
        one class accounts for an extraordinary proportion of the important compounds, including
        all those mentioned above. This chemical family is made up of the polyketides. They are
        synthesized by multifunctional enzymes called polyketide synthases (PKSs), which are
        related to the fatty acid synthases that make the lipids essential for the integrity of
        cell membranes, but they carry out much more complex biosynthetic routines. Repeated rounds
        of carbon chain building and modification use a series of independently variable reactions
        selected according to a ‘program’ characteristic of each PKS (Reeves 2003). Recent research
        has focused on determining this program so as to be able to modify it in rational ways by
        genetic engineering and thus generate novel drug candidates. The resulting field of
        ‘combinatorial biosynthesis’ of ‘unnatural natural products’ has been given added urgency
        by the rise of multidrug-resistant pathogens, of which MRSA (methicillin-resistant 
        Staphylococcus aureus ) is simply the most discussed of a series
        of threats (Walsh 2003). How do PKSs work and how can we make new ones?
      
      
        Molecular Diversity
        The heart of PKS function is the synthesis of long chains of carbon atoms by joining
        (condensing) together small organic acids, such as acetic and malonic acid, by a so-called
        ketosynthase function. This uses the building units in the form of activated derivatives,
        called coenzyme A (CoA) esters, so we speak of acetyl-CoA and malonyl-CoA, for example. The
        special form of condensation that joins them is driven by loss of carbon dioxide. Thus,
        when acetyl-CoA, with two carbon atoms, joins with malonyl-CoA, with three carbons, one of
        the latter is lost and a chain of four carbon atoms results (Figure 1A). Further rounds of
        condensation extend the chain by two carbons at each step. If the chain-extender unit,
        instead of being malonyl-CoA, is methylmalonyl-CoA, which has four carbon atoms, the linear
        carbon chain is still extended by two carbons, and the ‘extra’ carbon forms a methyl side
        branch. More complex extender units produce more complex side branches.
        Choices of the number and type of the building units are variables in determining
        polyketide structure. Another concerns the keto groups (C=O) that appear at every alternate
        carbon atom in the growing chain as a result of the condensation process (accounting for
        the name polyketide). They may remain intact. Alternatively, some may be modified or
        removed by a series of three steps (Figure 1B), any of which may be omitted. This results
        in keto groups remaining at some points in the chain; hydroxyl groups (–OH), formed by
        reduction of a keto group, at others; double bonds between some adjacent carbon atoms,
        resulting from removal of the hydroxyl by loss of water (dehydration); or full saturation
        with hydrogen atoms elsewhere, arising by ‘enoyl’ reduction of the double bond (Figure 1C).
        A further variable concerns the stereochemistry of the hydroxyl groups and methyl or other
        carbon branches, each of which can exist in two possible configurations. Finally, the
        nascent carbon chain adopts different folding patterns after it leaves the PKS, and
        ‘tailoring’ enzymes can then add sugars or other chemical groups to it at many alternative
        positions, enabled by the pattern of chemical reactivity built into the polyketide by the
        PKS. The challenge has been to understand the programming of the PKS that accounts for this
        gamut of structural variation.
        During the 1990s, the ability to manipulate actinomycete genes, developed over previous
        decades, mainly using the model species 
        Streptomyces coelicolor (Hopwood 1999), was combined with
        chemical and biochemical experiments to begin to crack this ‘polyketide code’. The first
        studies were on organisms making antibiotics of the ‘aromatic’ family, which includes
        tetracycline and doxorubicin, as well as the model compounds actinorhodin (made by 
        S. coelicolor itself) and tetracenomycin. The main variable in
        their structure is carbon chain length, with few choices of different building units or
        keto group modification, so the programming would (in principle) be simple. The DNA
        sequences responsible for such PKSs revealed sets of genes encoding proteins, including
        ketosynthases, ketoreductases, and acyl carrier proteins (ACPs) (the unit of the PKS on
        which the growing carbon chain is tethered; see Figure 1A), that would come together to
        form a multicomponent PKS resembling a typical bacterial fatty acid synthase. In contrast,
        the DNA sequence of the gene set for the complex polyketide erythromycin, made by a
        relative of 
        Streptomyces called 
        Saccharopolyspora erythraea , which has more involved
        programming, revealed multifunctional proteins with the various enzymic functions carried
        out by active sites on the same polypeptide chain, as in a mammalian fatty acid
        synthase.
        The big surprise, though, was the finding of six sets, or modules, of such active sites,
        corresponding to the six rounds of condensation needed to build the carbon chain (Cortes et
        al. 1990; Donadio et al. 1991). The modules each contain an acyl transferase (to load the
        extender unit onto the enzyme), as well as a ketosynthase and an ACP domain, together with
        exactly those reductive activities needed to generate the required pattern of modification
        of the chain at each step of elongation. Thus was born an ‘assembly line’ model in which
        the program for the PKS is hardwired into the DNA and expressed in a linear array of active
        sites (domains) along the giant protein. This consists of the six chain-building modules,
        preceded by a short module for loading the starter unit and ending in a domain for
        releasing the completed carbon chain from the PKS. The carbon chain of the polyketide would
        be assembled and modified progressively as the molecule moved along the protein,
        interacting with each domain in turn, which would select extender units, make carbon–carbon
        bonds, and modify keto groups as appropriate, depending on the presence or absence of
        domains for the three steps in the reductive cycle.
        The model arose from the gene sequence, but was rapidly tested by mutating individual
        domains or adding or deleting whole modules and by observing predicted changes in the
        polyketide product. Soon, dozens of engineered compounds had been made, and the field
        mushroomed with the isolation of more and more clusters of genes for complex polyketides
        that both proved the generality of the model (with minor variations) and filled the need
        for spare parts for the engineering of countless new polyketides (Shen 2003). Several
        biotech companies were founded to exploit the potential for drug discovery.
      
      
        Aromatic PKS Programming
        Meanwhile, the programming of the aromatic PKSs was harder to understand. They had been
        found to contain only a single ketosynthase, which has to operate a specific number of
        times to build a carbon chain of the correct length, so how is this determined? How does a
        single reductive enzyme know which keto groups to modify? And how is the starter unit for
        building the carbon chain selected (the extender units are normally all malonyl-CoA, so no
        choice is involved)? Considerable progress had been made in constructing novel compounds by
        mixing and matching PKS subunits, but this was largely based on empirical knowledge about
        which components to put together (McDaniel et al. 1995). A specific subunit of the PKS,
        named the chain length factor (CLF), was deduced to have a major influence on carbon chain
        length (McDaniel et al. 1993), but this conclusion was not universally accepted in the
        absence of experimental evidence on its mode of action. Two recent publications by the
        Khosla laboratory at Stanford University describe significant advances in understanding
        aromatic PKS programming and promise to turn the spotlight back onto engineered members of
        this class of compounds as potential drug candidates by allowing rational manipulation of
        the two key variables: carbon chain length and choice of starter unit.
        In the first paper (Tang et al. 2003), the authors explore the hypothesis that the CLF
        exerts control over carbon chain length by associating closely with the ketosynthase, a
        protein with which it shares considerable amino acid sequence similarity, giving rise to a
        channel of a certain size at the interface between the two proteins. By systematically
        changing amino acids at four key positions in the CLF, the size of the channel was altered.
        Thus, large amino acid residues in the CLF of a PKS that makes a 16-carbon chain were
        replaced by less bulky residues found in one that builds a 20-carbon chain, and the chain
        length of the product increased as expected. The authors propose that the length of the
        channel is the main factor in controlling the number of chain-extension steps that can take
        place to fill it. While protein–protein interactions with other PKS subunits may modulate
        this chain length control, the work represents a major step in understanding and
        manipulating the chain length of aromatic polyketides.
        What about the choice of starter unit? Most aromatic polyketides start with acetyl-CoA.
        An important earlier publication by Leadlay and colleagues (Bisang et al. 1999) had shown
        that this is not loaded directly onto the PKS, as had been assumed, but is derived by loss
        of carbon dioxide from a molecule of malonyl-CoA previously loaded onto the enzyme. This
        decarboxylation is catalysed by the CLF as an activity independent of its role in
        influencing carbon chain length. There are, however, certain aromatic polyketides,
        including the anticancer drug doxorubicin, an antiparasitic agent called frenolicin, and
        the estrogen receptor agonist R1128, that have different starters.
        What Tang et al. (2004) have deduced, as described in this issue of 
        PloS Biology , is that the PKSs for these compounds consist of two
        modules of active sites. The components of each module are not activities carried on the
        same protein, as in the PKSs for the complex polyketides, but are all separate proteins.
        They form functional modules nevertheless. The newly recognized modules in the producers of
        compounds that start with nonacetate units have a dedicated ACP and a special ketosynthase
        that carries out a first condensation, joining the unusual starter unit to the first
        malonyl-CoA extender unit. The starter module then hands the resulting ‘diketide’ on to the
        second module (first reducing it, if appropriate, using reductive enzymes ‘borrowed’ from
        fatty acid biosynthesis) for typical extension by successive condensation with malonyl-CoA
        units to complete the chain. If the starter module is not present, the second module
        defaults to its typical habit of decarboxylating malonyl-CoA to acetyl-CoA and starts the
        chain with that.
        The excitement of the work for biotechnology is that it offers the prospect of
        engineering promising drug candidates by making novel combinations of starter and extender
        modules and perhaps of feeding the starter modules with a whole range of unnatural
        substrates (Kalaitzis et al. 2003). It is encouraging that already, in the
        proof-of-principle studies reported by Tang et al. (2004), some products with improved in
        vitro antitumor activity were obtained.
      
    
  

  
    
      
        
        For people who received their introduction to cancer genetics in college in the first
        half of the 1990s, everything looked simple and straightforward. It was the stuff you could
        explain to sincerely interested relatives who wanted to know what you were spending your
        time on. There were oncogenes and there were tumour suppressor genes. Oncogenes were
        overactive genes and proteins that somehow caused cancer because they were overactive;
        therefore, they were dominant. Tumour suppressor genes were genes that would normally
        prevent a tumour from happening and that needed to be inactivated for a tumour to start to
        form; both copies of a tumour suppressor gene had to be inactivated, and the mutation was
        recessive. If inactivation of these genes is a random process, it was understandable that
        people who inherit an inactivated copy of a tumour suppressor gene had a higher risk of
        developing the associated form(s) of cancer than people born with two normal copies, as
        postulated in Alfred Knudson's (1971) two-hit model. And, indeed, it was shown that in the
        tumours in these predisposed patients, the remaining wild-type copy of the tumour
        suppressor gene was lost, a process referred to as loss of heterozygosity.
        For me, in 1998 things started to change. Venkatachalam et al. (1998) published a paper
        in the 
        EMBO Journal describing a detailed study of tumours in mice lacking one
        copy of the p53 tumour suppressor gene (
        Trp53 ). This gene is known to be the most mutated gene in human cancer
        and its function to be central to many processes that are involved in the cellular
        prevention of cancer. Mice lacking both copies of this gene are for the most part viable,
        but succumb to cancer (mainly thymic lymphomas) at three to five months of age (Donehower
        et al. 1992). Mice born with one copy of the 
        Trp53 gene start to develop cancer at around nine months, and incidence
        increases with age.
        In their study, Venkatachalam and colleagues analysed an impressive group of 217 
        Trp53
        +/− mice of controlled genetic background and followed the fate of the 
        Trp53 wild-type allele in the tumours. According to the two-hit model, it
        was expected that in these tumours this copy would have been lost or inactivated. However,
        this turned out not to be the case. Half of the tumours from mice younger than 18 months
        were found to have retained the wild-type copy of 
        Trp53 , a number that increased to 85% in mice older than 18 months. In
        two tumours, the researchers sequenced the complete coding region of the remaining
        wild-type allele and showed it was structurally intact. To exclude the possibility of
        downregulation or inactivation at the level of protein expression, they irradiated
        tumour-bearing mice prior to sacrifice, a treatment known to increase p53 protein levels
        via posttranslational mechanisms. Their data showed the retained wild-type allele in these
        tumours was expressed normally and suggested it had a normal wild-type conformation.
        Next, the authors did a rigorous test of different functions of the p53 protein. They
        first tested whether the tumours showed amplification of Mdm2. This protein, whose
        expression is regulated by p53, stimulates breakdown of p53, thereby forming a negative
        feedback mechanism that keeps p53 levels low. Some tumours therefore amplify the 
        Mdm2 gene as a means of inactivating p53. However, this was not found in
        the tumours from the 
        Trp53
        +/− mice. Subsequently, the researchers tested to what extent the
        retained 
        Trp53 copy behaved normally. Irradiation of many tissues leads to
        p53-dependent apoptosis, and, indeed, in tumours that had retained the wild-type allele,
        irradiation did lead to an increase in apoptosis, whereas in tumours that had lost the
        wild-type allele, it did not.
        The p53 protein is known to function as a transcriptional regulator by either up- or
        down regulating target genes in response to different forms of cellular stress, including
        irradiation-induced DNA damage. The authors studied the expression of two p53-upregulated
        genes (
        Cdkn1a , which encodes p21, and 
        Mdm2 ) and one downregulated gene (
        Pcna ) in p53-positive tumours after irradiation and showed responses
        indicative of normal p53 function. Furthermore, it was shown that the p53 protein from the
        tumours was able to bind to a p53-binding DNA sequence in an in vitro setting. Finally,
        since it is known that p53 absence in tumours is correlated with chromosomal instability,
        Venkatachalam et al. (1998) used comparative genome hybridisation to compare this feature
        between p53-negative and p53-positive tumours and found a 5-fold greater stability in the
        latter.
        In short, this paper clearly showed that, at least in mice, in many 
        Trp53
        +/− tumours the wild-type allele of 
        Trp53 is not only retained, but also appears to function normally. This
        strongly suggested that a decrease of dosage in p53 is already sufficient for
        tumourigenesis, a phenomenon referred to as haploinsufficiency. Shortly before, the group
        of Moshe Oren (Gottlieb et al. 1997) had shown that a 
        Trp53
        +/− background leads to a greater than 50% reduction in p53 activity
        using a p53-responsive 
        lacZ reporter gene in transgenic mice. Venkatachalam and colleagues
        suggested the strong concentration dependence of p53 function could be explained by the
        fact that p53 functions as a tetramer. A 50% decrease in p53 monomers can easily be
        imagined to result in a greater than 50% decrease in functional tetramers, which in turn
        increases the chances of these cells becoming cancerous.
        This paper by Venkatachalam et al. (1998) made me realise how important it is to remain
        critical, even of long-established theories and models. Since then, haploinsufficient
        mechanisms have been described in more tumour suppressor genes in humans and mice (reviewed
        in Fodde and Smits 2002). For instance, in a recent paper in 
        PLoS Biology , Trotman et al. (2003) used mouse models to describe how
        the dosage of the 
        Pten tumour suppressor gene influences the occurrence of prostate cancer.
        Further genes have been described with other unexpected roles in the tumourigenic process.
        There is a long-standing debate in the literature about the number and role of mutations in
        a tumour, and without going into the details, it is clear that haploinsufficient mechanisms
        for tumour suppressor genes will greatly influence the statistics on which these
        discussions are based. At a time when microarray analysis has become a standard experiment
        and the many thousands of changes in tumour cells are analysed across the whole genome, it
        is important to keep in mind that the correct interpretation of this wealth of information
        might be more complicated than the widely accepted models would have us believe.
      
      
        
      
    
  

  
    
      
        
        During the last few years, we have seen enormous strides in our abilities to sequence
        genomes, and the information that has poured out of these sequences is quite astonishing.
        With more than 150 complete genome sequences now available and many laboratories rushing
        into microarray analysis, proteomic initiatives, and even systems biology, it seems an
        appropriate time to consider not just the opportunities those sequences present, but also
        their shortcomings. By far the most serious problem is the quality and degree of
        completeness of the annotation of those genomes. Most troublesome are the large numbers of
        open reading frames that have been identified by computer programs, but remain labeled as a
        “conserved hypothetical protein” when they occur in more than one genome or simply a
        “hypothetical protein” when they appear unique to the genome in question. Between them,
        these two categories of annotated open reading frames often represent more than half of the
        potential protein-coding regions of a genome.
        These annotations highlight just one portion of our ignorance about the information
        content of genomes and our lack of fundamental knowledge about the function of so many of
        the building blocks of cells. Unless we rectify this situation, it is likely to undermine
        many of the other “-omic” efforts currently underway. Here I advocate a rather
        straightforward approach to address this problem—focused initially on the bacterial
        genomes. In contrast to the numerous proposals for big science initiatives to understand
        the fundamental workings of biological organisms, I propose a small science, relatively
        low-tech approach that could have a dramatic pay off. A relatively small investment could
        yield a massive amount of information that would greatly enhance our current efforts to use
        genomic approaches to study life.
      
      
        Initial Proposal
        The initial proposal is directed at deciphering the role of the “hypothetical proteins”
        encoded in the microbial genomes and would involve a community-wide approach to determine
        the function of these hypotheticals based on solid, old-fashioned biochemistry. The essence
        of the idea is to undertake an interdisciplinary effort that couples our current
        bioinformatics capabilities to predict protein function with a directed exploration by
        experimental laboratories to test those predictions. I would encourage a consortium of
        bioinformaticians to produce a list of all of the conserved hypothetical proteins that are
        found in multiple genomes, to carry out the best possible bioinformatics analysis, and then
        to offer those proteins to the biochemical community as potential targets for research into
        their function. To energize laboratories with appropriate expertise to participate in this
        community-wide effort, I suggest that a special program be set up by one or more of the
        funding agencies so that laboratories undertaking the investigation of any particular
        protein receive a small grant upfront as a supplement to an existing grant. Upon completion
        of the project and the identification of the function, they would receive a further
        supplement to that grant as a reward. In this way, one might hope to rally some of the best
        biochemical talent and apply it to this problem of determining function for a wide range of
        new proteins. The cost of such an operation could be quite minimal, and the bureaucracy and
        review process could be equally simple. Here is a case where a modest infusion of funds
        could greatly enhance our ability to annotate both existing and new genome sequences and
        ensure that our current investments in genomic sequences yield the richest biological
        harvest possible. There are two key steps in the proposed plan.
      
      
        Key Steps
        The first step is to encourage some bioinformaticians with appropriate expertise in the
        functional annotation of genomes to form a consortium and undertake the assembly of a list
        of prime targets for which an experimental demonstration of function would be most
        valuable. Three general classes of such genes come to mind: (1) The conserved hypothetical
        genes. These belong to the set of genes that have orthologs in many other genomes, but for
        which no function has been experimentally determined in any case. A recent success among
        such genes is illustrated in Box 1. (2) The hypothetical genes. These form the set of genes
        that are predicted to be protein coding, but that lack similar genes in any other organism
        in GenBank. They, too, have no assigned function. (3) The misannotated genes. These genes
        are ones for which a function has been assigned, but for which there is a good reason to
        believe the annotation is incorrect.
        These sets of targets would be combined and arranged into a prioritized list in which
        each was accompanied by the best assessment of potential function. The priorities would be
        based on which genes were most likely to prove broadly informative. For instance, a
        conserved hypothetical gene that occurred in most genomes would be of higher priority than
        one that had only two orthologs. The list would be on a public Web site where these targets
        and the predicted functions could be examined and modified by alternative or additional
        predictions from other groups to guide future experimentation. As function was derived,
        that information could be presented and the target removed from the main list.
        The second step would be to invite experimentalists to peruse the list and find those
        potential genes whose protein products might lie within their realm of expertise so that
        they could use their experimental knowledge and reagents to quickly test for function.
        Initially, I would advocate allowing laboratory teams to pick and choose among the list and
        sign up to study just one of these open reading frames. I would recommend allowing one
        laboratory per open reading frame in the initial stages. A laboratory wishing to sign up
        would generate a short document highlighting why its expertise might be suitable for a
        particular protein. A one-page proposal should suffice, with no experimental plan demanded.
        At this point, a small panel could choose among competing efforts and the laboratory chosen
        would be given a small grant and up to six months to carry out its analysis. If it was
        successful in delineating the function of their target protein, a paper would be written
        and submitted for peer review. If the paper was accepted for publication, then an
        additional sum would be allocated as a supplement to the laboratory's existing grant. If,
        after six months, a laboratory had not managed to delineate the function, it would submit a
        short report describing the approaches that have been tried, with the results of its
        analyses. This would be posted on the public Web site and that target would then become
        open for analysis by other laboratories, under the same conditions as before.
        While the initial list of target genes should probably be based on a well-studied and
        experimentally tractable organism such as 
        Escherichia coli , I would not demand that the biochemical
        experiments be done on the 
        E. coli gene. Any of the orthologs would do, so long as the
        similarity was sufficiently strong to give high expectations that function would be
        conserved. In fact, for a laboratory that happened to be already working on one of the
        homologs, this program might provide an added bonus and greatly speed its work. I would
        also encourage both biochemical and genetic approaches, since one can never be certain when
        one method might be better than another. The list would, of course, also include conserved
        genes not found in 
        E. coli , but commonly distributed in other genomes. In
        particular, I would make a pitch for including all genes in 
        Mycoplasma genitalium , which, as the free-living organism with
        the fewest genes, might be the most suitable as a model system for in-depth understanding
        of its biology.
      
      
        The Importance of Community
        This proposal for experimental attack on hypothetical genes is really a very traditional
        approach that becomes large-scale simply because of the parallel nature of the
        implementation. It resembles the successful approach used by the Europeans to achieve the
        complete sequence of the 
        Saccharomyces cerevisiae genome (Goffeau et al. 1996). The
        results would significantly increase our functional knowledge of the genes within the
        microbial genomes thus far sequenced. Such annotation would be immediately applicable
        across orthologs and could dramatically improve the value of the sequenced genomes. This,
        in turn, would facilitate our ability to annotate new genomes as they appear. The proposal
        also reinforces the notion that the overwhelming value of bioinformatics is to generate
        hypotheses that can be tested experimentally. By enabling the community to join in this
        effort, we would also demonstrate that science really is the collaborative enterprise that
        requires all of our contributions, not just a select few. Finally, if this initiative
        succeeds, it would serve as a suitable model from which to begin the more daunting task of
        trying to annotate the functions of the complex eukaryotic genomes, such as the human
        genome.
      
      
        
      
    
  

  
    
      
        
        I am a clone. That is, I am a colony of cells that developed from a single fertilized
        egg cell. Most animals are clones like me. It is a slight oversimplification to say that
        all of an animal's cells are genetically identical to each other. Some cells have
        mutations. In mammals, some cells (red blood cells) lack a nuclear genome entirely. Some
        cells have viruses—and when it's in a cell, a virus is basically a gene—that other cells
        lack. But a typical animal is a clone in the sense that all its cells arise from that
        single fertilized egg cell.
        Not all animals, however, are clones. Sometimes two tiny embryos developing inside their
        mother will fuse together into a single embryo and continue developing. The resulting
        animal is not a clone, but a chimera: a conglomeration of two different cell lineages into
        a single organism. Some species of monkeys (marmosets) typically have chimeric blood, from
        having shared a blood supply with a twin in utero (Haig 1999), and rare cases of accidental
        chimerism are known from many animal species (Tremblay and Caltagirone 1973; van Dijk et
        al. 1996). In marine invertebrates, chimeric individuals often arise from the fusion of
        individuals later in development (Buss 1987). Here I want to draw attention to a remarkable
        form of chimerism found in armored scale insects. These insects (Figure 1) always develop
        not from a single fertilized egg but from two genetically different cells. One of these
        cells develops into a special organ (the bacteriome, which houses symbiotic bacteria) that
        has a nuclear genome different from that found in the rest of the body.
        Obligate chimerism—the presence of two genetically distinct cell lineages in every
        individual at each life stage—is found in a few families of scale insects, but nowhere else
        in nature. The avid naturalist wants to understand this sort of deep oddity for its own
        sake, but such understanding might have broader implications as well. For instance,
        although humans are not usually chimeras, we do have a quasi-chimeric phase in our life
        cycle: pregnancy. Some diseases of pregnancy are apparently due to conflicts between the
        genetically nonidentical tissues of mother and fetus (Haig 2002). And the main things that
        humans eat are also quasi-chimeras: the seeds of flowering plants. In a grain of wheat, for
        instance, the germ, the endosperm, and the bran have three different nuclear genomes, and
        the conflicts between them may be similar in some ways to the conflicts seen in human
        pregnancy (Alleman and Doctor 2000; Santiago and Goodrich 2000). Ultimately, we might learn
        something about the general principles of conflict and cooperation between maternal and
        embryonic tissues that govern these cases if we can understand the uniquely stable and
        intimate chimerism of armored scale insects.
      
      
        Two Different Cell Lineages
        In all sexual animals and plants, production of an egg cell involves meiosis, the
        complex cellular process (involving DNA replication, recombination, and two nuclear
        divisions) whereby one diploid nucleus (with two copies of each chromosome) becomes four
        genetically different haploid nuclei (each with one copy of each chromosome). Only one of
        these four haploid nuclei becomes the egg cell (oocyte). In ordinary animals, the other
        three nuclei (the polar bodies) degenerate—they never divide again and are lost or
        destroyed—and the oocyte is the single maternal cell that (after fusion with a single
        paternal cell, the spermatocyte) develops into the embryo. But in armored scales, the polar
        bodies fuse together into a triploid cell (with three copies of each chromosome), and this
        triploid cell also winds up in the embryo (Figure 2). The triploid cell derived from the
        polar bodies fuses with one cell from the embryo to become a pentaploid cell (with five
        copies of each chromosome). This pentaploid cell then proliferates to form the bacteriome
        of the embryo (Brown 1965). Each cell in the bacteriome thus contains two copies of the
        mother's complete genome, in addition to the same haploid paternal genome as the rest of
        the embryo. In contrast, the rest of the embryo contains just one copy of half of the
        mother's genome. The apparent function of the bacteriome is to house intracellular
        bacteria. During embryonic development, bacteria move from the mother's bacteriome into the
        cells of the embryo's bacteriome. The precise role of the bacteria is not known, but it is
        thought that they synthesize essential nutrients (Tremblay 1990), as they do in scale
        insects' close relatives, the aphids (Shigenobu et al. 2000).
        Now the story gets even stranger. If the individual is a male, then the genetic
        difference between his bacteriome and the rest of his tissues becomes even greater as he
        develops. This is because most armored scale insects have an unusual genetic system called
        embryonic paternal genome elimination (Herrick and Seger 1999). In males, the paternal
        genome is completely eliminated from most tissues very early in development—but it is never
        eliminated from the bacteriome. As a result, most of a male armored scale insect's tissues
        (including his sperm) have one copy of half of the mother's genome (the same genome as the
        oocyte from which he developed), but his bacteriome has two complete copies of the mother's
        genome and also has a paternal genome. Thus, 60% (three of five) of the gene copies in the
        male's bacteriome are absent from the rest of the male (Figure 2).
      
      
        Chimerism and Sibling Rivalry
        What could possibly be going on here? Why should scale insects, of all creatures, have
        obligate chimerism involving activated polar bodies? Essentially, we have no idea, largely
        because no one has even ventured a serious guess. When the phenomena were discovered, early
        in the 20th century, the theoretical tools for making sense of them were unavailable. One
        such tool is W. D. Hamilton's (1964a) theory of inclusive fitness, which holds that the
        degree of cooperation between two organisms (or tissues) must depend upon their degree of
        genetic relatedness. But the rise of Hamiltonian thinking coincided with the eclipse of
        classical cytogenetics in favor of the molecular biology of model organisms, and these
        remarkable little chimeras have languished in undeserved obscurity. Perhaps merely by
        looking at them with a modern eye, we can turn up some plausible hypotheses.
        Consider the special theoretical difficulty posed by chimerism between tissues derived
        from the oocyte and those derived from the polar bodies ejected by it during meiosis. Two
        siblings will typically exhibit some degree of sibling rivalry—their interests are not
        identical. If an individual were a chimera comprising full-sibling tissues (identical
        across approximately half of their genomes), there might be conflict between the two
        nonidentical cell lineages, as there is between the tissues of a mother mammal and her
        fetus (also identical across half of their genomes) during pregnancy (Haig 2002). This may
        explain why obligate sibling chimerism never evolves (except perhaps in the very limited
        case of blood cells between sibling marmosets). But the problem of cooperation between
        tissues that derive from the oocyte and those that derive from the polar bodies is, if
        anything, even greater. The oocyte and the polar bodies are 
        less closely related than two siblings would be, because the polar bodies
        are enriched for chromosome regions 
        not present in the oocyte.
        If there were no crossing over between homologous chromosomes during meiosis, then the
        first meiotic division would consistently separate the chromosomes derived from the
        mother's mother from the chromosomes derived from the mother's father, producing two cells
        that are not related to each other 
        at all (or, more precisely, exactly as closely related to each other as
        the mother's mother was to the mother's father). Crossing over prevents this, creating a
        mosaic of related and unrelated chromosome regions between the products of the first
        meiotic division and uncertain relationships between the final four meiotic products.
        Nonetheless, the consistently depressed relatedness between the oocyte and the polar bodies
        may help to explain why polar bodies are almost always eliminated—sibling rivalry might be
        even greater if some siblings were derived from each other's polar bodies.
      
      
        Towards an Explanation
        So how and why did two families of scale insects tame and domesticate their potentially
        fractious polar bodies, rather than killing them like normal animals do? There are at least
        three lines of thinking that seem promising for unraveling this mystery.
        
          Histological eusociality and relatedness.
          There are interesting parallels between, on the one hand, the chimerism seen in
          armored scale insects and, on the other hand, the eusociality (true sociality) seen in
          ants and honeybees. In ants and honeybees, sterile individuals (workers) provide
          nutrition to their potentially fertile siblings. In armored scale insects, a genetically
          distinct but ultimately sterile cell lineage (the bacteriome) provides nutrition to its
          potentially fertile “sibling” cell lineage (the rest of the scale insect)—though, of
          course, polar body-derived cells are “sibling” in a strange special sense. Like ants and
          bees, armored scale insects are effectively haplodiploid: males transmit only the
          chromosomes they inherited from their mother, and all of a male's sperm are identical to
          each other. This “clonality” of sperm boosts the relatedness between sisters, and
          Hamilton (1964b) pointed out that this high relatedness can explain the high level of
          cooperation between sisters seen in eusocial ants and honeybees. High levels of
          cooperation between sisters have since been found in haplodiploid thrips as well (Crespi
          1992). It is tempting to speculate that similar explanations can be applied to
          “histological eusociality” seen in the cooperation between related tissues in scale
          insects.
          This temptation increases when we consider another case from the old cytogenetics
          literature of apparent histological eusociality, though not of true permanent chimerism.
          This occurs in a few families of parasitoid wasps (Tremblay and Caltagirone 1973; Strand
          and Grbic 1997), in which cells derived from polar bodies form a membrane around the
          yolkless egg that is deposited in the wasp's insect host (which is often a scale
          insect!). Similar to the worker ant and the scale insect bacteriome, this membrane is
          thought to mediate nutrition of the developing embryo, and, similar to ants, honeybees,
          and scale insects, these wasps are haplodiploid.
          But although it is tempting to conclude that haplodiploidy plays the same role in
          promoting histological eusociality as it does in promoting organismal eusociality, the
          temptation should probably be resisted. In the case of the parasitoid wasps, the polar
          body-derived tissue has no paternal genome, so the clonality of sperm cannot boost its
          relatedness to anything. In the case of armored scale insect chimerism, the bacteriome
          does have a copy of the paternal genome, and that copy is identical to the paternal
          genome in the rest of the embryo, so in that sense the two tissues do have a high
          relatedness similar to the high relatedness of full siblings under haplodiploidy. But the
          scale insect bacteriome gets its copy of the paternal genome directly from the embryo, so
          the clonality of sperm (the source of elevated relatedness between haplodiploid sisters)
          apparently has nothing to do with it. Some other explanation is probably needed.
        
        
          Maternal interests.
          Note that, whatever the relationship between the polar body-derived tissues and the
          rest of the insect, the polar bodies contain the mother's complete genome. And while your
          polar bodies may be 
          less related to you, on average, than your siblings are, they are 
          more related to your siblings (and, of course, to your mother) than
          they are to you! Perhaps the polar bodies function to somehow enforce some maternal or
          family interest, nipping in the bud some sibling rivalry that would otherwise suppress
          the family's fitness. In haplodiploid groups, females are more closely related to their
          full sisters (with whom they share three-quarters of their gene copies) than to their
          brothers (with whom they share only one-quarter of their gene copies). So if there is
          competition between siblings for resources, females are expected to behave more
          antagonistically towards brothers and more cooperatively towards sisters. In contrast, a
          female is equally related to a son as she is to a daughter (each carries half of her gene
          copies). These asymmetries in relatedness lead to struggles in haplodiploid social
          groups, with a mother seeking to direct more resources towards sons and with sisters
          seeking to direct more resources away from their brothers and towards each other (Seger
          and Stubblefield 2002).
          It is difficult to see how such a struggle might play itself out in scale insects,
          which are hardly social insects. The only motile stage in a female's life is the first
          instar (“crawler”), after which she settles in one spot permanently. The male is slightly
          more mobile, having a motile (usually winged) adult form. Nonetheless, (1) the low
          motility of females, and the fact that they live mostly on long-lived woody plants, means
          that maternal kin may interact over long timescales, as in social insects; (2) some scale
          insects appear to make relatively sophisticated social decisions about where to settle,
          settling nearer to (and thus possibly competing more closely with) non-kin than kin
          (Kasuya 2000); (3) although most scale insects use phloem sap, an almost inexhaustible
          resource, armored scales use parenchyma tissues (Rosen 1990), which may be locally
          exhausted, and therefore may compete against neighbors for food. Thus, it is conceivable
          that females may compete against brothers and sisters for resources, that they may make
          decisions that affect the intensity of that competition, and that such decisions may have
          different optima from the perspectives of maternal versus paternal genes. Possibly, the
          presence of a massive contingent of maternal genes (a double dose of the complete
          maternal genome) in a nutritionally significant tissue like the bacteriome might somehow
          affect such decisions in ways favorable to maternal interests. Proximal mechanisms might
          include effects on signals or perceptions related to relatedness, gender, site quality,
          or satiety.
          Similar manipulation of intersibling interaction might be going on in the case of the
          parasitoid wasps that have polar body-derived membranes around their eggs. Sometimes
          these wasps lay a single unfertilized (male) egg and a single fertilized (female) egg
          into the same host. Both eggs divide to form embryos, which divide into a clone of many
          embryos. Some of the embryos become sterile “precocious” larvae that can apparently
          attack other larvae trying to use the same host—including, potentially, their own
          siblings (Ode and Hunter 2002). Here is a situation in which the polar body genes (in the
          membranes surrounding the proliferating embryos) might have very different selective
          optima for levels of between-sibling aggressiveness—and even for rates of
          development—than the genes in the embryos they surround, and (because they apparently
          mediate the nutrition of the embryos) they might be able to influence how the embryos
          develop.
        
        
          Gender crypsis.
          The endosymbiotic bacteria that dwell in the scale insects' bacteriomes are maternally
          inherited. Thus, from the perspective of the bacteria, male insects are deadends. Many
          maternally inherited bacteria have evolved to manipulate the hosts' genetic system for
          their own advantage. Some bacterial lineages induce parthenogenesis or feminize males.
          Bacteria may even evolve to suicidally kill male embryos that they find themselves in, if
          the death of the male frees up resources that his sisters can use (Majerus 2003). In
          order to do this, bacteria must respond to some cue that indicates the gender of the
          individual they are in. Potentially, a host could evolve resistance to such manipulation
          by maternally inherited bacteria by depriving those bacteria of cues indicating gender.
          In armored scales, the bacteriome has exactly the same genome (two complete copies of the
          mother's diploid genome and one complete copy of the father's genome) in all full
          siblings, whether they are male or female, and the same is usually true in mealybugs.
          This might explain why the bacteriome is the only tissue in which the father's genome
          remains present and active in both males and females.
        
      
      
        Prospects
        Scale insects and their bacteriomes challenge our notion of what an individual is. Is a
        scale insect's bacteriome a kind of sibling? Is it half sibling, half self? Is it a sterile
        slave, under control? Is it an extension of the mother, exerting control? In all other
        organisms, chimeras are temporary and unstable. How have scale insects suppressed the
        conflicts that normally tear chimeras apart? To approach such questions, we'll have to
        revive the empirical study of scale insect bacteriomes, combining approaches from recent
        studies of aphid bacteriomes (Braendle et al. 2003) and of human pregnancy (Haig 2002). We
        can better understand the nature of genetic conflicts in scale insects by studies of the
        genetic structure of scale insect populations, together with studies of sex ratio variation
        and the proximate mechanisms of sex determination. For simplicity, I have described only
        the most common of the huge variety of very different scale insect genetic systems and
        modes of bacteriome development (Tremblay 1977, 1990; Nur 1980). This diversity (greater
        than for the comparable cases of mammalian placentas and flowering-plant endosperms) means
        there is a huge scope for comparative ecological and genetic studies that could help
        elucidate general principles. The study of truly strange creatures can tell us what kinds
        of things are possible. That's why we will be so interested in any life found on another
        planet and why, in the meantime, we should take a close look at scale insects.
      
    
  

  
    
      
        
        Stuttering, with its characteristic disruption in verbal fluency, has been known for
        centuries; earliest descriptions probably date back to the Biblical Moses' “slowness of
        speech and tongue” and his related avoidance behavior (Exodus 4, 10–13). Stuttering occurs
        in all cultures and ethnic groups (Andrews et al. 1983; Zimmermann et al. 1983), although
        prevalence might differ. Insofar as many of the steps in how we produce language normally
        are still a mystery, disorders like stuttering are even more poorly understood. However,
        genetic and neurobiological approaches are now giving us clues to causes and better
        treatments.
      
      
        What Is Stuttering?
        Stuttering is a disruption in the fluency of verbal expression characterized by
        involuntary, audible or silent, repetitions or prolongations of sounds or syllables (Figure
        1). These are not readily controllable and may be accompanied by other movements and by
        emotions of negative nature such as fear, embarrassment, or irritation (Wingate 1964).
        Strictly speaking, stuttering is a symptom, not a disease, but the term 
        stuttering usually refers to both the disorder and symptom.
        Developmental stuttering evolves before puberty, usually between two and five years of
        age, without apparent brain damage or other known cause (“idiopathic”). It is important to
        distinguish between this persistent developmental stuttering (PDS), which we focus on here,
        and acquired stuttering. Neurogenic or acquired stuttering occurs after a definable brain
        damage, e.g., stroke, intracerebral hemorrhage, or head trauma. It is a rare phenomenon
        that has been observed after lesions in a variety of brain areas (Grant et al. 1999;
        Ciabarra et al. 2000).
        The clinical presentation of developmental stuttering differs from acquired stuttering
        in that it is particularly prominent at the beginning of a word or a phrase, in long or
        meaningful words, or syntactically complex utterances (Karniol 1995; Natke et al. 2002),
        and the associated anxiety and secondary symptoms are more pronounced (Ringo and Dietrich
        1995). Moreover, at repeated readings, stuttering frequency tends to decline (adaptation)
        and to occur at the same syllables as before (consistency). Nonetheless, the distinction
        between both types of stuttering is not strict. In children with perinatal or other brain
        damage, stuttering is more frequent than in age-matched controls, and both types of
        stuttering may overlap (Andrews et al. 1983).
      
      
        Who Is Affected?
        PDS is a very frequent disorder, with approximately 1% of the population suffering from
        this condition. An estimated 3 million people in the United States and 55 million people
        worldwide stutter. Prevalence is similar in all social classes. In many cases, stuttering
        severely impairs communication, with devastating socioeconomic consequences. However, there
        are also many stutterers who, despite their disorder, have become famous. For instance,
        Winston Churchill had to rehearse all his public speeches to perfection and even practiced
        answers to possible questions and criticisms to avoid stuttering. Charles Darwin also
        stuttered; interestingly, his grandfather Erasmus Darwin suffered from the same condition,
        highlighting the fact that stuttering runs in families and is likely to have a genetic
        basis.
        The incidence of PDS is about 5%, and its recovery rate is up to about 80%, resulting in
        a prevalence of PDS in about 1% of the adult population. As recovery is considerably more
        frequent in girls than in boys, the male-to-female ratio increases during childhood and
        adolescence to reach three or four males to every one female in adulthood. It is not clear
        to what extent this recovery is spontaneous or induced by early speech therapy. Also, there
        is no good way of predicting whether an affected child will recover (Yairi and Ambrose
        1999).
        The presence of affected family members suggests a hereditary component. The concordance
        rate is about 70% for monozygotic twins (Andrews et al. 1983; Felsenfeld et al. 2000),
        about 30% for dizygotic twins (Andrews et al. 1983; Felsenfeld et al. 2000), and 18% for
        siblings of the same sex (Andrews et al. 1983). Given the high recovery rate, it may well
        be that the group abnormalities observed in adults reflects impaired recovery rather than
        the causes of stuttering (Andrews et al. 1983).
      
      
        Changing Theories
        Over the centuries, a variety of theories about the origin of stuttering and
        corresponding treatment approaches have been proposed. In ancient Greece, theories referred
        to dryness of the tongue. In the 19
        th century, abnormalities of the speech apparatus were thought to cause
        stuttering. Thus, treatment was based on extensive “plastic” surgery, often leading to
        mutilations and additional disabilities. Other treatment options were tongue-weights or
        mouth prostheses (Katz 1977) (Figure 2). In the 20th century, stuttering was primarily
        thought to be a psychogenic disorder. Consequently, psychoanalytical approaches and
        behavioral therapy were applied to solve possible neurotic conflicts (Plankers 1999).
        However, studies of personality traits and child–parent interactions did not detect
        psychological patterns consistently associated with stuttering (Andrews et al. 1983).
        Other theories regard stuttering as a learned behavior resulting from disadvantageous
        external, usually parental, reactions to normal childhood dysfluencies (Johnson 1955).
        While this model has failed to explain the core symptoms of stuttering (Zimmermann et al.
        1983), it may well explain secondary symptoms (Andrews et al. 1983), and guided early
        parental intervention may prevent persistence into adulthood (Onslow et al. 2001). The
        severity of PDS is clearly modulated by arousal, nervousness, and other factors (Andrews et
        al. 1983). This has led to a two-factor model of PDS. The first factor is believed to cause
        the disorder and is most likely a structural or functional central nervous system (CNS)
        abnormality, whereas the second factor reinforces the first one, especially through
        avoidance learning. However, one should be careful to call the latter factor “psychogenic”
        or “psychological,” because neuroscience has shown that learning is not simply
        “psychogenic” but leads to measurable changes in the brain (Kandel and O'Dell 1992).
        In some cases, arousal actually improves stuttering instead of making it worse.
        Consequently, some famous stutterers have “treated” their stuttering by putting themselves
        on the spot. Anecdotally, the American actor Bruce Willis, who began stuttering at the age
        of eight, joined a drama club in high school and his stuttering vanished in front of an
        audience.
      
      
        Is Stuttering a Sensory, Motor, or Cognitive Disorder?
        Stuttering subjects as a group differ from fluent control groups by showing, on average,
        slightly lower intelligence scores on both verbal and nonverbal tasks and by delays in
        speech development (Andrews et al. 1983; Paden et al. 1999). However, decreased
        intelligence scores need to be interpreted carefully, as stutterers show a schooling
        disadvantage of several months (Andrews et al. 1983). Associated symptoms comprise delays
        in tasks requiring a vocal response (Peters et al. 1989) and in complex bimanual timed
        tasks such as inserting a string in the eye of a needle (Vaughn and Webster 1989), whereas
        many other studies on sensory–motor reaction times yielded inconsistent results (Andrews et
        al. 1983). Alterations of auditory feedback (e.g., delayed auditory feedback,
        frequency-altered feedback), various forms of other auditory stimulation (e.g., chorus
        reading), and alteration of speech rhythm (e.g., syllable-timed speech) yield a prompt and
        marked reduction of stuttering frequency, which has raised suspicions of impaired auditory
        processing or rhythmic pacemaking in stuttering subjects (Lee 1951; Brady and Berson 1975;
        Hall and Jerger 1978; Salmelin et al. 1998). Other groups have also reported discoordinated
        and delayed onset of complex articulation patterns in stuttering subjects (Caruso et al.
        1988; van Lieshout et al. 1993). The assumption that stuttering might be a form of
        dystonia—involuntary muscle contractions produced by the CNS—specific to language
        production (Kiziltan and Akalin 1996) was not supported by a study on motor cortex
        excitability (Sommer et al. 2003).
        Neurochemistry, however, may link stuttering with disorders of a network of structures
        involved in the control of movement, the basal ganglia. An increase of the neurotransmitter
        dopamine has been associated with movement disorders such as Tourette syndrome (Comings et
        al. 1996; Abwender et al. 1998), which is a neurological disorder characterized by repeated
        and involuntary body movements and vocal sounds (motor and vocal tics). Accordingly, like
        Tourette syndrome, stuttering improves with antidopaminergic medication, e.g., neuroleptics
        such as haloperidol, risperidone, and olanzapine (Brady 1991; Lavid et al. 1999; Maguire et
        al. 2000), and anecdotal reports suggest that it is accentuated or appears under treatment
        with dopaminergic medication (Koller 1983; Anderson et al. 1999; Shahed and Jankovic 2001).
        Hence, a hyperactivity of the dopaminergic neurotransmitter system has been hypothesized to
        contribute to stuttering (Wu et al. 1995). Although dopamine antagonists have a positive
        effect on stuttering, they all have side effects that have prevented them from being a
        first line treatment of stuttering.
      
      
        Lessons from Imaging the Brain
        Given reports on acquired stuttering after brain trauma (Grant et al. 1999; Ciabarra et
        al. 2000), one might think that a lesion analysis (i.e., asking the question where do all
        lesions that lead to stuttering overlap) could help to find the location of an abnormality
        linked to stuttering. Unfortunately, lesions leading to stuttering are widespread and do
        not seem to follow an overlapping pattern. Even the contrary has been observed, a thalamic
        stroke after which stuttering was “cured” in a patient (Muroi et al. 1999).
        In fluent speakers, the left language-dominant brain hemisphere is most active during
        speech and language tasks. However, early studies on EEG lateralization already strongly
        suggested abnormal hemispheric dominance (Moore and Haynes 1980) in stutterers. With the
        advent of other noninvasive brain imaging techniques like positron emission tomography
        (PET) and functional magnetic resonance imaging (fMRI), it became possible to visualize
        brain activity of stutterers and compare these patterns to fluent controls. Following
        prominent theories that linked stuttering with an imbalance of hemispherical asymmetry
        (Travis 1978; Moore and Haynes 1980), an important PET study (Fox et al. 1996) reported
        increased activation in the right hemisphere in a language task in developmental
        stutterers. Another PET study (Braun et al. 1997) confirmed this result, but added an
        important detail to the previous study: Braun and colleagues found that activity in the
        left hemisphere was more active during the production of stuttered speech, whereas
        activation of the right hemisphere was more correlated with fluent speech. Thus, the
        authors concluded that the primary dysfunction is located in the left hemisphere and that
        the hyperactivation of the right hemisphere might not be the cause of stuttering, but
        rather a compensatory process. A similar compensatory process has been observed after
        stroke and aphasia, where an intact right hemisphere can at least partially compensate for
        a loss of function (Weiller et al. 1995). Right hemisphere hyperactivation during fluent
        speech has been more recently confirmed with fMRI (Neumann et al. 2003).
        PET and fMRI have high spatial resolution, but because they only indirectly index brain
        activity through blood flow, their temporal resolution is rather limited.
        Magnetoencephalography (MEG) is the method of choice to investigate fine-grained temporal
        sequence of brain activity. Consequently, MEG was used to investigate stutterers and fluent
        controls reading single words (Salmelin et al. 2000). Importantly, stutterers were reported
        to have read most single words fluently. Nevertheless, the data showed a clear-cut
        difference between stutterers and controls. Whereas fluent controls activated left frontal
        brain areas involved in language planning before central areas involved in speech
        execution, this pattern was absent, even reversed, in stutterers. This was the first study
        to directly show a neuronal correlate of a hypothesized speech timing disorder in
        stutterers (Van Riper 1982).
        Thus, functional neuroimaging studies have revealed two important facts: (i) in
        stutterers, the right hemisphere seems to be hyperactive, and (ii) a timing problem seems
        to exist between the left frontal and the left central cortex. The latter observation also
        fits various observations that have shown that stutterers have slight abnormalities in
        complex coordination tasks, suggesting that the underlying problem is located around motor
        and associated premotor brain areas.
        Are there structural abnormalities that parallel the functional abnormalities? The first
        anatomical study to investigate this question used high-resolution MR scans and found
        abnormalities of speech–language areas (Broca's and Wernicke's area) (Foundas et al. 2001).
        In addition, these researchers reported abnormalities in the gyrification pattern.
        Gyrification is a complex developmental procedure, and abnormalities in this process are an
        indicator of a developmental disorder.
        Another recent study investigated the hypothesis that impaired cortical connectivity
        might underlie timing disturbances between frontal and central brain regions observed in
        MEG studies (Figure 3). Using a new MRI technique, diffusion tensor imaging (DTI), that
        allows the assessment of white matter ultrastructure, investigators saw an area of
        decreased white matter tract coherence in the Rolandic operculum (Sommer et al. 2002). This
        structure is adjacent to the primary motor representation of tongue, larynx, and pharynx
        (Martin et al. 2001) and the inferior arcuate fascicle linking temporal and frontal
        language areas, which both form a temporofrontal language system involved in word
        perception and production (Price et al. 1996). It is thus conceivable that disturbed signal
        transmission through fibers passing the left Rolandic operculum impairs the fast
        sensorimotor integration necessary for fluent speech production. This theory also explains
        why the normal temporal pattern of activation between premotor and motor cortex is
        disturbed (Salmelin et al. 2000) and why, as a consequence, the right hemisphere language
        areas try to compensate for this deficit (Fox et al. 1996).
        These new data also provide a theory to explain the mechanism of common fluency-inducing
        maneuvers like chorus reading, singing, and metronome reading that reduce stuttering
        instantaneously. All these procedures involve an external signal (i.e., other readers in
        chorus reading, the music in singing, and the metronome itself). All these external signals
        feed into the “speech production system” through the auditory cortex. It is thus possible
        that this external trigger signal reaches speech-producing central brain areas by
        circumventing the frontocentral disconnection and is able to resynchronize frontocentral
        decorrelated activity. In simple terms, these external cues can be seen as an external
        “pacemaker.”
      
      
        Future Directions in Research
        There are numerous outstanding issues in stuttering. If structural changes in the brain
        cause PDS, the key question is when this lesion appears. Although symptoms are somewhat
        different, it would be interesting to find out to what extent transient stuttering (which
        occurs in 3%–5% in childhood) is linked to PDS. It is possible that all children who show
        signs of stuttering develop a structural abnormality during development, but this is
        transient in those who become fluent speakers. If this is the case, it is even more
        important that therapy starts as early as possible if it is to have most impact. This
        question can now be answered with current methodology, i.e., noninvasive brain imaging
        using MRI.
        Given that boys are about four times less likely to recover from stuttering than girls,
        it is tempting to speculate that all stutterers have a slight abnormality, but only those
        that can use the right hemisphere for language can develop into fluent speakers. Language
        lateralization is less pronounced in women (McGlone 1980) and might therefore be related to
        the fact that women show an overall lower incidence in PDS. Again, a developmental study
        comparing children who stutter with fluent controls and, most importantly, longitudinal
        studies on these children should be able to answer these questions.
        It is unlikely that stuttering is inherited in a simple fashion. Currently, a
        multifactorial model for genetic transmission is most likely. Moreover, it is unclear
        whether a certain genotype leads to stuttering or only represents a risk factor and that
        other environmental factors are necessary to develop PDS. Again, this question might be
        answered in the near future, as the National Institutes of Health has recently completed
        the data collection phase of a large stuttering sample for genetic linkage analysis.
      
    
  

  
    
      
        
        Creative human beings are the torch-bearers of civilization. How does their creativity
        arise? What causes some minds/brains to achieve awe-inspiring artistic or scientific
        achievements? We cannot help but be fascinated by the fact that Shakespeare—a merchant's
        son with “small Latin and less Greek”—could emerge from the “nowhere” of rural Stratford to
        create the richest literary treasure in the English language. We wonder how Michelangelo—a
        stonecutter's son who also came from a rural nowhere—found within himself the vision to see
        the shape of David in a block of discarded marble or the apolcalyptic fresco of 
        The Last Judgment on the wall of the Sistine Chapel. What genetic
        influences shaped their brains to create—and to create these very specific wondrous things?
        How did their environments promote or impede them? Would Michelangelo have been great
        without the patronage of the Medicis or the competitive edge induced by Leonardo? Great art
        and great science are indeed often forged in the smithy of pain—with the fire fueled by
        self-doubt, obsessive preoccupation, sorrow, depression, competition, or economic
        needs.
        
        The Midnight Disease: The Drive to Write, Writer's Block, and the Creative
        Brain by Alice Weaver Flaherty unites two intrinsically fascinating domains of
        knowledge—the workings of the brain and the nature of creativity. Its author, a neurologist
        who has also become a writer by virtue of having published her first nonacademic book,
        draws on her knowledge of neuroscience, her medical career as a clinician, and her
        experiences as a patient. Early in the book, she describes her own hospitalization for
        manic-depressive illness, a disclosure that implicitly places her in the pantheon of other
        artists who have suffered from serious mental illness and provides her with
        lustre-by-association. The result of all these juxtapositions is, however, a somewhat
        disconcerting blend of pop-science and pop-confessional genres. The author frequently talks
        to us in the first person, but one is not quite sure which person (the neuroscientist, the
        doctor, or the patient) is actually speaking. In other words, this book has a jarring lack
        of a strong single voice, despite a knack for often finding a fine turn-of-phrase or a
        clever word choice.
        Given that the book topic is promising and that the author can often write very well, it
        is dismaying that this book is not better than it is. It is written for the intelligent lay
        public, many of whom avidly collect and read “brain books” to expand their minds. Most
        painful is the fact that this book is filled with factual errors, glib and misleading
        generalizations, and careless misstatements. Perhaps most shocking and most erroneous, we
        are told (by a neurologist!) that “The tips of the temporal lobe can be lopped off without
        much changing a person's behavior.” HM, the most famous patient to receive bilateral
        temporal lobectomy, remains frozen in a past linked to a never-changing present because he
        lost the capacity to retain new memories. Temporal lobe syndromes are discussed more
        accurately later in the book, but that is a weak excuse for this early error.
        We are also told that “manic depression is a genetically transmitted syndrome” (when, in
        fact, no replicable genetic loci have yet been identified), that “a very high proportion of
        manic depressives become writers” (the lifetime prevalence rate of bipolar disorder is
        approximately 1%, and only a tiny proportion of that 1% are writers), and that
        “electrophysiology, because it is dangerous, is rarely performed” (electrophysiology
        tools—e.g., the study of evoked potentials or electroencephalograms—are noninvasive and
        frequently used; recordings of the activity of individual neurons with electrodes placed in
        the gray matter are indeed rare, but nothing from the context suggests that this particular
        type of electrophysiology is being discussed). There are many more such careless
        misstatements. The intelligent lay reader deserves better than this.
        The book raises and addresses a variety of interesting questions that have intrigued
        many thoughtful people for more than two millennia. What is the nature of creativity? What
        is the difference between skill and creativity? What is the relation between mental illness
        and creativity? Is creativity inhibited when mental illnesses are treated? What is the
        relation between mind and brain? The book also addresses some unique and interesting twists
        on these questions. Its focus is the domain of writing, drawing from the author's own
        experience of a compulsion to write, or hypergraphia, following a pyschic break. What is
        the relationship between hypergraphia and the brain? Between writer's block and the brain?
        Are these problems always pathological, or do they sometimes enhance creativity? Does that
        college student who can't finish a term paper have a “disease”? Can “mind-expanding” drugs
        that affect the brain enhance creativity?
        In short, 
        The Midnight Disease raises many important questions, but fails to
        address them completely and accurately. There is much more to learn, and much more to say,
        about the nature of creativity, its origins in the mind/brain and in the human genome, and
        its boundaries with health and disease.
      
    
  

  
    
      
        
        The AIDS crisis has brought to public notice what has always been generally true—that
        the existing business model for drug development leads to high prices and unequal access.
        There is now widespread dissatisfaction with drug prices in both the developed (Families
        USA 2003) and developing world (Correa 2000). Governments and health insurers are finding
        ways to deny access to the newest and priciest products. In the United States and other
        countries without a universal public health system, the uninsured simply cannot afford the
        newest medicines. In developing countries, life-saving medicines are priced beyond the
        reach of most people, a morally offensive outcome (TrueVisionTV 2003). Huge publicity
        surrounds negotiated price reductions for specific drugs in specific developing countries,
        yet the effect on the overall access problem is tiny.
        Today's high drug prices are a direct consequence of a business model that uses a single
        payment to cover both the cost of manufacture of a drug and the cost of the research and
        development (R&D) carried out by manufacturers to discover it. A 20-year patent-based
        marketing monopoly is then granted to the drug's developers to prevent their prices being
        undercut by ‘generic’ copies produced by manufacturers who do not have R&D costs to
        recover. Preventing such ‘free riding’ on R&D has become a global trade issue at the
        World Trade Organisation (WTO) (Drahos and Braithwaite 2002). The implementation of the
        TRIPS (Trade-Related Aspects of Intellectual Property Rights) agreement and a growing
        number of regional and bilateral agreements on intellectual property require most countries
        to implement tough patent systems that discourage or eliminate competition from
        manufacturers of generic medicines (Box 1).
        Unfortunately, monopoly-based business models have unpleasant side effects. Since the
        primary responsibility of any company is to maximise return on investment, it is
        unsurprising that there is pressure on pharmaceutical companies to set drug prices to
        whatever level gives the highest return, excluding those individuals who cannot afford to
        pay, rather than maximising the number of patients treated. There is also pressure to
        misuse the power given by patents, using them as anticompetitive weapons to block
        innovation and extend marketing monopolies. And there are growing fears that the huge
        growth in the use of patents is in itself starting to inhibit research (CIPR 2002;
        Anonymous 2003; Royal Society 2003). Something that is less well recognised is that this
        system is an enormously inefficient way of purchasing R&D. There is a considerable lack
        of transparency in pharmaceutical R&D investment, but the available data indicate that
        only about 10% of drug sales go towards R&D on new products. Only about one-quarter of
        new drug approvals are rated by the United States Food and Drug Administration (FDA) to
        have therapeutic benefit over existing treatments (NIHCM 2002; see Figure 1). Measured by
        investment, only about one-fifth of the 10% is invested in innovative products (Love
        2003a). There is also very little research for diseases that primarily afflict the poor
        (Trouiller et al. 2001; WHO 2003).
        Propping up the present structure for financing R&D (Figure 2A) is the widely held
        belief that the private sector plays a key role in the development of new medicines and
        that it is necessary to grant patents to incentivise private-sector financing. If this were
        true, it would make sense to tolerate all sorts of bad outcomes, because the fruits of
        R&D eventually benefit everyone. But granting a 20-year marketing monopoly on a
        patented invention is only one way to finance R&D, and the shortcomings of the present
        system are increasingly hard to ignore. Suggestions for alternatives are beginning to come
        from many quarters (Baker and Chatani 2002; CGSD 2003; Hubbard and Love 2003; Weisbrod
        2003). In this essay, we present practical proposals to modify trade rules based solely on
        intellectual property so that alternative policy instruments can be used to encourage
        innovation.
      
      
        A New Trade Framework
        Analysis of worldwide drug expenditure shows that spending varies, but is close to 1% of
        the gross domestic product (GDP) in most developed and developing countries (Love 2003b).
        Assuming that about a tenth of the revenue from the sale of drugs is ploughed back into
        R&D on new products, that means that countries already indirectly contribute about 0.1%
        GDP to support this. This contribution is enforced by trade agreements, which require the
        granting of patents to prevent ‘free riding’ via the purchase of generic drugs (see Box 1).
        Suppose the World Health Organisation (WHO) developed an R&D contribution ‘norm’ based
        upon this or a more appropriate figure and that there was international agreement that
        countries evaluated as meeting this norm would no longer be regarded as ‘free riding’.
        Trade rules could then be modified to allow countries to meet this norm 
        by any means , not just by the implementation of strict TRIPS
        intellectual property rules, as at present.
        Countries that met the norm would then be free to decide whether they wanted to follow a
        strictly patent-based system as at present, with high drug prices for 20 years, or
        experiment with new models based on the creation of separate competitive markets for sales
        and R&D (Figure 2B). Countries adopting the latter system would remove patents on final
        drug compounds, placing them in the public domain. This would allow them to become a freely
        traded commodity, creating a competitive manufacture and sales market with low generic
        prices. At the same time, in order to meet the required R&D contribution norm, they
        would have to create an efficient R&D virtual market alongside. However, the costs of
        this would be more than offset by the reduction in drugs prices, making substantial savings
        for that country overall.
      
      
        Business Models for an Effective Virtual R&D Market
        The existing system (Figure 2A), despite its failings, does lead to the development of
        new drugs. The challenge in creating a virtual R&D market is to find viable business
        models for successful drug development in the absence of marketing monopoly incentives.
        One obvious approach is direct funding of drug development. For example, the National
        Institutes of Health (NIH), the national agency in the United States, already spends $27
        billion per year on research, a substantial amount of which is directed towards drug
        development, including clinical trials. The NIH already has a track record in developing
        important drugs for severe illnesses, such as cancer or AIDS, showing that this is a viable
        model. It is also widely recognised that much of the research carried out across the world
        by similar agencies underpins the existing commercial research that leads to new drugs.
        Governments could expand direct funding for drug development, either through the
        existing structures in academia or through funding R&D arms of existing companies to
        carry out specific drug R&D. Such directed drug development funding could be similar to
        existing nonprofit development projects, such as those currently resourced to address
        treatments for neglected diseases like malaria and tuberculosis (TB). Examples of such
        projects are the Medicines for Malaria Venture (www.mmv.org), the Global Alliance for TB
        Drug Development (www.tballiance.org), the International AIDS Vaccine Initiative
        (www.iavi.org), the Drugs for Neglected Diseases Initiative (Butler 2003b) (www.dndi.org),
        and the Institute for One World Health (www.oneworldhealth.org).
        Many are doubtful that increased direct funding would generate sufficient incentives or
        be managed efficiently enough. An alternative market-based approach is one in which R&D
        organisations compete for rewards for specific R&D output, referred to by economists as
        a prize model (Wright 1983; Kremer 1998; Shavell and van Ypersele 2001). In a simple
        formulation, governments would place large sums into a fund that would be allocated every
        year to firms that bring new products to market. This could work with or without patents.
        If products were protected by patents or other intellectual property claims, the government
        could grant compulsory licenses (a procedure allowed by trade agreements to override
        monopoly rights on a patent, in return for compensation to rights owners; see Box 1) and
        permit rapid introduction of generic competition. The reward system could be a lump-sum
        payment, eliminating any incentive to continue to market the product, or a long-term payout
        structure, which would depend upon evidence of both usage and efficacy. Prize systems could
        be designed to be fairly similar to the current system, with big payoffs for successful
        entrepreneurs, but even with this approach, there would be huge opportunities to improve
        welfare. The reward system could be more rational than the existing system, allocating
        greater rewards for innovative products and less for ‘me too’ products that do not work
        better than existing products. Premiums could be given for therapies that address treatment
        gaps or for inventions that pave the way to new classes of drugs.
        Organisations competing for prizes might be expected to behave secretly to ensure that
        they are the ones to obtain ‘credit’ for the fruits of their work. However, progress in
        research is also driven by free exchange of information. It may be possible to design
        models that both reward R&D outputs and at the same time encourage complete and
        continuous openness with intermediate research outputs. There are now a number of examples
        of open collaborative public goods models (Cukier 2003), such as those used for the Human
        Genome Project. The proponents of such models point to the success of GNU/Linux in the
        software field as evidence that major projects can be undertaken with radically different
        business models. One of the benefits of complete openness is that it allows independent and
        open evaluation of R&D outputs, which helps in the allocation of ‘credit’ whether in
        the form or prizes or new research grants. The open-access publishing movement (Brown et
        al. 2003) has the potential to help in this process by allowing independent analysis of
        published science, which will help research funding agencies measure research outputs.
      
      
        Competitive Intermediators
        An R&D contribution norm, established by treaty, would ensure that the amount of
        money being spent on R&D is maintained. However, new mechanisms would be needed to
        collect the money to finance the R&D, as it would no longer come via drug sales. This
        could be via general taxation, although in countries with a private health insurance system
        this may be anathema. Many will also worry that a centralised national drug development
        agency taking decisions on R&D priorities and allocation of funds (via prizes or grants
        as discussed above) could easily become bureaucratic and inefficient.
        As a possible alternative, we propose a competitive financing scheme that would work
        through R&D investment intermediators. These R&D funds would be licensed and
        regulated (like pension funds). Their role would be to manage R&D assets on behalf of
        consumers. Individuals (or employers) would be required to make minimum contributions into
        R&D funds, much as there are mandatory contributions to social security or health
        insurance or to pension funds. Government would set the required contribution, but the
        individual (or employer) would be free to choose the particular intermediator that received
        their contributions. Intermediators would compete to attract funds to invest in R&D on
        the basis of their prowess for drug development and upon their priorities. Different
        business models for financing R&D could be tested in such a market, with intermediators
        experimenting with prize systems, direct investments in profit or nonprofit entities, open
        collaborative public good models, or other approaches.
      
      
        A Change for the Common Good
        We believe the economics of a change in the paradigm for funding R&D are highly
        favourable. Taken together, the two core steps of changing the trade framework and moving
        away from marketing monopolies can change the world in a positive way. We can raise global
        R&D levels as a matter of policy and ensure that resources flow into the areas of the
        greatest need, and we can do so knowing that the poor and the rich will have access to new
        inventions at marginal cost. Policy-makers will be weaned from their current unhealthy
        addiction to ever-higher levels of intellectual property rights as the only instrument to
        raise R&D levels, a path that has increasingly reached diminishing returns or become
        counterproductive. With new instruments to address the overall levels of R&D
        investment, policy-makers can more constructively address the well-known inefficiencies in
        the patent system without the fear that global R&D levels will suffer and explore
        alternative models (Butler 2003a). At the same time, the system of prescribing medicines
        will be transformed by a substantial reduction in the distorting influences of the current
        multibillion-dollar industry of marketing medicines to doctors and (increasingly) directly
        to the public. Similarly, without marketing monopolies to protect, there will be far less
        spent to influence the governments that set the rules that regulate such monopolies. If
        implemented worldwide, one of our most vexing ethical dilemmas can be resolved in a manner
        that actually promotes the Doha Declaration on TRIPS and Public Health mandate to encourage
        access to medicine for all.
      
      
        
      
    
  

  
    
      
        
        By next summer, more than 40% of 
        Streptococcus pneumoniae strains in the United States will
        resist both penicillin and erythromycin, according to a recent prediction from the Harvard
        School of Public Health. The forecast, based on mathematical modeling, was published in the
        spring of 2003. It's too early to tell whether that prediction is precisely on track,
        according to the senior author on that paper, Marc Lipsitch. But no one doubts that
        multidrug resistance in this common bug—responsible for diseases that range from sinus
        trouble and ear infections to meningitis and pneumonia—is speeding up.
        It is the certain fate of all antibacterials to be fought off eventually by the
        pathogens they target. The fact that the process is accelerating has been alarming public
        health officials for some time, especially in the United States. We need new ways to defeat
        disease, and we will need them forever.
      
      
        Tried and True—and Tired?
        Antibiotics have traditionally been plucked from nature's battleground. For billions of
        years, tiny organisms have engaged in an arms race, hurling toxic molecules at each other
        in the struggle to prosper. Nearly all of today's antibiotics are versions of weapons long
        wielded by microbes and fungi. Chemical synthesis of entirely human-created antibiotics has
        so far yielded only fluoroquinolones, a group of broad-spectrum antibiotics that includes
        Cipro, which became famously scarce during the 2001 anthrax scare, and linezolid
        (trade-named Zyvox), which is effective against some resistant strains of 
        Staphylococcus , 
        Streptococcus , and 
        Enterococcus .
        The usual way to find a new antibiotic has been laborious screening of immense libraries
        of compounds, natural and otherwise. Some argue that screening chemical libraries is
        approaching a deadend. There may be diminishing returns from screening, but it's not quite
        dead yet: in October, researchers at the University of Wisconsin at Madison reported a new
        class of bacterial RNA polymerase inhibitors with antibiotic potential. They were found by
        screening for molecules that prevent 
        Escherichia coli from transcribing RNA.
        Christopher T. Walsh of Harvard Medical School says screening's problem may be simply
        that libraries aren't good enough. Marine organisms have not been studied well, he points
        out, and 90% of organisms in the biosphere can't be cultured in standard ways. He says,
        “We're missing 90% of them every time we go and look in nature.”
        Walsh is doing his bit to create new libraries. He and his colleagues have recently
        employed combinatorial biosynthesis to learn how to use part of the machinery for
        assembling cyclic peptide antibiotics to control their architecture. The result was a small
        library of natural product analogs, some of which have improved antibiotic activity against
        common bacterial pathogens. “There are dozens of such enzymatic domains that in principle
        one could clone, express, and test with other substrates. I view that as the kind of thing
        we should do,” he says. For example, Walsh suggests, it is a reasonable approach to
        second-generation improvement of daptomycin, the antibiotic most recently approved for sale
        in the United States.
      
      
        Improving on Nature
        Walsh collaborates with Chaitan Khosla of Stanford University on finding ways to make
        existing antibiotics better. They are studying biosynthesis of rifamycin, an antibiotic
        that is increasingly less effective against its prime target, tuberculosis (TB) (see Figure
        1). “In the course of learning about that pathway, we've learned a few interesting things
        lately about how that molecule is initiated, and we're trying to apply it in other
        contexts, especially in the context of erythromycin biosynthesis,” Khosla says. The idea
        would be to make a molecule that might be more effective against bacteria that are becoming
        resistant to rifamycin—and are already naturally resistant to molecules like
        erythromycin.
        “Basically, what we do is to try and figure out new ways to hijack the biosynthesis of
        antibiotics in nature so as to modify their structures with the goal of improving them,”
        Khosla explains. He works with an important class of natural antibiotics called polyketides
        that have generated dozens of drugs, including erythromycin.
        Polyketides are secondary metabolites (which give their producers a competitive
        advantage in their environment) produced mostly by bacteria and fungi and made by a complex
        and structurally diverse family of enzymes called polyketide synthases (see the primer by
        David Hopwood in this issue of 
        PLoS Biology ). Among them are the anthracyclines, a group of anticancer
        drugs and antibacterials that includes tetracycline. In this issue of 
        PloS Biology , Khosla and his colleagues report that they can make
        selective positional modifications in existing anthracycline antibiotics by starting in a
        different way with a different starting molecule. The molecule came from a natural
        anthracycline antibiotic, an estrogen receptor antagonist called R1128. R1128 is made via
        two modules of enzymes that work sequentially; the first module starts the process, and the
        second completes it. This division of labor permitted the researchers to tack the first
        R1128 module onto two other enzyme systems, thus engineering completely new anthracyclines.
        Some were more active in two types of assays than the natural parent molecule. “One setting
        was an assay on an estrogen-sensitive cancer cell line. Another setting was an assay to
        probe activity of an enzyme that's of particular interest in Type 2 diabetes, called
        glucose-6-phosphate translocase.” The work also revealed fundamental mechanistic features
        of the polyketide synthases, Khosla says.
        The researchers didn't study the new anthracyclines' effects on bacteria, but Khosla
        notes that the general principle should apply to other classes of compounds, although the
        details of how it's implemented will vary from system to system. He says, “The upshot of
        this paper is that it is now possible to modify a particular methyl group in just about any
        anthracycline antibiotic.”
      
      
        Finding New Targets
        Instead of searching for new antibiotics by modifying existing ones, some researchers
        are trying something completely different—first finding the most vulnerable targets in a
        bacterium and then designing something that hits one or more of them hard. “You have to
        understand a helluva lot more about how these little cells work. In fact, we think we
        understand a lot, but I think we can understand almost everything now that we have all the
        genomes,” says Lucy Shapiro of Stanford University School of Medicine. While having full
        genome sequences—more than 100 microbe sequences have been completed—is essential, Shapiro
        believes that knocking outs genes galore to find out which ones are necessary and going
        after them all is not a sensible strategy. She observes, “People have been doing that for a
        while with absolutely no success. That's really going after the problem with a Howitzer
        instead of with an intelligent approach.”
        So instead of screening libraries of existing compounds, Shapiro prefers using
        structural information about drug targets or their natural ligands to create new drugs, an
        approach known as rational drug design. And instead of looking at all essential genes in a
        bacterium and choosing one to target, she and her colleagues look at genetic circuitry that
        controls the cell cycle, the pathway that coordinates cell growth and differentiation. They
        have identified key control points, or nodes, in the circuitry for their favorite study
        subject, 
        Caulobacter crescentus . Thus, they have found critical genes
        encoding proteins that control several critical functions in the cell. Their first
        candidate was an essential enzyme, a methyltransferase called CcrM, that prevents a
        particular piece of DNA from being expressed in a cell by tagging it with a methyl
        group.
        Antibiotic discovery is all chemistry, Shapiro says, which is why she joined with
        biochemist Stephen J. Benkovic of Pennsylvania State University. They didn't know the
        structure of CcrM, Benkovic explains, but the literature about other methyltransferases
        suggested that the adenine molecule, which is the substrate for CcrM within DNA, binds to a
        specific region of the enzyme.
        The researchers designed adenine-like molecules that would bind to CcrM and then
        developed inhibitors. Benkovic says, “We already knew what kind of structure we wanted, and
        we simply fine-tuned it.” They worked their way through 1,000 inhibitor candidates, ending
        up with a small subset—no more than about 20—that not only inhibited CcrM, but also killed 
        Caulobacter very quickly.
        And not only inoffensive 
        Caulobacter . The compounds knock out other gram-negative
        bacteria, such as the pathogens 
        Brucella abortus and 
        Francisella tularensis . Some even killed off anthrax, a big
        surprise because it is gram-positive and so has much thicker cell walls than gram-negative
        bacteria. The researchers undertook an exhaustive series of experiments to identify which
        gram-positive bacteria would be affected by which compounds. The list of sensitive
        pathogens now includes multidrug-resistant 
        Streptococcus , 
        Staphylococcus , and 
        Mycobacterium tuberculosis .
        More recently, Shapiro reports, they have demonstrated efficacy against rats infected
        with anthrax or multidrug-resistant 
        Staph , although the compounds save only about 60% of the rats
        at present. She notes, “So we have a long way to go. But this has proven that if you go
        after something using some rational approach instead of hit-and-miss, you'll probably have
        more success than by the other method.”
        Benkovic points out that theirs is an entirely new class of compounds, small molecular
        weight compounds that can be made in a few steps. He says, “They don't look like the normal
        antibiotic, so that's why I think they're fairly unique.” The basic research was done under
        a grant from the Defense Advanced Research Projects Agency (DARPA), the United States
        Department of Defense's (DOD) central research and development organization, and once the
        researchers realized they wanted to develop drugs against three agents that have been
        considered bioterrorism threats — 
        Brucella , tularensis, and anthrax — they established a separate
        operation, Anacor Pharmaceuticals, which is developing them with DOD funding and without
        Shapiro. In her Stanford lab, she continues her fundamental research to define the complete
        genetic circuitry of 
        Caulobacter , hoping to identify additional nodes in the
        circuit. She says, “I am not doing it to develop antibiotics; that's what comes out of the
        work. My goal is to understand how the cell works. I think a lot of studies in pathogenesis
        should not be just to understand pathogenic organisms, but to understand the complete
        network of regulatory mechanisms that controls the bacterial cell.”
      
      
        Phage Therapy
        The most radical approach to new antibiotics may be the resurrection of an old idea:
        bacteriophage therapy (see Figure 2). Late in the 19th century, a researcher noticed that
        water from some of India's sacred rivers combated cholera. Some years later, the active
        agents were identified as viruses that infected bacteria. Such viruses are called
        bacteriophage, or phage for short. There were reports of phage success against dysentery,
        typhoid, and plague, and bacteriophage therapy had a brief heyday, especially in the 1920s.
        Results on other diseases were mixed, and with the appearance of antibiotics, phage therapy
        became unfashionable in the United States, although it has continued in Russia and Eastern
        Europe.
        Phage were the model organisms of choice for genetics research in the 1930s and 1940s,
        but became less fashionable as research tools when investigators moved on to eukaryotes. A
        few held on, like Ry Young of Texas A&M University, who has made phage-induced cell
        lysis his life's work. “The cell is basically genetically dead as soon as the phage goes in
        there, but it will keep living as sort of an infected zombie for as long as the phage wants
        it to, with virus particles accumulating inside the cell,” he explains. “Only when the
        phage is ready and has decided that it's the right time will it pull the trigger. And the
        cell blows up.” The freed phage then spew forth to infect new cells.
        Antibiotic resistance has led to new interest in phage therapy by several small biotech
        companies. Young continues basic research at Texas A&M, but has also joined one of
        them, GangaGen, providing bacteriophage expertise to its labs.
        Phage do kill pathogenic bacteria effectively, and they do it without penetrating human
        cells, which they can't even recognize. So what is keeping phage therapy out of the clinic?
        Problems that some doubt can be overcome.
        Because bacteria develop resistance to phage rapidly, phage therapy companies will need
        to direct cocktails against a single pathogen, according to Vincent Fischetti at The
        Rockefeller University. Phage are also antigenic, and the antibodies they stimulate will
        neutralize their effects during subsequent treatment, he says. But the chief problem
        appears to be regulatory—regulatory in the political, rather than the genetic, sense. When
        bacteriophage package their DNA, they occasionally include varying amounts of their hosts'
        DNA, too. This miscellany, Fischetti points out, is likely to make the Food and Drug
        Administration unhappy. “Phage normally are very fragile, their tails break, so lot-to-lot
        homogeneity could be a problem too,” he adds. “So even though it will work, I think they'll
        have an uphill battle.” Phage may well enter agricultural or veterinary use, he predicts,
        but are probably not going to be available to patients in the United States any time
        soon.
        Fischetti chose a different approach to phage therapy. It does not rely on phage
        themselves, but on enzymes that phage produce to smash their way out of their host bacteria
        so they can infect new hosts. He and his colleagues employ these enzymes externally to kill
        bacteria. He reports, “We now have enzymes that will kill 
        Strep pyogenes , pneumococci, 
        Strep pneumoniae , 
        Bacillus anthracis , 
        Enterococcus faecalis , and group B 
        Strep . The beauty of these enzymes is that they are targeted
        killing. You only kill the organism you intend to kill, without destroying or affecting the
        surrounding organisms that are necessary for health.”
        The enzymes can be loaded into a nasal spray that wipes out pathogens such as 
        Pneumococcus , 
        Staphylococcus , and group A 
        Strep on contact with mucous membranes. The strategy might
        prevent bacterial infections from spreading in close quarters like hospitals, nursing
        homes, and daycare centers. Fischetti says, “Clinical trials would tell us how often we had
        to treat, but more important, we'd have a reagent that could treat people who walk out the
        door of the hospital to eliminate or reduce the transmission of resistant organisms into
        the community. We don't have that capability right now.”
        Fischetti and his colleagues have moved on to using the enzymes systemically to wipe out
        
        Bacillus anthracis spores, preventing them from germinating and
        seething through the bloodstream, producing deadly toxins. An IV drip would be started
        after exposure to the spores. The method, Fischetti reports, is already successful in mice;
        clinical trials will determine how long treatment must be continued, perhaps a week or so.
        They have also eliminated septicemia from pneumocci with the same intravenous method.
        Up to now the enzymes must make contact with bacteria to kill, but Fischetti is hoping
        that a new generation of engineered enzymes will be able to kill pathogens inside cells
        too. A second disadvantage is that they are effective only against gram-positive bacteria,
        although that group includes many vicious pathogens.
        But phage enzymes seem to offer one very big advantage: resistance to them has yet to
        develop. Fischetti says, “We've tried very hard to identify resistant bacteria, but so far
        we haven't found resistant organisms in all three of the enzymes we're working with. It
        appears to be a very rare event, much rarer than resistance to antibiotics.” Fischetti
        cautions against expecting that gladsome state to last forever, but he points out that even
        if widespread resistance takes the same 40 or 50 years that antibiotics required to become
        significantly resistant, phage enzymes could buy researchers decades for inventing other
        approaches.
      
      
        Antibiotics in the 21st Century
        There is no shortage of ideas for unearthing new antibiotic candidates. Why are they so
        slow to enter medical practice? The bottleneck, researchers agree, lies in the development
        process of turning them into effective therapies. Several researchers blame the big
        pharmaceutical companies that got so big by leading the way to new drugs for battling
        infectious disease, but in recent years have dropped out. Fischetti complains, “These are
        the big companies that have the money to develop antiinfectives, but they leave it to small
        biotech companies, and it's not going to happen as rapidly as it should. I think it's
        really unconscionable for these big companies to drop the ball because it's not going to be
        a billion-dollar market for them and that's what they're looking for.”
        Half a billion at least, says Francis Tally, a big pharmaceuticals veteran who is now
        chief scientific officer at Cubist Pharmaceuticals, a biotech company located in Lexington,
        Massachusetts. According to Tally, Cubist produced daptomycin, approved in September 2003,
        by licensing it from Eli Lilly, which shelved the new compound after concluding its
        potential market was only $250 million.
        But, Tally argues, the size of the market is not the only barrier to new antibiotics.
        Combinatorial chemistry and the genomics revolution have simply not delivered on their
        early promise. “The pipeline is very dry,” he says. “There's been a real lag at the basic
        research level.”
        “Antibiotic discovery is hard,” Shapiro says. “It's a huge long process to get a decent
        antibiotic.” Walsh agrees. “It's easier to find inhibitors of particular enzymes for
        particular processes—and a very long road to convert that into something for
        development.”
        In the meantime, there is a rising clamor to slow down the rate at which bacteria
        develop resistance. Doctors are exhorted to cut back on prescribing antibiotics and decline
        to prescribe for viral diseases, which antibiotics can't combat, even when their patients
        badger them.
        But even if antibiotic consumption slowed, we will still need new antibiotics. “I always
        say it's not a matter of if, it's only a matter of when,” says Walsh. “There will always be
        a need for new antibiotics because the clock starts ticking on the useful lifetime of any
        antibiotic once you start to use it. That cannot be argued.”
      
    
  

  
    
      
        
        From Bob Clark's snug office in Boise, Idaho, where he manages the United States
        government's Joint Fire Science Program (JFSP), he figures his computer provides fingertip
        reach to just about everybody who's anybody in wildfire research. This points to a primary
        need of nations worldwide in combating the scourge of recurrent wildfires: tools and
        technology suited to the job. It's no small order in places as economically, socially, and
        ecologically varied as, say, Brazil, South Africa, Australia, Indonesia, and the United
        States, which are among the countries where wildfire creates the greatest havoc.
        More than 750,000 acres (303,500 hectares) were burned in southern California alone
        during last year's wildfires. The 2000 season was one of the country's worst on record,
        destroying 8.4 million acres (3.4 million hectares), more than double the decade's 10-year
        average. Australia's summer months around the turn of 2002–2003 brought perhaps the worst
        drought in a century to the populous southeast and the biggest fire season for two decades.
        Mountain forests were extensively burned and more than 500 houses were lost. In 2002,
        Brazil suffered 217,000 wildfires, a number that is almost certainly too low because remote
        imaging cannot detect many fires under the forest canopy. In Indonesia, wildfires that
        burned for months during 1997–1998 were later estimated to have released the equivalent to
        13%–40% of annual global carbon emissions from fossil fuels, inflicting smoke-related
        ailments on thousands.
        Where wildfire is concerned, the many differences between such countries can perhaps be
        pinned down to two essentials. The first is whether a blaze occurs in temperate or tropical
        forest, and the second is whether the nation is developed or developing.
        “The science can be rock solid, but it can only go so far before social, economic, and
        political pressures take over,” Clark says. “That's what a forest service manager's job is,
        picking the best option based on all those considerations.”
        Unfortunately, having science-based options that are applicable to local conditions is
        largely a luxury for developed countries. Managers there can choose to let a fire burn
        under hopefully contained conditions, a policy known in the United States as “wildland fire
        use.” They can set experimental crown fires to study their effects, as was done recently
        in"journal" Canada (Figure 1). And they can take preemptive measures, such as reducing fuel
        in the forest to lower fire hazard.
        The two main fuel-reduction methods are mechanical removal of combustible materials and
        controlled or “prescribed” burning (Figure 2). During Bill Clinton's administration,
        prescribed burns were encouraged in protected areas, but thinning was allowed only for
        trees with trunks of nine inches (22.8 cm) in diameter or less. Under George W. Bush,
        prescribed burning remains a choice, but the United States Department of Agriculture's
        (USDA) Forest Service policy is much more focused on mechanical means. The argument runs
        that there's been too much concern about removing trees, when what counts most is the
        enhanced fire-resistance of the thinned habitat.
        Fire hazard reduction methods must be tailored to an understanding of fuel
        characteristics in a given area, says David Peterson of the Forest Service's Pacific
        Wildland Fire Sciences Laboratory in Seattle, Washington. “There's no uniform way of doing
        it, partly because, as scientists, we haven't given the management folks any quantitative
        guidelines.” Working with other ecologists, social scientists, and economists, he's
        currently producing just such guidelines for the dry interior forests of the Pacific
        Northwest. “One thing we don't want to do is take choices out of the hands of field
        managers working at the local level.”
      
      
        Forecasting Tools: Models and Simulations
        For those choices to be meaningful, managers need reliable information on the risk of
        wildfire outbreaks and on the future behavior of existing fires. This requires models and
        simulations that incorporate climatic conditions, particularly wind (Figure 3). At the
        Forest Service's Fire Sciences Laboratory in Missoula, Montana, researchers have created a
        “gridded wind” tool based on the engineering discipline of computational fluid dynamics.
        The program maps wind speed and direction using a digital elevation model, which is a grid
        of elevation points every 30–100 feet (9–30.5 meters) over a terrain 10–40 square miles
        (25.9–103.6 square kilometers) in size. This map forms the floor of a box extending up to
        five miles (eight kilometers) high, which is subdivided into a million or more cubes. Wind
        flow from either real observations or estimates can be entered into the software, and the
        layer of cubes nearest the grid floor is used to create surface wind maps at resolutions of
        every 100 meters (109 yards) or less. In contrast, the usual resolution of weather
        forecasts is 12 kilometers (7.5 miles), down to 4 kilometers (2.5 miles) in some urban
        areas.
        “Two or three years ago, we couldn't have done this simulation on a single-processor
        laptop,” says one of its developers, physical engineer Bret Butler. “It would have taken
        two or three days. Now we can do it in a matter of hours.”
        The ability of these maps to show varying wind flow in valleys, at midslope and on
        ridgetops, is just the beginning. The next step is to feed these data into models that
        predict wildfire spread. Butler and colleagues have coupled their gridded wind technology
        to a fire growth model and tested it against the actual spread of several wildfires,
        including in Southern California last summer. Maps of actual and predicted surface winds
        showed strong similarities, encouraging Butler to foresee an ideal scenario in which fire
        fighting teams enter wind flow data online or by telephone to a central base where gridded
        wind maps and fire growth simulations are generated within hours, before operational
        decisions are made.
        Yet he admits that challenges remain, including the current inability of fire behavior
        simulation to account for diurnal winds in addition to cold front-driven flow. In
        mountainous terrain, for example, winds often move up-canyon in the morning and down-canyon
        in the evening. Moreover, the effect of vegetation on wind is not yet included in such
        models.
        Those issues and others are being addressed by researchers working on improvements to
        the regional weather forecasts of so-called mesoscale models. At the Forest Service's Rocky
        Mountain Research Center in Fort Collins, Colorado, meteorologist Karl Zeller and
        colleagues are contributing calculations of biological processes to mesoscale weather
        models. Their algorithms not only can account for diurnal winds but can predict the effects
        on local weather when vegetation takes in carbon dioxide and releases water vapor. This
        process can produce different fluxes of carbon dioxide drawn into the canopy and water
        vapor coming out, depending largely on the type of vegetation and its canopy density.
        Zeller's group has analyzed current mesoscale forecasts in the Rocky Mountains and found
        that in the daytime, they often are too hot in the high country and too cold in the plains.
        Water vapor estimates are too low in the mountains and too high in the plains, which Zeller
        thinks is because the models feed off soil moisture estimates, not off vegetation. In
        coupling his team's new biophysical interface to gridded wind and mesoscale forecast
        models, Zeller says “point forecasts” are being developed that can focus on a prescribed
        burn area or even a single house.
      
      
        Wildfire and Species Diversity
        Fitting the appropriate mix of strategies to a given situation is an issue that has also
        received close attention in Australia. After the bushfires of 2002–2003, media commentators
        called for increased “hazard reduction burning” in national parks, prompting ecologists
        around the country to distribute a joint statement declaring that such a strategy would not
        further reduce bushfire risk, but would actually threaten biodiversity. Australian species
        are often well-adapted to fire, and researchers have learned that different fire
        regimes—meaning the type of fire, its intensity, severity, extent, season, and
        frequency—favor different species (Box 1). In the southeast of Australia, prescribed burns
        of high frequency and low intensity can alter the habitat in ways that therefore threaten
        survival of numerous plant and animal species.
        “A generic problem or conundrum seems to be that species which do not prosper under
        relatively frequent fires can be found in most fire-prone environments,” notes Ross
        Bradstock, principal research scientist in the New South Wales Department of Environment
        and Conservation. He says it's very difficult to determine how human interventions in
        various habitats can foster the coexistence of species that have different fire regime
        requirements.
      
      
        Fire Suppression and Tropical Forests
        As tough as such questions are to answer in developed countries, they pale compared to
        the problems of tropical forest wildfire researchers and managers in developing countries.
        In these countries, a destructive cycle of human behavior begins with land-clearing and
        burning for farming, logging, mining, road-building, and other uses that open gaps in the
        rainforest's canopy cover. This lets in sunlight and air, reducing the forest's ability to
        smother fire by trapping moisture, and it encourages the growth of smaller, more fire-prone
        plants. The first wildfires that occur are bad, but successive ones can eventually
        transform tropical forest to scrub savanna (Figure 4). Of course, the remaining forest is
        thereby broken into fragments that continue to suffer incursions at their edges, as the
        cycle continues.
        In a recent paper in Science, Michigan State University Amazon expert Mark Cochrane
        pointed out that prescribed burning is ineffective in tropical forests, because the
        collateral damage outweighs any benefits. Indeed, tools and technologies employed in
        temperate conditions can seldom be applied usefully to tropical forests without significant
        alterations.
        “One of the main issues in fire science is that the U.S. has no capacity to develop new
        tools,” charges Ernesto Alvarado, a research scientist at the University of Washington in
        Seattle. He's been working for several years with United States Forest Service and
        Brazilian scientists on field studies in Mata Grosso, the southernmost state of the
        Brazilian Amazon. He says that fire prediction simulations developed decades ago have not
        yet been replaced by ones that account for tropical wildfire extremes, including either
        large-scale crown fires or surface fires, which often reach only 10 centimeters (3.9
        inches) in height and move slowly but can burn for weeks and kill many trees.
        Fire behavior models don't work for tropical surface fires because the physics are
        different from those in temperate forests, he explains. A slow wind generated from the
        unburned forest blows toward the fire, forcing the small flames to advance against, rather
        than with, the wind. Another difference is that the fuel is mostly leaf litter, not conifer
        needles or sticks.
        Alvarado and colleagues light experimental fires in clear-cuts to determine factors
        limiting ignition and spread. Such experimental work is rare in tropical forests, where
        observation and description still predominate. But the team also monitors surface
        wildfires, measuring fire length, spread, and heat release.
        “We're trying to find applications that people can use to control fires or to explain
        implications of fire policy,” he says. Most wildfires originate from deliberately set
        burns. For example, many farmers still clear land by the ancient method of slash-and-burn,
        in which forest is chopped, left to dry, and then burned. These farmers are now banned by
        Brazilian federal law from burning at the height of the dry season, mid-July to
        mid-September. They cut in May, but if the rains come early in September, they can't burn
        after the ban ends and must wait until the next season, with nowhere to grow their crops in
        the meantime. Alvarado thinks a more flexible burning schedule is a solution.
        The challenge is to pass on technological understanding to decision-makers. For example,
        even ranchers in Mata Grosso's economic elite usually haven't heard of fire management
        techniques, says Amazonian ecologist Carlos Peres at the University of East Anglia in the
        United Kingdom. Educational projects from nongovernmental organizations have helped to turn
        some farmers away from heavy reliance on slash-and-burn techniques, but fire suppression
        information remains to be distributed on the frontiers.
        “What we really need are very large areas of primary forest that effectively serve as
        fire breaks,” he says. Conservation plans have been made by the federal government in
        collaboration with international agencies, but implementation remains a question,
        particularly given the high level of economic pressure from multinational resource
        developers eager to enter the Amazon. Major roads through the jungle are also on the
        drawing board. “Different categories of conservation units can be gazetted on paper, but in
        practice they're a long way from working. Someone draws lines on a map high in an office in
        Brasilia, but when you go out to that place in the forest, no one knows it's a conservation
        zone.”
      
      
        Fire Prevention: Developing the Technology
        Information transfer faces similar barriers in much of Southeast Asia, as Canadian
        forestry researchers discovered during a five-year project (now winding up) to create a
        computerized early warning tool for wildfire outbreaks. The program was instigated after
        the 1997–1998 fires created a regional haze hazard, largely because of peat deposits up to
        20 meters (21.8 yards) thick that had become susceptible to burning in swampy forests
        drained and cleared for development.
        Michael Brady, who managed the Canadian project in Jakarta, points out that headmen in
        remote communities are still likely to believe that wildfires start spontaneously, by
        grasses rubbing together or even by magic. A fire scientist whose doctorate is in tropical
        forest peat dynamics, Brady sees the project as a medium to strengthen regional fire
        ecology in general. “In some ways, that's more important to me than the tool itself.”
        The tool is a variation of the Fire Danger Rating System used in Canada and, with
        various permutations, in many other countries. The Canadian system has two components, one
        for indexing fire weather and another predicting fire behavior. The weather component
        models moisture input and output in fuels generically classed as fine, moderate, and heavy.
        Brady and Indonesian university scientists grouped grasslands in the fine fuel category,
        fallen leaves and litter as medium, and peat and woody materials as heavy. They spent three
        years calibrating these fuels to local weather conditions, examining moisture dynamics and
        performing ignition tests. In developed countries, fuels are further specified in numerous
        classes for fire behavior prediction, but that requires decades of field work. Brady's team
        concentrated instead on helping key agencies in seven Southeast Asian countries, especially
        Indonesia and Malaysia, to obtain and use the appropriate computing tools.
        Brady doesn't expect immediate results in terms of reducing acreage burned. “Canada and
        the U.S. still have huge fire problems after working on it for a century.” But he does hope
        for a change of thinking, away from a current fascination in the region with satellite
        imaging of “hot spots” where fires are likely to be occurring. Fire danger rating
        concentrates on where fires are most likely to begin. “It allows you to add prevention into
        your management program.”
      
      
        Beyond Prevention
        In South Africa, “retention” is a conservation buzzword referring to strategies that, in
        a sense, go beyond prevention of problems. What ecologists hope to retain is biodiversity
        in the midst of changes that can't be stopped, and their methods are producing major
        repercussions throughout government. The work is centered on the Cape Floristic Region of
        Africa's southwestern tip. Almost 90,000 square kilometers (34,750 square miles) in area,
        it's the world's smallest floral kingdom. A conservation plan was launched in 1998 that has
        drawn cooperation from tourism, mining, water use, agricultural, and land use planning
        groups.
        The project has the ambitious goal of protecting not only the usual biodiversity
        patterns of conservation areas but also the spatial components of evolutionary processes
        that enable species to adapt to potentially harmful changes. This entails a complex effort
        to determine which parts of developed and undeveloped lands are most necessary to such
        processes, including rivers, sand movement corridors, gradients from uplands to coastal
        lowlands, and major wilderness areas. University of Port Elizabeth botanist Richard
        Cowling, one of the scheme's principal architects, estimates that it might require 60%–70%
        of the region's landscape.
        As in Australia, fires are important to the Cape's biodiversity, but too-frequent burns
        are a problem. Cowling thinks that by consolidating mountainous megawilderness under the
        project's plan and protecting spatial transitions between fire-prone areas and those that
        resist fire, managers could move toward allowance of natural fire regimes. The current
        problem, he says, is that protected areas usually stop short of the transition to
        semidesert areas that are privately owned. When fire spreads from public to private land,
        the government often gets sued. Under the evolving Cape plan, landowners will sit on
        governing boards, and property that they contract for conservation will be tax-exempt.
        The Cape plan has attracted millions of dollars in support from the World Bank and other
        international sources, but Cowling regards that achievement as much less important than the
        progress made in gaining support from various interest groups. “The key issue is the extent
        to which you can get biodiversity concerns mainstreamed to other sectors,” he says. Threats
        to habitat retention, not least of which is wildfire, endanger every species. “It's about
        making people realize that biodiversity is the basis upon which all other things will
        succeed.”
      
      
        
      
    
  

  
    
      
        
        Open access is gaining momentum. Authors are submitting papers in ever-increasing
        numbers to open-access journals. Several prominent research sponsors, including the
        Wellcome Trust, the Max Planck Society, the Centre National de la Recherche Scientifique
        (CNRS), and the Institut National de la Santé et de la Recherche (INSERM), have recently
        pronounced that open access is the best way for the researchers they support to publish
        their work. Several established commercial and not-for-profit publishers have announced
        plans to experiment with open-access models for some or all of their journals.
        Delighted and encouraged, we gear up for the launch of 
        PLoS Medicine this autumn—the next step in our efforts to bring the
        benefits of open access to the entire scientific and medical community. We aim to make 
        PLoS Medicine a premier journal, providing open access to the best
        medical research to researchers, to physicians and other caregivers, and to the public.
        The case for open access to medical research is even stronger than it is for basic
        research in biology. There are more interested parties: patients and their advocates;
        biotechnology and pharmaceutical companies that develop drugs and medical devices; doctors,
        nurses, and other healthcare providers; and health policy-makers at the national and
        international levels. The goal of the medical research enterprise is—or should
        be—scientifically, ethically, and socially responsible medicine, which means research that
        will benefit patients worldwide.
        The reality looks somewhat different. Large investments into basic research have not yet
        lived up to their full potential to save lives and improve their quality. Doctors,
        patients, and their advocates do not have ready access to the combined peer-reviewed
        evidence from medical research. The prices for the latest drugs often put them out of reach
        of patients in poor countries or poor patients in countries without universal healthcare
        systems. Moreover, research focuses disproportionately on the potentially lucrative
        treatments for diseases of wealthy societies, shortchanging the poorer countries, which
        bear the greatest burden of disease.
        A medical journal by itself cannot change this reality. But with the help of researchers
        and practicing physicians around the world who recognize the need and opportunity for
        change, we seek to create a journal that promotes medical research and practice that is
        both scientifically rigorous and compassionate.
        Open access to this literature will strengthen the medical research community by giving 
        all stakeholders free and immediate access to the latest medical
        research, along with new and more powerful search tools and links between the literature
        and other relevant information.
        
        PLoS Medicine will be an international, modern, general medical journal,
        covering all areas in the medical sciences, from basic studies to large clinical trials and
        cost-effectiveness analyses. We will concentrate on human studies that enhance our
        understanding of disease epidemiology, etiology, and physiology; the development of
        prognostic and diagnostic technologies; and trials that test the efficacy of specific
        interventions and those that compare different treatments. We will publish original
        research and commentary that promotes translation both of basic research into clinical
        investigation and of clinical evidence into practice.
        A truly broad medical journal is an ambitious project, but we want 
        PLoS Medicine to promote an integrated understanding of the patient—to
        make it easy for people to read outside their specialty area. “Doctors are systems
        biologists,” as one medical researcher put it, and inspiration can often be found in
        unfamiliar territory.
        Articles published in 
        PLoS Medicine will be rigorously peer-reviewed. Academic and professional
        editors, supported by expert peer-reviewers, will select those studies that drive research
        forward—in this case, toward medical applications and benefits for patients.
        This issue of 
        PLoS Biology contains two “human” studies that met our criteria for
        excellence and originality, a paper by Howard Chang and colleagues (found at DOI:
        10.1371/journal.pbio.0020007) on the microarray analysis of tumors and one by Sarah
        Rowland-Jones and coworkers (found at DOI: 10.1371/journal.pbio.0020020) that examines how
        HIV exhausts the capacity of the immune system. Similar papers submitted in the future will
        be published in 
        PLoS Medicine , alongside studies that have more direct implications for
        clinical practice. This issue also contains several articles describing more basic advances
        with medical implications: a study by Terry van Dyke and colleagues (found at DOI:
        10.1371/journal.pbio.0020022) describing a new mouse model for breast cancer, a report on a
        novel approach to drug synthesis by Chaitan Khosla and coworkers (found at DOI:
        10.1371/journal.pbio.0020031), and an article by Stephen Dowdy et al. (found at DOI:
        10.1371/journal.pbio.0020036) on targeted modulation of p53 activity. 
        PLoS Biology will continue to solicit and publish such articles, but we
        will bring them—and similar ones published elsewhere—to the attention of the readers of 
        PLoS Medicine .
        Like 
        PLoS Biology , 
        PLoS Medicine must be a community journal to achieve its goals. If you
        are a researcher or an individual anywhere in the world who has a stake in medical research
        and if the goals of 
        PLoS Medicine outlined here resonate with you, please contact us. 
        PLoS Medicine will accept submissions beginning in April 2004, and we are
        looking for advocates who will help to spread the word about open access and 
        PLoS Medicine in the global medical community and for investigators who
        will submit excellent research, review submitted articles, and contribute editorials and
        commentaries. 
        PLoS Medicine is and will stay a work in progress, and we want to consult
        with as many people as possible, both before the launch and as 
        PLoS Medicine evolves. Sign up to join the 
        PLoS Medicine community at http://www.plos.org/medicine and help us to
        make the best of medical research and practice accessible to a global audience.
      
    
  

  
    
      
        
        The ability to taste food is a life-and-death matter. Failure to recognise food with a
        high enough caloric content could mean a slow death from malnutrition. Failure to detect a
        poison could result in near-instant expiration. And now, as researchers begin to understand
        some of the nuts and bolts of taste perception, it seems that the sense of taste may also
        have more subtle effects on health.
      
      
        The Basics of Taste
        At the front line of the taste sensory system are the taste buds—onion-shaped structures
        on the tongue and elsewhere in the mouth (Figure 1). Up to 100 taste receptor
        cells—epithelial cells with some neuronal properties—are arranged in each taste bud. In the
        tongue, the taste buds are innervated by the chorda tympani (a branch of the facial nerve)
        and the glossopharyngeal nerve. These nerves carry the taste messages to the brain.
        Taste is the sense by which the chemical qualities of food in the mouth are
        distinguished by the brain, based on information provided by the taste buds. Quality or
        ‘basic taste’, explains Bernd Lindemann, now retired but an active taste researcher in
        Germany for many years, is a psychophysical term. Large numbers of people describe
        different tastants and then statistical analyses are used to define the important tastes.
        ‘The number of taste qualities has varied over the years’, says Lindemann. ‘We are now
        settling at around five, though I would not be surprised if some additional qualities pop
        up’.
        The five qualities that Lindemann refers to are salty, sour, bitter, sweet, and umami,
        the last being the Japanese term for a savoury sensation. Salty and sour detection is
        needed to control salt and acid balance. Bitter detection warns of foods containing
        poisons—many of the poisonous compounds produced by plants for defence are bitter. The
        quality sweet provides a guide to calorie-rich foods. And umami (the taste of the amino
        acid glutamate) may flag up protein-rich foods. Our sense of taste has a simple goal,
        explains Lindemann: ‘Food is already in the mouth. We just have to decide whether to
        swallow or spit it out. It's an extremely important decision, but it can be made based on a
        few taste qualities’.
      
      
        From Physiology to Molecular Biology
        Taste has been actively researched for many decades. During the 20th century,
        electrophysiologists and other researchers worked hard to understand this seemingly simple
        sense system. Then, in 1991, the first olfactory receptors were described. These proteins,
        which are exposed on the surface of cells in the nose, bind to volatile chemicals and allow
        us to detect smells. This landmark discovery, in part, encouraged many established taste
        researchers to investigate the molecular aspects of taste.
        The olfaction results also enticed researchers from other disciplines into the taste
        field, including collaborators Charles Zuker (University of California, San Diego [UCSD],
        La Jolla, California, United States) and Nick Ryba (National Institute of Dental and
        Craniofacial Research [NIDCR], Bethesda, Maryland, United States). About six years ago,
        explains Zuker, who previously worked on other sensory systems in flies, ‘there was a
        disconnect between our understanding of sensations in the case of photoreception,
        mechanoreception, touch, and so on and what we knew about taste’. There was evidence, says
        Ryba, that a class of protein receptors called G-protein-coupled receptors (GPCRs) were
        involved in sweet and bitter taste, ‘but the receptors weren't known, so we started to look
        for them …. These molecules are intrinsically interesting, but more importantly, they
        provide tools with which we can dissect out how taste works’.
      
      
        Bitter, Sweet, and Umami Receptors
        The bitter receptors fell first to the onslaught of the UCSD–NIDCR team and other
        molecular biologists. In 1999, the ability to taste propylthiouracil, a bitter tasting
        compound, had been linked to a locus on human Chromosome 5p15. Reasoning that this
        variation might be due to alterations in the coding sequence for a bitter receptor, the
        UCSD–NIDCR researchers used the draft of the human genome to search for sequences that
        resembled GPCRs on Chromosome 5p15. ‘That was how we found T2R1, the first bitter receptor,
        and, subsequently, a whole family of T2Rs’, says Zuker.
        
          
            Researchers want to know: how is taste coded?
          
        
        All these receptors, says Zuker, are coexpressed in bitter taste receptor cells, a
        result that contradicts other research showing that different bitter-responsive cells react
        to different bitter molecules. ‘To me’, says Zuker, ‘it makes sense that all the bitter
        receptors would be expressed in each bitter taste cell. We just need to know if something
        is bitter to avoid death’, not the exact identity of the bitter tastant.
        The sweet receptor story started in 1999 with the identification of two putative
        mammalian taste receptors, GPCRs now known as T1R1 and T1R2. In early 2001, four groups
        reported an association between the mouse 
        Sac locus, which determines the ability of mice to detect saccharin, and
        T1R3, a third member of the T1R family. The UCSD–NIDCR team subsequently showed that the
        T1R2 and T1R3 heterodimer (a complex of one T1R2 and one T1R3 molecule) forms a broadly
        tuned sweet receptor, responsive to natural sugars and artificial sweeteners, and that a
        homodimer of two T1R3 molecules forms a low-affinity sugar receptor that responds to high
        concentrations of natural sugars only. All sweet detection, concludes Zuker, is via the
        T1R2 and T1R3 receptors.
        And umami? A truncated glutamate receptor was identified as an umami receptor by
        researchers at the University of Miami (Florida, United States) School of Medicine in 2000.
        Zuker, however, believes that the one and only umami receptor is a heterodimer of T1R1 and
        T1R3. In October 2003, Zuker and his coworkers reported that mice in which either T1R1 or
        T1R3 has been knocked out show no preference for monosodium glutamate (MSG), an umami
        tastant. However, other researchers reported in August 2003 that T1R3 knockouts retain some
        preference for MSG. ‘We believe this is due either to the truncated glutamate receptor or
        another unknown receptor’, says lead author Sami Damak (Mount Sinai School of Medicine, New
        York, New York, United States). Damak says he does not know why the two sets of T1R3
        knockout mice behaved differently, but the UCSD–NIDCR researchers suggest that the residual
        response to MSG seen by Damak et al. is a response to the sodium content of MSG. Damak is
        not alone, however, in thinking there may be more than one umami receptor (and additional
        sweet receptors).
        Commenting on these recent discoveries, taste expert Linda Bartoshuk (Yale University
        School of Medicine, New Haven, Connecticut, United States) says that ‘it is lovely to see
        all these details, especially as they confirm what we already believed conceptually’. For
        example, she says, it is no surprise that there are many bitter receptors but probably only
        one sweet receptor. ‘There are so many poisons and it makes perfect sense to have lots of
        receptors feeding into a common transduction pathway. Sweet is a different problem. In
        nature, there are many molecules with structures similar to sugar that we must not eat
        because we cannot metabolise them. So I would have predicted one or at most a few highly
        specific sweet receptors’.
      
      
        What about Salty and Sour Receptors?
        The salty and sour receptors may be very different from the GPCRs involved in bitter,
        sweet, and umami perception, which bind complex molecules on the outside of the cell and
        transmit a signal into the cell. For salty and sour perception, the taste cell only needs
        to detect simple ions. One way to do this may be to use ion channels—proteins that form a
        channel through which specific inorganic ions can diffuse. Changes in cellular ion
        concentrations could then be detected and transmitted to the nervous system.
        Physiologist John DeSimone (Virginia Commonwealth University, Richmond, Virginia, United
        States) says there are at least two ion channel receptors for salt in rodent taste receptor
        cells. The first of these is the epithelial sodium channel, a widely expressed channel that
        can be blocked specifically with the drug amiloride. In rats, says DeSimone, only 75% of
        the nerve response to salt can be blocked by amiloride, so there is probably a second
        receptor. This, he says, seems to be a generalist salt receptor—the amiloride-sensitive
        channel only responds to sodium chloride—and may be the more important receptor in
        people.
        Sour tastants are acids, often found in spoiled or unripe food. DeSimone's current idea
        is that strong acids enter taste cells through a proton channel (probably a known channel
        present on other cell types) while weak acids, like acetic acid (vinegar), enter as neutral
        molecules and then dissociate to lower intracellular pH. DeSimone believes that he has
        identified the proton channel involved in sour taste as well as an ion channel that could
        be the second salt receptor, and he plans to do knockout experiments on both. If these
        channels are essential elsewhere in the body, as DeSimone suspects, to avoid lethality he
        will need to construct conditional knockouts in which the channel is lost only in the taste
        receptor cells.
        Zuker, meanwhile, is not convinced that the current ion channel candidates for salt and
        sour perception are correct. And, he says, GPCRs could also be involved in these
        modalities. ‘There is a precedent for that’, he claims, noting that extracellular calcium
        is sensed by a GPCR.
      
      
        Taste-Coding
        With many taste receptors now identified, researchers are turning to a long-standing
        question in taste perception: how is taste coded? When we eat, our tongue is bombarded with
        tastants. How is their detection and transduction of information organised so that the
        appropriate response is elicited? Taste physiologist Sue Kinnamon (Colorado State
        University, Fort Collins, Colorado, United States) explains the two theories of
        taste-coding. In the ‘labelled-line’ model, sweet-sensitive cells, for example, are hooked
        up to sweet-sensitive nerve fibres that go to the brain and code sweet. If you stimulate
        that pathway, says Kinnamon, ‘you should elicit the appropriate behavioural response
        without any input from other cell types’. In the ‘cross-fibre’ model, the pattern of
        activity over many receptors codes taste. This model predicts that taste receptor cells are
        broadly tuned, responding to many tastants. Support for this theory, says Kinnamon, comes
        from electrical recordings from receptor cells and from nerves innervating the taste buds
        that show that one cell can respond to more than one taste quality.
        Zuker and Ryba's recent work strongly suggests that taste-coding for bitter, sweet, and
        umami fits the labelled-line model in the periphery of the taste system. Their expression
        data show that receptors for these qualities are expressed in distinct populations of taste
        cells. In addition, in early 2003, they reported that, as in other sensory systems, a
        single signalling pathway involving the ion channel TRPM5 and PLCβ2, a phospholipase that
        produces a TRPM5 activator, lies downstream of the bitter, sweet, and umami receptors. When
        the UCSD–NIDCR researchers took PLCβ2 knockout mice, which did not respond to bitter,
        sweet, or umami, and engineered them so that PLCβ2 was only expressed in bitter
        receptor-expressing cells, only the ability to respond to bitter tastants was regained.
        These data, says Zuker, support the labelled-line model.
        The latest data supporting the labelled-line model came last October when Zuker and
        colleagues described mice in which a non-taste receptor—a modified κ-opioid receptor that
        can only be activated by a synthetic ligand—was expressed only in cells expressing T1R2,
        sweet-responsive cells. The mice were attracted to the synthetic ligand, which they
        normally ignore, indicating that dedicated pathways mediate attractive behaviours. The
        researchers plan similar experiments to see whether the same is true for aversive
        behaviours.
        Even with all these molecular data, the cross-fibre model of taste-coding still has its
        supporters—just how many depends on whom one talks to. Both Damak and Kinnamon, for
        example, believe that there is at least some involvement of cross-fibre patterning even in
        the taste receptor cells. But, says neurobiologist and olfaction expert Lawrence C. Katz
        (Duke University, Durham, North Carolina, United States), ‘the onus is now on people who
        believe otherwise [than the labelled-line model] to provide compelling proof for the
        cross-fibre theory because now, at least at the periphery, the evidence is compelling for a
        labelled line for bitter, sweet, and umami’. Bartoshuk also says the debate is decided in
        favour of the labelled-line model in the periphery. The crossfibre model is an interesting
        historical footnote, she comments.
        Whether this putative link between taste perception and health can be confirmed and
        whether it will be possible to manipulate food preferences to improve health remain to be
        seen. However, it seems certain that, as in the past five years, the next five years will
        see large advances in our knowledge of many aspects of taste, a fascinating and important
        sensory system.
      
      
        What Next—and Why Study Taste Anyway?
        The periphery of the taste sensory system has yielded many of its secrets, but
        relatively little is known about the transduction pathways in taste, how taste cells talk
        to the nervous system, or about events further downstream in the brain. How are signals
        from taste receptors integrated with those from olfactory receptors to form a
        representation of complex food flavours, for example? With their expanding molecular
        toolbox, researchers can now delve deeper into these aspects of taste perception. This may
        tell us not only about taste but about how the nervous system in general is put together,
        says Ryba.
        But understanding taste is not just an academic exercise. It has practical uses too.
        DeSimone suggests that by understanding salt receptors, it may be possible to design
        artificial ligands to help people lower their salt intake. As Kinnamon succinctly puts it,
        ‘Can you imagine eating potato chips and not having the salty component?’ An artificial
        salt receptor ligand could make salt-free foods a palatable option for people with high
        blood pressure. Lindemann also sees a great future in artificial ligands for taste
        receptors. The sense of taste is partly lost in elderly people, he says, so better
        tastants—effectively ‘chemical spectacles’—might give them back their pleasure of eating
        and thereby improve their quality of life.
        Finally, some aspects of taste may be inextricably tied up with general health, says
        Bartoshuk. Many people who can taste propylthiouracil are also ‘supertasters’—they have
        more fungiform papillae, structures containing taste buds, on their tongues than
        non-tasters (Figure 2). Supertasters find vegetables bitter—particularly brassicas, like
        Brussel sprouts—so they tend to eat fewer vegetables as part of their regular diet than
        non-tasters. ‘Being a supertaster affects your taste preferences, your diet, and ultimately
        your health’, claims Bartoshuk.
      
    
  

  
    
      
        
        As the year-long celebration of the 50
        th anniversary of the discovery of the structure of DNA came to an end,
        the engaging autobiography of one of the participants further enlivened the drama of this
        event. Maurice Wilkins, now 87, postpones the account of his involvement in the DNA affair
        until the second half of the book. Recounting his background and interesting life before
        DNA (34 years) in plain but telling sentences brings to life a character that is almost as
        much out of the ordinary as those of the more flamboyant James Watson and Francis
        Crick.
        Wilkins' first six years in New Zealand (a Garden of Eden) were followed by a long,
        vividly described trip to England, where the family eventually settled in Birmingham. His
        boyhood was marked by immersion in astronomy and telescope-making, but saddened by the
        painful illness of his sister. Success in school physics was the key for getting into
        Cambridge, where he reveled in the world of Ernest Rutherford, Mark Oliphant, and John
        Bernal. Given his leftist leanings, it was inevitable that Wilkins would become involved in
        the pacifist movement in Cambridge, with its close connection to the Communist Party.
        Perhaps too much involvement led to a low degree grade in 1938 and no hope of remaining at
        Cambridge. Instead, he returned to Birmingham and joined the Luminescence Lab being
        established by John Randall, a man with whom he would be closely connected for many
        decades. The work there contributed to Randall's scheme for making radar practical in air
        defense—the cavity magnetron that may have turned the course of World War II.
        Early in 1944, Oliphant, then at Birmingham, left to work on the atomic bomb at Berkeley
        and took Wilkins along. Life in Berkeley was exciting, but beneath the excitement of bomb
        work and mixed feelings upon its success at Hiroshima, Wilkins read Erwin Schrodinger's 
        What Is Life? Along with others who were to unravel the secrets of DNA,
        this planted the seed. When, after three transitional years, Randall became head of Kings
        College London's physics department and director of a biophysics research unit sponsored by
        the Medical Research Council (MRC), Wilkins was his deputy. The attack on DNA structure
        soon began.
        That X-ray diffraction might play a major role in this search rested on two pillars
        unique to England. One was the British lead in using X-ray diffraction to determine
        molecular structures—a crown jewel built on the work of the Braggs (father and son),
        Bernal, and Dorothy Hodgkin. The other was the pre-World War II work of William Astbury in
        showing that DNA fibers displayed some crystallinity that, if developed, might be the basis
        of helping to determine the structure. Wilkins confides that in 1950 he knew little of how
        such X-ray analysis might be done. But in that year he was presented with an opportunity in
        the form of samples of carefully prepared calf DNA, given to him by a Swiss chemist, Rudolf
        Signer. With this DNA, much better fibers could be obtained and much sharper diffraction
        diagrams emerged.
        The exploitation of this advance, however, became mired in a colossal error in Randall's
        management of the group. Without telling Wilkins, he wrote to Rosalind Franklin, who was on
        her way to join the DNA effort, that Wilkins was withdrawing from DNA work and that she
        would take over. Unaware of this, Wilkins and Alec Stokes continued their work and reported
        at a meeting in Cambridge in July 1951 that DNA chains were probably in a helical
        conformation with a diameter of 20 Å. At the close of the meeting, Franklin assailed
        Wilkins, saying that he should stop his DNA work (as Randall had written would be the
        case). Understandably, but regrettably, the two groups continued working in isolation from
        each other.
        Matters worsened. In October, Watson arrived at Cambridge and set up DNA structure
        studies with Crick. They quickly arrived at a three-stranded helical structure. But
        Franklin and Wilkins soon demolished it. Likewise, a three-stranded model at Kings College
        had a very short life. As if to trump these failures, Bragg at Cambridge and Randall at
        Kings agreed that DNA studies at Cambridge should stop and that the work should continue
        only at Kings. Mismanagement and noncooperation were taking their toll. Franklin was moving
        toward a two-stranded structure, but away from helices. Indeed, in mid-1952 she initiated a
        discussion with an announcement about the death of the helix. Mysteriously, she put aside a
        striking photo of the diffraction pattern of B-DNA (one of the two major structural forms
        of DNA) that emerged in early 1953 as a perfect signature of the helical form. But 1952
        continued downhill. Even Wilkins stopped DNA work that November.
        Suddenly, in the new year, life returned to the DNA effort. Linus Pauling had just
        published a structure (three-stranded) that did not long survive, but the entrance of the
        world's leading structural chemist into the race reawakened everyone to the centrality of
        DNA structure. In January, Raymond Gosling gave to Wilkins the very well-oriented
        diffraction photo of B-DNA that he and Franklin had taken in July 1952. Wilkins assumed
        that it was given to him to do as he wished; a few days later, he showed it to Watson.
        Though hardly an expert in X-ray diffraction, Watson sensed that it was strong evidence for
        helices and sketched it for Crick on his return to Cambridge. Later that January, Franklin
        announced she would be moving from Kings College to Birkbeck College to join Bernal's
        group. In giving her final seminar, she switched from her earlier insistence that B-DNA was
        nonhelical, but did not show the photo that gave the strongest evidence for helicity. This
        shift put Franklin in a position to move forward on the structure of DNA, but without
        others' resorting to model building, the goal would have remained elusive.
        Finally, in mid-February, Max Perutz, who was a member of the MRC committee overseeing
        the Biophysics Unit at Kings College, passed on to Crick his copy of a report from that
        unit. This report contained Franklin's results that the phosphates were on the outside and
        that the A-form of DNA had a special crystalline arrangement called the monoclinic C2 space
        group. From his work with proteins, Crick saw immediately that the chains in the helical
        structure must be antiparallel and that there were probably two chains entwined. Watson
        used other data in the report to deduce that there were indeed two chains, not three or
        four. Erwin Chargaff had recently shown that in the base composition of all DNAs examined,
        adenines and thymines as well as guanines and cytosines are equal, i.e., A = T and G = C.
        Now released from the ban on DNA studies, Watson and Crick engaged in a frantic search
        using model building. They found a unique way to fit the bases in the structure by pairing,
        and by March 7 they had the double-helix model constructed: it obeyed the Chargaff ratios,
        it fit the X-ray data for B-DNA, and it provided a rational way to encode and transmit
        genetic information to subsequent generations.
        Wilkins was invited to view the model in Cambridge. He found it stunning. Watson asked
        him to be a coauthor of the paper. Wilkins, true to his character, declined, as he had not
        been involved in the final monumental stage. Back in London, Franklin had already moved to
        Birkbeck. She received the news of the discovery with equanimity. But a later examination
        of her notebooks showed that she had moved to favor helices and a two-chain (or possibly a
        one-chain) model.
        With the rather complicated story of the greatest discovery in biology in the century
        now reasonably complete, what is one to make of it? There are many answers. I will mention
        only three.
        The first is the key role played by model building. In fiber diffraction there is not
        enough information, by orders of magnitude, to locate every atom, as would be possible in
        diffraction by perfect crystals that give thousands of sharp reflections. Instead, the
        fiber diagram can only provide cues and some specifics, such as the repeat distance. Model
        building is a way of bringing into the picture previously determined bond distances and
        bond angles of components such as the purine and pyrimidine bases and the sugars that are
        unavailable from the fiber diagram. That this was not seen at Kings College left the
        researchers there well behind in a field that they had pioneered.
        A second lesson is the importance of bringing the full knowledge of single crystal
        analysis to fiber diagram interpretation. That Franklin and Wilkins missed noting that the
        monoclinic C2 space group meant that the chains in the fiber had to be antiparallel robbed
        them of an important clue to the structure.
        And third, the management of the Biophysics Unit at Kings College was a recipe for
        failure. Riddled by secrecy, diffuse lines of authority, the absence of strategies, and a
        lack of open congeniality, all so well described by Wilkins, who refers to it as Randall's
        Circus, this unit is a model of how not to succeed in group research.
        DNA research continued at Kings College in a gradually improving environment: important
        details were worked out. But there was no real renewal, such as aiming at how DNA is
        configured to accommodate proteins in the nucleus. Wilkins enjoyed being included in the
        subsequent awards—the Lasker and the Nobel prizes. With Crick, he was annoyed by Watson's
        rendering of events in 
        The Double Helix . The final chapter of his own autobiography addresses
        the criticism that some have leveled against his cold relation with Franklin, but also his
        happiness in newfound family life. Research gradually gave way to the pursuit of pacifist
        goals in a number of organizations and to the popularization of science. His has been a
        useful life, a part of which contributed to the great revolution in biology. It is good to
        have the insight that this book presents in a candid and personal way.
      
    
  

  
    
      
        
        Symbiosis, an interdependent relationship between two species, is an important driver of
        evolutionary novelty and ecological diversity. Microbial symbionts in particular have been
        major evolutionary catalysts throughout the 4 billion years of life on earth and have
        largely shaped the evolution of complex organisms. Endosymbiosis is a specific type of
        symbiosis in which one—typically microbial—partner lives within its host and represents the
        most intimate contact between interacting organisms. Mitochondria and chloroplasts, for
        example, result from endosymbiotic events of lasting significance that extended the range
        of acceptable habitats for life. The wide distribution of intracellular bacteria across
        diverse hosts and marine and terrestrial habitats testifies to the continued importance of
        endosymbiosis in evolution.
        Among multicellular organisms, insects as a group form exceptionally diverse
        associations with microbial associates, including bacteria that live exclusively within
        host cells and undergo maternal transmission to offspring. These microbes have piqued the
        interest of evolutionary biologists because they represent a wide spectrum of evolutionary
        strategies, ranging from obligate mutualism to reproductive parasitism (Buchner 1965;
        Ishikawa 2003) (Box 1; Table 1). In this issue of 
        PLoS Biology , the publication of the full genome sequence of the
        reproductive parasite 
        Wolbachia allows the first genomic comparisons across this range
        (Wu et al. 2004).
      
      
        Lifestyle Extremes in Insect Endosymbionts
        At one end of the spectrum, beneficial endosymbionts provide essential nutrients to
        about 10%–15% of insects and provide models for highly specialized, long-term mutualistic
        associations (Figure 1). These ‘primary’ endosymbionts are required for the survival and
        reproduction of the host, most of which feed on unbalanced diets such as plant sap, blood,
        or grain, and occur within specialized host cells called bacteriocytes (or mycetocytes)
        (Baumann et al. 2000; Moran and Baumann 2000). Molecular phylogenetic analyses demonstrate
        stability of these obligate mutualists over long evolutionary periods, ranging from tens to
        hundreds of millions of years. By allowing their hosts to exploit otherwise inadequate food
        sources and habitats, the acquisition of these mutualists can be viewed as a key innovation
        in the evolution of the host (Moran and Telang 1998). Owing to their long-term, stable
        transmission from generation to generation (vertical transmission), these cytoplasmic
        genomes have been viewed as analogs to organelles.
        By contrast, reproductive parasites of insects, including 
        Wolbachia (O'Neill et al. 1998) and the more recently discovered
        endosymbiont in the Bacteroidetes group (also called CFB or CLO) (Hunter et al. 2003),
        propagate in insect lineages by manipulating host reproduction. These maternally inherited
        bacteria inflict an impressive arsenal of reproductive alterations to increase the
        frequency of infected female offspring, often at the expense of their hosts. Such
        mechanisms include cytoplasmic incompatibility, parthenogenesis, and male killing or
        feminization. As parasites, these bacteria rely on occasional horizontal transmission to
        infect new populations (Noda et al. 2001) and, by directly altering reproductive patterns,
        may be a causative agent of host speciation (Bordenstein et al. 2001).
        Comparative molecular analysis of insect endosymbionts over the past decade has provided
        new insights into their distribution across hosts, their varying degrees of stability
        within host lineages (ranging from cospeciation to frequent host-switching), and their
        impressive genetic diversity that spans several major bacterial groups. More recently,
        studies in genomics of obligate mutualists—and now 
        Wolbachia —illuminate mechanisms of host–symbiont integration
        and the distinct consequences of this integration in various symbiotic systems. In
        addition, since hosts and symbionts often have different evolutionary interests, the
        diverse features of insect–bacterial associations can be understood as different outcomes
        in the negotiation of genetic conflicts. Some recent highlights and tantalizing research
        areas are described below.
      
      
        Endosymbiont Genomes: Spanning the Gamut from Static to Plastic
        The distinct lifestyle of endosymbionts has clear effects on rates and patterns of
        molecular evolution. Compared to free-living relatives, endosymbionts are thought to have
        reduced effective population sizes due to population bottlenecks upon transmission to host
        offspring and, in the case of obligate mutualists that coevolve with their hosts, limited
        opportunities for gene exchange. The nearly neutral theory of evolution (Ohta 1973)
        predicts accelerated fixation of deleterious mutations through random genetic drift in
        small populations, a phenomenon that has been observed in endosymbionts (Moran 1996;
        Lambert and Moran 1998). Over time, this lifestyle-associated accumulation of deleterious
        mutations may negatively affect the fitness of both the host and symbiont.
        It is increasingly clear the distinct lifestyle of endosymbionts also shapes the
        architecture and content of their genomes, which include the smallest, most AT-rich
        bacterial genomes yet characterized (Andersson and Kurland 1998; Moran 2002). A common
        theme is substantial gene loss, or genome streamlining, which is thought to reflect an
        underlying deletion bias in bacterial genomes combined with reduced strength or efficacy of
        selection to maintain genes in the host cellular environment. As a result of gene loss,
        these bacteria completely rely on the host cell for survival. Because they cannot be easily
        cultured apart outside of the host for traditional genetic or physiological techniques,
        analysis of genome sequence offers a valuable tool to infer metabolic functions that
        endosymbionts have retained and lost and to elucidate the steps in the evolutionary
        processes of genome reduction.
        Since 2000, full genome sequences have been published for 
        Buchnera of three aphid host species, 
        Wigglesworthia of tsetse flies, and 
        Blochmannia of ants (Shigenobu et al. 2000; Akman et al. 2002;
        Tamas et al. 2002; Gil et al. 2003; van Ham et al. 2003). Parallels among these mutualist
        genomes include their small size (each smaller than 810 kb), yet retention of specific
        biosynthetic pathways for nutrients required by the host (for example, amino acids or
        vitamins). However, genomes also show signs of deleterious deletions. Early gene loss in 
        Buchnera involved a few deletions of large contiguous regions of
        the ancestral genome and often included genes of unrelated functions (Moran and Mira 2001).
        These ‘large steps’ imply that genome reduction involved some random chance (due to the
        location of genes in the ancestral chromosome) and selection acting on the combined fitness
        of large sets of genes, rather than the fitness of individual loci. Such deletions are
        apparently irreversible in obligate mutualists, which lack recombination functions and
        genetic elements, such as prophages, transposons, and repetitive DNA that typically mediate
        gene acquisition. The scarcity of these functions, combined with limited opportunities to
        recombine with genetically distinct bacteria, may explain the unprecedented genome
        stability found in 
        Buchnera compared to all other fully sequenced bacteria (Tamas
        et al. 2002) and a lack of evidence for gene transfer in other mutualist genomes. Stability
        also extends to the level of gene expression, as obligate mutualists have lost most
        regulatory functions and have reduced abilities to respond to environmental stimuli (Wilcox
        et al. 2003).
        The 
        Wolbachia genome presented in this issue allows the first genome
        comparisons among bacteria that have adopted divergent evolutionary strategies in their
        associations with insects (Wu et al. 2004). Like other parasites, but unlike long-term
        mutualists, 
        Wolbachia may experience strong selection for phenotypic
        variation, for example, to counter improved host defenses, to compete with distinct 
        Wolbachia strains that coinfect the same host, or to increase
        its transmission to new host backgrounds. High levels of recombination in 
        Wolbachia (for example, Jiggins et al. 2001) may allow rapid
        genetic changes in this parasite and may be catalyzed by the exceptionally high levels of
        repetitive DNA and mobile elements in its genome (Wu et al. 2004). Other bacteria that
        colonize specialized niches for long periods and lack co-colonizing strains also possess
        high levels of repetitive chromosomal sequences. For example, among ulcer-causing 
        Helicobacter pylori in primate guts, repetitive DNA mediates
        intragenomic recombination and may provide an important source of genetic variation for
        adaptation to dynamic environmental stresses (Aras et al. 2003). The potential
        contributions of repetitive DNA and phage to intragenomic and intergenomic recombination in
        
        Wolbachia are exciting areas of research (Masui et al. 2000).
        The 
        Wolbachia genome also provides a valuable tool for future
        research to test whether plasticity extends to gene content variation among 
        Wolbachia strains and labile gene expression patterns.
        Between these two extremes of obligate mutualism and reproductive parasitism lies a
        spectrum of secondary symbionts of insects, most of which have not yet been studied in
        detail. Such ‘guest’ microbes transfer among diverse host species (Sandström et al. 2001),
        may provide more subtle or occasional benefits (for example, relating to host defense
        against parasitoids [Oliver et al. 2003]), and could represent an intermediate stage
        between a free-living lifestyle and obligate endosymbiosis. Genome-level data from these
        secondary symbionts promise to shed light on the range of lifestyles between obligate
        mutualism and reproductive parasitism and on the early stages in the transition to each.
        Microarray-based comparisons of gene content among 
        Escherichia coli , a facultative mutualist of tsetse flies (
        Sodalis glossinidius ), and a relatively young mutualist of
        weevils (
        Sitophilus oryzae primary endosymbiont [SOPE]) show that genome
        streamlining in the endosymbionts may preclude extracellular existence, and highlight
        modifications in metabolic pathways to complement specific host physiology and ecology (Rio
        et al. 2003). In addition, these endosymbionts may employ similar mechanisms as
        intracellular parasites in overcoming the shared challenges of entering host cells,
        avoiding or counteracting host defense mechanisms, and multiplying within a host cellular
        environment (Hentschel et al. 2000). The rapidly growing molecular datasets for secondary
        (or young primary) insect endosymbionts have identified pathways that are considered to be
        required for pathogenicity, such as Type III secretion (Dale et al. 2001, 2002). Such
        pathways may therefore have general utility for bacteria associated with host cells and may
        have evolved in the context of beneficial interactions.
      
      
        Genetic Conflicts and Host–Symbiont Dynamics
        Given their diverse evolutionary strategies, insect endosymbionts also provide a rich
        playing field to explore genetic conflicts (Frank 1996a, 1996b), which might involve the
        mode of symbiont transmission, the number of symbionts transmitted, and the sex of host
        offspring. Genetic conflicts described between organelle and nuclear genomes of the same
        organism (Hurst 1995) can provide a context to understand the evolutionary dynamics of
        insect–bacterial associations and the diverse outcomes of these relationships. For example,
        the uniparental (maternal) mode of inheritance of both mitochondria and insect
        endosymbionts may reflect host defense against invasion by foreign microbes with strong
        deleterious effects, which spread more easily under biparental inheritance (Law and Hutson
        1992).
        Host–symbiont conflicts over offspring sex ratio are quite apparent in reproductive
        parasites (Vala et al. 2003). While the bacteria favor more female offspring and employ a
        variety of mechanisms to achieve this, the host typically favors a more balanced sex ratio.
        This conflict may lead to changes in the host that counter the symbiont's effect on sex
        ratio. For example, the spread of 
        Wolbachia in a spider mite population caused selection on host
        nuclear genes that decrease the symbiont-induced sex ratio bias (Noda et al. 2001).
        Obligate mutualists also experience genetic conflicts with the host regarding
        transmission mode and number. In general, symbionts generally favor dispersal out of the
        host to avoid competition with their close relatives, while hosts are expected to restrict
        symbiont migration and thus reduce the virulent tendencies (Frank 1996b). In obligate
        mutualisms, there may be little room for negotiation. For example, the highly conserved,
        host-controlled determination of aphid bacteriocytes (Braendle et al. 2003) and the
        phylogenetic congruence observed in numerous studies suggest that aphids have won this
        conflict over symbiont transfer. However, the number of bacteria transmitted may be more
        flexible and is known to vary among aphid taxa (Mira and Moran 2002). Models indicate that
        the fixation rate for symbiont-beneficial (selfish) mutations increase with the number of
        symbionts transmitted, reflecting greater efficacy of selection among bacteria within a
        given host (Rispe and Moran 2000).
      
      
        Prospects
        In sum, the past few years have witnessed a surge of new empirical and theoretical
        approaches to understand the dynamics of bacterial–insect relationships. These tools have
        shed light on the roles of recombination, selection, and mutation on endosymbiont genome
        evolution and have highlighted parameters that shape the outcome of genetic conflicts
        between hosts and symbionts. These data provide a foundation for studying the evolution of
        mutualism and parasitism and modes of transitions between them. In the near future, we can
        look forward to full genome sequences that span a broader ecological and phylogenetic
        diversity of endosymbionts and provide a richer comparative framework to test existing
        models and develop new ones.
        Developments in endosymbiosis are important not only to questions in basic research, but
        may have important practical applications. Blood-feeding insects such as mosquitoes and
        tsetse flies are vectors for parasites that cause significant global infectious diseases
        such as malaria, dengue virus, and trypanosomiasis, many of which have frustrated attempts
        at vaccine development. The same insects that transmit these devastating human parasites
        often possess a diversity of mutualistic and parasitic bacterial endosymbionts. A very
        promising and urgent area of endosymbiont research is the manipulation of these bacteria to
        control vector populations in the field. Current studies already provide evidence that
        endosymbiont manipulation is a promising strategy to reduce the lifespan of the insect
        vector or limit its transmission of disease-causing parasites (Aksoy et al. 2001;
        Brownstein et al. 2003). Each advance in our understanding of endosymbiont genomics and
        evolutionary dynamics represents one step closer to that goal.
      
      
        
      
    
  

  
    
      
        
        Evolution is a complex phenomenon that requires a broad understanding of many areas of
        biology for us to appreciate it fully. Moreover, the field has expanded rapidly, especially
        since the development of molecular techniques in the past two to three decades. Futuyma's
        classic text on evolution (1998) contains 26 chapters totaling 763 pages. To cover the
        topic in only eight chapters and 145 pages, as the Charlesworths have done in 
        Evolution: A Very Short Introduction , is no mean feat.
        Their book is one of a series of short introductions, published by Oxford University
        Press, covering an eclectic array of subjects that aim to provide an accessible yet
        stimulating read for anyone wanting a thorough introduction to a topic. In this small
        volume, the Charlesworths have succeeded on both fronts and provide an excellent account of
        the core issues for a broad range of readers. One of the reasons for the book's appeal is
        that the authors draw on a range of carefully chosen human traits to illustrate their
        points. By contrast, most evolutionary textbooks (other than those purely on human
        evolution) tend to focus on nonhuman organisms. As with traits in every other organism,
        many human and human-related characteristics have evolved via genetic drift and natural
        selection, and they provide an effective means of convincing readers of the reality and
        relevance of evolution. For example, to explain how mutation can cause the loss of a
        function, the authors discuss the relatively poor sense of smell in humans, as compared
        with many other mammals, using an example of a vestigial ‘pseudogene’ of a human olfactory
        receptor gene. They also discuss tooth decay, enzyme aesthetics, heritable differences,
        cancer and other diseases, and the ability to taste and so on.
        Although the topics the Charlesworths choose to focus on are certainly appropriate, they
        provide only a brief mention of one important process—development. Evolutionary
        developmental biology is a burgeoning field that can provide interesting and important
        insights into our understanding of the mechanisms of evolution. For example, the absence of
        eyes in cavefish, rather than being the result of a degenerative process, might be the
        result of selection on genes that govern feeding morphology, a selection process that has
        included suppression of eye development (Pennisi 2002). Such developmental mechanisms and
        constraints can actually alter the direction of evolution. Although the key forces driving
        evolution are usually thought of as mutation, genetic drift, natural selection, and
        divergence, the developmental pathways from genes to phenotypes, along with associated
        developmental constraints, can also determine the rate and direction of evolution.
        In Chapter 7, the authors discuss five topics that have traditionally been hard to
        understand from an evolutionary point of view. These ‘difficult problems’ are ageing,
        altruism, human consciousness, complex adaptations, and the origin of living cells.
        Difficult problems can be interpreted in two ways: those that, although hard to solve, have
        either been explained or will eventually be explained by modern evolutionary theories, and
        those problems that cannot be fully resolved with our current understanding but leave room
        for learning about additional mechanisms or factors. The Charlesworths generally consider
        only those problems of the former type—the explained ones. However, I think that some of
        the more intractable problems should be described in more detail. For instance, complex
        adaptation might be fully explained by mutations and natural selection, but additional
        unknown mechanisms might be essential for the evolution of the complex traits. I realize
        that opponents of modern evolutionary theory, such as creationists, have often cited these
        traditional problems to support their conclusion that modern evolutionary theory is wrong;
        but progress always depends on the consideration of new ideas, and there might be important
        mechanisms still to be discovered that play a key role in evolution. Describing potentially
        intractable problems might also spur on young readers who are thinking of studying
        evolutionary biology with the hope that there are still some theoretical battles to be
        conquered.
        Who is the target audience of this book? For many books, the topics chosen and the
        writing style can perhaps provide clues to the nature of the readers. For instance, 
        The Blind Watchmaker by Richard Dawkins (1990) is a good introductory
        book for those interested in natural selection because it seems to be written mainly for
        individuals who either oppose or do not understand the role of natural selection. In the
        Charlesworths' book, providing evidence for evolution occupies 49 of the 130 pages. They
        explain how the similarities between living creatures can be understood in terms of
        evolution (Chapter 3) and subsequently discuss evidence from the geographical distributions
        of living and fossil species (Chapter 4). My first impression was that this part occupies
        too large a proportion of the book. However, Chapter 3 serves as a good introduction to the
        basic background of biology, such as the gene, DNA, and cells. When I read a recent article
        about a teaching controversy concerning evolution (Scott and Branch 2003), I began to
        appreciate the importance—at least in the United Kingdom and the United States—of
        convincing readers of the reality and cogency of evolution and evolution theory by astutely
        providing them with the evidence to judge for themselves.
        In Japan, there seem to be few people who deny the facts of evolution, although there
        are many ideologically motivated books opposing natural selection and Darwinism. To
        convince creationists of evolution is usually extremely difficult, if not impossible,
        because they will never doubt their assumption that God created humankind. Education of
        young and curious people, however, can make a difference. This is where I think the book
        will be most successful, but this book should not just be limited to young people—I can
        recommend it to anyone who wants to know about evolution. Moreover, I can recommend it to
        Japanese students not only as an introduction to evolution, but also as an exercise in
        reading a well-written and engaging English text.
      
    
  

  
    
      
        
        In 1959 Richard Feynman delivered what many consider the first lecture on
        nanotechnology. This lecture, presented to the American Physical Society at the California
        Institute of Technology, prompted intense discussion about the possibilities, or
        impossibilities, of manipulating materials at the molecular level. Although at the time of
        his presentation, the manipulation of single molecules and single atoms seemed improbable,
        if not impossible, Feynman challenged his audience to consider a new field of physics, one
        in which individual molecules and atoms would be manipulated and controlled at the
        molecular level (Feynman 1960).
        As an example of highly successful machines at the “small scale,” Feynman prompted his
        audience to consider the inherent properties of biological cells. He colorfully noted that
        although cells are “very tiny,” they are “very active, they manufacture various substances,
        they walk around, they wiggle, and they do all kinds of wonderful things on a very small
        scale” (Feynman 1960). Of course, many of these “wonderful things” that he was referring to
        are a result of the activities of proteins and protein complexes within each cell.
        The field of nanotechnology has indeed emerged and blossomed since Feynman's 1959
        lecture, and scientists from many disciplines are now taking a careful look at the protein
        “machines” that power biological cells (Drexler 1986). These “machines” are inherently
        nanoscale, ranging in width from a few nanometers (nm) to over 20 nm, and have been
        carefully refined by millions of years of evolution.
        As a graduate student in molecular biology, I have been especially interested in
        creative approaches to bridging the fields of biology and nanotechnology. Both DNA and
        protein molecules possess a number of intrinsic characteristics that make them excellent
        candidates for the assembly of dynamic nanostructures and nanodevices. Properties such as
        the site-specific molecular recognition among interacting protein molecules, the
        template-directed self assembly of complementary DNA strands, and the mechanical properties
        of certain protein complexes have enabled bionanotechnologists to envision a molecular
        world built “from the bottom up” using biological-based starting materials.
        In my own research, I have been very interested in investigating protein interactions
        and protein pathways on a genome-wide scale. In many ways, protein pathways are analogous
        to nanoscale “assembly lines,” since protein pathways often involve a series of proteins
        that act in successive order to yield a particular molecular “product” or perform a
        particular molecular function. While these protein-based “assembly lines” are commonplace
        within biological cells, they prompt two interesting questions with respect to the field of
        nanotechnology. First, can we mimic these multicomponent protein-based “assembly lines” on
        nanofabricated surfaces? And, second, can we tailor these “nanoscale assembly lines” to
        perform new and unique tasks?
        Nanomechanical protein complexes, such as the rotary ATP synthase complex, have also
        generated much interest from a nanotechnology standpoint (Soong et al. 2000). These protein
        complexes enable highly controlled mechanical motion at the nanoscale and may some day lead
        to novel rotary machines that function as molecular motors for a variety of nanoscale
        applications.
        In order to fully exploit these nanoscale protein machines, it is of prime importance to
        be able to position individual proteins and protein complexes at the nanoscale.
        Progress in this area has recently been reported by Yan et al. (2003), who developed a
        method to construct two-dimensional protein arrays using DNA-directed templates. Building
        on work pioneered by Nadrian Seeman (Seeman 2003), Yan et al. constructed two-dimensional
        DNA “nanogrids” by exploiting the pairing that occurs between complementary DNA strands
        (Figure 1). The two-dimensional DNA nanogrid exhibits a repeating periodic structure
        (Figure 1B) due to the inherent qualities of the individual DNA tiles that make up the
        nanogrid (Figure 1A). The distance between adjacent tile centers is approximately 19 nm
        (approximately 4.5 turns of the DNA double helix plus the diameter of two DNA helices).
        Yan et al. utilized these DNA nanogrids to assemble periodic protein nanoarrays. The DNA
        nanogrid, in this case, served as a molecular scaffold for the self assembly of protein
        molecules into ordered arrays. In order to control the location of protein assembly, Yan et
        al. first tethered a covalently linked biotin moiety to the central region of each DNA
        tile. The biotin was covalently linked to one of the DNA strands at the position
        corresponding to the center of the tile. This design resulted in a uniform array of
        biotinylated tiles, with each biotin moiety separated by about 19 nm. The authors then
        added streptavidin, a protein that has a strong binding affinity for biotin, to form a
        periodic streptavidin protein array on top of the biotinylated DNA lattice. The resulting
        array represents the first periodic, self-assembled DNA lattice in which individual protein
        molecules are precisely positioned into a periodic array with nanometer dimensions.
        It is interesting to consider some of the applications of self-assembled protein arrays.
        Soong et al. (2000) demonstrated that the ATP synthase protein complex could be used to
        power the rotation of an inorganic nickel “nanopropeller.” ATP synthase is a multisubunit
        protein complex with a domain that rotates about its membrane-bound axis during the natural
        hydrolysis of ATP within a cell. Soong et al. attached a nanoscale inorganic “propeller” to
        the rotary stalk of ATP synthase, creating a “rotary biomolecular motor.” It is intriguing
        to consider the construction of an ordered array of ATP synthase driven nanomachines, each
        positioned precisely along a DNA scaffold, similar to that described by Yan et al. Such an
        assembly, combined with proposed “nanogears” (Han et al. 1997), may one day enable the
        construction of nanoscale variations of the traditional “gear-train” and “rack-and-pinion”
        gearing systems. Construction of such systems may facilitate the design of machines that
        can transmit and transform rotary motion at the nanoscale.
        In addition to rotary biomolecular motors, proteins that undergo substantial
        conformational changes in response to external stimuli might also find some interesting
        uses in nanoarrays. Dubey et al. (2003) are working on methods to exploit the pH dependent
        conformational changes of the hemagglutinin (HA) viral protein to construct what they term
        viral protein linear (VPL) motors. Proteins that undergo substantial conformational changes
        in response to environmental stimuli may facilitate the design of nanoscale machines that
        produce linear motion (Drexler 1981), as opposed to rotary motion. At neutral pH, the HA
        2 polypeptide forms a compact structure composed of two α-helices folded
        back onto each other. At low pH, HA
        2 undergoes a substantial conformational change, which results in a
        single “extended” helix. This conformational change results in a linear mechanical motion,
        with a linear movement of approximately 10 nm (Dubey et al. 2003). It would be interesting
        to investigate the applications of ordered arrays of dynamic VPL motors, since an array of
        such “hinge” structures may enable the coordinated linear movement of hundreds of tethered
        macromolecules in a synchronous manner.
        The work of Yan et al. (2003) has opened up exciting new avenues in the field of
        nanotechnology and has provided the molecular framework for the construction of dynamic
        protein-based assemblies. It is foreseeable that variations of these same DNA scaffolds
        will eventually be used for the design and construction of more complex protein-based
        assemblies, such as nanoscale “assembly lines” or periodic arrays of dynamic motor
        proteins. This work is important to me because it demonstrates not only that it is possible
        to create uniform arrays of protein biomolecules using biomolecular scaffolds, but the
        study also emphasizes the important role that molecular biology will undoubtedly play as
        the field of nanotechnology matures.
        As the field of nanotechnology continues to evolve, it is likely that we will see many
        more nanotechnology applications utilizing biological macromolecules. Toward the end of
        Richard Feynman's 1959 lecture, he quipped, “What are the possibilities of small but
        movable machines? They may or may not be useful, but they surely would be fun to make.”
      
    
  

  
    
      
        
        As a student I always marvelled at the sight of single cells in culture moving over
        artificial surfaces and exhibiting membrane ruffles and protrusions. However, while I found
        cultured cells fascinating I always wondered how cells are able to move and regulate their
        shape in the context of a whole organism where so many space constraints exist and where
        all cellular processes have to be tightly regulated. Some answers to my questions began to
        emerge in a paper written by Baum and Perrimon (2001), in which the authors showed the
        expression and regulation of the actin cytoskeleton and of actin binding proteins in a real
        epithelium.
        The cytoskeleton is a meshwork of protein polymers extending throughout the cytoplasm.
        It not only provides structural support for the cell but also plays a central role in a
        range of dynamic processes from signalling to endocytosis and intracellular trafficking. A
        particularly clear example of this is the use of actin cytoskeleton as a “wool” for
        knitting multiple dynamic structures such as lamellae, filopodia, and stress fibres. These
        structures determine cell shape and also produce the driving force accompanying many types
        of cellular movements including muscle contraction and cell division. We know many details
        about some of the proteins that modulate the dynamics of actin in these structures.
        However, most of them have been found biochemically and their function has been elucidated
        primarily using in vitro and cell culture assays of actin assembly. What about these
        proteins in the context of a developing organism? How do cells generate a spatially and
        temporally ordered network of actin filaments represented at the tissue level? To answer
        these questions, we need to move to experimentally accessible multicellular organisms, such
        as 
        Drosophila , which offers virtually unlimited possibilities as a
        model system for the genetic and molecular analysis of biological processes.
        Baum and Perrimon (2001) analyzed the function of a number of proteins involved in actin
        dynamics within the context of a developing epithelium—the follicle cells that surround the
        germ line cyst during 
        Drosophila oogenesis. These cells have a simple polarised
        arrangement of actin filaments, which provides a useful system to study the spatial
        organisation of the actin cytoskeleton.
        Taking advantage of the ability to generate clones of cells lacking specific proteins,
        the authors identified new functional roles for actin regulators such as CAP (a 
        Drosophila homologue of adenylyl cyclase-associated proteins),
        Enabled (Ena) and Abelson (Abl). These proteins had been well characterized in cell culture
        and in vitro studies, but little was known about their function in a developing organism.
        Clones of cells lacking CAP (Figure 1), a protein known to inhibit actin polymerisation,
        maintained their epithelial polarity but had higher levels of actin and defects in the
        apical actin organisation. This result indicates that the inhibitory activity of CAP is
        restricted to one side of the cells, thus demonstrating that actin dynamics can be
        independently modified at opposite poles of an epithelium. Ena, a member of the Ena/VASP
        family proteins that catalyse filament formation, and Abl, a protein kinase that binds CAP
        in mammalian cells, were found to work with CAP in this process. The authors proposed that
        CAP, Ena, and Abl regulate the level and spatial organization of actin in the follicle
        cells.
        In contrast to the spatially restricted functions of CAP, Ena, and Abl, profilin and
        cofilin were shown to regulate actin filament formation throughout the cell cortex, a more
        global function that matches the results obtained in cell culture experiments. In summary,
        this study showed how proteins can organise actin in space and began to highlight some of
        the differences and similarities between cells in culture and in vivo. The functions
        revealed in the follicular epithelium were consistent with the roles previously shown in
        mammalian systems, but the experiments on intact tissue began to reveal a spatial and
        temporal functional dimension that could not have been observed in cell culture.
        These experiments could be expanded to large-scale screens (St Johnston 2002), but this
        would be time consuming and could encounter the problem that some genes will be cell
        lethal, preventing the analysis of their function in actin dynamics. However, two more
        recent reports (Kiger et al. 2003; Rogers et al. 2003) describe a complementary and
        exhaustive search for regulators of cytoskeletal dynamics by taking advantage of genomic
        resources and the powerful RNA interference (RNAi) technique (Hutvágner and Zamore 2002).
        RNAi allows individual genes to be knocked out in a simple and controlled fashion.
        Kiger et al. (2003) used RNAi in two different cell lines of 
        Drosophila to screen a number of genes involved in signalling
        and cytoskeletal dynamics. They targeted 994 genes, of which 160 produced phenotypes in the
        experiment. The range of phenotypes varied from specific defects in the actin and tubulin
        cytoskeleton to others affecting cell cycle progression, cytokinesis, and cell shape. They
        also showed that only about 40% of the genes had similar loss-of-function phenotypes in
        both cell lines. This alone indicates an important limitation of many tissue culture
        experiments, since the same protein can have different effects depending on the cell type.
        Another valuable element of this work is that clustering of genes with similar phenotypes
        leads to the identification of pathways and networks of genes that are involved in
        cytoskeletal function.
        Rogers et al. (2003), using only one 
        Drosophila cell line, studied the effects of proteins involved
        in the formation of lamellae. The authors looked at the effects of loss of function in 90
        genes known to be involved in actin dynamics and the formation and activity of the lamella.
        As well as confirming the function of many proteins already known to play a role in this
        process, this analysis allowed them to find interactions between genes and to build genetic
        pathways.
        Together these two studies reveal that RNAi screens in tissue culture can be a powerful
        tool for finding new functions of known and uncharacterized genes, and new relationships
        between genes. However, this is only the beginning, and the genes identified in this manner
        will have to be tested in vivo, in systems like that of Baum and Perrimon, where specific
        functions can be assessed in time and space within the confines of real organisms. The
        focus must be to understand how all these molecular events and regulation cascades operate
        in individual cells to contribute to the generation of changes in a whole individual.
        Increasingly, the attention of developmental biologists is being drawn from genes and their
        products towards cells (Kaltschmidt and Martinez Arias 2002). The future, it seems to me,
        lies in the combination of in vitro systems, cell culture, and in vivo studies. I hope to
        apply this view in my analysis of the process of dorsal closure in 
        Drosophila embryos, as an example of how signalling pathways
        coordinate and regulate the activity of the cytoskeleton in the generation of shape and
        morphogenetic movements (Jacinto et al. 2002).
      
    
  

  
    
      
        
        Upon arrival from Europe, now more than two decades ago, I was taken aback by the level
        of violence in the American media. I do not just mean the daily news, even though it is
        hard getting used to multiple murders per day in any large city. No, I mean sitcoms,
        comedies, drama series, and movies. Staying away from Schwarzenegger and Stallone does not
        do it; almost any American movie features violence. Inevitably, desensitization sets in. If
        you say, for example, that 
        Dances with Wolves (the 1990 movie with Kevin Costner) is violent, people
        look at you as if you are crazy. They see an idyllic, sentimental movie, with beautiful
        landscapes, showing a rare white man who respects American Indians. The bloody scenes
        barely register.
        Comedy is no different. I love, for example, 
        Saturday Night Live for its inside commentary on peculiarly American
        phenomena, such as cheerleaders, televangelists, and celebrity lawyers. But 
        SNL is incomplete without at least one sketch in which someone's car
        explodes or head gets blown off. Characters such as Hans and Franz (“We're going to pump
        you up!”) appeal to me for their names alone (and yes, I do have a brother named Hans), but
        when their free weights are so heavy that their arms get torn off, I am baffled. The
        spouting blood gets a big laugh from the audience, but I fail to see the humor.
        Did I grow up in a land of sissies? Perhaps, but I am not mentioning this to decide
        whether violence in the media and our ability to grow immune to it—as I also have over the
        years—is desirable, or not. I simply wish to draw attention to the cultural fissures in how
        violence is portrayed, how we teach conflict resolution, and whether harmony is valued over
        competitiveness. This is the problem with the human species. Somewhere in all of this
        resides a human nature, but it is molded and stretched into so many different directions
        that it is hard to say if we are naturally competitive or naturally community-builders. In
        fact, we are both, but each society reaches its own balance between the two. In America,
        the squeaky wheel gets the grease. In Japan, the nail that stands out gets pounded into the
        ground.
        Does this variability mean, as some have argued, that animal studies cannot possibly
        shed light on human aggression? “Nature, red in tooth and claw” remains the dominant image
        of the animal world. Animals just fight, and that is it? It is not that simple. First, each
        species has its own way of handling conflict, with for example the chimpanzee (
        Pan troglodytes ) being far more violent than that equally close
        relative of ours, the bonobo (
        P. paniscus ) (de Waal 1997). But also within each species we
        find, just as in humans, variation from group to group. There are “cultures” of violence
        and “cultures” of peace. The latter are made possible by the universal primate ability to
        settle disputes and iron out differences.
        There was a time when no review of human nature would be complete without assertions
        about our inborn aggressiveness. The first scientist to bring up this issue, not
        coincidentally after World War II, was Konrad Lorenz (1966). Lorenz's thesis was greeted
        with accusations about attempts to whitewash human atrocities, all the more so given the
        Nobel Prize winner's native tongue, which was German. But Lorenz was hardly alone. In the
        USA, science journalist Robert Ardrey (1961) presented us as “killer apes” unlikely to ever
        get our nasty side under control. Recent world events have done little to counter this
        pessimistic outlook.
        The opposition argued, of course, that aggression, like all human behavior, is subject
        to powerful cultural influences. They even signed petitions to this effect, such as the
        controversial 
        Seville Statement on Violence (Adams et al. 1990). In the polarized
        mind-set of the time, the issue was presented in either-or fashion, as if behavior cannot
        be both learned and built upon a biological foundation. This rather fruitless
        nature/nurture debate becomes considerably more complex if we include what is usually left
        out, which is the ability to keep aggression under control and foster peace. For this
        ability, too, there exist animal parallels, such as the habit of chimpanzees to reconcile
        after fights by means of a kiss and embrace. Such reunions are well-documented in a
        multitude of animals, including nonprimates, such as hyenas and dolphins. They serve to
        restore social relationships disturbed by aggression, and any animal that depends on
        cooperation needs such mechanisms of social repair (Aureli and de Waal 2000; de Waal 2000).
        There are even indications that in animals, too, cultural influences matter in this regard.
        This may disturb those who write culture with a capital 
        C , and hence view it as uniquely human, but it is a serious possibility
        nonetheless.
        Nonhuman culture is currently one of the hottest areas in the study of animal behavior.
        The idea goes back to the pioneering work of Kinji Imanishi, who in 1952 proposed that if
        individuals learn from one another, their behavior may over time grow different from that
        of individuals in other groups of the same species, thus creating a characteristic culture
        (reviewed by de Waal 2001). Imanishi thus brought the culture concept down to its most
        basic feature, that is, the social rather than genetic transmission of behavior. Since
        then, many examples have been documented, mostly concerning subsistence techniques, such as
        the sweet potato washing of Japanese macaques (
        Macaca fuscata ) and the rich array of tool use by wild
        chimpanzees, orangutans (
        Pongo pymaeus ), and capuchin monkeys (
        Cebus spp.) (Whiten et al. 1999; de Waal 2001; Hirata et al.
        2001; Perry et al. 2003; van Schaik et al. 2003). However, much less attention has been
        paid to 
        social culture , which we might define as the transmission of social
        positions, preferences, habits, and attitudes.
        Social culture is obviously harder to document than tool use. In human culture, for
        instance, it is easy to tell if people eat with knife and fork or with chopsticks, but to
        notice if a culture is egalitarian or hierarchical, warm or distant, collectivistic or
        individualistic takes time and is difficult to capture in behavioral measures. A
        well-documented monkey example of social culture is the inheritance of rank positions in
        macaque and baboon societies. The future position in the hierarchy of a newborn female can
        be predicted with almost one hundred percent certainty on the basis of her mother's rank.
        Females with relatives in high places are born with a silver spoon in their mouth, so to
        speak, whereas those of lowly origin will spend their life at the bottom. Despite its
        stability, the system depends on learning. Early in life, the young monkey finds out
        against which opponents it can expect help from her mother and sisters. When sparring with
        peer A she may utter screams that recruit massive support to defeat A. But against peer B
        she can scream her lungs out and nothing happens. Consequently, she will come to dominate A
        but not B. Experiments manipulating the presence of family members have found that when
        support dwindles dominant females are unable to maintain their positions (Chapais 1988). In
        other words, the kin-based hierarchy is maintained for generation after generation through
        social rather than genetic transmission.
        Returning to the issue of aggressive behavior, here the effects of social culture can be
        felt as well. Without any drugs or brain lesions, one experiment managed to turn monkeys
        into pacifists. Juveniles of two different macaque species were placed together, day and
        night, for five months. Rhesus monkeys (
        Macaca mulatta ), known as quarrelsome and violent, were housed
        with the more tolerant and easy-going stumptail monkeys (
        M. arctoides ) (Figure 1). Stumptail monkeys easily reconcile
        with their opponents after fights by holding each others' hips (the so-called “hold-bottom”
        ritual), whereas reconciliations are rare in rhesus monkeys. Because the mixed-species
        groups were dominated by the stumptails, physical aggression was rare. The atmosphere was
        relaxed, and after a while all of the monkeys became friends. Juveniles of the two species
        played together, groomed together, and slept in large, mixed huddles. Most importantly, the
        rhesus monkeys developed peacemaking skills on a par with those of their more tolerant
        group mates. Even when, at the end of the experiment, both species were separated, the
        rhesus monkeys still showed three times more reconciliation and grooming behaviors after
        fights than typical of their kind (de Waal and Johanowicz 1993). Primates thus can adopt
        social behavior under the influence of others, which opens the door to social culture.
        Not unlike rhesus monkeys, baboons have a reputation for fierce competition and nasty
        fights. With the study by Robert Sapolsky and Lisa Share published in this issue of 
        PLoS Biology , we now have the first field evidence that primates can go
        the flower power route (Sapolsky and Share 2004). Wild baboons developed an exceptionally
        pacific social tradition that outlasted the individuals who established it. For years,
        Sapolsky has documented how olive baboons (
        Papio anubis ) on the plains of the Masai Mara, in Kenya, wage
        wars of nerves, compromising their rivals' immune systems and pushing up the level of their
        blood cortisol (Sapolsky 1994). An accident of history, however, selectively wiped out all
        the male bullies of his main study troop. As a result, the number of aggressive incidents
        dropped dramatically. This by itself was not so surprising. It became more interesting when
        it was discovered that the behavioral change was maintained for a decade. Baboon males
        migrate after puberty, hence fresh young males enter troops all the time, resulting in a
        complete turn-over of males during the intervening decade. Nevertheless, compared with
        troops around it, the affected troop upheld its reduced aggression, increased friendly
        behavior, and exceptionally low stress levels. The conclusion from this natural experiment
        is that, like human societies, each animal society has its own ecological and behavioral
        history, which determines its prevalent social style.
        It is somewhat ironic that at a time when researchers on human aggression are
        increasingly attracted, albeit with a far more sophisticated approach, to the Lorenzian
        idea of a biological basis of aggression (Enserink 2000), students of animal behavior are
        beginning to look at its possible cultural basis. There is no reason for animals with a
        development as slow as a baboon (with adulthood achieved in five or six years) not to be
        influenced in every way by the environment in which they grow up, including the social
        environment. How this influence takes place is a point of much debate, and remains unclear
        in the case of the peaceful male baboons in the Masai Mara. Given their mobility, the males
        themselves are unlikely transmitters of social traditions within their natal troop.
        Therefore, Sapolsky and Share look at the females for an answer—female baboons stay all
        their lives in the same troop. By reacting positively to certain kinds of behavior, for
        example, females may be able to steer male attitudes in a new direction. This complex
        problem is hard to unravel with a single study, especially in the absence of
        experimentation. Yet, the main two points of this discovery are loud and clear: social
        behavior observed in nature may be a product of culture, and even the fiercest primates do
        not forever need to stay this way.
        Let us hope this applies to humanity as well.
      
    
  

  
    
      
        
        In the wake of declarations supporting open access to research literature from
        international bodies including the Organization for Economic Cooperation and Development
        (OECD) and the United Nations' World Summit on the Information Society (WSIS), advocates
        and critics of the movement appear to have agreed that the issue warrants a robust, ongoing
        dialogue—a development undoubtedly in the interest of the scientific community, regardless
        of its ultimate outcome.
        To the extent that listserv messages, editorials, and conference presentations are
        representative of more widespread reactions to the debate, there appear to be a number of
        common misconceptions about what open access is and what problems it can or cannot solve.
        Over the next few months in 
        PLoS Biology , we plan to explore the more pervasive of these
        misunderstandings, in an effort to expose the real challenges that need to be overcome and
        to identify some possible solutions. Here we address the first of these—the perception that
        the publication-charge model puts an unfair burden on authors. Subsequently, we will
        address concerns about the long-term economic viability of the open-access model, the
        integrity and quality of work published in open-access journals, and the effect that open
        access will have on scholarly societies.
      
      
        Publication Charges—Nothing New
        By charging authors a fee to have their work published in lieu of charging readers to
        access articles, open-access publishers such as the Public Library of Science (PLoS) and
        BioMed Central (BMC) have transformed the traditional publishing system. This reliance on a
        seemingly untested revenue stream has generated skepticism that authors will be both
        willing and able to pay publication charges.
        Publication fees are not a phenomenon born of the open-access movement. Many authors
        regularly pay several thousands of dollars in page charges, color charges, correction
        costs, reprint costs, and other fees to their publisher, even when such costs are entirely
        voluntary. In the 
        EMBO Journal , for example, authors are allowed six pages of text free,
        but are then charged $200 per page beyond that. A review of recent issues shows that almost
        all authors exceed six pages, voluntarily paying on average over $800 to publish their
        articles.
        Furthermore, in addition to paying other publication charges, authors may be willing to
        pay extra for their articles to be made open access, as several publishers have recently
        recognized. A recent survey of authors in the 
        Proceedings of National Academy of Science (
        PNAS ) found that although 
        PNAS already makes its content freely available after six months, nearly
        50% of 
        PNAS authors expressed a willingness to pay an “open-access surcharge” of
        $500 or more to make their papers available for free online immediately upon
        publication—this above and beyond the $1,700 in page charges that the average 
        PNAS author already pays (Cozzarelli et al. 2004).
        Although we recognize that authors who submit to 
        PLoS Biology may well be a self-selected group of enthusiastic
        open-access supporters, we have found that nearly 90% of those who submit manuscripts do
        not request a fee waiver, and the few who do still offer to pay some portion of the
        fee.
        The concern about authors' ability to pay publication charges will become less pressing
        as governments, funding organizations, and institutions increasingly support open-access
        publication on their researchers' behalf. More funding agencies are joining the Howard
        Hughes Medical Institute, the Wellcome Trust, and others who have already designated funds
        for open-access publication. (For more information about these funders' announcements and
        other international policy statements relevant to open access, see
        http://www.plos.org/openaccess.)
        Universities, too, are supporting open access directly by setting aside funds for
        open-access publication through institutional memberships with BMC and PLoS or through
        discretionary funds that faculty can tap into to pay publication charges. Such approaches
        reduce authors' reliance on individual grants to support charges directly and ensure equal
        access to publishing options that require such payments.
      
      
        The Disenfranchised
        Even with the steady increase in sources to pay publication fees, detractors claim that
        open-access publishing may lead to a situation in which some authors are simply unable to
        publish their work due to lack of funds. The response to this concern is that the ability
        of authors to pay publication charges must never be a consideration in the decision to
        publish their papers. To ensure that this happens, PLoS has a firewall in place such that
        neither the editors nor the reviewers know which authors have indicated whether or not they
        can pay. Because all work judged worthy of publication by peer review should be published,
        any open-access business model should be designed to account for fee waivers, just as
        publishers have always absorbed some authors' inability to pay page and color charges. PLoS
        grants full or partial publication-charge waivers to any author who requests them, no
        questions asked.
        In part, the savings to institutions, hospitals, nongovernmental organizations, and
        universities provided by open-access publications could help to establish funds for
        researchers who are less well supported. In the developing world, as free online access to
        scientific literature is increasingly seen as a political imperative, organizations such as
        the World Health Organization, the Oxford-based International Network for the Availability
        of Scientific Publications, and Brazil's SciELO are likely to become more willing to pay
        open-access publication charges for authors who cannot afford them. The Open Society
        Institute (OSI) already pays such costs for universities and other organizations in a
        number of countries in which the foundation is active by way of a PLoS Institutional
        Membership that grants waived publication charges to authors while providing compensatory
        revenue for PLoS.
        Perhaps the real misconception about the unfair burden that open access places on
        authors resides in the terminology—the term “author charge” is itself misleading.
        Publication fees are not borne purely by authors, but are shared by the many organizations
        whose missions depend on the broadest possible dissemination and communication of
        scientific discoveries. Some of those may provide funding for open-access publication as
        intermediaries between authors and journals, as OSI does. Others—including many
        government-financed funding agencies—do so directly through their research grants to
        scientists. In both cases, funding open access is an effective way to fulfill mandates for
        public access to and accountability over scientific research and to ensure that all worthy
        research is published.
      
    
  

  
    
      
        
        For the estimated 800 million people, living largely in developing countries, without
        enough food to eat, the main food risk is starvation. But if you ask, ‘When does food
        actually kill?’ in a country such as the United Kingdom, ‘Not that often’ is the short
        reply you would give after reading Hugh Pennington's book 
        When Food Kills: BSE, E. coli, 
        and Disaster Science . The two food-borne diseases that occupy much of
        the book, 
        Escherichia coli 0157 and bovine spongiform encephalopathy
        (BSE), kill humans very rarely, although the ramifications and implications of these few
        deaths for science, regulators, and government are large.
        As Pennington clearly explains, there is still much uncertainty in the science of BSE,
        and the eventual UK death toll from the human form may be as low as a few hundred, with
        even the most pessimistic expert assessments putting the upper bound as fewer than 5,000.
        Food-borne 
        E. coli 0157 kills fewer than a dozen people a year in the
        UK.
        Whilst each death is a terrible tragedy and an indescribably harrowing experience for
        those close to the victim, these figures are small when compared with other ways in which
        food kills. Epidemiologists estimate that the dietary contributions to cardiovascular
        disease and cancer between them kill more than 100,000 people a year in Britain. Yet we
        hear much more about BSE and 
        E. coli as food risks. For instance, a recent study by the
        King's Fund (http://www.kingsfund.org.uk/pdf/healthinthenewssummary.pdf) reports that the
        rate of news coverage in the UK of a death from variant Creutzfeldt-Jakob disease, the
        human form of BSE, is nearly 23,000 times that for a death from obesity.
        In his characteristically diverting and obscurely erudite way, Pennington describes this
        discrepancy between public perception and magnitude of risk by referring to an article on
        railway accidents published in 1859 by one Dionysius Lardner. The systematic and much more
        revealing analyses of risk perception by psychologists such as Paul Slovic over the past 25
        years do not get a mention.
        In fact, one of the hallmarks of Pennington's style is his enthusiasm for taking his
        reader down little-known historical byways. Whether it be the drowning (possibly suicide)
        of King Ludwig II of Bavaria in the Starnberger See or the treatment of James Norris in
        Bethlehem Lunatic Asylum in 1814, Pennington has an almost endless supply of anecdotes to
        provide peripheral colour to his main narrative. Indeed, on some occasions his delight in
        the detail makes it hard to see where the main narrative is leading, although his aim is to
        show that similar conclusions can be drawn about risk management in food, transport, oil
        rigs, and other fields.
        Anyone who has heard Hugh Pennington speak will know that he has a remarkably direct and
        engaging style, which he translates into the written word with verve. Already on page 2, he
        gets us into the mood by referring to a sample from a five-year-old girl sent for analysis
        at the start of the Lanarkshire 
        E. coli outbreak of 1996: ‘It was a stool. The word carries the
        impression of firmness, even of deliberate effort in its production. Hers was not’. His
        laconic sense of humour is also reflected in many of the wittily irrelevant or tangential
        photographs. My personal favourites are ‘Her Majesty in Gloves’ on page 44 and ‘Turds on
        Campsite Track’ on page 101.
        The Lanarkshire 
        E. coli 0157 outbreak, which in late 1996 affected 202 people
        and killed eight, was very much Pennington's show. He chaired the public enquiry that led
        eventually to a change in the law, requiring all butchers in the UK handling cooked and raw
        meat to be licensed. The license itself is less important than the training in food safety
        management principles that precedes it. The butcher John Barr (and his staff), whose shop
        was the primary source of the outbreak, apparently did not know that you have to keep raw
        meat and ready-to-eat products separate to avoid cross-contamination with dangerous
        pathogens, such as 
        E. coli 0157, that can occur in raw meat. Pennington's
        authoritative and blow-by-blow account shows failings not only in the butcher (who was,
        incidentally, Scottish Master Butcher of the Year in 1996), but also in the inspectors who
        had visited his shop eight times in the previous two years. They had not, apparently,
        picked up that Barr and his staff employed the same knives for cutting up raw and cooked
        meat, nor that they used a ‘biodegradable’ cleaning fluid, not realising that this is not
        the same as ‘biocidal’.
        The second theme, BSE, is given somewhat shorter treatment. Nevertheless, Pennington
        goes into some detail in assessing the prion theory of transmissible spongiform
        encephalopathies (he argues that a nucleic acid is not also involved). He also reviews the
        sequence of events that led the UK government in the early 1990s to conclude that there was
        not likely to be a risk to human health and to be slow to change its view. This and the
        concluding part of the book (see below) draw heavily on the Phillips Enquiry into BSE.
        Although this enquiry focussed on the response of the UK government, its lessons are
        relevant to other countries where BSE has emerged in recent years, including many European
        countries, Japan, Canada, and the United States.
        In his book 
        Mountains of the Mind , Robert Macfarlane writes: ‘[F]or the hunter risk
        wasn't optional—it came with the job. I sought risk out, however. I courted, in fact paid
        for it. This is the great shift which has taken place in the history of risk…. [I]t became
        a commodity’. Pennington reflects a similar shift in attitude to food risk over the past
        half century or so. Back in 1938, although it was known that over 2,500 people a year in
        Britain died from drinking raw milk, the risk was not seen as large enough to warrant
        legislation to make pasteurisation compulsory. We are now used to much higher standards of
        food safety, and we can, as a society, enjoy the luxury of fear of relatively minor
        risks.
        Nevertheless, there are important lessons from past failures for all involved in food
        safety (and in other areas of risk management), and Pennington discusses some of these in
        his concluding chapters. He emphasises the need to continually review the evidence
        underpinning risk assessments, to communicate effectively with the media, to ensure that
        actions to manage risks are effectively implemented and audited. Notably, he refers to the
        importance of inclusiveness and openness about risk and uncertainty in decision-making:
        ‘[I]f [this] becomes the norm, it will be possible to say that good has come out of
        tragedy’.
      
    
  

  
    
      
        
        It is becoming increasingly apparent that the vast blue expanse of ocean—the last
        frontier—is not as inexhaustible as it once seemed. While we have yet to fully explore the
        reaches of the sea, technology has granted humans the ability to harvest its wealth. We can
        now fish anywhere, at any depth, for any species. Like the American frontier range's bison
        and wolf populations brought to the brink of extinction swordfish and sharks are the
        ocean's most pursued prizes. The disadvantages associated with the depth and dimensions of
        this open range, however, have long obscured the real consequences of fishing. Indeed,
        scientists have the formidable challenge of assessing the status of species whose home
        covers over 75% of the earth.
        Three recent highly publicized papers—a trifecta detailing troubled waters—call
        attention to overfishing's contributions to the dramatic declines in global fisheries.
        Delving into the past, Jeremy Jackson and colleagues (2001) combined local historic records
        with current estimates to detail the ecological impacts of overfishing, Reg Watson and
        Daniel Pauly (2001) drew attention to distortions of global catches, and Ransom Myers and
        Boris Worm (2003) highlighted the depletion of the majority of the largest ocean predators.
        While some have valid criticisms of the assumptions and aggregation of historic data used
        to assess the global situation, few disagree with the overriding conclusion that humans
        have drastically altered not only fish biodiversity, but, increasingly, the ocean
        itself.
        Recent reports by the United Nation's Food and Agriculture Organization (FAO) which
        maintains the world's most complete global fisheries database, appear to validate the
        conclusions of these studies. The most recent FAO report states that 28% of global stocks
        are significantly depleted or overexploited, and 47% are either fully exploited or meet the
        target maximum sustainable yield. Only 24% of global stocks are either under- or moderately
        exploited. As the sea is increasingly harvested, many ecologists wonder how the ecosystem
        will continue to function (Jackson et al. 2001). Although economic and social
        considerations often supercede scientific assessments, science will continuously be called
        upon to deliver management options that will straddle the needs for conservation and
        production, even in areas where there is only subsistence fishing (Box 1). As scientists
        debate the details of global fisheries assessment, they are also including studies of the
        long-term ecosystem effects and options for recovery efforts. Like was done on the open
        range, shall we conserve or farm the sea—or both?
      
      
        Catches, Collapses, and Controversies
        The FAO began keeping fisheries records in 1950. Unfortunately, an enormous amount of
        data comes directly from each country's fishing industry, which is often biased as a result
        of unreported discarding, illegal fishing, and the misreporting of harvests. For example,
        mid-level Chinese government officials seeking promotions systematically enhanced China's
        fisheries numbers in recent years—which inflated and skewed international catch rates.
        The FAO data show that catches, excluding a recent surge in anchoveta and China's
        suspect numbers, reached a peak of 80 million metric tons in the late 1980s and have since
        begun to decline. Regional studies validate these trends. “Most of the line fish around the
        coast of South Africa are depleted to 5%–15% of pristine levels,” says George Branch, a
        marine biologist from the University of Cape Town (Cape Town, South Africa). Meryl
        Williams, Director General of WorldFish in Penang, Malaysia, notes that the Asia-specific
        database called TrawlBase (www.worldfishcenter.org/trawl/) confirms that the region's
        commercial species have been depleted to 10%–30% of what they were 30–40 years ago.
        Obtaining accurate information on highly migratory species is challenging, to say the
        least. It is not hard to imagine that data quality is the biggest disadvantage to any
        scientific assessment. Of the 50 managed stocks in the northeast Atlantic Ocean—including
        invertebrates, sport fishes, and major commercial finfish—data are kept on only one-fifth
        of the species. There are 250 fish species in the region, but only 55 species are of
        commercial interest and merit inquiry. “We know next to nothing about noncommercially
        fished species,” notes Jeff Hutchings, a conservation biologist at Dalhousie University
        (Halifax, Nova Scotia, Canada). And that is where fisheries have adequate access to current
        monitoring programs. “With the recent expansion of the Taiwanese and Chinese fleets, we
        don't have the kind of sampling programs needed for those kinds of fisheries,” says Rick
        Deriso, a fisheries scientist with the Inter-American Tropical Tuna Commission (IATTC) (La
        Jolla, California, United States).
        Couple these inadequacies with previously unknown bycatch rates (i.e., the fish caught
        in addition to the target catch) and illegal catches, and it is easy to see that the task
        is formidable. The FAO estimates that roughly one-quarter of the marine commercial catch
        destined for human consumption—some 18–40 million metric tons of fish—is thrown back in the
        sea, a harvested catch that is never utilized or counted. It is estimated that the illegal,
        unreported, and unregulated (IUU) fisheries surpass allowed fishing quotas by 300%. IUU
        fishers operate in areas where fishing is not permitted, use banned technologies or
        outlawed net types, or underreport catches. “The IUU fishery for Patagonian toothfish
        expanded rapidly in the mid-1990s, likely on the order of 20–30 vessels,” says Andrew
        Constable, an ecological modeler at the Australian Antarctic Division (Kingston,
        Australia), who also works with the Scientific Committee of the Commission for the
        Conservation of Antarctic Marine Living Resources (Hobart, Australia). “These rates of IUU
        fishing could reduce stocks to threshold levels in some areas in two to five years,” he
        adds.
        Often overlooked is the inescapable fact that even sustainable harvest rates reduce fish
        populations quickly. “If the goal is a productive fishery, we're automatically talking
        about up to a 70% decline in population across the board,” says Deriso. The FAO's Chief of
        Marine Resource Services, Jorge Csirke, states that “from a stock point of view, there is
        no way to preserve integrity of wild stocks and exploit them at the same time.” Indeed, the
        United States' National Marine Fisheries Service (NMFS) considers optimal harvest rates to
        be between 40%–60% of virgin levels. But once fish populations dip below the 10%–20% mark,
        declines are of serious concern.
        Atlantic cod in Canadian waters suffered a total population collapse and are now on
        Canada's endangered species list (Figure 1). From 2 billion breeding individuals in the
        1960s, Atlantic cod populations have declined by almost 90%, according to Hutchings. While
        advisors called attention to declining cod stocks, Constable notes that by the time a
        significant declining trend has been detected by traditional catch assessments, stocks are
        likely to be in poor shape, if not already depleted.
        Given the task of compiling data on only the economically important species, fisheries
        biologists developed a single-species management approach in the 1960s, which assumed that
        fisheries affect each species in isolation. This approach, although now rife with problems,
        served the community and the politicians well during the decades of abundant resources.
        “They brought the approach of single-species management to near-perfection,” says Boris
        Worm, a marine ecologist at the Institute for Marine Science in Kiel, Germany. A growing
        discontent with the model, in addition to greater awareness of ecological interactions,
        however, prompted Worm and his Dalhousie University colleague Ransom Myers to question the
        sustainability of the single-species approach. Attempting a comprehensive assessment, their
        widely cited recent paper (Myers and Worm 2003) indicated that the global ocean has lost
        more than 90% of large predatory fishes, such as marlin, sharks, and rays.
        However, this new approach to assess fish stocks is not without its critics. Fisheries
        biologists point out that the nuances of management contained in fisheries data—such as
        altered fisher behavior, the variable “catchability” of individual species, and altered
        gear use—were discounted in the Myers and Worm (2003) assessment and led to
        misinterpretations for some species, notably tropical tunas (Figure 2). A number of tuna
        biologists have expressed concern that these omissions have left the mistaken impression
        that all tuna species are among the list of declining predators (Hampton et al 2003). Worm
        acknowledges that his approach can be improved, but says, “The whole point of our paper was
        to aggregate species to communities to see what the overall ecosystem is doing.”
      
      
        Ecosystem Sustainability
        Despite the controversy, most agree that the large predators, particularly sharks,
        skates, rays, and marlin, are in the most dire straits. Unlike other lower-trophic order
        species, the wholesale removal of top predators has enormous effects on the rest of the
        ecosystem. One consequence is that overall reproduction rates can potentially suffer. Fish
        size, gender, and age at maturity have a substantial impact on individual species'
        reproduction rates. Since larger fish are the most susceptible to fishing, the population's
        age structure can shift as individuals, particularly females, are fished out. For example,
        a 23-inch (59-cm) female vermilion rockfish can produce 17 times the young of a 14-inch
        (36-cm) fish. Given uncertainties with population dynamics, the fact that basic biological
        data are missing makes the job even harder. While knowledge of these components is still
        quite spotty, tuna inventories, for example, have started collecting gender data on
        catches.
        Daniel Pauly, a fisheries biologist at the University of Vancouver (Vancouver, British
        Columbia, Canada), has shown that increased fishing has caused the industry to “fish down
        the food web,” or systematically move to lower trophic levels over time as higher ones were
        depleted (Pauly et al. 1998). The impact to ecosystems is only beginning to be uncovered.
        “If you fish out an abundant predator, the species that it was eating or competing with
        will increase,” says Worm. “The problem is that the ecosystem may change in such a way that
        recovery is inhibited because a species niche space is taken or altered.”
        Fisheries science has taken steps to increase the quality of data in recent years.
        “Traditional fishery models assumed that a fishery was a homogenous thing—like bacteria in
        a bottle—rather than a spatially diverse system,” says Pierre Kleiber, a fisheries
        biologist with the Pacific Islands Fisheries Science Center of the NMFS (Honolulu, Hawaii,
        United States). He adds that recent work accounts for spatial diversity. In addition,
        fisheries are now dealing with the inherent uncertainty of their work and are factoring
        that into models and decision-making. “Uncertainty didn't used to be dealt with at all in
        formulating fishery management advice,” confirms Keith Sainsbury, a marine ecologist with
        the Commonwealth Scientific and Industrial Research Organisation (CSIRO) (Clayton, South
        Victoria, Australia), adding that its absence gave rise to an awful lot of troubles.
        “Traditional models tended to assume perfect data with no holes in it,” says Kleiber. “Now
        we've tried to craft a model to fit the realities of missing data.”
        As well as incorporating spatial diversity and uncertainty, researchers are beginning to
        comprehend the ecological damage caused by different types of fishing gear. Indeed,
        trawling the bottom of the seafloor for groundfish can destroy a half-acre footprint of
        habitat (Figure 3). Detailed reports document that, depending on the habitat's stability,
        bottom trawling can not only remove fish from seafloor habitats, but alter bottom relief
        such that it compromises the ability of other fish to survive (NRC, 1002). In Australia,
        for example, lingcod rely on undisturbed bottom relief to lay their eggs, while other
        groundfish species depend on complex seafloor habitats for the majority of their food.
        “Science is getting more realistic, but it is getting more difficult,” says Branch.
        Ecological models are far more complex than traditional fisheries models, says Csirke,
        adding that more model variables make it more difficult to apply to fisheries, an industry
        whose focus is, understandably, not conservation. Despite its incorporation into national
        fisheries policies, ecosystem-based management remains a loosely defined term. It is not a
        well-defined concept because it is not possible to optimize every species, says Deriso.
        An additional concern to scientists is that of biomass resilience in the face of
        environmental changes. Francisco Chavez, a biologist with the Monterey Bay Aquarium
        Research Institute (Moss Landing, California, United States), recently demonstrated that
        over a 25-year period, warmer and cooler Pacific waters tilt the distribution of anchoveta
        versus sardines, both open-ocean dwellers (Chavez et al. 2003). Indeed, El Niño influenced
        the crash of the heavily fished Peruvian anchoveta industry in the late 1970s. These
        examples illustrate how susceptible fisheries are to environmental fluctuations. When the
        biomass of a population is reduced, it is much more sensitive to environmental change. We
        do not know how environmental fluctuations like these will affect the natural production of
        young fish, says Kleiber, expressing the concern that without a better understanding of
        climate, fisheries scientists end up trying to estimate moving targets.
        In the end, many scientists have their doubts about the influence of science on
        decision-making. “My personal view is that it's naïve to think that modifying and improving
        models will necessarily lead to improved natural resource management,” says Simon Jennings,
        a fisheries biologist with the United Kingdom's Centre for Environment, Fisheries and
        Aquaculture Science in Lowestoft. Indeed, the International Council for the Exploration of
        the Seas (Copenhagen, Denmark) recently recommended a total ban on North Sea and Irish Sea
        cod stocks, based on single-species assessment. Although the more intensive ecosystem-based
        models could not have produced a more stringent recommendation, politicians allowed
        harvests at roughly half of last year's catch.
      
      
        To Conserve or to Farm?
        While lowering fisheries' effort seems the most logical approach to the recovery of
        depleted fisheries, social and economic concerns often stymie political action. Yet demand
        for seafood continues. Therefore, scientists also are investigating both conservation and
        alternative production options.
        Given the social, economic, and political problems associated with that, managers have
        often used closures to help a hard-hit species recover. In many cases, however, the
        recovery time for exploited species is longer than once thought (Hutchings 2000). “Based on
        the available information, it is not unusual for fish populations to show no or little
        recovery even after 15 years,” says Hutchings. “All else being equal, we predict the
        earlier the age of maturity, the faster the rate of recovery,” he adds. And that depends on
        environmental conditions as well. “In the case of Antarctic species, some overexploited
        populations remain at less than 5% pre-exploitation abundance after 30 years,” says
        Constable.
        One management strategy to recover species is to create marine protected areas (MPAs),
        zones that restrict all removal of marine life (Box 2). A number of marine ecologists are
        staunch supporters of MPAs for both conservation and fishery's recovery. What looked like
        sustainability in the past were fisheries out of our reach—naturally protected areas—says
        Pauly, adding that our increasing ability to harvest fisheries necessitates the creation of
        MPAs now. In theory, these areas are refugia for fishes to reproduce, spilling over not
        only healthy adults but also potentially transporting thousands of viable young—seeding
        surrounding waters. To date, less than 1% of the ocean's area is protected, which hinders
        the ability to conclusively determine if spillover rates have the predicted impact on
        fishery's recovery.
        A review of 89 studies of MPAs by Ben Halpern, a student at the University of
        California, Santa Barbara (Santa Barbara, California, United States), demonstrated that the
        average number of fish inside a reserve increases between 60%– and 150% (Halpern 2003). In
        addition, 59% of the sites had increased diversity. While the numbers inside the reserves
        look good, the crucial condition of larval spillover has yet to be proven. Most scientists
        involved in the debate agree that MPAs should be one component in an overall management
        scheme, but worry that until the crucial element of fishing effort is resolved, MPAs may
        just displace the vast industrial fleets.
        In terms of simply producing fish for global food needs, aquaculture (also known as fish
        farming) is another, increasingly popular, option. In 2001, the European Union produced 17%
        of total fishery's production via aquaculture. These numbers are projected to steadily
        increase, but some question whether aquaculture would be sufficient to supply what has been
        lost by overexploited fisheries.
        Concentrated in coastal areas, aquaculture has aroused numerous concerns. Indeed, in
        developed countries, most operations grow carnivorous fish, which necessitates growing fish
        to feed fish. While the process has become more efficient in recent years, due in part to a
        growing reliance on vegetarian diets, it still takes about 3 pounds (1.36 kg) of fish to
        create 2.2 pounds (1 kg) of desirable meat (Aldhous 2004). Yet, the total catch of food
        fish continues to grow, as do concerns about nutrient runoff and estuary pollution
        resulting from aquaculture. Increasingly, coastal residents often complain about the
        aesthetics of such activities, and there is also new research that indicates that
        farm-raised fish harbor more cancer-causing pollutants than wild species (Hites et al.
        2004).
        To alleviate many of these concerns, open-ocean aquaculture is now being considered.
        Indeed, the NMFS is set to propose a Code of Conduct for Offshore Aquaculture, which would
        open up the 200-mile (322-km) United States Exclusive Economic Zone to net pens seaward of
        coastal state boundaries and authorities. The Sea Grant program in conjunction with
        interested business, is also currently assessing the carrying capacity of open-water pens
        as well as their potential environmental impact. Given increased industrial interest and
        unchanging demand for seafood, many think farming the sea may be around the corner.
        Undoubtedly, scientific effort will continue to inform both conservationists and
        industry about fisheries' capacity and potential recovery options. As attitudes towards
        fisheries continue to change, increased understanding of the ecological underpinnings
        should help strike a more informed balance between fisheries' conservation and production.
        “The big mistake is suggesting that you can manage fish stocks,” says Niels Daan, a
        biologist with the Netherlands Institute for Fisheries Research (IJmuiden, The
        Netherlands). “In my opinion, we can only manage human activity.”
      
      
        
      
    
  

  
    
      
        
        We are two of the scientist members of the President's Council on Bioethics. In late
        2001, we were invited by the President of the United States to serve on this Council. The
        Bioethics Council was appointed by the President to “monitor stem-cell research, to
        recommend appropriate guidelines and regulations, and to consider all of the medical and
        ethical ramifications of biomedical innovation…. This council will keep us apprised of new
        developments and give our nation a forum to continue to discuss and evaluate these
        important issues.”
        This was a difficult invitation to accept. On the one hand, the President's views on the
        use of human embryonic stem cell research and somatic cell nuclear transfer techniques were
        well-known and in conflict with our own beliefs about the costs and benefits of the use of
        progressive technologies to advance biomedical research. On the other hand, we were
        grateful that the President, despite his views in opposition to these therapies, was
        willing to invite serious biomedical scientists to help formulate advice to him—and
        ultimately to contribute to the development of national policy—on these critically
        important advances.
        We knew that on this originally 18-member (but for most of the past two years a
        17-member) Council, as scientists we would be in the minority in our belief of the good to
        be gained through these and other areas of biomedical research. We were also aware that
        some others on the Council had strong opposing views. Thus, it was only with the assurances
        of the Council chairman, Leon Kass of the University of Chicago, and of the President of
        the United States himself that we were persuaded that our voices would be heard and
        integrated into the statements of the Council. Furthermore, we felt, and continue to feel,
        that bioethical issues are important not only to all biologists, but also to society at
        large, and thus especially worthy of engaging debate and discussion.
        Two recently issued reports of the Council, “Beyond Therapy: Biotechnology and the
        Pursuit of Happiness” (http://bioethics.gov/reports/beyondtherapy/index.html) and
        “Monitoring Stem Cell Research” (http://bioethics.gov/reports/stemcell/index.html), are
        therefore of deep concern to us. We discuss them in turn below.
      
      
        Concerns about the “Beyond Therapy” Report
        The “Beyond Therapy” report deals with issues of direct concern for every thoughtful
        person. However, in the interests of setting straight the record of our views, as Council
        members and scientists, on the content of this report and for a proper assessment of the
        scientific content of the “Beyond Therapy” report, we feel it is important to point out
        aspects of the report for which we had requested revisions and for which those requests
        were declined.
        In the discussions of preimplantation genetic diagnosis, the specter of designer babies
        is raised by implying that selecting embryos for intelligence and other traits, such as
        temperament is a possibility. Scientifically, this simply is highly unlikely and indeed may
        not even be feasible. While such scientific unlikelihood is mentioned in passing in the
        report, it is easy to take away from the report the feeling that such genetic manipulation
        will happen and is even imminent.
        The report also claims that “the underlying impulse driving age-retardation research is,
        at least implicitly, limitless, the equivalent of a desire for immortality.” Furthermore,
        the title of Chapter 4 of the report, “Ageless Bodies,” implies that immortality is the
        goal of this research, despite all reliable scientific evidence to the contrary. Such a
        title is not consistent with the knowledge, stated in that chapter, that there is no
        scientific basis for immortality and implies that, by seeking to maintain and extend
        “youth,” research into aging, including stem cell research, is predominantly to serve
        vanity. Also, without presenting scientific or reliable evidence, the report presents the
        opinion that research into prolonging healthy life may result in a lifetime obsession with
        immortality. Hence, this chapter in the report falls short of explaining the serious
        challenge of preventing and curing age-related disease to extend health—very different from
        attempting immortality.
        The same chapter offers a sensational quote from a researcher that “the real goal [of
        aging research] is to keep people alive forever.” The request that quotes from researchers
        more representative of the biomedical research community also be included was declined.
        This leads to a misleading misrepresentation of the motivation of reputable researchers in
        the field of aging.
        In suggesting that slowing biological aging may increase the disjunction between “social
        aging” (the age at which children are exposed to “adult” images and concepts) and
        “biological aging” (expected lifespan), only one view, a conservative one, of the supposed
        “best” way to raise children is presented. The report also suggests, with no clear
        reasoning behind it, that longer lives will somehow undermine human determination to
        contribute as much as one can during a lifetime. Despite requests for inclusion of material
        that would allow for a balanced treatment of these topics, the report minimized discussion
        of potential positive aspects of slowing biological aging, such as prolonged good
        health.
        Finally, the report repeatedly emphasizes a “profound and mysterious” link between
        longevity and fertility, thereby leaving the reader with the distinct but erroneous
        impression that anything done to extend healthy life will be traded for decreased
        fertility, despite the fact that current scientific literature, which was made available
        for inclusion in the report, shows a 
        lack of any necessary 
        mechanistic linkage of the two.
      
      
        Concerns about the “Monitoring Stem Cell Research” Report
        With respect to the “Monitoring Stem Cell Research” report, we feel that some facts that
        would help the public and scientists better assess the content of the report were not
        brought out clearly or were omitted entirely. First, from the published scientific
        literature in peer-reviewed journals on stem cells, a major message can be distilled:
        namely, the vast difference that currently exists in our understanding of, and the
        potential utility of, embryonic versus adult stem cells as sources of material for research
        and clinical purposes. In brief, human stem cells have been isolated from a variety of
        embryonic, fetal, and adult tissue sources. However, enormous differences exist in purity,
        properties, data reproducibility, and understanding of cells from these different sources.
        Much of our ignorance is related to the relative paucity of funding for research using
        embryonic stem cells.
        Years of rigorous and careful research in animal models have documented that embryonic
        stem cells have great utility for scientific studies. This work has also rigorously and
        reproducibly established the great plasticity of these cells and supports the opinion that
        human embryonic stem cells possess the greatest broadest potential and promise for clinical
        applications. As well as therapeutic uses, important potential applications include studies
        of embryonic stem cells bearing complex genotypes susceptible to poorly understood common
        human diseases and testing and screening drug efficacy.
        The report does not make clear that the best-characterized adult stem cells are
        hematopoietic stem cells. Currently, major difficulties and inadequate understanding exist
        with most other types of adult stem cells reported to date. In addition, many experiments
        suggesting that adult stem cells have broad plasticity may be incorrectly interpreted owing
        to an error caused by an experimental artifact of cell fusion present in some unknown
        proportion of the experiments. Research on some of the reported adult stem cell
        preparations may conceivably in the future demonstrate that they, too, like hematopoietic
        stem cells, can also be prospectively identified, “single cell cloned,” expanded
        considerably by growth in vitro with retention of normal chromosome structure and number,
        and preserved by freezing and storage at low temperatures. But it should be strongly
        cautioned that this has not been done for most adult stem cell preparations, and, even if
        possible, it is not clear that any of the just-mentioned procedures will be accomplished in
        the near future, owing to the technically very demanding nature of such experiments.
        We feel it is important to emphasize a point that the report mentioned, that the
        reported isolation and properties of multipotent adult progenitor cells (MAPCs) must be
        reproduced in additional laboratories for any reliable interpretation of the results
        reported with these cells. After considerable effort, this has still not been achieved.
        Thus, in the reported results, the possible significance of the reported isolation and
        properties of human MAPCs is left unclear, as is their potential as a source of stem cells
        for clinical purposes. Hence, a strong overall caution is that many of the reports on the
        properties of cells differentiated from adult stem cell preparations are to date
        preliminary and incomplete. If results with any isolated and characterized adult stem cells
        are validated, it will then be very important to compare their properties—and those of any
        more differentiated cells that can be derived from them—with other stem cell sources, such
        as the well-characterized hematopoietic stem cells, and with human embryonic stem cell
        preparations.
        Two major considerations argue strongly for non-commercial, federal, peer-reviewed
        funding to be made available for this work. The first is the sustained effort this work
        will require. The second is the importance of reliable and unbiased design of experiments
        and of open, public availability of the complete findings.
      
      
        Reasons for Our Concern
        In being concerned about the content of these reports, neither of which makes any
        recommendations for legislative or policy actions, are we worrying too much? We think not.
        Indeed, already, sadly as a result of the way the sections on aging research in the report
        were written, the myth that longevity has an inevitable tradeoff of diminished fertility is
        now gaining a further foothold: witness the January 26, 2004, issue of the 
        The New Republic . In it, an article about this report of the Council
        falls right into the trap: it states, “But changes come with longer life. Worms and mice
        that are altered for extended lifespans become sterile, or barely reproduce.” The public is
        done a disservice when science is presented incompletely; myths are then perpetuated.
        This is but one example of the dangers that three of the Council members who are
        scientists (the two of us along with Michael Gazzaniga of Dartmouth College) pointed out,
        in a Commentary within the edition of the “Beyond Therapy” report published by the Dana
        Foundation in November 2003. In that Commentary, we stated that “Our concern … is that,
        moving forward, the debate carry on with all of the scientific evidence—or as much as such
        a widespread public discussion can include—and take care not to leave an erroneous
        impression as to the nature of the potential problems at hand.” We ended the Commentary by
        saying “We urge both good reading and 
        critical reading!” (our italics).
        These reports had as their premise the aim of neutrality in the scientific analysis of
        the issues addressed. But our concern is that some of their contents, as in the few
        examples outlined above, may have ended up distorting the potential of biomedical research
        and the motivation of some of its researchers. Continuing discussions will form the basis
        for future decisions on these topics; keeping such discussion open and balanced is of
        paramount importance.
      
      
        
      
    
  

  
    
      
        
        In 1967, mule deer in a research facility near Fort Collins, Colorado, in the United
        States apparently began to react badly to their captivity. At least, that was the guess of
        researchers working on the natural history and nutrition of the deer, which became listless
        and showed signs of depressed mood, hanging their heads and lowering their ears. They lost
        appetite and weight. Then they died—of emaciation, pneumonia, and other complications—or
        were euthanized. The scientists dubbed it chronic wasting disease (CWD), and for years they
        thought it might be caused by stress, nutritional deficiencies, or poisoning. A decade
        later, CWD was identified as one of the neurodegenerative diseases called spongiform
        encephalopathies, the most notorious example of which is bovine spongiform encephalopathy
        (BSE), more commonly known as mad cow disease. Nowadays, CWD is epidemic in the United
        States. Although no proof has yet emerged that it's transmissible to humans, scientific
        authorities haven't ruled out the possibility of a public health threat. The media have
        concentrated on this concern, and politicians have responded with escalated funding over
        the past two years for fundamental research into the many questions surrounding this
        mysterious disease.
        Quite apart from how little is yet known about CWD, media interest is reason enough to
        step up investigation of it, says Mo Salman, a veterinary epidemiologist at Colorado State
        University in Fort Collins. He's been scientifically involved with BSE, since it was first
        discovered among cattle in the United Kingdom in 1986. He recalls predicting that lay
        interest in BSE would wane after five years. Instead, the disease was found in the
        mid-1990s to be capable of killing humans who ate tainted beef. “I was wrong, and it really
        changed my way of thinking, to differentiate between scientific evidence and the public
        perception,” Salman admits. “Because CWD is similar to BSE, the public perception is that
        we need to address this disease, to see if it has any link to human health.”
        
          
            CWD is the only spongiform encephalopathy known to naturally infect both
            free-ranging and captive animals, a situation that greatly complicates efforts to
            monitor, control, or eradicate it.
          
        
      
      
        Increasing Attention
        In 2001, the United States' Department of Agriculture (USDA) declared an emergency after
        CWD was first diagnosed in deer east of the Mississippi River, indicating a potential
        nationwide problem. This year, the USDA is developing a herd certification program to help
        prevent the movement of infected animals in the game farming industry. This will bolster
        monitoring already underway in virtually every state, including postmortem examinations of
        game killed by hunters and by sharpshooters in mass culling operations.
        By June 2003, brain tissue from more than 111,000 animals had been sampled in North
        America, and 629 were found to have tested positive for CWD. That's a small epidemic
        compared to the thousands of BSE cases detected in cattle in the United Kingdom, but CWD is
        thought to be slow-spreading and perhaps lurking undiscovered elsewhere. So far, the United
        States and Canada are the only countries in which it has been identified, apart from a few
        imported cases in the Republic of Korea, but surveillance has not been thorough in North
        America and is virtually nonexistent in the rest of the world.
        Considered 100% fatal once clinical signs develop, CWD has struck three species of the
        cervid family—mule deer, white-tailed deer, and Rocky Mountain elk—which roam wild and are
        raised on farms for meat and hunting. It's the only spongiform encephalopathy known to
        naturally infect both free-ranging and captive animals, a situation that greatly
        complicates efforts to monitor, control, or eradicate it.
        The economic costs are hard to quantify, but a 2001 survey by the United States
        Department of Commerce's Bureau of Census shows that big-game hunters nationwide spend more
        than US$10 billion annually for trips and equipment. By far, their main target is deer.
        Wildlife watching of large land mammals, principally deer, drew 12.2 million participants
        in 2001. The North American Deer Farmers Association represents owners of 75,000 cervid
        livestock raised for their meat and for velvet antler, a health-food supplement made from
        antlers. These animals are valued at more than US$111 million.
        Over the past two years, the federal government's emphasis on CWD has been “quite high”
        compared to other wildlife diseases, says USDA staff veterinarian Dan Goeldner. “In no
        small part, that's because the disease has cropped up in new places, and those are states
        that have political clout.” It has now been found in ten more states beyond what became
        known as the endemic region of Colorado and neighboring Wyoming (Figure 1). Last year, the
        USDA received US$14.8 million to monitor and manage the disease, and Goeldner says the
        department expects to get about US$16 million this year.
      
      
        The Prion Diseases
        Those figures don't include scientific research funded by other organizations, such as
        the US$42.5 million received by the United States' Department of Defense in 2002 to start
        up a National Prion Research Program. The prion is the protein-like agent that causes
        transmissible spongiform encephalopathies (TSEs). Its normal function is uncertain, but
        when it misfolds into an abnormal or “infectious” form, it causes the microscopic holes and
        globs of toxic, misshapen protein found in the brains of TSE victims. Unlike viruses,
        prions don't contain nucleic acids—only protein. Without DNA or RNA to issue biochemical
        commands, abnormal prions shouldn't be able to convert normal prions to the infectious
        state, but that's exactly what they do (Box 1).
        Prion diseases occur in many species. In domestic sheep and goats, prion diseases occur
        as scrapie, which has a virtually worldwide distribution. North America and Europe have
        also reported rare cases of TSE in ranched mink. Humans get kuru, Creutzfeldt-Jacob disease
        (CJD), and Gerstmann-Sträussler-Scheinker syndrome—all rare—and BSE itself manifests in
        people as a variant form of CJD. Since the United Kingdom outbreak, BSE has been discovered
        in more than 20 countries, most recently in North America. As public fear rose of possible
        CWD transmission to humans who eat infected venison, the United States' Centers for Disease
        Control and Prevention (CDC) released a report last year of its investigation into several
        deaths among venison eaters who might have had a TSE. The report concluded that none of the
        deaths could be attributed to venison, but it nevertheless cautioned that animals showing
        evidence of CWD should be excluded from the human and animal feed chains (Box 2).
        CWD is the least understood of all the prion diseases. Its origins are unknown and may
        well never be discovered. The question is largely academic, unless one hypothesis is proven
        true, that it derives from scrapie. In that case, the knowledge might help in efforts to
        control the two diseases through herd and flock management.
        Researchers are working to determine the minimum incubation time of CWD before clinical
        signs appear, now roughly estimated at 15 months in deer and 12—34 months in elk. They're
        trying to discover whether CWD strains exist that can affect the length of the disease
        process and different regions of the brain or that can infect different species, including
        humans. They are also investigating the period during which the prion is passed on, as well
        as its modes of transmission. They want to know whether disease reservoirs exist in the
        bodily fluids of hosts, in the environment, or both. They're racing to develop a diagnostic
        test that can be performed on live animals, enabling identification of the disease before
        clinical signs appear, which would eliminate the need to kill thousands of apparently
        healthy animals in areas where CWD is detected. But among the first things they need to
        clarify are CWD's distribution across North America and its prevalence.
        
          
            “…the disease has cropped up in new places, and those are states that
            have political clout.”
          
        
      
      
        An Initial Step: Improved Surveillance
        “Before you can start to control CWD, you need to understand where it is and how much of
        it you have,” says veterinary pathologist Beth Williams of the University of Wyoming in
        Laramie. “So I think you really need surveillance.” Research on its pathogenesis and
        transmission will help to develop better diagnostic tools, which will improve surveillance,
        adds Williams, who first identified the disease as a TSE more than a quarter-century
        ago.
        Colorado State's Salman argues that current surveillance is primarily a series of
        reactions to reported cases, rather than a systematic strategy designed to determine where
        and at what prevalence the disease exists and where it's absent. The estimated prevalence
        is about 1% in elk and 2.5% in deer. But Salman says, “We don't have a good idea of areas
        in which we are saying we haven't found the disease because these areas are not yet, in my
        estimation, negative for the disease. Scrapie is a wonderful example of systematic
        surveillance but, to be fair to the decision-makers and technical people involved with CWD,
        surveillance on wildlife species is very difficult.”
        The USDA's Goeldner declares, “We have the goal and the hope to eradicate the disease
        from the farm population.” But Colorado Department of Wildlife veterinarian and CWD expert
        Mike Miller warns, “Given existing tools, it seems unlikely that CWD can be eradicated from
        free-ranging populations once established.”
        The gold standard of diagnosis is based on examination of the brain for spongiform
        lesions and abnormal prion aggregation. Suspect animals are decapitated and their bodies
        incinerated. “This is an approach that nobody wants, including the people who have to
        implement it,” says wildlife ecologist Michael Samuel, principal investigator in the United
        States Geological Survey–Wisconsin Cooperative Wildlife Research Unit at the University of
        Wisconsin in Madison.
        Nevertheless, when three white-tailed deer shot by hunters in the south-central part of
        that state during the fall of 2001 were diagnosed with CWD, the state government took swift
        action. By the spring of 2003, almost 40,000 deer had been sacrificed and sampled for the
        disease, both within and without a 411 square-mile (1065 square-kilometer) region dubbed
        the eradication zone. There the goal was to remove as many deer as possible, whereas the
        plan in contiguous outlying areas was to reduce density to about ten deer per square mile.
        CWD is thought to spread more efficiently in high-density populations, and normal densities
        in Wisconsin are 50–100 deer per square mile, about five times that of Colorado and
        Wyoming. The main objectives of the Wisconsin culling were to discover where the disease
        existed and its prevalence in affected areas. In the eradication zone, it was 6%–7%,
        although in the outlying region it was only 1%–2%. Samples elsewhere in the state tested
        negative.
      
      
        In Search of a Live Assay
        A key to combating the spread of CWD is to put into widespread use a preclinical
        diagnostic test on live animals. Miller and colleagues recently developed and validated the
        first such assay, based on a biopsy of lymphoid tissue, where the infectious agent is known
        to incubate. They showed that tonsillar biopsies taken from live animals can confirm
        disease at least 20 months prior to death and up to 14 months before the onset of clinical
        signs. Although the method is a useful screening tool, it requires much time and training.
        Each deer must be anaesthetized and blindfolded, placed in a restraint, its mouth held open
        with a gag, the tonsil visualized with a laryngoscope, and the biopsy taken with endoscopic
        forceps. Lymphoid tissue sampling was first used as a preclinical test in sheep scrapie.
        “Many attempts have been made to develop and evaluate tests for live animals, but it is
        fraught with difficulties,” declares TSE specialist Danny Matthews of the United Kingdom
        Government's Veterinary Laboratories Agency in Weybridge. He says that a live test for BSE
        in cattle is likely to be evaluated shortly by the European Food Safety Authority, but
        warns of a major problem: test samples are collected early in the incubation, whereas brain
        pathology only arises two to three years later. This creates long delays in determining
        whether a positive preclinical test result is, in fact, accurate: “How can one do an
        appropriate evaluation?”
        Matthews notes that blood appears to be a useful medium for testing scrapie in sheep,
        but current technology cannot deliver a tool applicable across a range of different scrapie
        genotypes. “Like sheep, elk and mule deer do have a peripheral pathogenesis, which suggests
        that the blood test route may have some potential, especially if the genotype variability
        is more restricted than in sheep.”
      
      
        Transmission Mysteries
        Scrapie can be vertically transmitted from mother to offspring, either in the womb or
        from the transfer of infected germ plasm. It also can be transmitted horizontally, from any
        one animal to another. CWD, the only other known contagious TSE, is thought to be
        transmitted solely by as-yet-undetermined direct or indirect horizontal contact. It
        probably is not transmitted through infected feed, as is the case for BSE.
        A number of scientists are currently on the trail of suspected CWD disease reservoirs.
        Saliva is a leading candidate, because clinical signs of CWD include excessive thirst,
        drinking, and drooling. Work with lab animals suggests that the infectious agent might be
        produced in salivary glands and, if so, it could be transmitted through social
        interactions. Feces is also a possible reservoir because animals nose in the ground for
        feed, and urine is yet another candidate, because it is involved in the scenting activities
        of cervids.
        Soil could be an environmental reservoir, because cervids ingest dirt to supplement
        their diets with minerals. Bucks also lick soil on which does have urinated to ascertain
        their mating status. University of Wisconsin soil science professor Joel Pedersen has
        discovered that abnormally folded prions stick to the surface of some soil types, such as
        clay, resisting environmental and chemical damage. “Captive elk contracted CWD when
        introduced into paddocks occupied by infected elk more than 12 months earlier, despite
        fairly extensive efforts to disinfect the enclosures,” Pedersen notes. He has begun a
        five-year project to characterize interactions between infectious prions and soil particles
        and determine the extent to which infectivity is retained.
        No matter how CWD is transmitted between cervids, the likelihood of human susceptibility
        seems low. Laboratory evidence has demonstrated a molecular barrier against such
        cross-species infection, based on the failure of abnormal cervid prions to efficiently
        convert normal human prions to the infectious state. Likewise, abnormal cervid prions don't
        easily convert normal cattle prions, suggesting that cattle won't get CWD and pass it on to
        humans who eat tainted beef. While cattle can contract CWD if inoculated with the
        infectious agent, long-term studies placing cattle in close proximity to diseased cervids
        have resulted in no cases of natural transmission. Williams summarizes what all this
        suggests: “Never say never, but based on the [molecular] work, the CDC's findings, and the
        epidemiology, we certainly don't have evidence that humans have gotten CWD.”
      
      
        
      
    
  

  
    
      
        
        Many damaged and mutant polypeptides, as well as some normal proteins, have a tendency
        to aggregate in cells. Some protein aggregates are capable of “dividing” and propagating in
        cells, leading to formation of similar aggregates in daughter cells or even in neighboring
        cells due to “infection.” These self-propagating protein aggregates are called prions and
        constitute the basis of prion diseases. The infectious agent in these diseases is an
        abnormal conformation of the PrP protein (PrP
        Sc ), which makes it protease-resistant and initiates its aggregation
        (Prusiner 1998). The abnormal aggregated species can recruit normal soluble PrP molecules
        into aggregates, thus inactivating them. The aggregates of PrP
        Sc can proliferate within cells and be transmitted to other cells and
        tissues, leading to the spread of neurotoxicity.
      
      
        Prion Domains
        While so far only one prion protein is known in mammals, several prion-like proteins
        capable of forming self-propagating aggregates have been found in various yeast species.
        The common structural feature of yeast prion proteins is the so-called prion domain,
        characterized by the high content of glutamines (Q) and asparagines (N) (DePace et al.
        1998; Michelitsch and Weissman 2000), also known as the Q/N-rich domain. The prion domains
        are the major structural determinants that are solely responsible for the polypeptide
        aggregation and propagation of the aggregates. Interestingly, the mammalian PrP
        Sc is fundamentally different from yeast prions, since it lacks a
        Q/N-rich domain, indicating that distinct structural features are responsible for its
        ability to form self-propagating aggregates. The Q/N-rich domains in yeast prions are
        transferable in that, when fused to a heterologous polypeptide, they confer prion
        properties to this polypeptide. With a low probability, soluble proteins with prion domains
        can change conformation to form self-propagating aggregates, which can be transmitted to
        daughter cells (Lindquist 1997) (Figure 1). As with PrP
        Sc , yeast prions efficiently recruit soluble molecules of the same
        species, thus inactivating them (Lindquist 1997; Chernoff 2001; Wickner et al. 2001). Also
        with low probability, the aggregation-prone conformation of yeast prion proteins can
        reverse to a soluble functional conformation. Certain yeast prion proteins, when in soluble
        conformation, function in important pathways; e.g., Sup35 (forming [PSI
        + ] prion) controls termination of translation, and Ure2 (forming [URE3
        + ] prion) controls some membrane transporter systems. Aggregation of
        these proteins leads to phenotypes (e.g., suppression of nonsense mutations or transport
        defects) inherited in a non-Mendelian fashion owing to the nonchromosomal basis of the
        inheritance.
      
      
        Inheriting Variations
        A remarkable feature of yeast prion proteins is their ability to produce distinct
        inherited “variants” of the prion. For example, [PSI
        + ] prion could exist in several distinct forms that suppress
        termination of translation to different degrees. These “variants” of yeast prions are
        analogous to different prion “strains” of PrP
        Sc , which cause versions of the disease with different incubation
        periods and different patterns of brain pathology. The molecular nature of distinct PrP
        Sc strains is determined by specific stable conformations of PrP.
        Similarly, “variants” of yeast prions are explained by different stable conformation states
        of the corresponding prion proteins (Chien et al. 2003). Strict conformation requirements
        for aggregate formation can also explain interspecies transmission barriers, where prion
        domains of Sup35 derived from other yeast species cannot cause formation of [PSI
        + ] prion in Saccharomyces cerevisiae, in spite of a high degree of
        homology. This observation is very intriguing, especially in light of a recent finding that
        prion conformation of some proteins is required for formation of prions by the other
        proteins. For example, for de novo formation of [PSI
        + ] prion, a distinct prion [RNQ
        + ] should be present in a cell (Derkatch et al. 2001; Osherovich and
        Weissman 2001), probably in order to cross-seed Sup35 aggregates. This is in spite of
        relatively limited homology between the prion domains of these proteins. The apparent
        contradiction between the interspecies transmission barriers of very homologous prion
        proteins and possible cross-seeding of aggregates by prion proteins with more limited
        homology represents an interesting biological problem. On the other hand, this apparent
        contradiction may indicate that prion formation is a more complicated process than we
        currently think and that it may involve many cellular factors.
      
      
        What Do Prions Do?
        Although yeast prions have been studied for almost ten years, very little is known about
        their biological significance. We do not know the functions of the majority of proteins
        that can exist as prions. Even if a function of prion proteins, such as with Sup35 or Ure2,
        is known, we do not understand the biological significance of their “prionization,” i.e.,
        that they aggregate and propagate in the aggregated form. A very intriguing and unexpected
        finding was that formation of [PSI
        + ] prion causes a wide variety of phenotypic alterations, which depend
        on the strain background (True and Lindquist 2000). In fact, comparison of yeast strains of
        different origin, each with and without [PSI
        + ] prion, showed that certain strains with [PSI
        + ] prion have different sensitivity to stresses and antibiotics than
        their non-prion derivatives, despite their genetic identity. In some strains, cells with
        [PSI
        + ] prion demonstrated better survival than their non-prion counterparts
        in the presence of inhibitors of translation or microtubules, heavy metals, low pH, and
        other deleterious conditions, which of course gives a strong advantage to the [PSI
        + ] cells. It is likely that some genomic mutations could be suppressed
        and therefore become silent when termination of translation by Sup35 is partially
        inactivated in [PSI
        + ] prion cells (Lindquist 2000; True and Lindquist 2000). [PSI
        + ] could also reveal previously silent mutations or their combinations.
        It was hypothesized that switches between prion and non-prion forms of Sup35 enhance
        survival in fluctuating environments and provide a novel instrument for evolution of new
        traits.
      
      
        Q/N Does Not Necessarily a Prion Make
        Searching genomes of various species demonstrated that a relatively large fraction of
        proteins (between 0.1% and 2%) contain Q/N-rich domains (Michelitsch and Weissman 2000) or
        polyQ or polyN sequences. These domains are often found in transcription factors, protein
        kinases, and components of vesicular transport. The Q/N-rich domains usually are not
        evolutionary conserved and their functional role is largely unknown. Some of the Q/N-rich
        or polyQ domains facilitate aggregation of polypeptides, especially if expanded owing to
        mutations. Such expansion of the polyQ domains in certain neuronal proteins could cause
        neurodegenerative disorders, e.g., Huntington's disease or several forms of ataxia.
        Importantly, aggregates formed by polypeptides with the Q/N-rich or polyQ domains are not
        necessarily self-propagating aggregates, i.e., prions. In fact, there are additional
        structural properties of the polypeptides that provide the self-propagation (see below).
        Even if a protein with a polyQ domain does not form a prion, its aggregation may depend on
        certain prions. For example, recent experiments demonstrated that [RNQ
        + ] prion dramatically stimulated aggregation of fragments of
        recombinant human huntingtin or ataxin-3 with an expanded polyQ domain cloned in yeast
        (Osherovich and Weissman 2001; Meriin et al. 2002). [RNQ
        + ] facilitated the nucleation phase of the huntingtin fragment
        aggregation, suggesting that this prion can be directly involved in seeding of the
        aggregates. The major question now is whether there are analogous prion-like proteins in
        mammalian cells that are involved in aggregation of huntingtin or ataxin-3 and subsequent
        neurodegenerative disease.
        The first indication that mammalian proteins with Q/N-rich domains can form
        self-propagating prions came from recent work with a regulator of translation cytoplasmic
        polyadenylation element-binding protein (CPEB) from 
        Aplysia neurons (Si et al. 2003). The neuronal form of this protein has a
        Q/N-rich domain similar to the prion domains of yeast prions. The Q/N-rich domain from CPEB
        (CPEBQ), when fused to green fluorescent protein (GFP), conferred upon it prion-like
        properties. The CPEBQ–GFP fusion polypeptide existed in yeast cells in one of the three
        distinct states, i.e., soluble, many small aggregates, or few large aggregates. Mother
        cells almost always gave rise to daughter cells in which the CPEBQ–GFP polypeptide was in
        the same state, indicating the ability of these aggregates to be inherited, i.e., to
        self-propagate. Furthermore, full-length 
        Aplysia CPEB protein, when cloned in yeast, can also exist in two
        distinct states, soluble and aggregated, which is an inherited feature. Very unexpectedly,
        unlike other prions, the aggregated state of CPEB was more functionally active than the
        soluble form (Si et al. 2003). These data strongly suggest that metazoan proteins with
        Q/N-rich domains are potentially capable of forming prions. The challenge now will be to
        establish whether CPEB can exist as a self-propagating aggregate in 
        Aplysia or mammalian neurons.
      
      
        Mystery of Propagation
        What makes protein aggregates in yeast propagate? The key cellular element that is
        critical for this process is molecular chaperone Hsp104 (Chernoff et al. 1995). This factor
        is specifically required for maintenance of all known prions within generations and
        probably is not involved in prion formation (i.e., initial protein aggregation). [PSI
        + ] yeast cells have about 60 seeds of this prion (although this number
        differed in different [PSI
        + ] isolates), and maintenance of about this number of seeds after cell
        divisions requires functional Hsp104 (Eaglestone et al. 2000). In fact, in the absence of
        Hsp104, prion aggregates continue to grow without increase in number and are rapidly lost
        in generations (Wegrzyn et al. 2001). Since this chaperone can directly bind to protein
        aggregates and promote there disassembly (Glover and Lindquist 1998), it was suggested that
        the main function of Hsp104 in prion inheritance is to disaggregate large prion aggregates
        to smaller elements, thus leading to formation of new seeds (Kushnirov and Ter-Avanesyan
        1998). Interestingly, although Hsp104 is conserved among bacteria, fungi, and plants,
        animal cells do not have this chaperone or its close homologs. Therefore, if yeast-type
        prions with Q/N-rich domains exist in animal cells, there should be alternative factors
        that disaggregate large prion aggregates into smaller species in order to keep the number
        of seeds relatively constant and thus maintain the prions.
        The fact that some proteins with Q/N-rich domains form self-propagating aggregates,
        while others can aggregate but cannot form prions, suggests that there should be some
        structural elements either within the Q/N-rich sequence or close to it that confer the
        ability to propagate. In an article in this issue of 
        PLoS Biology by Osherovich et al. (2004), the authors examined sequence
        requirements for prion formation and maintenance of two prion proteins, Sup35 and New1.
        They noted that both prion proteins contain an oligopeptide repeat QGGYQ in close proximity
        to Q/N-rich sequences and examined the functional significance of the repeats for
        aggregation and maintenance of the prions. In New1, in contrast to a deletion of the N-rich
        domain, deletion of the repeat did not affect aggregation of the protein or formation of
        the prion, but abrogated inheritance of the prion. With Sup35, the situation was somewhat
        more complicated, since repeats adjacent to Q/N-rich domain affected both protein
        aggregation and prion maintenance while more distant repeats affected only the prion
        inheritance. The authors suggested that the oligopeptide repeats facilitate the division of
        aggregates, either by serving as binding sites for Hsp104 or by altering the conformation
        of the polypeptides in aggregates to promote access for Hsp104 (Figure 2).
        The likely possibility was that the oligopeptide repeats could be interchangeable
        between different prions, leading to creation of novel chimeric prions. In fact, the
        authors constructed an F chimera, a fusion protein having the N-rich domain of New1 and the
        oligopeptide repeat of Sup35. This fusion polypeptide efficiently formed prion [F
        + ]. Furthermore, when the oligopeptide repeat sequence was added to a
        polyQ sequence, this fusion polypeptide also acquired the ability to form self-propagating
        aggregates. This work, therefore, clarifies the architecture of prions by defining two
        structural motifs in prion proteins that have distinct functions in aggregation and
        propagation. Interestingly, not all yeast prions have similar oligopeptide repeat motifs,
        indicating that distinct structures could confer prion properties to polypeptides that can
        aggregate. It would be important to identify these structures in order to understand the
        mechanisms of aggregate propagation. The work of Osherovich et al. (2004) may help to
        identify proteins from mammalian cells, plants, and bacteria that can potentially form
        prions. Finding these novel prions could be of very high significance since they may
        provide insight into a wide range of currently unexplained epigenetic phenomena.
      
    
  

  
    
      
        
        In what is usually referred to as the most famous experiment in embryology, Hans Spemann
        and Hilde Mangold (1924) showed that a specific region in early frog embryos called the
        blastopore lip can induce a second complete embryonic axis, including the head, when
        transplanted to a host embryo. Most of the axis, including the nervous system, was derived
        from the host, whose cells were induced to form an axis by the graft, therefore named the
        organizer. Induction refers to the change in fate of a group of cells in response to
        signals from other cells. The signal-receiving cells must be capable of responding, a
        property termed competence. The Spemann–Mangold organizer. which—as the transplantation
        experiment shows—is able to turn cells whose original fate would be gut or ventral
        epidermis into brain or somites, is the prototypical inducing tissue. And neural induction
        has for a long time been regarded as a process by which organizer signals, in their normal
        context, redirect ectodermal cells from an epidermal towards a neural fate. The nature of
        the neural inducer or inducers and the mechanism of neural induction have been and remain
        hot topics in developmental biology.
        For half a century after Spemann and Mangold, studies on amphibians monopolized the
        subject, and even more recently, a large part of the progress in analyzing organizer
        formation and function and neural induction was based on amphibians, mostly the model
        species 
        Xenopus laevis . In the past few years, however, work in other
        vertebrate and nonvertebrate chordate systems has come to play an important role in the
        field and has shed light on generalities and differences among chordates. If the present
        primer uses 
        Xenopus to illustrate the process, it is because it accompanies
        an article in this issue of 
        PLoS Biology dealing with neural development in this species (Kuroda et
        al. 2004) and, of course, because of the experience of this author. Here I shall outline
        the understanding of organizer formation and neural induction as it has evolved over recent
        times and attempt to integrate recent results from different species into a common
        pattern.
      
      
        Cortical Rotation and Nuclear Localization of β-Catenin
        The frog egg is radially symmetrical around the animal–vegetal axis that has been
        established during oogenesis. Fertilization triggers a rotation of the cortex relative to
        the cytoplasm that is associated with the movement of dorsal determinants from the vegetal
        pole to the future dorsal region of the embryo (Gerhart et al. 1989). (A brief
        parenthetical point is in order here. Conventionally, the side of the amphibian and fish
        embryo where the organizer forms has been called dorsal, with the opposite side labeled as
        ventral. This axis assignment does not project unambiguously onto the clearly defined
        dorsal–ventral polarity of the larva, as pointed out forcefully in recent publications
        [Lane and Smith 1999; Lane and Sheets 2000, 2002]. In these papers, a new proposal is made
        for polarity assignments in the gastrula that, I believe, has some merit, but also presents
        some difficulties. As the conventional approach of equating organizer side with dorsal
        seems to remain in wide use at present, I shall apply this convention, albeit with the
        reservation above.)
        While the nature of the dorsal determinants is still in dispute, it is clear that the
        consequence of their translocation is the nuclear localization of β-catenin in a wide arc
        at the future organizer side (Figure 1) (Schneider et al. 1996; Schohl and Fagotto 2002).
        Nuclear localization of β-catenin appears to be the first event that determines
        dorsal/ventral polarity in the 
        Xenopus and zebrafish embryos (Hibi et al. 2002). No comparable
        early event appears to be involved in amniote (e.g., chick and mouse) embryos.
      
      
        Induction by the Organizer: Antagonizing Bone Morphogenetic Protein
        As gastrulation starts, the Spemann–Mangold organizer, which includes mostly axial
        mesodermal precursors, was classically believed to instruct naïve ectoderm to convert to
        neural tissue. In transplant or explant studies, animal ectoderm that forms epidermis, when
        undisturbed, is susceptible to neural induction by the organizer. This fact prompted a
        search for neural inducers that eventually led to the identification of several substances
        with the expected properties—organizer products that can neuralize ectoderm. Their
        molecular properties were at first surprising: they proved to be antagonists of other
        signaling factors, mostly of bone morphogenetic proteins (BMPs) and also of WNT (a secreted
        protein homologous to the 
        Drosophila Wingless protein) and Nodal factors (Sasai and De
        Robertis 1997; Hibi et al. 2002). These observations led to the formulation of a “default”
        model of neural induction (Weinstein and Hemmati-Brivanlou 1997), which states that
        ectodermal cells will differentiate along a neural pathway unless induced to a different
        fate. The heuristic simplicity and logical cogency of this model facilitated its wide
        acceptance, although it did not explain the processes that set the “default.” Some of these
        processes have been the subject of subsequent studies that were conducted in several
        different species, and this has led to a more refined (and probably more accurate)
        picture.
      
      
        The Role of Fibroblast Growth Factor
        For example, additional signaling pathways are now known to operate. Recent work on
        neural induction comes to two major conclusions: (i) the fibroblast growth factor (FGF)
        signaling pathway plays a major role in this process, and (ii) neural specification starts
        well before gastrulation and thus before the formation and function of the organizer.
        Studies on the role of FGF in early 
        Xenopus development initially discovered its role in mesoderm
        induction and the formation of posterior tissues (Kimelman et al. 1992). And while the
        involvement of FGF in neuralization was observed early in this system (Lamb and Harland
        1995; Launay et al. 1996; Hongo et al. 1999; Hardcastle et al. 2000), in view of the
        impressive effects seen with Chordin and other BMP pathway antagonists, the relevance of
        FGF in neural specification in amphibians and fish was slow to be recognized. It took
        elegant studies, mostly in chick embryos (Streit et al. 2000), and their eloquent
        exposition (Streit and Stern 1999; Wilson and Edlund 2001; Stern 2002) to turn the tide,
        but there is now no doubt that the FGF signaling pathway plays a major role in the
        specification and early development of the neural ectoderm in chordates.
        FGF does not seem to behave as a classical organizer-derived neural inducer, however.
        Maternal FGF mRNA and protein appear to be widely distributed in the early embryo, and at
        least one FGF family member is expressed primarily in the animal, pre-ectodermal region
        during blastula stages (Song and Slack 1996). A detailed study of the regions where
        different signaling pathways are active during embryogenesis (Schohl and Fagotto 2002)
        showed that the entire ectoderm is probably exposed to FGF signals at or prior to the time
        of neural induction, with the more vegetal, mesoderm-proximal region of the ectoderm being
        exposed to higher levels. Thus, exposure to FGF is required to endow the ectoderm with the
        competence to respond to additional signals that will act later on its way towards neural
        specification. Such a process was deduced from experiments in the chick, where an FGF
        signal must be followed by exposure to organizer signals to sensitize the tissue to BMP
        antagonists that ultimately stabilize the neural fate (Stern 2002).
        An exciting recent study shows that exposure of the epiblast (ectoderm) to FGF induces,
        after a time delay, a transcription factor named Churchill. Churchill expression inhibits
        cell ingression leading to mesoderm formation; the cells remaining in the epiblast assume a
        neural fate (Sheng et al. 2003). The time delay in Churchill induction appears to be the
        key in explaining how one signal, FGF, can be involved in mesodermal and neural development
        at the same time in cells that are in close proximity. The question how FGF signaling can
        lead to different outcomes was also addressed in a study on neural specification in
        ascidians (Bertrand et al. 2003). Here, the FGF signal leads to neural induction through
        the coordinated activation of two transcription factors, Ets1/2 and GATAa, whereas FGF does
        not activate GATAa during its function in mesoderm formation. Thus, similar input leads to
        distinct output as a result of different responses by target tissues, stressing the
        importance of competence in this inductive process.
      
      
        Molecular Predisposition
        Not surprisingly, then, attention has turned to the target tissues and to the
        prepatterns that might already exist. In 
        Xenopus , it was long known that the animal region or
        pre-ectoderm is not uniform or naïve, in that the dorsal, organizer-proximal region is
        predisposed towards a neural fate (Sharpe et al. 1987). The paper by Kuroda et al. (2004)
        adds much information about neural specification before gastrulation in 
        Xenopus and the factors involved in this process. The authors
        identify a region in the dorsal ectoderm of the blastula that they name the “blastula
        Chordin- and Nogginexpressing” (or BCNE) region (Figure 2). They show that this region,
        which I prefer to simply call dorsal ectoderm, expresses 
        siamois , 
        chordin , and 
        Xnr3 , another β-catenin target. The dorsal ectoderm or BCNE is fully
        specified as anterior neural ectoderm, as excision of this region led to headless embryos,
        and explants differentiated into neural tissue in culture, even when the formation of any
        mesodermal cells was blocked by interference with nodal signaling (Kuroda et al. 2004).
        Kuroda et al. (2004) further show that induction of anterior neural tissue initiated by
        β-catenin requires Chordin, whereas formation of posterior neural tissue does not. This
        latter point concerns an issue not yet mentioned here, namely anterior–posterior patterning
        of the neural ectoderm, a process that occurs in concert with neural induction per se. This
        patterning appears to involve the interaction of various signaling factors, including FGF,
        BMP, WNT, and retinoic acid, all of which act as posteriorizing factors (Kudoh et al.
        2002). Suppression of BMP signaling by expression of its antagonists is the condition that
        specifies the dorsal ectoderm or BCNE as future anterior neural ectoderm; in contrast,
        posterior neural ectoderm may form under the influence of FGF even in the presence of BMP
        signaling. The work by Kuroda et al. (2004) thus shows that initial specification of
        anterior neural ectoderm in 
        Xenopus , as in other vertebrates, takes place before
        gastrulation and does not require organizer signals; this is not to say that full
        differentiation and patterning of the nervous system could be achieved without organizer
        participation.
      
      
        Induction and Competence
        The formation of the vertebrate nervous system thus depends on multiple signaling
        pathways, such as the FGF, BMP, and WNT signaling cascades, that interact in complex ways
        (e.g., Pera et al. 2003). In contrast to the classical view, neural induction is not
        exclusively promoted by organizer-derived signals, in that earlier signals and intrinsic
        processes that determine ectodermal competence are prominently involved. Whether inductive
        signals or competence of responding tissue is more important in embryology has been
        debated, much like the nature–nurture controversy in the behavioral arena. Current work has
        given some boost to the competence side of the argument, but, as in behavior, the truth
        lies somewhere in between, though not necessarily at the halfway mark. Studies such as
        those discussed here bring us closer to finding the answer to this question.
      
    
  

  
    
      
        
        Although the word ‘revolution’ should not be used lightly in science, there is no other
        way to describe the recent explosion in our awareness and understanding of RNA-mediated
        gene silencing pathways. The central player in RNA-mediated gene silencing is a
        double-stranded RNA (dsRNA) that is chopped into tiny RNAs by the enzyme Dicer. The tiny
        RNAs associate with various silencing effector complexes and attach to homologous target
        sequences (RNA or DNA) by basepairing. Depending on the protein composition of the effector
        complex and the nature of the target sequence, the outcome can be either mRNA degradation,
        translational repression, or genome modification, all of which silence gene expression
        (Figure 1). Present in plants, animals, and many fungi, RNA-mediated gene silencing
        pathways have essential roles in development, chromosome structure, and virus resistance.
        Although the mechanistic details are still under investigation, RNA-mediated silencing has
        already provided a powerful tool for studying gene function and spawned a fledgling
        industry that aims to develop novel RNA-based therapeutics to treat human diseases
        (Robinson 2004).
        Many biologists first learned of RNA-mediated gene silencing in 1998 following the
        discovery, in the nematode worm 
        Caenorhabditis elegans (Fire et al. 1998), of a process called
        RNA interference (RNAi), in which dsRNA triggers sequence-specific mRNA degradation. The
        roots of RNA-mediated silencing, however, can be traced back 15 years, when a handful of
        botanical labs stumbled across strange cases of gene silencing in transgenic plants. To
        highlight the many seminal contributions of plant scientists to the field, we offer here a
        personal perspective on the origins and history of RNA-mediated gene silencing in
        plants.
      
      
        Early Silencing Phenomena
        Starting in the late 1980s, biologists working with transgenic plants found themselves
        confronted with a ‘bewildering array’ of unanticipated gene silencing phenomena
        (Martienssen and Richards 1995). Most intriguing were cases in which silencing seemed to be
        triggered by DNA or RNA sequence interactions, which could occur between two separate
        transgenes that shared sequence homology or between a transgene and homologous plant gene.
        Several early examples supplied the prototypes for two types of RNA-mediated gene silencing
        that are recognized today. In one type, silencing results from a block in mRNA synthesis
        (transcriptional gene silencing [TGS]); in the second type, silencing results from mRNA
        degradation (posttranscriptional gene silencing [PTGS]) (Figure 1).
        TGS was revealed when two different transgene complexes were introduced in sequential
        steps into the tobacco genome. Each complex encoded different proteins, but contained
        identical gene regulatory regions (promoters). Unexpectedly, the first transgene complex,
        which was stably active on its own, often became silenced in the presence of the second
        (Figure 2). The promoters of the silenced transgenes acquired DNA methylation, a genome
        modification frequently associated with silencing. Silencing and methylation were reversed
        when the transgene complexes segregated from each other in progeny, suggesting that
        interactions between the common promoter regions triggered silencing and methylation
        (Matzke et al. 1989; Park et al. 1996).
        PTGS was discovered in two ways. One involved experiments to evaluate antisense
        suppression, a promising approach at the time for selectively silencing plant gene
        expression. In theory, antisense RNA encoded by a transgene should basepair to the
        complementary mRNA of a plant gene, preventing its translation into protein. Although the
        control ‘sense’ transgene RNAs are unable to basepair to mRNA and hence should not induce
        silencing, they often inexplicably did (Smith et al. 1990). In another type of experiment,
        efforts to enhance floral coloration in petunia by overexpressing a transgene encoding a
        protein involved in pigment synthesis led paradoxically to partial or complete loss of
        color (Figure 2). This resulted from coordinate silencing (‘cosuppression’) of both the
        transgene and the homologous plant gene (Napoli et al. 1990; Van der Krol et al. 1990),
        later shown to occur at the posttranscriptional level (De Carvalho et al. 1992; Van
        Blokland et al. 1994) A related phenomenon, called quelling, was observed in the
        filamentous fungus 
        Neurospora crassa (Romano and Macino 1992). Similarly to TGS,
        PTGS was often associated with DNA methylation of transgene sequences (Ingelbrecht et al.
        1994).
        Two influential papers appeared in the early 1990s. One reported the discovery of
        RNA-directed DNA methylation in transgenic tobacco plants (Wassenegger et al. 1994). This
        was the earliest demonstration of RNA-induced modification of DNA, a process that we return
        to below. A second study showed that plant RNA viruses could be both initiators and targets
        of PTGS. Plants expressing a transgene encoding a truncated viral coat protein became
        resistant to the corresponding virus, a state achieved by mutual degradation of viral RNA
        and transgene mRNA (Lindbo et al. 1993). In addition to forging a link between RNA virus
        resistance and PTGS, this study included a remarkably prescient model for PTGS that
        featured an RNA-dependent RNA polymerase (RDR), small RNAs, and dsRNA, all of which were
        later found to be important for the RNAi. PTGS was subsequently shown in 1997 to protect
        plants naturally from virus infection (Covey et al. 1997; Ratcliff et al. 1997). Transgene
        PTGS thus tapped into a preexisting natural mechanism for combating viruses.
        To recap: by 1998—the year in which RNAi was reported—plant scientists had documented
        sequence-specific RNA degradation (PTGS), sequence-specific DNA methylation that triggered
        TGS, and RNA-directed DNA methylation. They had also proposed models for PTGS involving
        dsRNA (Lindbo et al. 1993; Metzlaff et al. 1997), small RNAs, and RDR (Lindbo et al.
        1993).
      
      
        RNAi
        RNAi was discovered in experiments designed to compare the silencing activity of
        single-stranded RNAs (ssRNAs) (antisense or sense) with their dsRNA hybrid. While only
        marginal silencing of a target gene was achieved after injecting worms with the individual
        strands, injection of a sense–antisense mixture resulted in potent and specific silencing
        (Fire et al. 1998). This unequivocally fingered dsRNA as the trigger of silencing. Shortly
        thereafter, dsRNA was shown to provoke gene silencing in other organisms, including plants
        (Waterhouse et al. 1998). Indeed, the relatedness of RNAi, PTGS, and quelling was confirmed
        when genetic analyses in worms, plants, and 
        Neurospora identified common components in the respective
        silencing pathways (Denli and Hannon 2003). This included the aforementioned RDR, which can
        synthesize dsRNA from ssRNA templates (see Figure 1). PTGS is now accepted as the plant
        equivalent of RNAi.
        The discovery of RNAi established a requirement for dsRNA in silencing, but details of
        the mechanism remained unclear. In 1999, plant scientists studying PTGS provided a crucial
        clue when they detected small (approximately 25 nucleotide-long) RNAs corresponding to
        silenced target genes in transgenic plants (Hamilton and Baulcombe 1999). They proposed
        that the small RNAs provided the all-important specificity determinant for silencing.
        Consistent with this, a rapid succession of studies in 
        Drosophila systems demonstrated that 21–23 nucleotide ‘short
        interfering'RNAs (siRNAs), derived from cutting longer dsRNA, can guide mRNA cleavage
        (Zamore et al. 2000; Elbashir et al. 2001); identified RISC (RNA-induced silencing
        complex), a nuclease that associates with small RNAs and executes target mRNA cleavage
        (Hammond et al. 2000); and identified Dicer, the enzyme that chops dsRNA into short RNAs
        (Bernstein et al. 2001) (see Figure 1).
        RNAi/PTGS was detected originally in experiments involving transgenes, injected RNAs, or
        viruses. Did the RNAi machinery also generate small RNAs for host gene regulation?
        Strikingly, the newly discovered siRNAs were the same size as several ‘small temporal’
        RNAs, first identified in 1993 as important regulators of developmental timing in worms
        (Lee et al. 1993; Reinhart et al. 2000). Everything came together in 2001 when heroic
        cloning efforts unearthed dozens of natural small RNAs 21–25 nucleotides in length, first
        from worms and flies and later from plants and mammals (Lai 2003; Bartel 2004). Similar to
        siRNAs, the natural small RNAs, dubbed microRNAs (miRNAs), arise from Dicer processing of
        dsRNA precursors and are incorporated into RISC (Denli and Hannon 2003). In many cases,
        miRNAs effect silencing by basepairing to the 3′ ends of target mRNAs and repressing
        translation (see Figure 1). miRNAs are now recognized as key regulators of plant and animal
        development. Identifying their target genes and full range of action are areas of intense
        research (Lai 2003; Bartel 2004).
        Up until 2002, RNAi/PTGS and miRNAs were the most avidly studied aspects of RNA-mediated
        gene silencing. The next major advance, however, abruptly turned attention back to
        RNA-guided modifications of the genome. By 2001, plant scientists working on RNA-directed
        DNA methylation and TGS had demonstrated a requirement for dsRNAs that are processed to
        short RNAs, reinforcing a mechanistic link to PTGS (Mette et al. 2000; Sijen et al. 2001).
        This established the principle of RNA-guided genome modifications, but the generality of
        this process was uncertain because not all organisms methylate their DNA. Widespread
        acceptance came with the discovery in 2002 of RNAimediated heterchromatin assembly in
        fission yeast (Hall et al. 2002; Volpe et al. 2002). This silencing pathway uses short RNAs
        produced by Dicer and other RNAi components to direct methylation of DNA-associated
        proteins (histones), thus generating condensed, transcriptionally silent chromosome regions
        (heterochromatin) (see Figure 1). Targets of this pathway include centromeres, which are
        essential for normal chromosome segregation. The RNAi-dependent heterochromatin pathway has
        been found in plants (Zilberman et al. 2003) and 
        Drosophila (Pal-Bhadra et al. 2004) and likely represents a
        general means for creating condensed, silent chromosome domains.
      
      
        More Lessons from Plants
        Plant scientists can chalk up other ‘firsts’ in RNA-mediated gene silencing. Systemic
        silencing, in which a silencing signal (short RNA or dsRNA) moves from cell to cell and
        through the vascular system to induce silencing at distant sites, was initially detected in
        plants in 1997 (Palauqui et al. 1997; Voinnet and Baulcombe 1997) and later in worms (Fire
        et al. 1998), although not yet in 
        Drosophila or mammals. Viral proteins that suppress silencing by
        disarming the PTGS-based antiviral defense mechanism were discovered by plant virologists
        in 1998 (Anandalakshmi et al. 1998; Béclin et al. 1998; Brigneti et al. 1998; Kasschau and
        Carrington 1998). One of these, the p19 protein of tombusviruses, acts as a size-selective
        caliper to sequester short RNAs from the silencing machinery (Vargason et al. 2003). A
        recent study suggests that animal viruses encode suppressors of RNA-mediated silencing (Li
        et al. 2004).
        Although RNA-mediated gene silencing pathways are evolutionarily conserved, there are
        various elaborations in different organisms. For example, the plant 
        Arabidopsis has four Dicer-like (DCL) proteins, in contrast to
        mammals and worms, whose genomes encode only one Dicer protein (Schauer et al. 2002). The
        RDR family has also expanded in 
        Arabidopsis to include at least three active members. An
        important goal has been to determine the functions of individual family members. Previous
        studies in 
        Arabidopsis have shown that DCL1 is needed for processing miRNA
        precursors important for plant development (Park et al. 2002; Reinhart et al. 2002), but
        not for siRNAs active in RNAi (Finnegan et al. 2003). The paper by Xie et al. (2004) in
        this issue of 
        PLoS Biology delineates distinct functions for DCL2, DCL3, and RDR2.
        Nuclear-localized DCL3 acts with RDR2 to generate short RNAs that elicit DNA and histone
        modifications; DCL2 produces short RNAs active in antiviral defense in the cytoplasm of
        cells. This study illustrates nicely how RNA silencing components have diversified in
        plants to carry out specialized functions.
        By identifying small RNAs as agents of gene silencing that act at multiple levels
        throughout the cell, molecular biologists have created a new paradigm for eukaryotic gene
        regulation. Plant scientists have figured prominently in RNA-mediated silencing research.
        Instrumental to their success was the early ability to produce large numbers of transgenic
        plants, which displayed a rich variety of gene silencing phenomena that were amenable to
        analysis. The agricultural biotechnology industry provided incentives to find ways to
        stabilize transgene expression and use transgenic approaches to modulate plant gene
        expression and to genetically engineer virus resistance. As exemplified by the petunia
        cosuppression experiments, nonessential plant pigments provide conspicuous visual markers
        that vividly reveal gene silencing. The history of gene silencing research shows once again
        that plants offer outstanding experimental systems for elucidating general biological
        principles.
      
    
  

  
    
      
        
        One of the hallmarks of human nature is our remarkably flexible behaviour, especially in
        the social domain, which is perhaps also a major reason for our relative evolutionary
        success. Our social skills are already being honed in childhood and early adolescence, when
        we quickly become very adept at forming and breaking alliances within and between groups
        and spend much of our time engaged in complex social interactions. At best, these
        interactions enrich our society; at worst, they become ‘Machiavellian’ and exploitative.
        While science might appear removed from such politics, many scientists would probably agree
        that science is in fact a social enterprise, sharing many characteristics with other human
        pursuits, and that any claim to greater scientific truth can only be accorded over decades,
        even centuries.
        I have always been fascinated by social intelligence, particularly of the
        ‘Machiavellian’ kind, and found myself wondering at the start of my doctoral research how
        one might use neuroimaging to study social intelligence in the human brain. I was also
        interested by the fact that some of this flexible behaviour is shared with other primates
        such as chimps, bonobos, and even monkeys, who also spend inordinate amounts of time in
        social interactions, working out social hierarchies. However, it was not immediately
        obvious how one might go about designing experiments that would address these somewhat
        intangible issues of social behaviour.
        Trawling the scientific literature, I came across the concept of reversal learning.
        While it is obviously important that we can learn arbitrary associations between stimuli
        and actions, it is also extremely important that we can relatively easily break these
        associations and learn others. If we learn that choosing a certain object leads to a
        reward, it would be rather maladaptive to keep choosing this object when it was no longer
        associated with a reward but, say, a punishment instead. In order to accommodate complex
        behaviour, we need to be able to adapt or reverse the learning patterns when things
        change.
        For a long time, it was thought that complex behaviour depended crucially on the
        prefrontal cortex of the brain, but it was not clear which parts were important for
        reversal learning. This was investigated in a classic paper by the eminent neuroscientists
        Susan Iversen and Mortimer Mishkin (1970), who studied lesions in monkeys, with elegant and
        important results. The authors lesioned discrete parts of the prefrontal cortex in
        different monkeys and showed convincingly that these lesions had differential effects on
        the animals' ability to reverse rewarding associations in an object reversal task. When the
        inferior prefrontal convexity and parts of the lateral orbitofrontal cortex (which is the
        ventral part of the prefrontal cortex over the orbits) (see Figure 1) were lesioned, the
        monkeys became significantly impaired with respect to object reversal learning.
        Specifically, they continued to respond much longer than controls to an object that was no
        longer rewarded on the first reversal trial.
        This was not the case for monkeys who had had the medial parts of the orbitofrontal
        cortex lesioned. These monkeys were not completely unaffected by the lesion, but showed
        moderate impairment on all but the first of the object discrimination reversals.
        Furthermore, they had moderate difficulty withholding response between trials on an
        auditory differentiation task. These results strongly suggested a differential role for the
        lateral and medial parts of the orbitofrontal cortex.
        Although the paper was not published in a high-profile journal, this elegant and very
        significant result has had a huge influence on subsequent research. The paper, like many
        other great papers, was ahead of its time, and it took almost a decade before the citations
        started to pick up (at last online count, on February 1, 2004, of the ISI database, the
        paper had generated 229 citations since 1981).
        Iversen and Mishkin (1970) persuasively demonstrated the importance of the orbitofrontal
        cortex in reversal learning, and other studies have since extended this result in nonhuman
        primates. One study demonstrated that single neurons in the macaque orbitofrontal cortex
        change their responses to a visual cue after a single trial in which the reward association
        of the visual cue is reversed (Thorpe et al. 1983). Another lesion study in marmosets by
        Dias et al. (1996) found that the orbitofrontal cortex is essential for the performance of
        emotion-related reversal learning tasks.
        There was also some evidence that humans with lesions to the orbitofrontal cortex have
        problems with reversal learning, but the lesions, caused by neurological insult, were not
        very clean or focal (Rolls et al. 1994). In addition, it had also become clear that lesions
        to the orbitofrontal cortex were associated with impairments in emotional and social
        behaviour, characterised by disinhibition, social inappropriateness, and irresponsibility
        (Anderson et al. 1999).
        These interesting but nonconclusive results in humans spurred us on to use neuroimaging
        on a modified version of a probabilistic reversal learning task designed by Julia Hornak
        and John O'Doherty (Hornak et al. 2004), whose preliminary data suggested that patients
        with surgical lesions to the orbitofrontal cortex were impaired. The subjects' task was to
        determine, by trial and error, which of two stimuli was the more profitable to choose and
        to keep track of this, reversing their choice when a reversal occurred. By design, the
        actual reversal event was not easy to determine, since ‘money’ could be won or lost on both
        stimuli, but a choice of the rewarding stimulus would in general give larger rewards and
        smaller punishments. The converse was true of the punishing stimulus; losing a large amount
        of money would often (but not always) signal that a reversal had occurred.
        We used functional magnetic resonance imaging to show that dissociable activity in the
        medial orbitofrontal cortex was correlated with the magnitude of the monetary gains
        received, while activity in the lateral orbitofrontal cortex was correlated with the
        monetary losses incurred (O'Doherty et al. 2001). This dissociation between the functions
        of medial and lateral orbitofrontal cortex seemed to mirror Iversen and Mishkin's initial
        dissociation in monkeys, in which the lateral orbitofrontal cortex was linked, in both
        cases, to the reversal trials. However, owing to the probabilistic nature of the task, in
        which receiving a monetary punishment did not always signal reversal, our imaging study did
        not reveal the cortical localisation of reversal trials. In addition, our task used money
        as the secondary reinforcer, which might be a powerful influence on humans but has little
        biological relevance for other animals, and certainly none in the social domain that I was
        interested in.
        One way to solve these problems was to use facial expressions rather than money as the
        reinforcing stimuli. This made sense, given that the key to social intelligence is the
        ability to detect subtle changes in communication and act upon these changes rapidly as
        they occur. Such changes in social behaviour are often based on facial expression and come
        so naturally to humans (and are in place so early in child development) that some might
        argue that this functionality is essentially innate. However, our human social behaviour is
        sufficiently flexible that we can easily learn to adapt our behaviour to most facial
        expressions. For example, other people's neutral expressions do not normally indicate that
        our behaviour should change, but it is easy to think of social contexts in which a neutral
        expression does indeed imply that our current behaviour is inappropriate and should
        change.
        I designed a reversal task in which the subject's overall goal was to keep track of the
        mood of two people presented in a pair and, as much as possible, to select the ‘happy’
        person, who would then smile. Over time, the person with the ‘happy’ mood (who would smile
        when selected), changed his/her mood to ‘angry’. This person thus no longer smiled when
        selected, but instead changed to a facial expression that signalled that he/she should no
        longer be selected. In the main reversal task, the facial expression used to cue reversal
        was an angry expression (the most natural facial expression to cue reversal), while in the
        second, control, version of the reversal task, a neutral expression was used. By using two
        different reversal tasks in which different facial expressions signalled that behaviour
        must change, we were able to determine which brain areas were specific to general reversal
        learning, rather than just to reversal following a particular expression, such as
        anger.
        We used functional magnetic resonance imaging to show that the ability to change
        behaviour based on facial expression is not reflected in the activity of the fusiform face
        area (which invariably appears to reflect only identity and not valence), but that general
        reversal learning is specifically correlated with activity in the lateral orbitofrontal and
        anterior cingulate/paracingulate cortices (as well as other brain areas, including the
        ventral striatum and the inferior precentral sulcus) (Kringelbach and Rolls 2003).
        This result confirmed and extended the results from Iversen and Mishkin's original
        paper. Further confirmation came from the neuropsychological testing, carried out by Julia
        Hornak on human patients with surgical lesions to the orbitofrontal cortex, which showed
        that bilateral (but not unilateral) lesions to the lateral orbitofrontal cortex produce
        significant impairments in reversal learning (Hornak et al. 2004). Yet, as always, these
        results are not conclusive and raise many new issues. It is, for instance, not presently
        clear what other areas of the brain are necessary and sufficient for reversal learning.
        Among the other brain areas we found relating to general reversal learning in our study,
        the ventral striatum is, for instance, an obvious candidate (Cools et al. 2002). In
        addition, functional magnetic resonance imaging is essentially a correlative technique,
        with poor temporal information, which makes it very difficult to infer causal relations
        between brain regions. Thus, further investigations, e.g., with magnetoencephalography,
        will still be required to gain temporal information on the milliseconds scale. I take heart
        from a friend, a very distinguished scientist, who states that the price for having spent a
        lifetime in cutting-edge research is that 99% of his (and other scientists') research is
        wrong—perhaps not completely wrong, but certainly wrong in the details. I would like to
        think that the original result from the Iversen and Mishkin paper is among the rare 1%, but
        the trouble with such foresight is that it lacks the vantage point of true hindsight.
        In his masterpiece, 
        The Prince , Niccolò Machiavelli offers a rather pessimistic view on
        human nature, in which ‘love is held by a chain of obligation which, since men are bad, is
        broken at every opportunity for personal gain’. It may be that our capacity for rapid
        reversal learning is sometimes used for less than noble pursuits, both in science and in
        interpersonal relations in general, but we would be in real trouble if we couldn't learn to
        change.
      
    
  

  
    
      
        
        The next generation of life scientists are currently undergraduates—and the success of
        this generation depends upon the quality of the education they receive. It is clear the
        expectations for undergraduate education are changing (Collins et al. 2003). When the
        National Research Council published its recommendations for changing the undergraduate
        training of future life scientists, the BIO2010 report, access to student-based research
        was a primary recommendation: “Colleges and universities should provide all students with
        opportunities to become engaged in research …” (National Research Council 2003). As every
        investigator knows, research begins in the literature, not in the laboratory. Therefore, an
        unstated assumption of the BIO2010 report was that students need to have unencumbered
        access to the research literature in order to engage in research and become scientific
        leaders in the 21st century.
        Early in my teaching career, I discussed graduate student preparation with a colleague
        at MIT. He said new graduate students knew about the different methods, they could even
        recite fine definitions—but if you asked them which method would be best to answer a
        particular question, they were uncertain. This reinforced my attitude towards teaching and
        testing. I realized that teaching science to students should be modeled on the way all
        scientists learn new information: in the context of an interesting question and on a
        need-to-know basis. This new style of teaching, “applied education,” would require me to
        reorganize reading materials for students, since most textbooks are written by someone who
        already knows all the information and has organized it accordingly. For example, describing
        membrane structure, protein structure, and signal transduction in Chapters 5, 12, and 15,
        respectively (spanning 227 pages) is not helpful for most students. It makes more sense to
        cover these three topics in close succession.
        Gradually, I converted all my courses over to this “applied education” format in which
        students were learning new information the same way all other scientists do. I began by
        asking questions that could be answered by learning the information provided by textbooks
        or the literature. With time, I realized that published research papers are ideal teaching
        tools because they cover information in the context of an interesting question and new
        material is presented as needed. This led me to collect series of related papers to create
        my own course materials (see
        www.bio.davidson.edu/courses/Molbio/Publicschedule.html#anchor99574051). So, for example,
        in my classes students first read the elegant paper by Munro and Pelham (1987) that
        uncovered the tetrapeptide lysine–aspartic acid–glutamic acid–leucine (KDEL) retention
        signal for proteins destined to remain in the endoplasmic reticulum lumen. Then, students
        read four additional papers, one of which is composed of weak data and overinterpreted
        analysis. Through this series of papers, students learn to trust their own assessment of
        the data rather than the authors': this is a very substantial improvement in student
        thinking and in their attitude towards the literature. I do not emphasize the particular
        details of these paper, but I do want the students to gain higher-order thinking skills.
        Therefore, my tests consist of figures from research papers that the students have never
        seen before. They are asked to interpret the figures as they appear in the papers and/or to
        design new experiments to answer a new question, given what they have learned from the
        published figure. Testing them in this way, students very quickly understand that
        memorizing details is not productive, but learning how to read scientific literature and
        design well-controlled experiments is much more rewarding (see
        www.bio.davidson.edu/courses/Molbio/molecular.html#2003exams). Based on this success, I
        have designed my genomics course on the “applied education” principle (see below; see also
        www.bio.davidson.edu/genomics).
      
      
        Access to Information Changes Education
        When I was a graduate student (in the late 1980s and early 1990s), PubMed was restricted
        to those institutions that could afford the subscription fee; now PubMed is freely
        available to all who have Internet access. This change in access to PubMed has
        significantly improved undergraduate training by providing students with the opportunities
        to do literature searches for their lab reports, papers, seminars, and of course original
        research. Free access to information in the life sciences has continued to evolve with the
        newest phenomenon in publishing—open-access journals. PubMed Central
        (http://www.pubmedcentral.nih.gov/) is a rich repository of and portal to open access
        articles, BioMed Central (http://www.biomedcentral.com/) publishes a growing number of
        open-access journals, and there are a few new open-access education journals such as 
        Cell Biology Education (http://www.cellbioed.org) and the 
        Journal of Undergraduate Neuroscience
        Education (http://www.funjournal.org). As the newest player in the open-access arena, 
        PLoS Biology has further enriched the growing espritdes-corps of
        publishing and has already improved undergraduate education. My students now have equal
        access to a growing portion of the literature that Nobel laureates and investigators at
        wealthy institutions enjoy.
        Interestingly, the push towards open access has led many subscription-based journals to
        permit “free access” two to 12 months after publication. These time-delayed free-access
        journals are helpful for course adjustments in the subsequent academic year, but not the
        current semester. Unfortunately, owing to the high cost of subscriptions for many journals,
        the library at my institution (like many other libraries) is forced to make difficult
        choices about which journals we can afford. The number of journal subscriptions goes down
        in proportion to the rise of subscription costs, but fortunately this loss is being offset
        by the creation of new open-access journals.
      
      
        The Promise of the Internet
        I have been teaching undergraduates since 1993 and have noticed a trend in the way I
        teach—increasingly, I have provided research papers to my students so they can learn to
        read those papers and improve their critical thinking skills. One reason for my increased
        use of research papers is the development of PDFs. When I first started using journal
        articles in my molecular biology course, the class had to meet in the library so we could
        pass around the bulky bound volumes to detect the important subtleties often lost in
        photocopied versions of figures. Later, I learned how to scan the figures and generate Web
        pages so that I could project the images in class and so that students could print
        laser-quality versions of papers (see http://www.bio.davidson.edu/molecular). Now I use PDF
        files for students to print and for me to display in class with no loss of information due
        to reformatting or resolution problems (Figure 1).
        With my increased confidence from using research papers in my molecular biology class, I
        began experimenting with research papers for my introductory students. First-year students
        are not ready to critically evaluate complex data, but they are beginning their first
        forays into reading review articles and occasionally original research papers. When
        introductory students make presentations of their findings in laboratory courses,
        increasing numbers are utilizing PubMed and PDF reprints when they are available.
        Students have been reading primary research papers since well before PDF files became
        available, but the increased access to papers online and the improved quality of the format
        has significantly enhanced the use of research and review papers in the undergraduate
        curriculum. It is common for students in upper-level lecture and lab courses to read papers
        (DebBurman 2002; Hall and Harrington 2003; Kitchen et al. 2003; Mulnix 2003), and seminar
        courses are usually dominated by student presentations of literature (Wright and Boggs
        2002; Hales 2003; Lom 2003).
        It is worth noting that most colleges and universities are being told to reduce
        expenditures, and one frequent target of money-saving measures is the ever-increasing costs
        of library journal subscriptions. This fiscal reality will erode the pedagogical gains made
        by faculty who are already meeting one of the goals of the BIO2010 report by immersing
        students in the research literature. However, open-access journals are proving to be
        virtual oases in a desert of pay-per-view journals that are available on a sliding scale
        that favors the richest and biggest institutions.
      
      
        Using Open-Access Resources for Creative Teaching …
        During the past three years, I have taught an undergraduate course in genomics
        (www.bio.davidson.edu/genomics) in which I capitalize on a confluence of two trends in the
        field: public domain databases and open-access journals (Campbell 2003). In my genomics
        class, students have three assignments for which they are required to mine databases for
        sequence, transcriptome, and proteome information (see
        www.bio.davidson.edu/courses/genomics/2003/cain/home.html). But genomics courses are not
        the only beneficiaries, since other classes at many institutions (e.g., introductory
        biology, biochemistry, cell development, genetics, microbiology, molecular biology [see
        http://www.bio.davidson.edu/courses/Molbio/standardsHP.html#anchor78181983], and
        neuroscience) require students to mine public domain databases (Dyer and LeBlanc 2002;
        Honts 2003). This year, we introduced genome database searching to our introductory biology
        students (see www.bio.davidson.edu/people/macampbell/Hope/DQ/DQ9.html and
        www.bio.davidson.edu/people/macampbell/Hope/DQ/DQ10.html). First-year students use Genome
        Browser and BLAST to determine the molecular causes of cystic fibrosis and Huntington
        disease, respectively. The benefit of public databases and open-access literature to
        educators is obvious and immediate. Images can be used in lectures, and papers can be
        distributed easily and on short notice for class use. There is no need to worry about
        limited access due to subscription costs nor an obligation to obtain copyright permission
        from publishers, which is a bothersome and sometimes expensive process for busy faculty
        members. By reducing nonproductive busy work for faculty, open-access journals have already
        created an environment that is improving undergraduate education today with long-term
        benefits in creating research-ready graduate students.
        Students who are exposed to publicly available literature through their coursework often
        develop an expectation that all research papers will be freely available to them from any
        computer and become frustrated if they do not have access to all the journal articles they
        want and need to read. Increasingly, I have students sending me PDF files of open-access
        journal articles they have read and want to share with me. Who would have guessed that free
        access to journals would result in students mining the literature for relevant papers and
        sending them to their instructors for consideration? In addition to papers related to their
        own classes and research, students also enjoy learning about “hot topics” from scientific
        publications and those stories that quickly reach the popular press. Examples include the
        use of DNA microarrays and sequencing to identify the causative agent for SARS (Wang et al.
        2003) and a good review article of small inhibitory RNA (Dillin 2003).
        Two common educational goals are to encourage students to become skeptical of
        unsubstantiated claims and to enable students to evaluate data critically. One way to
        accomplish these goals is to capitalize on the natural curiosity of students and ask them
        to compare topics in the popular press to that in the scientific literature (see
        http://www.bio.davidson.edu/courses/genomics/2003/poulton/p21.html). Open-access journals
        make these two educational goals much more feasible because students can utilize current
        findings immediately without having to wait for interlibrary loans, which can take up to
        two weeks, can cost up to $20 per article, and can result in poor-quality black-and-white
        photocopies.
      
      
        … and for Thought-Provoking Testing
        If we want students to achieve higher levels of thinking (Bloom et al. 1956), we need to
        model our courses so students can learn by examples and are rewarded for learning to
        critically evaluate data and for inspecting evidence before believing claims made by
        authors (Brill and Yarden 2003). Students quickly figure out what intellectual behaviors
        are rewarded in exams. If exam questions simply require students to regurgitate factoids,
        then higher levels of thinking are unlikely to be demonstrated by students. It is difficult
        to create good exam questions that cover the course material and reward students who have
        learned to read critically and to interpret data. Over the last few years, increasingly I
        have turned to current literature to find raw data for my exam questions. For example, for
        my genomics class in Fall 2003, I used a paper published in 
        PLoS Biology that utilized DNA microarrays to analyze the life cycle of
        malaria-causing 
        Plasmodium (Bozdech et al. 2003). I asked students to interpret several
        figures, using their own words (Figure 2). Owing to my choosing to use an open-access
        journal, my students also had full access to the supporting information, which two students
        utilized to enhance their answers. For this question, these two produced answers that were
        better than mine. Another exam question required students to mine a database associated
        with the Bozdech paper (see http://malaria.ucsf.edu/index.php). Students were asked to
        combine what they learned from the paper and the course and choose new proteins (in
        addition to the ones described in Bozdech et al. [2003]) that would make good candidates
        for vaccines based on the timing of gene transcription. In order to answer this question,
        students performed the first steps in real research, which rewards students for learning
        higher-order thinking skills.
        At the end of their exam, students were given an opportunity for extra credit points (a
        maximum of three points out of 100 available on the exam) if they provided constructive
        criticism directly to the database curators. About 70% of the students sent comments,
        including this one: “In recently using your database, I found it difficult to search the 
        Plasmodium gene expression data with multiple constraints. For example,
        it would be helpful if there were a way to identify all the genes within a certain
        functional group that fell within certain time or amplitude constraints. Is this possible
        in this database?” The curators very professionally responded to the students' suggestions,
        which resulted in three new search capacities being added to the database, as can be seen
        on the left side of the main page (see http://malaria.ucsf.edu/index.php). As a result of
        these professional interactions, students became participants in a community of scholars,
        interacting with investigators at the University of California, San Francisco, while taking
        their exams.
        The use of open-access journals for teaching and testing has already improved my
        courses. I can provide exam questions that are more interesting, more educational, and more
        current. Furthermore, I accomplish two tasks simultaneously: I keep abreast of new
        developments in my field and I write exam questions. But what do students think? While I
        have not formally assessed student attitudes, I have collected information from
        end-of-semester course evaluations, including the following comments: “One of the best
        parts of the entire course for me were the exams. The exams really gave me an opportunity
        to show how I could work through real problems. This class definitely increased my critical
        thinking skills. Each test presented me with new ideas and problems to work through. I
        enjoyed the idea that each exam would be a learning experience.”
      
      
        The Future
        Teaching is a lot like raising children. Like parents, teachers provide learning
        opportunities in part by modeling the behavior we want our students to learn. By choosing
        the most current literature as testing material, my students realize that I read the
        literature to stay current in my field and that there are always new opportunities to
        learn, analyze, and design experiments, etc. By my choosing open-access papers such as
        those published in 
        PLoS Biology , my students benefit from free access to published research
        results. Free access to research literature enhances student learning and helps produce the
        next generation of graduate students, who are then better trained. Open-access publishing
        provides the right mix of benefits for educators and students alike.
      
    
  

  
    
      
        
        
          “… a complete, comprehensive understanding of odor … may not seem a
          profound enough problem to dominate all the life sciences, but it contains, piece by
          piece, all the mysteries.” — Lewis Thomas
        
        One of the oldest beliefs about human perception is that we have a poor sense of smell.
        Not only is this a general belief among the public, but it appears to have a scientific
        basis. Recent genetic studies show a decline in the number of functional olfactory receptor
        genes through primate evolution to humans. Human evolution was characterized by the gradual
        ascendance of vision and reduction of smell, evidenced in the anthropological record by the
        progressive diminution of the snout as the eyes moved to the middle of the face to subserve
        depth vision (Jones et al. 1992). Concurrently, the use of an arboreal habitat and the
        adoption of an erect posture moved the nose away from the ground, with its rich varieties
        of odors.
        However, some recent behavioral studies suggest that primates, including humans, have
        relatively good senses of smell. Resolution of this paradox may come from a larger
        perspective on the biology of smell. Here we begin by reassessing several overlooked
        factors: the structure of the nasal cavity, retronasal smell, olfactory brain areas, and
        language. In these arenas, humans may have advantages which outweigh their lower numbers of
        receptors. It appears that in the olfactory system, olfactory receptor genes do not map
        directly onto behavior; rather, behavior is the outcome of multiple factors. If human smell
        perception is better than we thought, it may have played a more important role in human
        evolution than is usually acknowledged.
      
      
        Gene Studies
        From rodents through the primate series to humans there is a progressive reduction in
        the proportion of functional olfactory receptor genes (Rouquier et al. 2000; Gilad et al.
        2004). Mice have approximately 1,300 olfactory receptor genes, of which some 1,100 are
        functional (Young et al. 2002; Zhang and Firestein 2002), whereas humans have only some 350
        functional genes of approximately 1,000 (Glusman et al. 2001; Zozulya et al. 2001). The
        conclusion seems obvious: the low number of functional olfactory receptor genes in humans
        compared with rodents—and presumably most other mammals—is directly correlated with the
        evolutionary decline in the human sense of smell.
      
      
        Behavioral Studies
        Although these conclusions seem incontrovertible, they are challenged by some recent
        behavioral studies. One type of study shows that much of the olfactory system can be
        removed with no effect on smell perception. The olfactory receptor genes map
        topographically onto the first relay station, a sheet of modules called glomeruli in the
        olfactory bulb. Up to 80% of the glomerular layer in the rat can be removed without
        significant effect on olfactory detection and discrimination (Bisulco and Slotnick 2003).
        If the remaining 20% of the glomeruli—and the olfactory receptor genes they represent—can
        subserve the functions of 1,100 genes, it implies that 350 genes in the human are more than
        enough to smell as well as a mouse.
        Another type of study has tested smell perception in primates, and has shown that,
        despite their reduced olfactory receptor gene repertoire, primates, including humans, have
        surprisingly good senses of smell (Laska et al. 2000). Comparing the data on smell
        detection thresholds shows that humans not only perform as well or better than other
        primates, they also perform as well or better than other mammals. When tested for
        thresholds to the odors of a series of straight-chain (aliphatic) aldehydes, dogs do better
        on the short chain compounds, but humans perform as well or slightly better than dogs on
        the longer chain compounds, and humans perform significantly better than rats (Laska et al.
        2000). Similar results have been obtained with other types of odors.
        A third type of study demonstrating human olfactory abilities shows that in tests of
        odor detection, humans outperform the most sensitive measuring instruments such as the gas
        chromatograph.
        These results indicate that humans are not poor smellers (a condition technically called
        microsmats), but rather are relatively good, perhaps even excellent, smellers (macrosmats)
        (Laska et al. 2000). This may come as a surprise to many people, though not to those who
        make their living by their noses, such as oenologists, perfumers, and food scientists.
        Anyone who has taken part in a wine tasting, or observed professional testing of food
        flavors or perfumes, knows that the human sense of smell has extraordinary capacities for
        discrimination.
      
      
        The Mystery
        Here, then, is the mystery: how can one reconcile a relatively high sensitivity to smell
        with a relatively low number of olfactory receptors in the nose? To answer this question, I
        think we need to look beyond the olfactory receptor genes and consider olfaction in its
        full behavioral context. This requires considering several overlooked aspects of the
        olfactory system: the nasal cavity, the oropharyngeal cavity, the olfactory brain, and the
        role of language. In this article I focus on behaviors related to conscious perception of
        ordinary smells. Pheromones, and the rich world of unconscious effects of odors and
        pheromones, are beyond the present scope (cf. Jacob et al. 2004), though they undoubtedly
        will add to the general conclusions.
      
      
        The Filtering Apparatus of the Nasal Cavity
        A marked difference between the noses of primates and other mammals is that in nearly
        all nonprimate mammals, the nasal cavities contain at the front a much-convoluted filtering
        apparatus (formed by the ethmo- and maxillo-turbinals) covered with respiratory membrane.
        This filtering apparatus is a biological air conditioner (Negus 1958) with three key
        functions: cleaning, warming, and humidifying the inspired air. An important function of
        the filtering apparatus is presumably to protect the nasal cavity from infections. In many
        mammals, air drawn into the nose is often highly contaminated with bacteria from fecal
        material, decaying animal and plant material, and noxious fumes from the environment, all
        of which attack the olfactory epithelium. Rodents are susceptible to chronic rhinitis,
        which causes substantial loss of functioning olfactory receptor cells (Hinds et al.
        1984).
        This filtering, however, might have negative consequences for odor detection. Warming
        and humidification presumably enhance the odor-stimulating capacity of the inhaled air, but
        cleaning would remove odor molecules by absorbing them into the lining of the epithelium,
        an effect which could be large depending on the size of the filtering apparatus. If so,
        mammals with large snouts might have a large inventory of olfactory receptors at least in
        part to offset the loss of odor molecules absorbed by the filtering apparatus.
        How do these considerations relate to humans? The evolution of humans involved lifting
        the nose away from the noxious ground environment as they adopted a bipedal posture (Aiello
        and Dean 1990). This would have reduced the need for the filtering apparatus and with it
        the losses of absorbed odor molecules. The large numbers of olfactory receptors and
        receptor cells would have come under reduced adaptive pressure and could accordingly be
        reduced in proportion.
        By this hypothesis, during human evolution the snout could be reduced in dimensions and
        complexity without compromising the ultimate amounts of odorized air reaching the olfactory
        epithelium. The reduced snout allowed the eyes to come forward and lie closer together to
        promote more effective stereoscopic vision. Thus, vision could become more dominant in
        humans without sacrificing unduly the sense of smell. Tests of this hypothesis are needed,
        including calculations of air flows and odor losses through the filtering apparatus in
        mammals with extensive filtering apparatuses compared with the simpler nasal cavities of
        primates.
      
      
        Humans Receive Richer Retronasal Smells
        Being carried in with inhaled air (the orthonasal route) is not the only way for odor
        molecules to reach the olfactory receptor cells. Odor molecules also reach the olfactory
        receptor cells via the retronasal route, from the back of the oral cavity through the
        nasopharynx into the back of the nasal cavity. Although the orthonasal route is the one
        usually used to test for smell perception, the retronasal route is the main source of the
        smells we perceive from foods and liquids within our mouths. These are the smells that
        primarily determine the hedonic (i.e., pleasurable or aversive) qualities of foods, and
        that, combined with taste and somatosensation, form the complex sensation of flavor. It is
        likely, for several reasons, that this is an important route for smell in humans.
        First, with the adoption of bipedalism, humans became increasingly wide ranging, with
        concomitant diversification of diet and retronasal smells. Second, the advent of fire,
        perhaps as early as 2 million years ago (Wrangham and Conklin-Brittain 2003), made the
        human diet more odorous and tasty. From this time also one can begin to speak of human
        cuisines of prepared foods, with all their diversity of smells. Wrangham and
        Conklin-Brittain (2003) support the view that prepared cuisines based on cooked foods are
        one of the defining characteristics of humans. Third, added to the cooked cuisines were
        fermented foods and liquids, with their own strong flavors. These developments occurred
        among the early hunter-gatherer human cultures and continued through the last ice age. With
        the transition to agricultural and urban cultures 10,000 years ago, human cuisines changed
        by the advent of animal domestication, plant cultivation, use of spices, and of complex
        procedures, such as those for producing cheeses and wines, all of which produced foodstuffs
        that especially stimulate the smell receptors in the nose through the retronasal route and
        contribute to complex flavors.
        These considerations suggest the hypothesis that the retronasal route for smells has
        delivered a richer repertoire of smells in humans than in nonhuman primates and other
        mammals (see Figure 1). Research on retronasal olfaction is being actively pursued
        (reviewed in Deibler and Delwiche 2004). Studies are needed of the evolutionary pressures
        on this route in addition to the pressures on the evolution of the snout.
      
      
        Humans Smell with Bigger and Better Brains
        Comparisons of the decreasing size of the olfactory system relative to expansion of the
        visual, auditory, and somatosensory systems usually focus on the olfactory bulb and lateral
        olfactory tract, which are relatively small. However, what matters more are the central
        olfactory brain regions that process the olfactory input as the basis for smell
        perception.
        These regions are more extensive in humans than is usually realized. The dedicated
        olfactory regions include the olfactory cortex, the olfactory tubercle, the entorhinal
        cortex, parts of the amygdala, parts of the hypothalamus, the mediodorsal thalamus, the
        medial and lateral orbitofrontal cortex, and parts of the insula (Neville and Haberly
        2004). These regions are involved in immediate processing of odor input and probably
        subserve the specific tasks of smell detection and simple smell discrimination. For more
        complex tasks, memory becomes important in comparing smells, thus involving the temporal
        and frontal lobes (e.g., Buchanan et al. 2003) and the specifically human higher
        association areas. It may be hypothesized that these regions enable humans to bring far
        more cognitive power to bear on odor discrimination than is possible in the rodent and
        other mammals.
        The reduced repertoire of olfactory receptor genes in the human is thus offset by the
        expanded repertoire of higher brain mechanisms. Rather than being restricted to a tiny part
        of the brain, olfactory processing of complex smells, such as those produced by human
        cuisines, draws on the enlarged processing capacity of the human brain.
      
      
        Language Is Necessary for Human Smell
        In the enlarged processing capacity for perceiving and discriminating odors, language
        plays a critical role. This seems paradoxical, for we have great difficulty describing a
        smell in words. Insight into this difficulty comes from the finding that different smells
        are represented in the olfactory bulb by different patterns of olfactory glomerular
        activity. These patterns function as virtual “odor images” (Xu et al. 2003). It has been
        hypothesized that these odor images provide the basis for discrimination between odors,
        analogous to the way that retinal images are the basis for discrimination of visual pattern
        stimuli. The complex patterns constituting odor images may be considered as analogous to
        the complex patterns constituting visual images of faces. And just as we are very good at
        recognizing a human face, yet have difficulty describing it in words, we have a hard time
        describing and verbally comparing odor images.
        Because of this difficulty, describing a smell or a taste in words is very demanding. A
        professional wine tasting, for example, requires many steps: analysing both orthonasal and
        retronasal perception, comparing the two in memory with each other and with all other wines
        to be compared, identifying the constituent properties separate from the hedonic qualities,
        and finding the words to describe the process as it unfolds, leading to the final
        formulation to characterize the quality of the wine and identify it as distinct from all
        others. It may be characterized as hard cognitive work that only a human, among all the
        animals with olfactory organs, can do. It may be argued that this is what humans are
        adapted to do (Wrangham and Conklin-Brittain 2003).
        This cognitive work is largely independent of the numbers of peripheral receptor cells
        and their genes. A good analogy is with language. There are some 17,000–20,000 auditory
        nerve fibers in the rat and cat and some 25,000–30,000 in the human (cf. Hall and Masengill
        1997). This modest increase in the input from the peripheral auditory receptors provides
        little basis for the development of human speech and language, which had much more to do
        with the increase in the central brain mechanisms that elaborate the input. It may be
        hypothesized that a similar conclusion applies to human olfaction.
      
      
        Implications for Systems Biology
        A general result from these considerations is that there appears not to be a one-to-one
        relation between the number of olfactory receptor genes and the detection and
        discrimination of odors. This implies that we are dealing with a fundamental problem in
        relating genes to systems behavior: a given set of genes may not map directly onto a given
        behavior. In this respect the mystery being addressed here is a caution for the new era of
        “systems biology” and against any belief that behavior can be related directly to genomes,
        proteomes, or any other type of “-ome.” We are reminded instead that the functional ecology
        of the body is dependent on many factors.
      
      
        Conclusions
        Much about the sense of smell seems enigmatic and conflicting. This is partly because of
        the inherent difficulties in presenting smell stimuli, and partly because there is not yet
        a recognition of all the relevant mechanisms that are involved.
        It may be hoped that the hypotheses and mechanisms discussed here can help to address
        and resolve the mystery of the apparent noncorrelation of olfactory receptor gene numbers
        with smell acuity, and in doing so stimulate a major reassessment of human smell
        perception. Such an effort cuts across many academic disciplines. Molecular biologists need
        to continue their efforts to characterize the olfactory genomes of humans and nonhuman
        mammals more closely, to compare how different organisms sample odor space. Physiologists
        need to devise high-throughput systems to test these odor spaces. Behavioral
        neuroscientists need to develop increasingly accurate tests of olfactory function that
        enable comparisons across different species. Psychologists need to explore even more
        vigorously the subtle ways that smells can influence human behavior. Anthropologists and
        paleontologists need to study the olfactory parts of the cranium and face from this new
        perspective, to reassess the role that both orthonasal and retronasal smell may have played
        in primate and human evolution.
        The factors reviewed here suggest that the sense of smell is more important in humans
        than is generally realized, which in turn suggests that it may have played a bigger role in
        the evolution of human diet, habitat, and social behavior than has been appreciated. All of
        these considerations should stimulate a greater interest in this neglected sense.
      
    
  

  
    
      
        
        This book, the latest in the excellent Monographs in Population Biology series from
        Princeton University Press, is a work of advocacy in which the authors argue that
        evolutionary theory is incomplete and that, in consequence, we are failing fully to
        understand phenomena as disparate as ecosystem development and the interplay of genes and
        culture in shaping human evolution. What we are missing, they argue, is an appreciation of
        niche construction, the process by which an organism modifies the abiotic and biotic
        environment in which it is subject to natural selection. The authors' major assertion is
        that the importance of niche construction is so great that it should be regarded “after
        natural selection, as a second major participant in evolution” and that it is “not just an
        important addition to evolutionary theory” but “requires a reformulation of evolutionary
        theory”. Bold claims indeed.
        After introducing the conceptual framework Odling-Smee et al. set out a series of
        arguments to support this position, the first of these being the empirical case for the
        existence of niche construction. Niche construction, as broadly interpreted here, is
        everywhere. Animals build nests, burrows, and protective cases and so alter the environment
        they experience in a way that may select for further adaptations. The changes caused by
        some animal species, such as beavers and earthworms, are of a sufficient magnitude that the
        environment experienced by a host of other species is affected. Many plant species also
        modify the environment they experience by generating organic litter; influencing
        hydrological and biogeochemical cycles; affecting temperature, humidity, and light regimes;
        and, over the longer term, determining the make up of the atmosphere. Decomposer and
        chemoautotrophic microorganisms similarly influence biogeochemical transformations, while
        parasitic species can manipulate the behaviour and internal environment of the hosts they
        infect. Perhaps less obvious examples of niche construction are the many types of migration
        and cultural evolution that, like physical transformations, cause the organism's
        descendants to experience a different selective environment.
        After this broad, accessible survey, the authors change key rather abruptly and explore
        two-locus, frequency-dependent population genetics. The novelty here is that selection on
        one locus depends on the history of gene frequencies at the other, “niche construction”,
        locus. In an extension, gene frequencies at one locus affect an environmental variable with
        its own dynamics that in turn influences the second locus. As one would guess, the models
        display a range of potentially interesting dynamics, though generalisations and broad
        conclusions are sparse. We guess the aim of the chapter is to illustrate that environmental
        feedbacks can be potent and general agents of evolutionary change, but the restriction of
        the theory to such a narrow model, with very technical explanation, risks losing the few
        readers who we suspect will stay the course (did we really need a rederivation for
        haplodiploids?).
        Perhaps aware of the dangers of getting bogged down in detail, the argument then moves
        to proving a case for the universality of niche construction. Invoking the second law of
        thermodynamics and Maxwell's Demon, the authors lead us through a challenging thesis that
        concludes that the persistence of life on earth requires both natural selection and niche
        construction, thereby justifying some of the bold claims for their new theory. We think
        they are technically correct, but we are concerned that the demonstration of the
        inescapability of niche construction, as defined here, does not guarantee that it will
        actually tell us new and important things about the world, as the theory of natural
        selection has.
        The remainder of the book explores the implications of niche-construction thinking for
        evolutionary biology, ecology, and the human sciences, and in our view is the most
        successful part. Though it is rare for the authors to offer new analysis and insight, their
        sideways look at many issues from the niche-construction viewpoint often offers interesting
        new angles on old problems, and suggests new avenues of enquiry that may be the book's
        greatest legacy. A good example of this is their convincing and timely argument that a more
        explicit recognition of evolution's role in environmental feedbacks will help to unify
        population/community ecology and ecosystem science.
        The chief argument for the prosecution is that niche construction is common but not
        pervasive, and that wherever ecologists and evolutionists have found interesting examples
        of it, they have developed appropriate theory and concepts to understand its ramifications.
        For example, some of the clearest examples of niche construction occur in plant succession
        where, as F.E. Clements realised nearly a hundred years ago, early-succession plants
        frequently modify their environment in ways that allow other species to replace the
        pioneers. Interestingly, the strict Clementsian theory of facilitation, niche construction 
        avant la lettre , has given way to a more pluralistic theory of
        succession. It is a great pity that the authors give so little space to plants and plant
        ecology, as it here that some of the finest examples of niche construction are found, as
        well as the best-developed conceptual framework for studying the roles of environmental
        feedback.
        Other areas where biologists have well-developed theories of the influences and impacts
        of niche construction include co-evolutionary theory, where the environmental feedbacks are
        largely biotic, and Dawkins' theory of the extended phenotype. Very close to some of the
        arguments discussed here, and generously acknowledged, is the idea developed by Jones,
        Lawton, and colleagues of ecosystem engineers, species that have a major impact on the
        abiotic environment experienced by a large number of species.
        A major strength of the book is that it reveals common processes and patterns underlying
        disparate biology in consistently interesting ways. Its chief contribution is thus not to
        tell us new things about how nature works but to link together many different aspects of
        ecology under an umbrella of theory that may in the future lead to new insights. Do they
        deliver on their grand claims? Time will tell, but our view is that they don't. They
        engagingly admit that for their project to succeed the new theory must earn its keep by
        producing significant new biology—something which has yet to occur. However, the great need
        for the biological and human sciences to integrate across subdisciplines, as the authors
        bravely attempt here, makes this a hugely worthwhile book. Its breadth of scope and its
        boldness in creating syntheses have resulted in a stimulating and challenging read.
      
    
  

  
    
      
        
        Francis Hamilton, the Briton who first described zebrafish (
        Danio rerio ) in 1822, would be astounded to see the scientific
        attention now afforded to this two-inch-long native of Indian rivers. A fish with no
        economic worth was how he described this little creature. Yet recently, the European Union
        awarded 12 million Euros to the ZF-MODELS research consortium to study zebrafish models for
        human development and disease. When and why did zebrafish swim from home aquaria into
        research labs, and what can we learn about our biology from this surprising source?
      
      
        The Early Days
        It was the late 1960s when phage geneticist George Streisinger began to look for a model
        system in which to study the genetic basis of vertebrate neural development. His passion
        for tropical fish led him to the humble zebrafish. He was a ‘visionary’, remembers
        neurobiologist Judith Eisen (University of Oregon, Eugene, Oregon, United States), ‘who
        laid the groundwork for the use of zebrafish as a developmental model’.
        Eisen, who now heads her own research group, went to Oregon in 1983 to work on 
        Xenopus neural development but soon became attracted to
        zebrafish as a model organism. By the early 1980s, she explains, Streisinger had worked out
        many of the genetic tricks needed to tackle zebrafish development. What's more, the fish
        had ‘wonderful embryology’. The embryo, which develops outside its mother, is transparent.
        ‘You can see different cell types, watch individual cells develop, do transplantation
        experiments’, Eisen enthuses, ‘and development is quick but not too quick’. Being able to
        watch individual neurons developing in real time opened up whole new avenues of research
        for Eisen and other neurobiologists.
      
      
        Fast Forward to the Big Screen
        The properties of zebrafish that attracted Eisen soon attracted people interested in
        other aspects of vertebrate development to the stripy tiddler (Figure 1). As Eisen
        comments: ‘No other developmental model has risen to prominence so quickly’. These days
        more than 3,000 researchers are listed on ZFIN, a United States–based information resource
        for the zebrafish research community.
        The speedy expansion was driven in great part by two genetic screens initiated in
        1992–1993 by Christiane Nüsslein-Volhard in Tübingen, Germany, and Wolfgang Driever and
        Mark Fishman in Boston, Massachusetts. The aim of both screens was to identify genes with
        unique and essential functions in zebrafish development, and in 1996 an issue of the
        journal 
        Development was dedicated to the mutants that had been isolated and
        characterised. These screens, says Ralf Dahm (Max Planck Institute for Developmental
        Biology, Tübingen, Germany), project manager of the ZF-MODELS consortium, ‘were the first
        major zebrafish projects, and they showed that zebrafish was a model organism to be
        reckoned with’.
        ‘That was a fantastic time’, says Derek Stemple, then a postdoc with Driever but now a
        group leader at the Wellcome Trust Sanger Institute (Cambridge, United Kingdom) and a
        ZF-MODELS participant. ‘From Wolfgang's lab, I was able to take the mutations that affected
        notochord development, and have been studying them ever since’. The notochord is an
        embryonic structure that forms the primitive axial skeleton of the developing embryo, and
        because mutations affecting notochord development result in shortened embryos, seven of the
        affected genes have been named after the dwarves in Snow White—zebrafish, like some other
        developmental models, have many imaginatively named mutants. Stemple now knows the identity
        of six of these mutated genes, all of which lead to disruption of basement membrane around
        the notochord.
        Many mutants from those first two screens are still used by developmental biologists,
        but another set of mutants has recently been isolated by Nancy Hopkins, Amgen Professor of
        Biology at the Massachusetts Institute of Technology (Cambridge, Massachusetts, United
        States). About ten years ago, Hopkins started to develop insertional mutagenesis in
        zebrafish (Figure 2). In this approach, mutations are caused by the random insertion of
        viral DNA throughout the fish genome. The inserted DNA acts as a tag, making cloning of
        mutated genes very straightforward, although the efficiency of the initial insertional
        mutagenesis is much lower than that of the chemical mutagenesis used in the 1992–1993
        screens. Hopkins has isolated 550 mutants in her screen, representing around 400 different
        genes, and has cloned more than 300 of these genes to date. Some of the fruits of this
        project are published in this issue of PLoS Biology. Hopkins's group is now collaborating
        with 25 external laboratories on the annotation of the mutant collection with funding from
        the National Center for Research Resources, part of the United States National Institutes
        of Health.
        The Tübingen researchers did another chemical mutagenesis screen between 2000 and 2001,
        and are now starting a third screen of 6,000 genomes as part of the ZF-MODELS project.
        ‘Each of our screens has built on the previous one by including more specific assays’,
        explains Dahm. Mutagenesis for the third screen is underway, but the assays, which include
        looking for defects that specifically affect adults, are still at the pilot stage; this
        autumn, the project's executive committee, which is headed by Nüsslein-Volhard, will decide
        which assays to use in the full-scale screen. ‘Just over half the 17 partners in the
        consortium will come to Tübingen to do screens’, predicts Dahm. ‘By bringing in expertise
        in different systems in this way we should greatly increase the efficiency of the
        screen’.
      
      
        What Else Will ZF-MODELS Do?
        The ZF-MODELS consortium, which is funded under the European Union's Sixth Framework
        Programme, aims to establish zebrafish models for human diseases, discover genes that will
        lead to the identification of new drug targets, and gain fundamental insights into human
        development. ‘We will mainly focus on using advanced technologies that have recently become
        available’, says scientific coordinator Robert Geisler (Max Planck Institute for
        Developmental Biology). For example, Geisler's lab will use DNA chip technology to
        investigate gene expression patterns in zebrafish mutants and so provide increased
        knowledge of the regulatory pathways that act in zebrafish development.
        Consortium members will also use ‘reverse genetics’ to investigate these pathways. In
        reverse genetics, researchers start with a gene of interest and investigate the phenotypic
        effect of altering its activity; by contrast, in ‘forward genetics’ the starting point is
        to look for a particular phenotype and then hunt out the altered gene that is causing it.
        Two reverse genetics approaches will be used by the consortium. Gene expression will be
        transiently knocked down with morpholinos, short segments of the gene that block its
        function. In addition, a recently developed technique known as TILLING (targeting induced
        local lesions in genomes) will be used to knock out gene activity permanently.
        The first step in TILLING is to mutagenise male zebrafish and mate them with untreated
        female fish, explains Stemple, whose group is one of three ZF-MODELS partners who will use
        this approach. Offspring are raised to adulthood, and the DNA of each individual is then
        genotyped for the exon of interest. The consortium already has a collection of 6,000 such
        individuals, and once a fish carrying a mutation in the gene of interest has been
        identified, it will be outcrossed to produce offspring, half of which will carry the
        desired mutation on one of their chromosomes. ‘It is then a matter of identifying these
        heterozygote fish and incrossing them to get homozygous fish in which you can see the
        phenotype that correlates with that mutation’, says Stemple.
        As well as helping to produce knock-outs for other researchers, Stemple is also using
        the TILLING technique to develop zebrafish models for muscular dystrophy. Among the genes
        that are important in notochord development are those that encode laminins. This led
        Stemple into studying muscular dystrophy because laminins are involved in the human
        disease. ‘When we used morpholinos to disrupt [the production of] dystroglycan, a laminin
        receptor, we got a good model for muscular dystrophy’, he explains. Now, he plans to use
        TILLING to disrupt up to 30 other genes known to be involved in human muscular dystrophy.
        ‘In particular, we will look for hypomorphic mutants, fish that are viable but on the edge
        of falling apart’. These mutants can be used to identify small molecules that push the fish
        into muscular dystrophy. Finding molecules that can cause a disease in this way ‘might give
        us a handle on something to fix the disease’, says Stemple.
        In another strand of the ZF-MODELS project, zebrafish expressing green fluorescent
        protein (GFP) in specific cells or tissues will be generated and characterised (Figure 3).
        In such fish, developing structures can be easily imaged over time in the living embryo.
        One researcher working on this aspect of the project is Stephen Wilson, Professor of
        Developmental Genetics at University College London (United Kingdom). GFP lines can be made
        either by attaching to the GFP gene regions of DNA that control, or ‘drive’, GFP expression
        in selected cell types or by allowing the GFP gene to insert randomly in the genome and
        looking for fish with specific expression patterns. ‘There are now many lines of fish
        available with different GFP expression patterns’, says Wilson, ‘and it is important to
        catalogue their expression so that people can use the most appropriate lines for their
        research’.
        Wilson's own interest is in neuroanatomy. Together with Jon Clarke, another
        developmental neurobiology group leader at University College London, he plans to analyse
        GFP lines in which small groups of neurons or particular parts of neurons are labelled, and
        in this way start to build a detailed reconstruction of early brain neuroanatomy. This, in
        combination with other work on zebrafish carrying mutations affecting neural development,
        will give the team ‘a better picture of how a vertebrate brain is built’.
        A final, important aspect of the ZF-MODELS project, adds Dahm, is database construction.
        ‘We will be developing a set of databases that will integrate all of the project data’, he
        explains. ‘In addition, we hope to integrate our data with that of ZFIN in the United
        States to make one central zebrafish resource’.
      
      
        But Fish Aren't People
        The researchers of the ZF-MODELS consortium are understandably excited about
        participating in what will, says Geisler, bring an already strong European zebrafish
        community closer together. But zebrafish researchers in the United States are also excited
        by the ZF-MODELS project. ‘We need big lab models like ZF-MODELS in developmental biology’,
        says Hopkins, noting that the days of small groups working in isolation are long gone. This
        consortium, adds Howard Hughes Medical Institute Investigator Leonard Zon (Harvard Medical
        School, Boston, Massachusetts, United States), ‘will not only have an effect on European
        zebrafish science but also on how it is done in the United States’.
        But how much can zebrafish tell us about human development and disease? A lot, say
        zebrafish researchers. ‘Fish really are just little people with fins’, says Hopkins. ‘Of
        course, there are developmental differences between people and fish, and no one pretends
        that we can answer every question about human development in zebrafish’. Nevertheless,
        zebrafish studies can provide valuable clues to the genes involved in human diseases and to
        potential targets for therapeutic interventions. Hopkins provides the following
        illustration: ‘We have been doing “shelf screens”, in which we go back to our collection of
        mutants to find all those that affect the development of a single organ. When Zhaoxia Sun,
        a postdoc in my lab who now has an independent position at Yale Medical School, screened
        three-day-old fish for cystic kidney disease, she found 12 different genes. Two were known
        to cause human cystic kidney disease, so we knew we were in the human disease pathway
        somewhere, but we had no idea what the other genes were’. Hopkins and Sun have since
        identified the remaining genes, and these point to a single pathway being involved in the
        human disease.
        Developmental geneticist Didier Stainier (University of California, San Francisco,
        California, United States) is also using zebrafish to study organ development, in
        particular, heart development. The zebrafish heart is like the early human heart—a tube
        with an atrium, ventricle, and valves. ‘Everything we have found in the fish is relevant to
        the human heart’, says Stainier. ‘Obviously, there are additional processes involved in
        humans, but the basic outline of heart development in fish and people is largely similar’.
        Stainier has a collection of zebrafish in which valve formation is faulty. ‘Some of the
        genes we have found will be involved in human congenital valve defects’, he predicts.
        Knowing the identity of these genes will be useful diagnostically, but, in addition,
        zebrafish studies can reveal exactly what has gone wrong at a cellular level. The ability
        to follow individual cells as organs develop is key to this, says Stainier, who reported in
        March that fibronectin is required for heart development because, by regulating the
        polarisation of epithelial cells, fibronectin ensures the correct migration of myocardial
        cells. And in this issue of PLoS Biology, Stainier's lab have identified another zebrafish
        gene that is involved in heart development—cardiofunk, which encodes a special type of
        muscle protein.
      
      
        A Proliferation of Zebrafish Models of Human Disease
        Many researchers are now recognising the value of zebrafish models of human disease.
        Over the past three to four years, says Zon, this area of research has become a growth
        industry. The interest in disease models has grown hand-in-hand with the development of
        morpholinos to knock out specific genes, and the advent of TILLING, says Zon, ‘has set off
        a whole new fury. There are now large numbers of investigators who will try to knock out
        their favourite gene and come up with a model’.
        Zon has worked on disease models for blood (Figure 4), blood vessel, and heart disorders
        but is currently studying zebrafish models of cancer. ‘We started by doing chemical
        mutagenesis and screened for cell-cycle mutants. These were embryonic lethals, but when we
        looked at heterozygote carriers of these mutations, some developed cancer at a high rate as
        adults’. Now Zon and his colleagues have returned to the cell-cycle mutant that yielded
        this cancer-susceptible heterozygote and are using embryos in high-throughput screening
        assays to look for small molecules that can suppress the cell-cycle phenotype. These
        molecules, reasons Zon, may have potential as anticancer drugs.
      
      
        And the Future of Zebrafish Research?
        Bigger and bigger seems to be the consensus. Chemical screens like Zon's for anticancer
        drugs can be set up for other human diseases such as muscular dystrophy. Work like
        Stainier's on organ development may have applications in tissue engineering. ‘If we can
        find out what drives differentiation in zebrafish’, he suggests, ‘we might be able to do
        the same for human cells’, making human tissue replacement therapy a practical possibility.
        And while many zebrafish researchers will continue to study development, others are now
        moving into the realms of physiology and behavioural studies.
        Geisler sums up zebrafish developmental research past, present, and future as follows:
        ‘No other [vertebrate] organism offers the same combination of transparent and accessible
        embryos, cost-effective mutagenesis screening, and, more recently, a sequenced genome,
        [DNA] chip, GFP, and knockout technology’. Add to that the potential of zebrafish embryos
        as a screening platform for small molecule libraries and the new technologies that allow
        forward and reverse genetics, and it is clear that zebrafish are not about to revert to
        being pretty pets swimming in small tanks in the corner of the living room.
      
      
        Where to Find Out More
        ZF-MODELS
        More details of the work included in this European Union Integrated Project can be found
        at http://www.zf-models.org
        ZFIN
        The ZFIN Web site, at http://z.n.org/ZFIN, provides an extensive database for the
        zebrafish community including genetic, genomic, and developmental information; search
        engines for zebrafish researchers and laboratories; listings of meetings; and links to many
        other zebrafish sites, including sites with movies of zebrafish development.
        The special issue of 
        Development (Dec 1; 1996; 123: 1–461) on the first two mutagenesis
        screens contains 37 research articles and can be freely accessed at
        http://dev.biologists.org/content/vol123/issue1/index.shtml
      
    
  

  
    
      
        
        Functional magnetic resonance imaging—fMRI—opens a window onto the brain at work. By
        tracking changes in cerebral blood flow as a subject performs a mental task, fMRI shows
        which brain regions “light up” when making a movement, thinking of a loved one, or telling
        a lie. Its ability to reveal function, not merely structure, distinguishes fMRI from static
        neuroimaging techniques such as CT scanning, and its capacity to highlight the neural
        substrates of decisions, emotions, and deceptions has propelled fMRI into the popular
        consciousness. Discussions of the future of fMRI have conjured visions of mind-reading
        devices used everywhere from the front door at the airport terminal to the back room of the
        corporate personnel office. At least one “neuromarketing” research firm is already trying
        to use fMRI to probe what consumers “really” think about their clients' products.
        But will fMRI's utility in the real world ever match the power we currently imagine for
        it? Is fMRI likely to leave the clinic for widespread use in the courtroom or the
        boardroom? Are there neuroethical nightmares just around the corner? Or are all these vivid
        specters really just idle speculations that will never come to pass?
      
      
        150,000 Grains of Rice
        To understand the potential, and the limitations, of fMRI, it's helpful to know how the
        technique works. The heart of the apparatus is a large donut-shaped magnet that senses
        changes in the electromagnetic field of any material placed in its center, in
        particular—when a head is scanned—the blood as it flows through the brain. When a region of
        the brain is activated, it receives an increased flow of oxygenated blood (the extremely
        rapid redirection of blood within the active brain is one of the underappreciated wonders
        supporting neural activity). This influx of oxygenated blood alters the strength of the
        local magnetic field in proportion to the increase in flow, which is detected and recorded
        by the imaging machinery.
        The resolution of the best fMRI machines—the smallest “volume picture element,” or
        voxel, they can distinguish and make an image of—is currently about 1.5 mm ×1.5 mm × 4 mm,
        the size of a grain of rice. There are approximately 150,000 of these little volumes in the
        typical brain, and the immense computers hooked up to the scanners record and integrate
        signals from all of them. In a typical experiment, a subject, lying still with his head
        surrounded by the magnet, does nothing for thirty seconds, then performs some task for
        thirty seconds, then lies still for thirty seconds. For each voxel, the signal during the
        task is compared to the signal at rest; those areas of the brain with stronger signals
        during the task are presumed to be processing the information that underlies the
        performance of the task (Figure 1). According to Joy Hirsch, Director of the Functional
        Magnetic Resonance Imaging Research Center at Columbia University, fMRI represents a
        “quantum leap” over any previous technology for imaging the brain. “It enables us for the
        first time to probe the workings of a normal human brain,” she says. “It's really opening
        the black box.”
        The first caveat about fMRI's imaging power, though, and one that every neuroimager
        stresses, is that a voxel is a long way from a neuron. There are an estimated 100 billion
        neurons, so at best, an fMRI is signaling blood flow changes associated with the increased
        activity of tens of thousands of neurons. As a result, says Hirsch, fMRI “falls short when
        we want to ask about more detailed brain processes. We're not learning that much about how
        neurons are doing local computing.” While resolution will improve over time, it seems
        unlikely that fMRI will ever detect the activity of individual neurons, and so its ability
        to dissect the “fine structure” of thought is inherently limited. (Even should it become
        possible to detect and integrate the workings of every neuron in the brain, it would still
        be far from clear how neuronal firing patterns translate into coherent, perceived thoughts,
        and this gap is unlikely to be bridged by any advance in imaging technology alone.)
        These limitations have not prevented fMRI researchers from making some major discoveries
        about brain function, however. Hirsch, for instance, showed in one study that minimally
        conscious individuals still process human speech, and in another, that those who become
        bilingual as young children employ overlapping language areas in the cerebral cortex, while
        those who learn a second language later in life use a different area for the second
        language. The key strength of fMRI, she says, is that it provides the ability to test these
        kinds of hypotheses about structure–function relationships in the normal brain.
      
      
        All Sizes Do Not Fit One
        But the hypotheses that can be tested and the conclusions that can be drawn are still
        largely about group averages, not about the functionings of individual brains, and therein
        lies a second major caveat about the use of fMRI beyond the clinic. John Gabrieli,
        Associate Professor of Psychology at Stanford University, has shown that distinct
        activation patterns in the brains of dyslexic children normalize as they improve their
        reading skills (Figure 2). It seems like a small leap from there to including an fMRI as
        part of the workup for a schoolchild struggling in the classroom. But, Gabrieli cautions,
        that small leap in fact traverses a huge chasm, on one side of which is the group data from
        which conclusions are drawn in studies, and on the other side, the application of these
        conclusions to the individual child. “At the moment, fMRI would be among the most useless
        things to do. We would love to get it to the point that it would be useful [on an
        individual basis],” he says, but it's not there yet. “There is no single-subject
        reliability,” says Gabrieli. “Where we are now, I'm not aware of any applications for which
        it would be responsible to interpret an individual scan [based on group data].”
        There are similar limitations to most other applications of fMRI—while conclusions can
        be made about aggregated data, individual scans are for the most part too hard to
        interpret. There is not yet any real understanding of how brain patterns change over time
        in an individual, or how interindividual differences should be interpreted in relation to
        the conclusions that are valid for groups. This makes fMRI an unlikely tool for job
        screening, for instance. While one study has shown a brain signature in a group of white
        people that is associated with racial bias, denying a particular individual a job on the
        basis of such a scan would likely lead straight to a lawsuit, with experts debating whether
        
        this scan in 
        this individual on 
        this day does or doesn't reflect his underlying racial attitudes.
        On the other hand, Hirsch has used individual scans to help locate a patient's language
        structures that must be spared during neurosurgery. “If you are a neurosurgeon planning a
        resection, you don't want an average brain at all. Millimeters matter.” But her success is
        precisely because she is not using group data to make inferences about the individual—she
        is not leaping over the chasm, but instead is toiling entirely on the other side of it.
        “The goal is personalized medicine,” she says.
      
      
        A Little Guilty Knowledge Is a Dangerous Thing
        Even this kind of personalized approach with fMRI is fraught with problems when
        researchers attempt to apply it outside the clinic, because of limitations in the
        technology itself. One researcher with firsthand knowledge of these problems is Daniel
        Langleben, Assistant Professor of Psychiatry at the University of Pennsylvania School of
        Medicine. In 2002, Langleben showed that when subjects were hiding information in an
        attempt to deceive (so-called guilty knowledge), they had intense activity in five distinct
        brain areas not seen when they were telling the truth. In effect, Langleben used the fMRI
        as a lie detector. It is potentially even more powerful than a standard polygraph test, he
        says, because there are thousands of brain regions which can be scanned for
        deception-triggered variation, versus only three variables—skin conductance, respiration,
        and blood pressure—used in the standard polygraph. Not surprisingly, Langleben got a lot of
        press after he announced his results, and his experiment led directly to speculation that
        we might eventually see fMRIs installed at airports, scanning the brains of would-be
        terrorists trying to deceive security screeners, or in courtrooms, catching perjurers
        red-handed (or perhaps red–anterior-cingulate-gyrused?).
        Langleben is enthusiastic about the potential for an fMRI-based lie detector, and has
        even applied to the Department of Justice for a grant to develop the technology (they
        turned him down, saying it was too expensive). But he is also clear about how difficult it
        will be to get one that really works outside the highly structured confines of the research
        lab. “We are a long way from making a working polygraph,” he says. Even with a “Manhattan
        Project” type effort, he speculates it would take at least ten years. “There are still
        essential discoveries to make along the way,” he says, “and there's a good chance it would
        end in total failure.” It's not just a matter of developing the imaging technology, he
        stresses—“we'll need fundamental developments in semantics, too.” This is because “a lot
        still depends on how you ask the question”—the subtlest of differences can dramatically
        shift which areas of the brain respond. Given the sensitivity of the fMRI result to such
        seemingly minor perturbations, it's hard to imagine it could be reliably adapted to the
        hurly-burly of an airport security checkpoint.
        Even well-performed scans done in topnotch clinics may not easily find their way into
        the courtroom. Perhaps the least likely use of fMRI is in determining if a defendant is
        telling the truth, according to Hank Greely, Professor of Law at Stanford Law School, since
        compelling someone on trial to submit to an fMRI could be seen as a violation of the Fifth
        Amendment right against self incrimination, just as giving spoken testimony against oneself
        is. On the other hand, says Greely, DNA samples and fingerprints can be compelled—whether a
        brain scan is more like testifying or more like submitting to a blood test is an open
        question. Still, for the moment, scanning under duress simply isn't feasible, since all you
        have to do to ruin a good scan is move your head. Motion-correcting algorithms can be used,
        but they are nowhere near advanced enough to correct for large-scale movements by an
        unwilling subject. It's much more likely that an fMRI of a willing defendant would be
        introduced to convince the jury he is telling the truth, or performed before trial to rule
        out an innocent suspect. While to Greely's knowledge fMRI evidence hasn't yet been used in
        court, “it's certain to be tried,” and the barrier to its admission will fall as both the
        reliability and the ease of administration increase. “The easier, the cheaper, the more
        pleasant a technique is, the more likely it is to be used in the legal system.”
        Other forensic uses of fMRI are likely to arrive sooner rather than later. Could scans
        showing diminished impulse control—a function controlled by several regions of the brain,
        including the striatum and the ventromedial prefrontal cortex—be used to support more
        lenient sentencing, or even acquit a defendant, because he couldn't control his violent
        impulses? Or alternatively, will those same scans be used to argue for harsher sentences,
        since the defendant is clearly “hardwired” to commit similar crimes again? Courts already
        consider other factors, such as a history of child abuse, in an attempt to more fully
        understand the psychological state of the defendant. Will brain scans be seen as the
        ultimate “objective” look into the mind of the person on trial?
        Deciding all these issues of admissibility will be judges who will need to weigh
        competing claims from lawyers with competing interests, backed up by expert witnesses with
        competing theories. Here, the desire to apply the science may rush ahead of its
        demonstrated validity.
        Langleben, for one, doesn't think fMRI will be legitimately ready for the courtroom for
        a long time. On the other hand, he says, “if you want to abuse this technique and claim
        that it works, you can create tests that will produce results—I can see how it could be
        done. We know enough to rig it.” But still, he says, “we have all the tools we need to
        prevent this—there are enough people who are sufficiently honest [who would counter the
        premature use of fMRI in these contexts].”
        For now, at least, given the problems inherent in current fMRI technology, the
        neuroethical nightmare scenarios of widespread brain scanning seem unlikely to come to
        pass, at least until radical advances make it far cheaper, much less invasive, far less
        sensitive to subtle perturbations, and with a much more robust ability to legitimately
        extrapolate from a finding about a group to a prediction about an individual. Where fMRI is
        concerned, “a penny for your thoughts” is currently more like “a million pennies for a
        group-averaged hemodynamic response to highly constrained stimuli under entirely artificial
        conditions.”
        In light of this, bioethical concerns about fMRI applications should perhaps be viewed
        not as predictions of a certain future but rather as worstcase scenarios, a reminder of
        what we want to avoid. “It's a funny thing about the bioethics field,” says Greely. “The
        general approach is to look for bad news.”
        While many of these “worst cases” seem highly unlikely to come to pass, Judy Illes, of
        the Stanford Center for Biomedical Ethics, thinks some action is warranted now, if only to
        generate a better understanding of the ethical dimensions of fMRI research. She notes that
        “bioethicists are often viewed as the ethics police,” but she doesn't see regulations as
        the right path to shape the future uses of fMRI. Instead, she thinks a coalition of
        involved parties—scientists, lawyers, ethicists, politicians—should work together to
        develop guidelines that all will find acceptable. “I'm not in the business of stopping
        anything.”
        What everyone apparently already agrees on is the need for carefully designed
        experiments and cautious interpretation of the data. “A huge message in imaging is that you
        really have to look at the experimental setup at the common-sense level,” says Gabrieli,
        and avoid the tendency to “pick the most dramatic interpretation.” “The public needs to be
        reminded of the limitations of these findings,” agrees Hirsch. And as Langleben puts it,
        expressing his skepticism that there will ever be a one-size-fits-all, foolproof fMRI mind
        reader: “I don't think we'll ever be able to be stupid about it.”
      
    
  

  
    
      
        
        
          This is the second in a series of three editorials that aim to address
          recurring concerns about the benefits and risks associated with open-access publishing in
          medicine and the biological sciences.
        
        Scientific societies serve their members, their broader scholarly communities, and the
        different components of their missions in many important ways. Making peer-reviewed
        literature immediately accessible, searchable, and reusable to anyone in the world with an
        Internet connection is a uniquely direct means of achieving a number of goals that are
        common to most scholarly associations and of advancing the diverse interests of their
        constituencies.
        Setting aside for the moment the question of how feasible it is for societies to alter
        their journals' access policies, there is by now a broad consensus that widespread open
        access to scientific publications is good for scientists and good for science. Society
        members want to maximize the impact of their work—and articles that are freely available
        online are cited more frequently than those that are not (Lawrence 2001). Most societies
        are committed to catalyzing innovations within and across scientific disciplines—and
        open-access archives of full-text literature provide a valuable tool for sharing
        information globally in order to accelerate the rate of scientific progress. Many societies
        articulate in their mission statements the goal of communicating the benefits of their
        members' discoveries with the public—and open-access publishing is a direct means to
        accomplish this goal.
        In addition to an interest in exploring new ways to serve their members and their
        missions, societies have another compelling reason to investigate open access for their
        journals: the rapidly changing landscape of scholarly publishing. From 1990 to 2000, the
        average price of an academic journal subscription increased 10% per year (Create Change
        2000). While society-run and nonprofit journals may not be the major contributors to those
        spiraling costs, societies that rely on revenues from subscriptions and site licenses may
        bear a disproportionate share of the negative consequences of skyrocketing serials prices.
        As libraries are forced for a variety of reasons (including decreased budgets and the
        increasing prevalence of “big deals” and journal bundling) to eliminate subscriptions,
        society journals may be among the hardest hit. Journals that appeal to a relatively
        specialized readership and those that are not part of larger publishing groups are
        particularly vulnerable to the contraction of serials collections that has already begun
        and will likely accelerate (Create Change 2000).
      
      
        A Society Is More Than a Journal
        The confluence of forces in favor of open access says nothing about its fiscal
        implications for scientific societies. As any systemic change in research or publishing
        would, the movement toward open access has generated concern about its ramifications for
        the scholarly associations that often serve as the backbones of scientific communities.
        However, the strength of those societies and their essential role in the communities they
        serve are precisely what should allay fears about the revenue-eroding effect that some
        argue would plague societies if they converted their traditional subscription-based
        journals to open access.
        Scientific societies perform an array of tremendously valuable functions for their
        constituents and disciplines. Researchers, educators, and others join societies for the
        many benefits of membership beyond simply discounted or “free” subscriptions to journals,
        so the concern that open-access publications would be the death knell of voluntary academic
        associations is misguided. As Elizabeth Marincola, executive director of the American
        Society for Cell Biology, recently noted, her society “offers a diverse range of products
        so that if publications were at risk financially, we wouldn't lose our membership base
        because there are lots of other reasons why people are members” (Anonymous 2003).
        While open-access publication can, in fact, be paid for in a number of different ways,
        there is no question that a transition toward the elimination of online access barriers
        requires most societies to restructure the business models for their journals. If journal
        subscriptions generate surplus revenue that supports other society activities, then the
        business model of the society as a whole may need to be examined. This is not to say that
        open-access journals cannot generate a surplus or profit—simply that they do not do so by
        restricting access to their primary research content.
      
      
        Testing the Open-Access Waters
        There are a number of societies that have already begun to take transitional steps to
        wean themselves from subscription revenues. One of the earliest societies to commit to
        open-access publication, the American Society for Clinical Investigation (ASCI) has since
        1996 provided the 
        Journal of Clinical Investigation (JCI) freely online and recently
        reaffirmed its commitment to open access: “The financing having been resolved, through
        author charges and other means,” John Hawley, the executive director of the ASCI writes,
        “the 
        JCI hopefully can bring the greatest benefit to its authors and readers,
        regardless of who they might be. It is in this spirit that the 
        JCI has always been free online, and will remain so” (Hawley 2003).
        In order to experiment cautiously with new access policies, several societies have
        implemented hybrid models of access-restriction for their publications. The American
        Physiological Society, for example, offers authors in 
        Physiological Genomics the option to pay a surcharge for their articles
        to be made freely available online immediately upon publication. A recent survey by the
        Joint Information Systems Committee (JISC) in the United Kingdom suggests that many authors
        would use such an option if it were more widely available: 48% of authors who had never
        published in an open-access journal and 60% of authors who had done so indicated that they
        would be willing to “pay a publisher of a journal sold according to the traditional
        subscription model an additional fee for them to make [the author's] particular paper ‘open
        access’” (JISC 2004).
        JISC is also directly encouraging society and nonprofit publishers to implement hybrid
        models and other open-access experiments and to launch new open-access journals by
        providing grants to offset the publication charges for authors during this transitional
        phase. In the long run, of course, open access will prove sustainable when more funders of
        research, in addition to interested third parties, designate funds specifically for the
        costs of publishing articles to be made freely available, searchable, and reusable
        online.
      
      
        Starting the Dialogue
        Reaching a “steady-state” system of open-access publishing by scientific societies will
        require three critical components: recognition that open access serves societies' members
        and missions; diversified revenue streams not solely dependent on subscription or
        site-license fees; and society publishers' making use of recent innovations in journal
        production and dissemination, which can dramatically reduce the costs of publishing. It is,
        after all, the increased efficiencies born of new technologies—from the Internet itself to
        electronic journal management systems—that have made the idea of open access possible. And
        while proponents of open access are confident that publication charges of around $1,500 per
        article will be sufficient to cover the costs of publishing an efficiently operated society
        journal, there is no question that many existing journals may need to update their
        infrastructure in order to make open access financially viable (PLoS 2004).
        There is also no question that many societies do not, at present, have a wealth of
        revenue streams beyond the proceeds from their journals, which they often use to fund
        valuable activities from education initiatives to annual meetings. As open-access journals
        become more established, however, and as the benefits of open access to scientific and
        medical literature become more apparent to society members, the demand for the broadest
        possible dissemination of research is only likely to grow. Those societies that embrace the
        developments taking place in scholarly publishing may well see their membership and
        publications thrive more than societies that cling to the potentially unstable status
        quo.
        In any case, a constructive discussion about the pitfalls to be avoided and the benefits
        to be gained through a transition to open-access publishing would be a worthy first step
        for any scientific society to take—and PLoS welcomes the questions, comments, and feedback
        of those who are intrigued by the potential that open access affords and want to learn
        more.
      
    
  

  
    
      
        
        Science looks set for a fundamentalist revival within the European Union. Its leading
        proponents are taking advantage of unprecedented political upheaval—as ten new Member
        States accede to the Union—to press their case for funding of basic research that is driven
        solely and independently by investigators themselves in the pursuit of excellence.
        The broad thrust of their appeal calls for the setting up of a new agency, most commonly
        referred to as a ‘European Research Council’. The ERC could be an entirely new organisation
        or a new division within an established body, run by a small staff able to draw on the best
        expertise available. It would administer a new fund from EU coffers, tagged the European
        Fund for Research Excellence, that would be valued modestly, initially at least, at much
        less than half of the EU's existing budget for research. Most importantly, dispersal of
        that fund would reflect the wishes of eminent peer reviewers, assessing competitive bids in
        search of the best science, rather than the judgements of Eurocrats, looking for the most
        politically and economically expedient solutions and operating on a lead time of two years
        or more.
        Although the modus operandi of the proposed ERC has still to be worked out, European
        scientists have been looking to the United States and at the way that the National Science
        Foundation and the National Institutes of Health operate, as well as to private
        institutions such as the Howard Hughes Medical Institute in the United States and the
        Wellcome Trust in the United Kingdom. In particular, they seek the independence and
        excellence achieved outside of the EU framework. More to the point, they are weary of the
        bureaucratic formulations that determine how the EU's research budget, currently known as
        the Sixth Framework Programme (2002–2006) and worth around €4.4 billion/year (or just over
        5% of all public spending on nonmilitary research in the region), is spent and distributed.
        The EU's guiding principle is often one of 
        juste retour , or fair reward, in which Member States traditionally seek
        to recover grants at least equal to their contributions to the EU pot (see Box 1).
        ‘Most of the Anglo-Saxon countries in Europe—the Scandinavian countries, the United
        Kingdom, the Netherlands—operate a peer review process and a research funding council
        process that's very similar to best practice in North America,’ says Michael Morgan, a
        consultant to the Wellcome Trust on European issues and former chief executive of the
        Trust's Genome Campus at Hinxton, near Cambridge, United Kingdom. ‘The French and Germans
        and others have elements of that but they also have what you might call more “state-funded
        science”, scientists as civil servants, and there is obviously much greater possibility of
        science being funded for less than the best scientific reasons,’ notes Morgan, referring to
        the opportunities for greater political influence on decision-making. ‘I'm not suggesting
        that that is the case, but it is the possibility,’ he adds.
        ‘What we need in Europe is something that should strictly adhere to the international
        standards of research funding and be evaluated by peer review,’ says Peter Gruss, professor
        of molecular cell biology at the University of Göttingen and president of the Max Planck
        Society in Munich, Germany. ‘The sole criterion has to be quality, not geographical
        distribution, not management capacity,’ he adds, alluding to the EU practice of 
        juste retour . ‘We want to encourage excellence in Europe. We want to
        have as a benchmark a European standard that should be as high as the standard is in the
        US.’
        Gruss acknowledges the tensions that the ERC proposal has generated among Member States:
        ‘I'm not saying that there aren't countries that have this standard—like the UK, parts of
        Germany, Sweden, and some other Nordic countries—but of course this is not the general
        European standard, and in order to get one and the same, the common standard, we need a
        common structure.’
      
      
        A Fund for Excellence
        The European Commission now appears ready to accept the need for a common structure that
        would have, as the Commission puts it, ‘more open and less binding’ programmes of basic
        research, in contrast to the Framework Programme, whose emphasis is on applied research
        with commercial objectives. The Commission expects to publish its endorsement of the ERC
        proposal this month, so that approval by the Council of the EU should follow later this
        year. On this timetable, setting up of the ERC could begin in 2006 when the next five-year
        Framework Programme, FP7, gets underway.
        Over the ERC's first five years, its grant is expected to grow from around €500
        million/year to €2 billion/year, and to derive from a reallocation of funds within the EU's
        budget rather than from any top-up contributions from Member States. Furthermore, Gruss
        released a legal opinion in March that advised how an ERC need not be an executive agency
        of the Commission, as many scientists had feared it would have to be under the terms of the
        EU Treaty, but could be established as an independent and autonomous body. The opinion is a
        real coup for the ERC lobbyists.
      
      
        Origins of the ERC
        Moves to establish an ERC are founded in a ‘new strategic goal’ for the EU that the
        leaders of its 15 Member States set during their European Council in Lisbon in March 2000.
        Over the first decade of the new millennium, they urged the EU ‘to become the most
        competitive and dynamic knowledge-based economy in the world’. They enthusiastically
        endorsed a notion, floated by the European Commission, of a European Research Area (ERA).
        ‘Research activities at national and [European] Union level must be better integrated and
        co-ordinated to make them as efficient and innovative as possible, and to ensure that
        Europe offers attractive prospects to its best brains,’ concluded the EU leaders, eager to
        reverse the flow of trained talent abroad, notably to North America. All appropriate means,
        they added, ‘must be fully exploited to achieve this objective in a flexible, decentralised
        and non-bureaucratic manner’.
        Two years later, at the European Council in Barcelona, the EU leaders went one step
        further by defining the target more precisely. ‘In order to close the gap between the EU
        and its major competitors,’ they said, ‘overall spending on R & D and innovation in the
        Union should be increased with the aim of approaching 3% of GDP by 2010. Two-thirds of this
        new investment should come from the private sector.’
        The scale of the challenge is illustrated by the latest figures for R & D
        expenditure, published in February by the Statistical Office of the European Communities
        (Eurostat). The EU's estimated R & D spending in 2002 was 1.99% of GDP, still far short
        of the US (2.80%) and Japan (2.98% in 2000), and a long way from the target of 3%.
        Emphasising the UK's uneasiness about the EU's escalating enthusiasm for a regional
        science base, the Royal Society (the UK national academy of science) poured scorn on the
        ‘ambitious’ GDP target by noting how the UK alone would have needed an extra £11 billion in
        2000, or more than 60% of total spending on R & D, to lift its ratio of 1.85% to the 3%
        target. The Royal Society also noted how public funding of R & D in the EU matches that
        in the US and Japan, with the disparity among GDP ratios reflecting the differentials in
        private investment in R & D, over which the EU has little control.
        Nevertheless, the challenge could not be ignored. According to Bob May, professor of
        mathematical biology at the University of Oxford, president of the Royal Society, and
        former UK Chief Scientist, such initiatives might be ‘driven more by political expediency
        than common sense, but the moment you see that train beginning to roll, there's a chance to
        do something useful with it’.
        Among the leading proponents of an ERC is Bernard Larrouturou, director general of the
        National Centre of Scientific Research (CNRS) in Paris, France. For Larrouturou, a
        biomathematician currently engaged in streamlining the organisation, the changes at the
        European level are a breath of fresh air. However, he is not convinced that funded
        investigators should expect to exclude Commission strategists entirely from their lives.
        The scientific community should lead an ERC, says Larrouturou, ‘but I do not like the idea
        that this should be completely under the guidance and wisdom of the scientific community
        with no strategy guidance. You cannot ask for 1 or 2 billion Euros every year and say there
        will not be any strategy and [that it will be done solely] on this basis of excellence.’
        And Larrouturou distances himself from the idea that basic and applied research can be
        treated separately because this suggests, wrongly he says, a conflict between the two.
        On these issues, Larrouturou moves onto some common ground with John Taylor, former
        director general of Research Councils UK and now chairman of Roke Manor Research, a UK
        subsidiary of Siemens, the German electronics group. Research Councils UK oversees spending
        of Britain's national research councils (currently, just over £2 billion from its 2004–2005
        Science Budget of nearly £2.7 billion). Interactions across disciplines and between
        scientists and technologists ‘are not helped by making artificial distinctions between this
        kind of research and that kind of research,’ says Taylor. ‘The distinctions I make are much
        more between top-down and bottom-up.’
        While Taylor is a joint architect of one proposal to create an ERC, he remains
        unconvinced that the research funding system is broken, especially from the UK's
        perspective, and needs to be fixed. Nor is he convinced that EU funds for an ERC will not
        affect national R & D budgets. ‘I'm middle of the road,’ he says. ‘Much greater
        collaboration is good. It has to be a slow process, with all the different cultures
        involved. Collaboration on various areas of science is an excellent way to go, provided you
        don't try to organise it from the top and legislate for it all to happen in a particular
        way and to a particular timescale. Excellence is key.’
        Taylor's cautions reflect his experience of the EU's Framework Programme and his
        reservations that any initiative from Brussels can be free of red tape. ‘If you want to do
        research, then you can't lay out beforehand all the answers you're going to get,’ he says.
        ‘And if you try to get people to stick rigorously to a plan, then you get a lot of silly
        things going on. If you try to form very complex bureaucratic organisations to do the
        research, you get a lot of delays and so on, so things are not very timely.’
        But the Framework Programme's failures need not spell disaster for the fledgling funding
        council, insists Lennart Philipson, former director general of the European Molecular
        Biology Laboratory (EMBL) in Heidelberg, Germany, and now an emeritus professor at the
        Karolinska Institute in Stockholm, Sweden. Drawing on his 11 years as head of EMBL, until
        1993, Philipson recalls how ‘pan-European peer review was the best method for distributing
        the funds of EMBL and EMBO [European Molecular Biology Organization]’. The continuing high
        status of the two organisations, he says, is testimony that the system works. In fact, EMBO
        is mentioned as a possible incubator for an ERC, in spite of its specialisation.
        Other proponents of the proposed research changes in the EU include 45 Nobel Laureates
        from Europe or of European origin, who headed a petition organised by EMBO. The European
        Life Scientists Organization (ELSO) organised another. Its president, Kai Simons, also the
        Director of the Max Planck Institute for Cell Biology and Genetics in Dresden, Germany,
        says research funding in Europe is just not working. ‘It's not geared for basic research—it
        has other aims,’ he notes. EU funds are ‘not grants, they are contracts with in-built
        milestones that have nothing to do [with basic research]. Basic research doesn't work like
        that.’
        The evaluation and peer review system is falling apart, continues Simons. He says that
        the best people are not interested in peer reviewing a system that doesn't work: ‘You're
        not attracting the peer reviewers that you need to maintain quality.’ But at last,
        acknowledges Simons, someone in Brussels is listening. ‘In the past two years there has
        been enormous progress.’
      
      
        Many Questions Remain
        Within a month of the Barcelona Council in 2002, the European Science Foundation (ESF),
        which brings together the funding agencies of 29 countries and acts as a bridge to
        Brussels, had formed a High Level Working Group to review the case for an ERC and how it
        might operate. The group, chaired by Sir Richard Sykes, Rector of Imperial College, London,
        United Kingdom, reported a year later, in April 2003. It endorsed the creation of an ERC as
        ‘the cornerstone for the ERA and the key approach to developing a locus for…long-term
        fundamental curiosity-driven research judged on the basis of excellence and merit’. The
        Sykes group also proposed, controversially, an enhanced ESF as the most effective medium
        for establishing an ERC swiftly.
        ‘Some people say that the ESF has no experience in funding large amounts… for research,’
        acknowledges Enric Banda, director general of the Catalan Research Foundation in Barcelona,
        Spain, who finished a five-year term as the ESF's chief executive at the end of 2003 and is
        credited with ‘waking up’ the foundation. ‘But certainly if you create a new
        [organisation], that's the same thing. So the ESF is in a good position because its member
        organisations are the funding agencies.’
        Bertil Andersson, who was a member of the Sykes Group before taking over from Banda at
        the ESF in January, also stakes the ESF's claim to nurture a fledgling ERC. But he accepts
        that any one of the respected national funding agencies, such as the German Research
        Foundation (DFG), or even a specialist body, such as EMBO, could do the job. ‘We don't need
        a new skyscraper in Brussels, but a lot of… peer review and running of the ERC could be
        done by existing bodies.
        ‘Compared to soccer, we have only the national leagues—we don't have the Champions
        League [the league of Europe's best teams],’ says Andersson. There is no competition for
        basic research grants across national boundaries in Europe, he insists. ‘The Swedish league
        is exciting, but the Champions League is more exciting.’
        In the meantime, while the Sykes group was still deliberating, the Council of the EU
        appointed another group of experts to evaluate the case for an ERC. Chaired by Federico
        Mayor, former director general of the United Nations Educational, Scientific, and Cultural
        Organization (UNESCO), the ERC Expert Group also delivered its verdict—a resounding
        endorsement—within 12 months.
        ‘The first and main task for the ERC should be to support investigator-driven research
        of the highest quality selected through European competition,’ concluded the Mayor report,
        published in December 2003. ‘In doing so, the ERC should create and support nodes of
        excellence in European universities and research institutions, strengthening the
        knowledge-base that underpins economic, industrial, cultural and societal development, and
        thereby stimulating European competitiveness and innovative capacity at all levels.’
        While few disagreed with the Mayor report's sentiments, the absence of a detailed
        analysis exposed underlying tensions over the rationale for an ERC. In the UK, in
        particular, some scientists seemed concerned that their mature and respected system for
        funding research risked dilution.
        ‘The British have always had doubts about what goes on in Europe,’ notes Kai Simons.
        ‘They always think that they can do it better. But the big problem for the British is that
        they are also too small to fund a new innovative area,’ he says. ‘Of course, we can do it
        without Britain, but they are an important part of Europe and it would be sad if they're
        not part of it.’
        The agnostic John Taylor, who was a member of the Mayor group, recalls his early
        reservations when the group convened. ‘I'm way beyond the euphoria; I'm into practical
        pragmatics,’ he notes. ‘My major input into the whole thing has been to get them to “get
        real” instead of just philosophising. They've been using the sort of, dare I say it, Gallic
        approach… of thinking about the reasons why, and the philosophy, and not thinking about
        what you would actually do.’
        Taylor dismisses the notion that wariness of the ERC is representative of a general
        antipathy in Britain towards European integration. ‘What we're saying is that science in
        the UK is not yet well-funded enough to say we would rather do this [the ERC] instead of
        the things that we're already trying to get done in the UK scene.’
        Anticipating the Mayor report's publication, the Royal Society quickly pulled together a
        detailed background paper late last year that identified ‘a number of problems that need
        resolution, although not necessarily through the establishment of any major new
        institutions within Europe’. An addendum followed in March, in direct response to the Mayor
        report. That addendum highlighted what it saw as the paucity of solid evidence in the Mayor
        report and, in some cases, the confusing data in the report's case for an ERC.
        On balance it looked as though the Royal Society, and as such the British science
        establishment as a whole, had weighed the disadvantages of an ERC as greater than its
        advantages, but Bob May is quick to refute this charge. ‘My vision and the Royal Society's
        vision of the ERC is that it will fund the very best science,’ he insists. ‘The Mayor
        committee itself was really good people who'd produced basically a good report…. I'm
        basically in favour of this European Research Council… provided it can be set up properly,
        which is by no means certain.’
        For May, and other scientists on the continent, the ERC offers a real chance to redress
        the balance of fortune in favour of young scientists. ‘The way to encourage science is to
        get the best people and set them free to express their creativity while they are young,
        which means bring them into the best laboratories—don't let them get entrained in
        hierarchies of deference to second-rate people,’ says May.
        ‘The most important single thing to create one Europe in science is a flexible
        postdoctoral programme that gets the best young people wherever they are and lets them go
        to the best places,’ enthuses May. An ERC will then foster those collaborations, he
        forecasts. ‘It won't ask whether they're 
        juste retour , whether they're serving some industrial purpose, it will
        just try to fund the best science. But I hope increasingly the best projects will involve
        collaborations, as they do in Britain, collaborations among institutions within
        Europe.’
      
      
        
      
    
  

  
    
      
        
        Why is the sky blue? Any scientist will answer this question with a statement of
        mechanism: Atmospheric gas scatters some wavelengths of light more than others. To answer
        with a statement of purpose—e.g., to say the sky is blue in order to make people
        happy—would not cross the scientific mind. Yet in biology we often pose “why” questions in
        which it is purpose, not mechanism, that interests us. The question “Why does the eye have
        a lens?” most often calls for the answer that the lens is there to focus light rays, and
        only rarely for the answer that the lens is there because lens cells are induced by the
        retina from overlying ectoderm.
        It is a legacy of evolution that teleology—the tendency to explain natural phenomena in
        terms of purposes—is deeply ingrained in biology, and not in other fields (Ayala 1999).
        Natural selection has so molded biological entities that nearly everything one looks at,
        from molecules to cells, from organ systems to ecosystems, has (at one time at least) been
        retained because it carries out a function that enhances fitness. It is natural to equate
        such functions with purposes. Even if we can't actually know why something evolved, we care
        about the useful things it does that could account for its evolution.
        As a group, molecular biologists shy away from teleological matters, perhaps because
        early attitudes in molecular biology were shaped by physicists and chemists. Even
        geneticists rigorously define function not in terms of the useful things a gene does, but
        by what happens when the gene is altered. Molecular biology and molecular genetics might
        continue to dodge teleological issues were it not for their fields' remarkable recent
        successes. Mechanistic information about how a multitude of genes and gene products act and
        interact is now being gathered so rapidly that our inability to synthesize such information
        into a coherent whole is becoming more and more frustrating. Gene regulation, intracellular
        signaling pathways, metabolic networks, developmental programs—the current information
        deluge is revealing these systems to be so complex that molecular biologists are forced to
        wrestle with an overtly teleological question: What purpose does all this complexity
        serve?
        In response to this situation, two strains have emerged in molecular biology, both of
        which are sometimes lumped under the heading “systems biology.” One strain, bioinformatics,
        champions the gathering of even larger amounts of new data, both descriptive and
        mechanistic, followed by computerbased data “mining” to identify correlations from which
        insightful hypotheses are likely to emerge. The other strain, computational biology, begins
        with the complex interactions we already know about, and uses computer-aided mathematics to
        explore the consequences of those interactions. Of course, bioinformatics and computational
        biology are not entirely separable entities; they represent ends of a spectrum, differing
        in the degree of emphasis placed on large versus small data sets, and statistical versus
        deterministic analyses.
        Computational biology, in the sense used above, arouses some skepticism among
        scientists. To some, it recalls the “mathematical biology” that, starting from its heyday
        in the 1960s, provided some interesting insights, but also succeeded in elevating the term
        “modeling” to near-pejorative status among many biologists. For the most part, mathematical
        biologists sought to fit biological data to relatively simple mathematical models, with the
        hope that fundamental laws might be recognized (Fox Keller 2002). This strategy works well
        in physics and chemistry, but in biology it is stymied by two problems. First, biological
        data are usually incomplete and extremely imprecise. As new measurements are made, today's
        models rapidly join tomorrow's trash heaps. Second, because biological phenomena are
        generated by large, complex networks of elements, there is little reason to expect to
        discern fundamental laws in them. To do so would be like expecting to discern the
        fundamental laws of electromagnetism in the output of a personal computer.
        Nowadays, many computational biologists avoid modeling-as-data-fitting, opting instead
        to create models in which networks are specified in terms of elements and interactions (the
        network “topology”), but the numerical values that quantify those interactions (the
        parameters) are deliberately varied over wide ranges. As a result, the study of such
        networks focuses not on the exact values of outputs, but rather on qualitative behavior,
        e.g., whether the network acts as a “switch,” “filter,” “oscillator,” “dynamic range
        adjuster,” “producer of stripes,” etc. By investigating how such behaviors change for
        different parameter sets— an exercise referred to as “exploring the parameter space”—one
        starts to assemble a comprehensive picture of all the kinds of behaviors a network can
        produce. If one such behavior seems useful (to the organism), it becomes a candidate for
        explaining why the network itself was selected, i.e., it is seen as a potential purpose for
        the network. If experiments subsequently support assignments of actual parameter values to
        the range of parameter space that produces such behavior, then the potential purpose
        becomes a likely one.
        For very simple networks (e.g., linear pathways with no delays or feedback and with
        constant inputs), possible global behaviors are usually limited, and computation rarely
        reveals more than one could have gleaned through intuition alone. In contrast, when
        networks become even slightly complex, intuition often fails, sometimes spectacularly so,
        and computation becomes essential.
        For example, intuitive thinking about MAP kinase pathways led to the long-held view that
        the obligatory cascade of three sequential kinases serves to provide signal amplification.
        In contrast, computational studies have suggested that the purpose of such a network is to
        achieve extreme positive cooperativity, so that the pathway behaves in a switch-like,
        rather than a graded, fashion (Huang and Ferrell 1996). Another example comes from the
        study of morphogen gradient formation in animal development. Whereas intuitive
        interpretations of experiments led to the conclusion that simple diffusion is not adequate
        to transport most morphogens, computational analysis of the same experimental data yields
        the opposite conclusion (Lander et al. 2002).
        As the power of computation to identify possible functions of complex biological
        networks is increasingly recognized, purely (or largely) computational studies are becoming
        more common in biological journals. This raises an interesting question for the biology
        community: In a field in which scientific contributions have long been judged in terms of
        the amount of new experimental data they contain, how does one judge work that is primarily
        focused on interpreting (albeit with great effort and sophistication) the experimental data
        of others? At the simplest level, this question poses a conundrum for journal editors. At a
        deeper level, it calls attention to the biology community's difficulty in defining what,
        exactly, constitutes “insight” (Fox Keller 2002).
        In yesterday's mathematical biology, a model's utility could always be equated with its
        ability to generate testable predictions about new experimental outcomes. This approach
        works fine when one's ambition is to build models that faithfully mimic particular
        biological phenomena. But when the goal is to identify all possible classes of biological
        phenomena that could arise from a given network topology, the connection to experimental
        verification becomes blurred. This does not mean that computational studies of biological
        networks are disconnected from experimental reality, but rather that they tend, nowadays,
        to address questions of a higher level than simply whether a particular model fits
        particular data.
        The problem this creates for those of us who read computational biology papers is
        knowing how to judge when a study has made a contribution that is deep, comprehensive, or
        enduring enough to be worth our attention. We can observe the field trying to sort out this
        issue in the recent literature. A good example can be found in an article by Nicholas
        Ingolia in this issue of 
        PLoS Biology (Ingolia 2004), and an earlier study from Garrett Odell's
        group, upon which Ingolia draws heavily (von Dassow et al. 2000).
        Both articles deal with a classical problem in developmental biology, namely, how
        repeating patterns (such as stripes and segments) are laid down. In the early fruit fly
        embryo, it is known that a network involving cell-to-cell signaling via the Wingless (Wg)
        and Hedgehog (Hh) pathways specifies the formation and maintenance of alternating stripes
        of gene expression and cell identity. This network is clearly complex, in that Wg and Hh
        signals affect not only downstream genes, but also the expression and/or activity of the
        components of each other's signaling machinery.
        Von Dassow et al. (2000) calculated the behaviors of various embodiments of this network
        over a wide range of parameter values and starting conditions. This was done by expressing
        the network in terms of coupled differential equations, picking parameters at random from
        within prespecified ranges, solving the equation set numerically, then picking another
        random set of parameters and obtaining a new numerical solution, and so forth, until
        240,000 cases were tried. The solutions were then sorted into groups based on the predicted
        output—in this case, spatial patterns of gene expression.
        When they used a network topology based only upon molecular and generegulatory
        interactions that were firmly known to take place in the embryo, they were unable to
        produce the necessary output (stable stripes), but upon inclusion of two molecular events
        that were strongly suspected of taking place in the embryo, they produced the desired
        pattern easily. In fact, they produced it much more easily than expected. It appeared that
        a remarkably large fraction of random parameter values produced the very same stable
        stripes. This implied that the output of the network is extraordinarily robust, where
        robustness is meant in the engineering sense of the word, namely, a relative insensitivity
        of output to variations in parameter values.
        Because real organisms face changing parameter values constantly—whether as a result of
        unstable environmental conditions, or mutations leading to the inactivation of a single
        allele of a gene—robustness is an extremely valuable feature of biological networks, so
        much so that some have elevated it to a sort of sine qua non (Morohashi et al. 2002).
        Indeed, the major message of the von Dassow article was that the authors had uncovered a
        “robust developmental module,” which could ensure the formation of an appropriate pattern
        even across distantly related insect species whose earliest steps of embryogenesis are
        quite different from one another (von Dassow et al. 2000).
        There is little doubt that von Dassow's computational study extracted an extremely
        valuable insight from what might otherwise seem like a messy and ill-specified system. But
        Ingolia now argues that something further is needed. He proposes that it is not enough to
        show that a network performs in a certain way; one should also find out why it does so.
        Ingolia throws down the gauntlet with a simple hypothesis about why the von Dassow
        network is so robust. He argues that it can be ascribed entirely to the ability of two
        positive feedback loops within the system to make the network bistable. Bistability is the
        tendency for a system's output to be drawn toward either one or the other of two stable
        states. For example, in excitable cells such as neurons, depolarization elicits sodium
        entry, which in turn elicits depolarization—a positive feedback loop. As a result, large
        depolarizations drive neurons to fully discharge their membrane potential, whereas small
        depolarizations decay back to a resting state. Thus, the neuron tends strongly toward one
        or the other of these two states. The stability of each state brings with it a sort of
        intrinsic robustness— i.e., once a cell is in one state, it takes a fairly large
        disturbance to move it into the other. This is the same principle that makes electronic
        equipment based on digital (i.e., binary) signals so much more resistant to noise than
        equipment based on analog circuitry.
        Ingolia not only argues that robustness in the von Dassow model arises because positive
        feedback leads to network bistability, he further claims that such network bistability is a
        consequence of bistability at the single cell level. He strongly supports these claims
        through computational explorations of parameter space that are similar to those done by von
        Dassow et al., but which also use strippeddown network topologies (to focus on individual
        cell behaviors), test specifically for bistability, correlate results with the patterns
        formed, and ultimately generate a set of mathematical rules that strongly predict those
        cases that succeed or fail at producing an appropriate pattern.
        At first glance, such a contribution might seem no more than a footnote to von Dassow's
        paper, but a closer look shows that this is not the case. Without mechanistic information
        about why the von Dassow network does what it does, it is difficult to relate it to other
        work, or to modify it to accommodate new information or new demands. Ingolia demonstrates
        this by deftly improving on the network topology. He inserts some new data from the
        literature about the product of an additional gene, 
        sloppy-paired , in Hh signaling, removes some of the more tenuous
        connections, and promptly recovers a biologically essential behavior that the original von
        Dassow network lacked: the ability to maintain a fixed pattern of gene expression even in
        the face of cell division and growth.
        Taken as a pair, the von Dassow and Ingolia papers illustrate the value of complementary
        approaches in the analysis of complex biological systems. Whereas one emphasizes simulation
        (as embodied in the numerical solution of differential equations), the other emphasizes
        analysis (the mathematical analysis of the behavior of a set of equations). Whereas one
        emphasizes exploration (exploring a parameter space), the other emphasizes the testing of
        hypotheses (about the origins of robustness). The same themes can be seen in sets of papers
        on other topics. For example, in their analysis of bacterial chemotaxis, Leibler and
        colleagues (Barkai and Leibler 1997) found a particular model to be extremely robust in the
        production of an important behavior (exact signal adaptation), and subsequently showed that
        bacteria do indeed exhibit such robust adaptation (Alon et al. 1999). Although Leibler and
        colleagues took significant steps toward identifying and explaining how such robustness
        came about, it took a subsequent group (Yi et al. 2000) to show that robustness emerged as
        a consequence of a simple engineering design principle known as “integral feedback
        control.” That group also showed, through mathematical analysis, that integral feedback
        control is the only feedback strategy capable of achieving the requisite degree of
        robustness.
        From these and many other examples in the literature, one can begin to discern several
        of the elements that, when present together, elevate investigations in computational
        biology to a level at which ordinary biologists take serious notice. Such elements include
        network topologies anchored in experimental data, fine-grained explorations of large
        parameter spaces, identification of “useful” network behaviors, and hypothesisdriven
        analyses of the mathematical or statistical bases for such behaviors. These elements can be
        seen as the foundations of a new calculus of purpose, enabling biologists to take on the
        much-neglected teleological side of molecular biology. “What purpose does all this
        complexity serve?” may soon go from a question few biologists dare to pose, to one on
        everyone's lips.
      
    
  

  
    
      
        
        It goes without saying that the cellular plasma membrane effectively creates a barrier
        between the inside (intracellular area) and outside (extracellular area) of the cell it
        defines. In order for the cell to sense and respond to its environment (including other
        cells and the supporting structures that comprise the extracellular matrix [ECM]) and for
        the environment to influence cell function (including cell growth and movement),
        bidirectional signaling across the plasma membrane has to be mediated by receptors and
        other structures. About two decades ago, it became widely appreciated that many of the cell
        surface receptors that mediate cell–cell and cell–ECM interactions were structurally and
        functionally related, and the term “integrins” was coined to reflect the capacity of
        members of this family to integrate the extracellular and intracellular environment (Hynes
        1987). Integrin-mediated interactions are vital to the maintenance of normal cell
        functioning because of their ability to mediate inside-out (intracellular to extracellular)
        and outside-in (extracellular to intracellular) signaling. Integrin dysfunctions are
        associated with numerous human disorders such as thrombosis, atherosclerosis, cancer, and
        chronic inflammatory diseases. Despite a total of nearly 30,000 integrin-related articles
        in the literature, intensive effort—more than 200 articles per month—continues to focus on
        understanding the roles of integrins in both physiological and pathological processes.
      
      
        The Integrin Family
        The integrin family comprises 20 or more members that are found in many animal species,
        ranging from sponges to mammals (Hynes 2002). They consist of two distinct, associated
        subunits (noncovalent heterodimers), where each subunit (α, β) consists of a single
        transmembrane domain, a large extracellular domain of several hundred amino acids (composed
        of multiple structural domains), and typically, a small cytoplasmic domain of somewhere
        between 20–70 residues (Figure 1). The extracellular domains bind a wide variety of
        ligands, whereas the intracellular cytoplasmic domains anchor to cytoskeletal proteins. In
        this manner, the exterior and interior of a cell are physically linked, which allows for
        bidirectional transmission of mechanical and biochemical signals across the plasma
        membrane, and leads to a cooperative regulation of cell functions, including adhesion,
        migration, growth, and differentiation. A central topic in the integrin research over the
        past decade has been the mechanism of inside-out activation (Liddington and Ginsberg 2002).
        In their resting state, integrins normally bind the molecules that activate them with low
        affinity. Upon stimulation, a cellular signal induces a conformational change in the
        integrin cytoplasmic domain that propagates to the extracellular domain. Integrins are
        transformed from a low- to a highaffinity ligand binding state. Such inside-out regulation
        of integrin affinity states is distinct from the outside-in signaling observed upon
        activation of most other transmembrane receptors (e.g., growth factor–growth factor
        receptor interactions), including integrins. The inside-out signaling protects the host
        from excessive integrin-mediated cell adhesion, which could, for example, lead to
        spontaneous aggregation of blood cells and have profound pathological consequences.
      
      
        The Heads and Tails of Inside-Out Signaling
        Mutational studies provided the initial hints that disruption of the non-covalent clasp
        between α and β cytoplasmic tails is clearly the event within the structure of the integrin
        that initiates inside-out signaling. Point mutations in the α and β cytoplasmic tails that
        are near the membrane or deletion of either region result in constitutive activation of the
        receptor (O'Toole et al. 1991, 1994; Hughes et al. 1995). Mutating a single specific
        residue in the cytoplasmic tail of either subunit led to integrin activation, but a double
        mutation, which would have allowed retention of a salt bridge between the subunits, did not
        (Hughes et al. 1996)—suggesting that integrin inside-out activation is dependent upon
        regulation of the interaction between the two subunits. In support of this hypothesis,
        peptides corresponding to α and β cytoplasmic tails have been shown to interact with each
        other (Haas and Plow 1996). Since these original observations, there has been an intensive
        effort to understand the mechanism for regulation of integrin activation by the cytoplasmic
        region (for a recent review, see Hynes 2002). On the road toward this goal, Ginsberg and
        colleagues discovered that the head domain of a cytoskeletal protein—talin—plays a key role
        in binding to integrin β cytoplasmic tails and inducing integrin activation (Calderwood et
        al. 1999). Many other intracellular proteins bind to the α and β cytoplasmic tails (Liu et
        al. 2000), but the importance of talin in integrin activation is particularly convincing
        since it has been confirmed by multiple laboratories (Vinogradova et al. 2002; Kim et al.
        2003; Tremuth et al. 2004) using various methods including overexpression and gene
        knockdown (siRNA) approaches (Tadokoro et al. 2003). In 2001, Springer and coworkers
        provided evidence for a model by which separation of the C-terminal portions of the α and β
        subunits results in inside-out activation. They showed that replacement of the
        cytoplasmic-transmembrane regions by an artificial linkage between the tails inactivates
        the receptor, whereas breakage of the clasp activates the receptor (Lu et al. 2001; Takagi
        et al. 2001). Shortly thereafter, the model gained direct and strong experimental support
        from a structural analysis in which the membrane-proximal helices of the two subunits were
        found to clasp in a weak “handshake” that could be disrupted by talin or constitutively
        activating mutations (Vinogradova et al. 2002). The model has been further verified by
        other biophysical studies (Kim et al. 2003) and extended to other integrins (Vinogradova et
        al. 2004). Since the membrane-proximal regions of integrin α and β cytoplasmic tails are
        highly conserved, the generalization of this signaling mechanism to all integrins was to be
        anticipated. A dynamic image of how such cytoplasmic unclasping occurs at the membrane
        surface can now be modeled (Figure 1) (Vinogradova et al. 2004).
      
      
        Straightening Out the Outside
        On the extracellular side, ground-breaking insights were provided when the crystal
        structure of the extracellular domain of integrin α
        v β
        3 (the nomenclature identifies the particular α and β subunits) was
        determined (Xiong et al. 2001). In addition to the exquisite structural details, the
        overall conformation was surprisingly bent (Figure 1), which contrasted with structures
        revealed by the earlier electron micrographic studies that showed an extended, stalk-like
        structure (Weisel et al. 1992). Springer and coworkers used a series of
        biochemical/biophysical experiments to suggest that the bent structure represents an
        inactive form of integrin (Takagi et al. 2002), whereas activation induces a switchblade
        shift that converts the bent form to the extended form (Figure 1). A molecular picture has
        emerged for integrin insideout activation where a cellular signal induces the
        conformational change of talin exposing its head domain allowing it to bind to the integrin
        β cytoplasmic tail. This interaction unclasps the complex between the cytoplasmic tails,
        which then allows a conformational shift in the extracellular domain from a bent to a more
        extended form for high-affinity ligand binding (Figure 1) (Takagi et al. 2002).
        The activated integrins may then undergo clustering whereby the transmembrane domain of
        each type of subunit (the α or β) interacts with itself—called homotypic oligomerization of
        the transmembrane domains (Figure 1) (Li et al. 2003). Ligand occupancy and receptor
        clustering initiates outside-in signaling that, in turn, regulates a variety of cellular
        responses (see below). The three steps in Figure 1 occur as part of a dynamic equilibrium,
        and perturbation of any step can shift the equilibrium, leading to transient, partial, or
        permanent integrin activation/inactivation depending on the extent of perturbation. For
        example, deletion of aIIb cytoplasmic tail completely removes the clasp and permanently
        activates the receptor (O'Toole et al. 1991), whereas a particular disease mutation may
        only impair the clasp and partially activate the receptor (Peyruchaud et al. 1997). While
        the model in Figure 1 is based on direct structural evidence for the cytoplasmic face
        (Vinogradova et al. 2002; Kim et al. 2003) and the extracellular domain (Takagi et al.
        2002), the changes in the transmembrane region remained speculative. In this issue of 
        PLoS Biology , Luo et al. (2004) provide what is, to our knowledge, the
        first experimental evidence for the transmembrane domain separation, an event suggested by
        the model shown in Figure 1. By selectively altering the residues that can interact with
        one another, the authors defined a specific transmembrane domain interface in resting α
        IIb β
        3 and showed that this interface is lost upon activation of this
        integrin. Backed by extensive structural and biochemical data on the integrin
        cytoplasmic/extracellular domains, this transmembrane domain study takes the next vital
        step toward a more complete understanding of the unclasping mechanism for integrin
        activation. Although the energy required for lateral separation of the transmembrane
        domains in membrane appears to be high, the third step in Figure 1 (clustering via
        transmembrane domain oligomerization) may compensate for it.
      
      
        Filling in the Pieces
        Despite the molecular level of our understanding of integrin activation, a number of key
        questions remain unresolved. Although we know that the membrane-proximal clasp on the
        integrin cytoplasmic face controls the integrin activation, the distal side of either the α
        or β cytoplasmic tails may also play a role in integrin activation, since other mutations
        indicate that the C-terminal membrane distal region is important in regulating integrin
        activation via a mechanism that is yet unknown. Thus, the picture for the cytoplasmic
        face-controlled inside-out activation may be substantially more complicated than specified
        in Figure 1. There may exist other factors, such as negative regulators, in cells that bind
        to the cytoplasmic tails or their complex, and control the conformational change required
        for integrin activation. Also, there may be pathways other than the talin-mediated one that
        lead to integrin activation. Structures of the integrin cytoplasmic face bound to talin and
        the many other proteins known to bind to the cytoplasmic tails of integrins will
        undoubtedly provide further insights. In the transmembrane region, although there is ample
        evidence for heterodimeric transmembrane domain association (Adair and Yeager 2002;
        Schneider and Engelman 2003; Gottschalk and Kessler 2004; Luo et al. 2004) and dissociation
        upon integrin activation (Luo et al. 2004), a definitive structural view is missing. Some
        studies have proposed that homo-oligomerization is essential for inducing integrin
        activation (Li et al. 2003). However, the data provided by Luo et al. do not appear to
        support this model. On the extracellular side, while the C-terminal unclasping and
        separation of the cytoplasmic and transmembrane regions appears to relieve the structural
        constraint and may allow the unbending of the extracellular domain to attain the
        high-affinity ligand binding state (Takagi et al. 2002), a thorough molecular understanding
        of this process awaits high resolution structures of the intact receptor in inactive and
        active forms.
      
      
        What About Outside-In?
        Upon the inside-out activation, integrins bind to specific extracellular matrix
        proteins. However, for the integrins to grip tightly to the extracellular matrix to mediate
        cell adhesion and migration, the integrin cytoplasmic domains must be anchored to the
        cytoskeleton (Giancotti and Ruoslahti 1999). This is achieved by “outside-in” signaling,
        i.e., when an integrin binds to the extracellular ligand, it clusters with other bound
        integrins, resulting in the formation of highly organized intracellular complexes known as
        focal adhesions that are connected to the cytoskeleton. The focal adhesions incorporate a
        variety of molecules, including the cytoplasmic domains of the clustered integrins,
        cytoskeletal proteins, and an extensive array of signaling molecules. The high local
        concentrations of these molecules facilitate cascades of downstream intracellular responses
        via protein–protein interactions, which are linked to the cytoskeleton as well as to
        complex intracellular signaling networks. Although many intracellular components involved
        in outsidein signaling have been identified, and much has been learned about various
        signaling pathways involved in outside-in signaling (Giancotti and Ruoslahti 1999), a
        molecular view of how the various events occur in time and space is still very uncertain.
        In particular, little structural insight has been obtained for early outside-in
        intracellular events following ECM–integrin binding, e.g., upon ECM engagement. How is the
        integrin cytoplasmic domain connected to the cytoskeleton? How is this connection regulated
        during cell adhesion and migration? The next wave of structural information may provide
        insights into these important and fertile areas of investigation.
      
    
  

  
    
      
        
        Throughout his career, Bela Julesz created new scientific disciplines by remarkable
        combinations of seemingly disparate approaches. The selection of his major discipline,
        which would eventually be called visual neuroscience, may have been serendipity or
        choice.
        When the unexpected Soviet invasion of Hungary in 1956 spurred his emigration to the
        United States, Bela Julesz, with his Hungarian doctorate in engineering, joined the
        numerous mathematical luminaries working at AT&T Bell Laboratories, such as John Tukey,
        Harry Nyquist, Claude Shannon, and John Kelly. One of the projects underway at the time was
        the creation of long random-number binary sequences that did not repeat. Bela told the
        story that he was assigned the problem of testing these number generators; he decided to
        use the best pattern recognizer that he knew of—the human visual system. The random bits of
        zeros and ones drawn from the random number sequences were plotted as sequential rows in an
        image. Any repeats, any correlations across space, would be instantly seen by the human
        visual system as patterns in the random dots. What caused Bela to choose this unusual
        approach to looking for patterns, combining computers and vision? His doctoral thesis
        research in network theory and television signals clearly influenced him, but it was
        quintessential Bela to give himself a hand up into a new field by building on his base of
        knowledge, moving in a new and unexpected direction using mathematical and psychological
        insight. He termed this talent “scientific bilingualism” (Julesz 1994).
        This success in exploiting the visual system, and the intellectual freedom intrinsic to
        the design of Bell Labs, provided Bela with the opportunity to use these new random dot
        patterns to explore the visual system. Most of us know well that we can use the small
        differences in the images in each eye to see depth. Sir Charles Wheatstone showed in 1838
        that if two different perspective images were observed through a stereoscope so that each
        eye observed only one view, a startlingly realistic three-dimensional image occurred.
        Oliver Wendell Holmes, stereoscope enthusiast, wrote of the experience that “the shutting
        out of surrounding objects, and the concentration of the whole attention, which is a
        consequence of this, produce a dreamlike exaltation…in which we seem to leave the body
        behind us and sail away into one strange scene after another, like disembodied spirits”
        (Holmes 1861).
        The basis of this three-dimensional perception was hotly debated between Wheatstone and
        fellow physicist Sir David Brewster. (Though it may seem odd for physicists to concern
        themselves with the physiology of optics, this was felt to be a natural extension of the
        study of the physics of optics.) Brewster opined that perspective was the source of the
        apprehension of an object's shape. Wheatstone insisted that the images in the each eye had
        identifiable landmarks that were combined to assign depth to the landmarks. Bela read much
        of the literature of that time, and he must have seen two greats as wrestling without
        either finding the overwhelming hold to pin down the other. More than one hundred twenty
        years after Brewster and Wheatstone, Bela realized that his random dot patterns could be
        used to probe this question. What Bela did was create a pair of identical random dot
        patterns. When viewed binocularly through a stereoscope (i.e., fused), they would be seen
        as a single surface. Then Bela took a central region from the right random dot pattern and
        displaced it minutely to the right. Now when the two patterns were fused, the central
        square was not seen double, but after a moment or two, eerily moved into depth, behind the
        surrounding region. In 1960, Bela's experiment with what eventually became known as Julesz
        random dot stereograms unambiguously demonstrated that stereoscopic depth could be computed
        in the absence of any identifiable objects, in the absence of any perspective, in the
        absence of any cues available to either eye alone. It was a perfect combination of
        psychological and mathematical insight and technology that solved this puzzle. (It is an
        interesting aside that Bela sent his first report to the 
        Journal of the Optical Society of America , where it was rejected; the 
        Bell Labs Technical Journal holds the now classic paper [Julesz 1960].
        The 
        Journal of the Optical Society of America published Bela's second paper
        [Julesz 1963].) The stereoscope had existed 125 years.
        Bela proposed in his book 
        Foundations of Cyclopean Perception (1971) that early in the vision
        process the two images from the two eyes were combined to form a single view, imbued with
        inherent depth information. The perceptual “cyclops within us” was proposed to analyze the
        visual world first, before the motion, color, and contrast systems began their perceptual
        operations. Bela's book is full of powerful visual experiments that make this point
        irrefragably; from his psychophysical analysis, binocular vision forces unexpected
        constraints on the rest of vision, 
        Q.E.D. Foundations of Cyclopean Perception is still considered one of the
        classics of modern psychophysics and continues to have profound relevance to both those
        entering the field and established investigators—over thirty years after its publication.
        At the time of his death, Bela had begun working on a second edition.
        His success in determining the sequence of visual processing using random dot
        stereograms led Bela to propose that the anatomical hierarchy of the visual system could be
        understood in part through visual psychophysics—he termed this approach “psychoanatomy.”
        His ingenious use of the stereogram established a new approach in the field of vision
        research and presaged the now common use of carefully controlled computational techniques
        in brain science. By this time Bela's reputation was established, and in 1983, he received
        a prestigious MacArthur Fellowship—the “genius award.” He used the funds for travel,
        including an annual peregrination to the California Institute of Technology, where I first
        met Bela in 1985.
        His seminars and lecture courses were enthusiastically received and endorsed by
        countless students, post-doctoral trainees, and faculty, as evidenced both by his
        formidable reputation and through the numerous citations of his work. His approach to
        presenting his research was modest and gently self-deprecating. He always encouraged young
        scientists; his joy and passion in their science were transmitted both through his warm
        persona and his suggestions of directions for future study. His insights guided my
        development of random dot kinematograms (i.e., movies) to examine how motion could be used
        to construct three-dimensional form (Siegel and Andersen 1988). He collaborated with Derek
        Fender, David Van Essen, and John Allman at the California Institute of Technology on the
        combination of the computer, the psychophysical approach, and the physiological
        experiment.
        Bela was a fount of ideas, each building on the prior's advance. His later passions were
        explorations of texture and attention, notably with Jonathan Victor and Dov Sagi. Bela's
        appealing hypothesis that textons (putative elements of textures) are represented at a
        cellular level is now questionable (Julesz et al. 1978). Bela was groping for an
        overarching computational theory for the representation of random geometry, but none was to
        be had. Nonetheless, the texton elements served useful duty in the demonstration that there
        were two stages to early vision—an effortless phase preceding attention and a guided
        identification phase (Sagi and Julesz 1985). Many contemporary laboratories examining
        vision, studying either perception or the activity of neurons, now incorporate designed,
        complicated, yet highly controlled stimuli that have evolved (knowingly or not) from Bela's
        original forays in the 1960s and 1970s. His continuing impact was recognized by his
        election to the National Academy of Science in 1987.
        In 1989, Bela retired from Bell Labs (by then he was a department head) and joined the
        Department of Psychology at Rutgers University to establish the Laboratory of Vision
        Research. Bela continued investigating mechanisms of form, texture, and stereopsis; his
        presence led to numerous studies into the implications of his original findings as well as
        new investigations into computational vision. His collaborations greatly aided the
        establishment of neuroscience at Rutgers. Bela wrote 
        Dialogues on Perception (1995), a wide-ranging intellectual effort, in
        which he uses classic dialectics to question both his own successes and those of his chosen
        field. In the book one reads of two competing intellects, a Bela who believes in his
        contributions to science and another Bela who is constantly belittling and judging his
        contributions.
        Throughout his career Bela Julesz was able to add language after language to his
        research imperative, becoming a true scientific polyglot. Although his arrival in the
        United States was propelled by political events beyond his control, his intellectual
        directions followed a chosen path “less traveled by, and that has made all the difference.”
        In 1956, an engineer set out from Hungary. By 2003, his unique combination of mathematical
        precision combined with deep biological insight had carried him to elegant solutions for
        seemingly intractable problems in visual neuroscience. Bela was always in dialogue, often
        with others, and often with himself. In the process, he would gently drive each of us, and
        himself, forward to our final destination of understanding the brain. Bela Julesz died on
        December 31, 2003, forty-seven years to the day after starting at Bell Laboratories.
      
    
  

  
    
      
        
        From a human perspective, sexes seem a relatively simple thing to get one's head
        around—there are females, and there are males. But our perspective seems biased and narrow
        when applied to life as a whole, says evolutionary biologist Laurence Hurst of the
        University of Bath, United Kingdom.“If you were a single-celled alga sitting in a pond, you
        wouldn't see the world as splitting into males and females.”
        In fact, different species have evolved a bewildering number of ways to mix and match
        the attributes of sexes. Some do not have males and females, but have adaptations that mean
        each individual performs a specific role during sex. There are other species of which every
        member is sexually equivalent, but individuals nevertheless divide into groups for the
        purposes of mating. And in some species, individuals make both eggs and sperm (Box 1). This
        biological diversity has produced a semantic muddle among biologists—everyone who thinks
        about the evolution of sexes seems to have a slightly different take on what a sex is. “The
        literature is highly confusing—we need to clarify our terminology,” comments Rolf Hoekstra,
        a geneticist at the University of Waageningen in the Netherlands.
        As things stand, there are three main aspects to the definition of a sex: who you are,
        who you can mate with, and who your parents are. The third part of this trinity—parental
        number—shows the least variation in nature. No known organism needs more than one mother
        and one father. But even this assumption is now starting to break down at the level of
        biological systems. In a recently discovered hybrid system within the harvester ant genus 
        Pogonomyrmex , queens must mate with two types of males to
        produce both reproductive individuals and workers (Figure 3). These ants are the first
        species known which truly has more than two sexes—with colonies effectively having three
        parents— argues Joel Parker of the University of Lausanne, Switzerland.
        Parker's ideas might reactivate evolutionary biologists' interest in sexes, which has
        lain somewhat dormant since the 1990s. It could also provide a new route to experiments—
        something often lacking in the field. Not everyone agrees that it makes sense to define the
        ants' genetic quirks as new sexes. Each ant is still only a mix the genes from no more than
        two parents, after all. But Parker believes that our current ideas about mating systems may
        not be adequate to describe the ingenuity of evolution. “Until you see a three-sex system,
        you don't know what it'll look like,” he says.
      
      
        Little and Large
        To address whether these ants have more than two sexes, we first need to look at other
        candidates for sexes, their numbers in different species, and how these systems evolved.
        One thing biologists do agree on is that males and females count as different sexes. And
        they also agree that the main difference between the two is gamete size: males make lots of
        small gametes—sperm in animals, pollen in plants—and females produce a few big eggs. But
        researchers also think that before males and females evolved, sex occurred between
        organisms with equal-sized gametes, a state called isogamy.
        Evolutionarily speaking, an isogamous species faces two pressures. Individuals can make
        more smaller gametes, thus increasing their potential number of offspring, or they can make
        fewer bigger gametes, thus giving their offspring a better start in life by providing them
        with more resources. Theoretical analyses suggest that this pressure is particularly great
        if being big carries large benefits, making isogamy unstable. The original identical
        gametes will evolve towards the opposite ends of the size spectrum.
        In many species, however, one size of gamete still fits all. The organisms that have
        hung on to isogamy are found among the less complex branches of life, such as fungi, algae,
        and protozoa. This might be because large gametes, yielding well-funded zygotes, are likely
        to be more strongly selected if the resulting offspring needs to grow into a large and
        complex organism. The benefits of large gametes in simple and unicellular organisms are not
        so obvious. Some support for this hypothesis comes from the algae belonging to the group
        Volvocales. The variation in gamete size within each species matches its degree of
        complexity. For example, the unicellular species 
        Chlamydomonas rheinhardtii is isogamous, while 
        Volvox rouseletti , which lives in balls of up to 50,000 cells,
        has large and small gametes (Figure 4).
      
      
        The Opposite of Sexes?
        The question of sexes, and their number, is complex in isogamous species. Such species
        still typically comprise different groups for mating purposes. They have genes that allow
        them to mate with everyone except those belonging to the same “mating type” (this is
        presumably to avoid inbreeding and to produce offspring that are genetically diverse to
        cope with environmental change or biological enemies). Species with mating types, rather
        than males and females, aren't limited to two interbreeding groups: the ciliate protozoan 
        Tetrahymena thermophila has seven, and the mushroom 
        Schizophyllum commune has more than 28,000, for example. Some
        biologists call these mating types sexes; others think that, in the absence of traits other
        than sexual compatibility or the lack thereof, it makes more sense to view species with
        many mating types as having no sexes, rather than lots.
        Yet most isogamous species have only two mating types. This seems perverse—it excludes
        half the population as potential mates without gaining the benefits of specialization in
        sexual biology. With William Hamilton, Hurst came up an explanation for this apparent
        inefficiency.
        Two-group mating systems, they proposed, evolved as a way for genes in the nucleus to
        police the DNA in organelles. Cellular structures with their own genomes, such as
        mitochondria and chloroplasts, can divide more rapidly than the cells that house them. If
        the inheritance of organelles was biparental, selfish mutations in their DNA could spread
        rapidly, Hurst and Hamilton showed. A nuclear gene that enforces uniparental inheritance of
        organelles, along with a label that allows such cells to recognize each other so that their
        nuclear genes can share the benefits of cytoplasmic policing, should be favored.
        The mating biology of isogamous species offers considerable support for this idea. The
        aforementioned 
        C. rheinhardtii , for example, comes in two mating types called
        plus and minus. When the two fuse, the plastid of the minus cell is detroyed. Most
        isogamous species that fuse cells have a similar mechanism. Male-killer parasites such as 
        Wolbachia , a parasite of arthropods, show the selection
        pressure that intracellular passengers can exert (see also the primer by Wernegreen in the
        March issue of 
        PLoS Biology ). And cellfusion experiments hint that biparental
        inheritance of organelles does indeed cause problems, says Hurst. “Hybrids are often
        rubbish, but they can be better if a drug is administered that inhibits the mitochondria of
        one cell line.”
        The species that have lots of mating types, such as ciliate protozoa, exchange nuclear
        DNA, but not cytoplasm, and hence not intracellular organelles. Since individuals are freed
        from the need to police their organelles or keep out parasites, selection favors the widest
        assortment of possible mates, and thus the evolution of a large number of mating types so
        that one's own type—which one can't mate with—is a small subset of the population. It is
        possible to imagine species with cytoplasmic policing likewise having many mating types,
        but such a situation would be much more prone to break down and be invaded by selfish
        agents than one with two clearly defined types, which is what we usually see in nature.
        Some have argued that cytoplasmic policing might also be a selective force for
        different-sized gametes. Sperm could be small so that they do not import mitochondria into
        the egg.
        More than a decade after he devised it, Hurst's is still the leading hypothesis
        explaining the number of mating types in a species. But experimental evidence remains
        frustratingly elusive. “I wouldn't say I was entirely satisfied,” says Hurst. “We've got
        all these ideas, and they turn out to be quite hard to test—there's no simple thing one can
        do on a single species.” There are species where the uniparental inheritance of organelles
        is not so strictly enforced, says Hoekstra, such as yeasts and plants. “It's not easy to
        see if selection [on organelles] is strong enough,” he says.
      
      
        Three's Company
        Yet even in a species such as 
        S. commune , with its thousands of mating types, each sexual
        encounter involves only two cells. Nor are we likely to find a species that defies this
        pattern. The technical difficulties of combining more than two sets of genetic information
        into one individual, and of parceling out that information during meiosis, must be vast,
        says Brian Charlesworth of the University of Edinburgh. “We've reached the point of two
        cells fusing, and stuck with that; two cells are probably just as good as three,” he
        says.
        The ant colonies that Parker suggests have three parents are a hybrid of the species 
        Pogonomyrmex rugosus and 
        P. barbatus . The hybrids have not yet been classed as a new
        species, but they are well established across the southwestern United States, and there is
        no evidence of contemporary gene flow between hybrids and their parent species.
        Each ant has one parent if it is male, because male ants are produced from unfertilized
        eggs, or two if it is female. But each sex also comes in two genetic strains. If a queen
        mates with a male of her own strain, her offspring will be queens, and if she mates with a
        male from the other strain, the sperm will give rise to workers. So, for a colony to
        function fully it—and the queens it produces, because workers raise queens—must have two
        fathers and one mother. And if any one group were to disappear, the population as a whole
        would go extinct—unlike fungal mating types, where it's easy to imagine that the species
        would carry on if a few disappeared. “If you lose any one, the whole thing collapses,” says
        Parker. “It's really different from any other system.”
        So, Parker argues, 
        Pogonomyrmex has four sexes: the males and females of each
        strain. The idea is particularly potent if one views a social insect colony as a
        “superorganism,” with the workers equivalent to the cells of a body. It's as if a female
        mates with one male to produce her offspring's somatic cells, and another to produce its
        germ cells. The ants form chaotic mating swarms, so most queens have no problem mating
        multiply and getting sperm from males of both strains, although one would expect that males
        would strongly favor mating with females of their own strain.
        It's not known how the system originated. Separating the worker and reproductive castes
        by genetics—other social insects do this by environment, that is, by rearing workers and
        reproductives differently—may allow selection to operate more efficiently on each lineage,
        and the workers may benefit from hybrid vigor: field researchers report them as being
        highly aggressive. In an echo of Hurst's hypothesis, the system also mixes mitochondrial
        and nuclear genes differently in queens and workers.
        Some evolutionary biologists, such as Charlesworth, do not consider 
        Pogonomyrmex '
        s mating types sexes, arguing that to define sexes in yet another way
        only confuses the picture further. “[The ants] are an interesting system, but I wasn't
        persuaded by Parker's interpretation,” Charlesworth says. “I'm not a fan of the idea that
        it's useful to use the word ‘sex’ to describe compatibility between mating types—it muddies
        the waters.” Others are more positive towards Parker's interpretation: “It deserves to be
        taken seriously,” says evolutionary biologist Eörs Szathmáry of the Collegium Budapest in
        Hungary. “He's thrown a stone in the water—now we need to see what kinds of ripples it
        makes. You can't falsify a definition in the way you can a hypothesis; what determines
        their fate is whether people find them useful or not.”
        Species in which some individuals give up their reproductive opportunities to form part
        of a breeding group, such as slime molds, might have a system similar to that of the ants,
        Parker believes. “There may be hidden mating incompatibilities,” he says. “Now [that]
        people know to look, we're going to start seeing more of these systems.”
      
      
        
      
    
  

  
    
      
        
        The biogerontologist David Sinclair and the bioethicist Leon Kass recently locked horns
        in a radio debate (http://www.theconnection.org/shows/2004/01/20040106_b_main.asp) on human
        life extension that was remarkable for one thing: on the key issue, Kass was right and
        Sinclair wrong. Sinclair suggested, as have other experts, including his mentor Lenny
        Guarente and the National Institute on Aging advisory council member Elizabeth Blackburn,
        that Kass and other bioconservatives are creating a false alarm about life extension,
        because only a modest (say, 30%) increase in human life span is achievable by biomedical
        intervention, whereas Kass's apprehensions concern extreme or indefinite life extension.
        Kass retorted that science isn't like that: modest success tends to place the bit between
        our teeth and can often result in advances far exceeding our expectations.
        
        Coping with Methuselah consists of seven essays, mostly on the economics
        of life extension but also including one essay surveying the biology of aging and one on
        the ethics of life extension. The economic issues addressed are wide ranging, including
        detailed analysis of the balance between wealth creation by the employed and wealth
        consumption in pensions and health care; most chapters focus on the United States, but the
        closing chapter discusses these issues in a global context. Each essay is followed by a
        short commentary by another distinguished author. Within their own scope, all of these
        contributions are highly informative and rigorous. Dishearteningly, however, all echo
        Sinclair's views about the limited prospects for life extension in the coming decades. In
        my opinion, they make three distinct oversights.
        The first concerns current science. Sinclair and several other prominent gerontologists
        are presently seeking human therapies based on the long-standing observation that lifelong
        restriction of caloric intake considerably extends both the healthy and total life span of
        nearly all species in which it has been tried, including rodents and dogs. Drugs that
        elicit the gene expression changes that result from caloric restriction might, these
        workers assert, extend human life span by something approaching the same proportion as seen
        in rodents—20% is often predicted—without impacting quality of life, and even when
        administered starting in middle age. They assiduously stress, however, that anything beyond
        this degree of life extension is inconceivable.
        I agree with these predictions in two respects: that the degree of life extension
        achieved by first-generation drugs of this sort may well approach the (currently unknown)
        amount elicitable by caloric restriction itself in humans, and that it is unlikely to be
        much exceeded by later drugs that work the same way. In two other ways, however, I claim
        they are incorrect. The first error is the assumption of proportionality: I have recently
        argued (de Grey 2004), from evolutionary considerations, that longer-lived species will
        show a smaller maximal proportional life-span extension in response to starvation, probably
        not much more than the same 
        absolute increase seen in shorter-lived species. The second error is the
        assertion that no other type of intervention can do better. In concert with other
        colleagues whose areas of expertise span the relevant fields, I have described (de Grey et
        al. 2002, 2004) a strategy built around the actual 
        repair (not just retardation of accumulation) of age-related molecular
        and cellular damage—consisting of just seven major categories of ‘rejuvenation therapy’
        (Table 1)—that appears technically feasible and, by its nature, is indefinitely extensible
        to greater life spans without recourse to further conceptual breakthroughs.
        The second oversight made both by the contributors to 
        Coping with Methuselah and by other commentators is demographic. Life
        expectancy is typically defined in terms of what demographers call a period survival curve,
        which is a purely artificial construction derived from the proportions of those of each age
        at the start of a given year who die during that year. The ‘life expectancy’ of the
        ‘population’ thus described is that of a hypothetical population whose members live all
        their lives with the mortality risk at each age that the real people of that age
        experienced in the year of interest. The remaining life expectancy of someone aged 
        N in that year is more than this life expectancy minus 
        N for two reasons: one mathematical (what one actually wants, roughly, is
        the age to which the probability of survival is half that of survival to 
        N ) and one biomedical (mortality rates at each age, especially advanced
        ages, tend to fall with time). My spirits briefly rose on reading Aaron and Harris's
        explicit statement (p. 69) of the latter reason. Unfortunately, they didn't discuss what
        would happen if age-specific mortality rates fell by more than 2% per year. An interesting
        scenario was thus unexplored: that in which mortality rates fall so fast that people's 
        remaining (not merely total) life expectancy increases with time. Is this
        unimaginably fast? Not at all: it is simply the ratio of the mortality rates at consecutive
        ages (in the same year) in the age range where most people die, which is only about 10% per
        year. I term this rate of reduction of age-specific mortality risk ‘actuarial escape
        velocity’ (AEV), because an individual's remaining life expectancy is affected by aging and
        by improvements in life-extending therapy in a way qualitatively very similar to how the
        remaining life expectancy of someone jumping off a cliff is affected by, respectively,
        gravity and upward jet propulsion (Figure 1).
        The escape velocity cusp is closer than you might guess. Since we are already so long
        lived, even a 30% increase in healthy life span will give the first beneficiaries of
        rejuvenation therapies another 20 years—an eternity in science—to benefit from
        second-generation therapies that would give another 30%, and so on ad infinitum. Thus, if
        first-generation rejuvenation therapies were universally available and this progress in
        developing rejuvenation therapy could be indefinitely maintained, these advances would put
        us beyond AEV. Universal availability might be thought economically and sociopolitically
        implausible (though that conclusion may be premature, as I will summarise below), so it's
        worth considering the same question in terms of life-span 
        potential (the life span of the luckiest people). Figure 1 again
        illustrates this: those who get first-generation therapies only just in time will in fact
        be unlikely to live more than 20–30 years more than their parents, because they will spend
        many frail years with a short remaining life expectancy (i.e., a high risk of imminent
        death), whereas those only a little younger will never get that frail and will spend rather
        few years even in biological middle age. Quantitatively, what this means is that if a 10%
        per year decline of mortality rates at all ages is achieved and sustained indefinitely,
        then the first 1000-year-old is probably only 5–10 years younger than the first
        150-year-old.
        The third oversight that I observe in contemporary commentaries on life extension, among
        which 
        Coping with Methuselah is representative, is the most significant because
        of its urgency. First-generation rejuvenation therapies, whenever they arrive, will surely
        build on a string of prior laboratory achievements. Those achievements, it seems to me,
        will have progressively worn down humanity's evidently desperate determination to close its
        eyes to the prospect of defeating its foremost remaining scourge anytime soon. The problem
        (if we can call it that) is that this wearing-down may have been completed long before the
        rejuvenation therapies arrive. There will come an advance—probably a single laboratory
        result—that breaks the camel's back and forces society to abandon that denial: to accept
        that the risk of getting one's hopes up and seeing them dashed is now outweighed by the
        risk of missing the AEV boat by inaction. What will that result be? I think a conservative
        guess is a trebling of the remaining life span of mice of a long-lived strain that have
        reached two-thirds of their normal life span before treatment begins. This would possess
        what I claim are the key necessary features: a big life extension, in something furry and
        not congenitally sick, from treatment begun in middle age.
        It is the prospect of AEV, of course, that makes this juncture so pivotal. It seems
        quite certain to me that the announcement of such mice will cause huge, essentially
        immediate, society-wide changes in lifestyle and expenditure choices—in a word,
        pandemonium—resulting from the anticipation that extreme human life extension might arrive
        soon enough to benefit people already alive. We will probably not have effective
        rejuvenation therapies for humans for at least 25 years, and it could certainly be 100
        years. But given the present status of the therapies listed in Table 1, we have, in my
        view, a high probability of reaching the mouse life extension milestone just described
        (which I call ‘robust mouse rejuvenation’) within just 
        ten years, given adequate and focused funding (perhaps $100 million per
        year). And nobody in 
        Coping with Methuselah said so. This timeframe could be way off, of
        course, but as Wade notes (p. 57), big advances often occur much sooner than most experts
        expect. Even the most obvious of these lifestyle changes—greater expenditure on traditional
        medical care, avoidance of socially vital but risky professions—could severely destabilise
        the global economy; those better versed in economics and sociology than I would doubtless
        be even more pessimistic about our ability to negotiate this period smoothly.
        Overpopulation, probably the most frequently cited drawback of curing aging, could not
        result for many decades, but the same cannot be said for breadth of access irrespective of
        ability to pay: in a post-9/11 world, restricted availability of rejuvenation therapies
        resembling that seen today with AIDS drugs would invite violence on a scale that, shall we
        say, might be worth trying to avoid.
        Am I, then, resigned to a future in which countless millions are denied many decades of
        life by our studied reluctance to plan ahead today? Not quite. The way out is pointed to in
        Lee and Tuljapurkar's (1997) graph of the average wealth consumed and generated by an
        individual as a function of age, reproduced in 
        Coping with Methuselah (p. 143). Once AEV is achieved, there will be no
        going back: rejuvenation research will be intense forever thereafter and will anticipate
        and remedy the life-threatening degenerative changes appearing at newly achieved ages with
        ever-increasing efficacy and lead time. This will bring about the greatest economic change
        of all in society: the elimination of retirement benefits. Retirement benefits are for
        frail people, and there won't 
        be any frail people. The graph just mentioned amply illustrates how much
        wealth will be released by this. My hope, therefore, is that once policy makers begin to
        realise what's coming they will factor in this eventual windfall and allocate sufficient
        short-term resources to make the period of limited availability of rejuvenation therapies
        brief enough to prevent mayhem. This will, however, be possible only if such resources
        begin to be set aside long enough in advance—and we don't know how long we have.
      
    
  

  
    
      
        
        Consider a piece of text, either this one that you are now reading or any other. Surely
        they are all pretty much alike, in so far as they are all run-on strings of characters. In
        this same sense, we can envision that all DNA strands are alike because all are monotonous
        polymers with the same general chemical makeup. Indeed, this is how we think of DNA when
        considering its basic function of inheritance, in which all parts of all chromosomes must
        be duplicated and then passed from one cell generation to the next. The capacity for
        inheritance is fundamentally a consequence of DNA's general molecular structure, and not of
        its sequence per se, as Watson and Crick (1953), and indeed Muller (1922) long before them,
        well appreciated. Muller did not know that genes are made of DNA, but he did realize that,
        whatever genes were made of, they must have a general capacity to replicate, regardless of
        the information they carry (Muller 1922).
        But sequence does matter when DNA fulfills its other, more directly functional role.
        When the DNA that makes up a gene is exposed and expressed, when a gene is serving its
        individual function, then the detailed sequence means all.
        So where does recombination (Box 1) fit in? Is recombination something that happens to
        DNA generally? Or does it happen to particular sequences? Bacteria have their chi (χ)
        sequence, which is a specific series of eight base pairs in the DNA of the bacterial
        chromosome that stimulate the action of proteins that bring about recombination (Eggleston
        and West 1997). Similarly, the immunoglobulin genes of mammals have recombination signal
        sequences that are involved in V-J joining—a kind of somatic recombination involving the
        joining of a variable gene segment and a joining segment to form an immunoglobulin gene
        (Krangel 2003). But does normal meiotic recombination depend on the local DNA sequence? In
        yeast, as well as mammals (mice and humans), the answer is partly yes, for it is clear that
        chromosomes have local recombination hotspots where crossing over is much more likely to
        occur than in other places on the chromosome. Recombination hotspots are local regions of
        chromosomes, on the order of one or two thousand base pairs of DNA (or less—their length is
        difficult to measure), in which recombination events tend to be concentrated. Often they
        are flanked by “coldspots,” regions of lower than average frequency of recombination
        (Lichten and Goldman 1995).
      
      
        Diverse Implications of Recombination Hotspots: The Study of Meiosis and the Mapping
        of Human Disease Alleles
        Recombination hotspots are of strong interest to at least two quite different groups of
        biologists. For geneticists and cell biologists who study meiosis, the existence of
        recombination hotspots offers a way to learn what other processes are associated with
        recombination. This is partly how we know that homologous crossovers in yeast and other
        eukaryotes are initiated by the cleavage of single chromosomes, called “double-strand
        breaks” (Box 1). It turns out that because of this causal linkage, the hotspots for
        doublestrand breaks and the hotspots for recombination are one and the same (Game et al.
        1989; Sun et al. 1989; Keeney et al. 1997; Lopes et al. 1999; Allers and Lichten 2001;
        Hunter 2003).
        For population geneticists, much of the interest in recombination hotspots comes from
        their possible effect on the patterns of DNA sequence variation along human chromosomes and
        from the possibility that these patterns could be used to map the position of alleles that
        cause disease. When multiple copies of the DNA sequence of a gene, or of a larger region of
        a chromosome, are aligned, they reveal the location and distribution of variation at
        individual nucleotide positions—single nucleotide polymorphisms (SNPs). Each particular
        sequence, or haplotype, will carry a configuration for the SNPs for that region (Figure 1).
        Investigators have long known that SNPs that are adjacent or near each other tend to be
        highly correlated in their pattern and to exhibit strong linkage disequilibrium (Box 1). It
        is this linkage disequilibrium that enables scientists to map the locations of mutations
        that cause heritable genetic diseases. If alleles that cause a disease have the same kind
        of linkage disequilibrium with nearby SNPs as SNPs generally have with each other, then one
        could search for genes with disease alleles by looking for a pattern of SNPs that is found
        only in people who have the disease. This general method for mapping disease alleles is
        called “association mapping,” and it is basically a search for linkage disequilibrium
        between disease alleles and other SNPs. Whether or not association mapping works depends on
        the actual patterns of linkage that occur among SNPs in human populations, and these
        patterns depend in turn on how much recombination has occurred in the past (as well as on
        other demographic and mutation processes).
        With the advent of larger human haplotype data sets, it has become clear that there are
        often fairly long regions with very high linkage disequilibrium (Daly et al. 2001; Patil et
        al. 2001; Gabriel et al. 2002). This pattern of variation has been characterized as
        occurring in “haplotype blocks,” which are apparent regions of low recombination (or high
        linkage disequilibrium). Figure 1 shows a hypothetical example of haplotype blocks among
        eight haplotypes for a series of SNPs found over a region of a chromosome. Given diverse
        evidence of recombination hotspots in humans, a much discussed question is whether
        recombination hotspots play a large role in the formation of the pattern of haplotype
        blocks (Wang et al. 2002; Innan et al. 2003; Phillips et al. 2003; Stumpf and Goldstein
        2003). The occurrence of haplotype blocks has inspired the HAPMAP project
        (http://www.hapmap.org/), which has the goal of identifying a subset of SNPs that capture
        most of the relevant linkage information in the human genome (IHC 2003). If one had a
        subset of all common SNPs, with one or two per haplotype block, then this subset would
        contain much of the available information for association mapping of disease alleles.
      
      
        The Evolution of Recombination and (Possibly) Recombination Hotspots
        Recombination is a nearly ubiquitous feature of genomes, and a great many theories have
        been put forward to explain why it would be evolutionarily advantageous for genes to
        regularly break with one another to join new genes (Barton and Charlesworth 1998). By and
        large these theories predict that recombination should occur more often where genes occur
        in higher concentration and that it should happen less often in areas of the genome where
        genes are spaced far apart. This expectation is roughly born out in the human genome, where
        recombination rates are higher in regions of the genome with higher gene density (Fullerton
        et al. 2001; Kong et al. 2002).
        To consider the possible evolutionary advantages of individual recombination hotspots,
        we can draw from theory on the evolution of recombination modifiers. In particular, recent
        population genetic theory has brought to light some fairly general circumstances for which
        mutations that raise recombination rates would be favored by natural selection (Barton
        1995; Otto and Barton 1997; Otto and Barton 2001; Otto and Lenormand 2002). The basic idea
        is that linkage disequilibrium can easily occur (for many reasons) between two (or more)
        polymorphic sites that are under selection. When this occurs, an allele that raises the
        recombination rate (and decreases the linkage disequilibrium) can cause selection to act
        more efficiently. If an allele that is under positive or negative selection always occurs
        with an allele at another locus that is also under selection (i.e., the two loci are in
        strong linkage disequilibrium), then selection cannot act on one locus independently of the
        second locus. As new, multilocus configurations of beneficial alleles are generated (by
        recombination) and increase in frequency by selection, the modifiers of recombination that
        caused the production of those beneficial configurations increase in frequency with them. A
        key piece of evidence supporting this kind of theory of the evolution of recombination is
        directional selection, like that which occurs in artificial selection experiments, which
        often generates a correlated elevation in recombination rates (Otto and Lenormand
        2002).
        Connecting these ideas about the evolution of recombination modifiers to the question of
        recombination hotspots, we come to the possibility that individual hotspots may have arisen
        as a byproduct of linkage disequilibrium between genes on either side of the hotspot that
        were under selection. This situation would create a kind of selection pressure favoring
        recombinant haplotypes and thus also favoring those chromosomes that happen to have a high
        recombination rate between the selected genes. If true, then we might expect local
        recombination rates (i.e., hotspots and coldspots for recombination) to fluctuate in
        location and intensity, in ways that would be hard to precisely predict without knowing
        what genes have been under selection and what patterns of linkage disequilibria there may
        have been.
        In this light, the paper by Ptak et al. (2004) in this issue of 
        PLoS Biology is especially interesting. They report that chimpanzees do
        not have a recombination hotspot in the TAP2 region where humans have a fairly well
        characterized recombination hotspot (Jeffreys and Neumann 2002). Ptak et al.'s is a
        statistical study of linkage disequilibrium in the TAP2 region of chimpanzees and humans,
        and is less direct than the sperm-typing study of Jeffreys and Neumann (2002). However the
        contrast in linkage patterns between humans and our closest relatives suggests that
        recombination hotspots can evolve fairly quickly.
      
      
        Functional Constraints on Recombination Hotspots
        As appealing as the recombination modifier theory of recombination hotspots may be,
        there is circumstantial evidence that argues against it and that suggests that
        recombination hotspots are not directly the byproduct of selection on alleles in linkage
        disequilibrium. Particularly important in this regard is that some wellstudied organisms
        (notably the worm 
        Caenorhabditis elegans and the fruitfly 
        Drosophila melanogaster ) have not shown evidence of
        recombination hotspots. If we compare these organisms with yeast and mammals, which do show
        hotspots, we gain some more insight into the factors affecting the evolution of
        hotspots.
        Recall that double-strand breaks are the sites where recombination is initiated during
        meiosis, and that this is true regardless of the presence of hotspots for both phenomena.
        Apparently it is the case in yeast and mammals that both recombination and double-strand
        breaks are also prerequisites for the proper formation of the synaptonemal complex (SC)
        (Figure 2) and thus for proper orientation of the spindle apparatus and accurate
        segregation of chromosomes during meiosis (Paques and Haber 1999; Lichten 2001; Hunter
        2003; Page and Hawley 2003). In contrast, neither double-strand breaks nor recombination
        appear to be required for the formation of the SC in 
        D. melanogaster or 
        C. elegans (Zickler and Kleckner 1999; MacQueen et al. 2002;
        McKim et al. 2002; Hunter 2003; Page and Hawley 2003). Double-strand breaks and
        recombination do indeed co-occur in these model organisms, and are required for proper
        chromosome segregation, but they occur after the formation of the SC. Both of these species
        have broad chromosomal regions where crossing over occurs at higher rates than others, but
        there have been no reports of local recombination hotspots.
        Recombination during meiosis seems to be required for proper chromosome segregation;
        however, in those organisms where recombination and double-strand-break hotspots occur,
        these phenomena are also required for proper formation of the SC. It is as if the
        recombination machinery has been partly co-opted for chromosome alignment in some
        eukaryotes more so than in others. The implication of these findings is that recombination
        hotspots are byproducts of other functional constraints associated with the recombination
        process. This does not rule out the evolutionary theory of recombination modifiers, or that
        the location and intensity of recombination hotspots may evolve rapidly, but it does
        suggest that we may not need to invoke the evolutionary modifier theory to explain the
        existence of recombination hotspots.
      
      
        Conclusions
        Recombination hotspots co-occur with double-strand-break hotspots in some eukaryotes,
        and together these phenomena appear to play an important role in the formation of the SC in
        those organisms. Given the limited phylogenetic occurrence of recombination hotspots (i.e.,
        their occurrence in some, but not all, species), general theories for the evolution of
        recombination may not be very helpful for understanding the existence of recombination
        hotspots. However, in those species where they do occur, it is quite possible that
        recombination hotspots do evolve in location and intensity. Furthermore, the presence of
        recombination hotspots in humans may have large effects on the length of local patterns of
        linkage disequilibrium (haplotype blocks) and thus on our ability to map disease alleles by
        their association with other markers.
      
      
        
      
    
  

  
    
      
        
        If necessity is the mother of invention, then its father is an inveterate tinkerer, with
        a large garage full of spare parts. Innovation (like homicide) requires motive and
        opportunity. Clearly, the predominant ‘motive’ during the evolution of a novel gene
        function is to gain a selective advantage. To understand why gene duplications represent
        the major ‘opportunities’ from which new genes evolve, we must first consider what
        constrains genic evolution.
        The vast majority of genes in every genome are selectively constrained, in that most
        nucleotide changes that alter the fitness of the organism are deleterious. How do we know
        this? Comparisons between genomes clearly demonstrate that coding sequences diverge at
        slower rates than non-coding regions, largely due to a deficit of mutations at positions
        where a base change would cause an amino-acid change. Gene duplication provides
        opportunities to explore this forbidden evolutionary space more widely by generating
        duplicates of a gene that can ‘wander’ more freely, on condition that between them they
        continue to supply the original function.
        Susumu Ohno was the first to comprehensively elucidate the potential of gene
        duplication, in his book 
        Evolution by Gene Duplication , published more than 30 years ago (Ohno
        1970). The prescience of Ohno's book is highlighted by the fact that his book has almost
        certainly been cited more times in the past five years than in the first five years after
        its publication.
      
      
        What Is the Evidence for the Importance of Gene Duplication?
        The primary evidence that duplication has played a vital role in the evolution of new
        gene functions is the widespread existence of gene families. Members of a gene family that
        share a common ancestor as a result of a duplication event are denoted as being paralogous,
        distinguishing them from orthologous genes in different genomes, which share a common
        ancestor as a result of a speciation event. Paralogous genes can often be found clustered
        within a genome, although dispersed paralogues, often with more diverse functions, are also
        common.
        Whole genome sequences of closely related organisms have allowed us to identify changes
        in the gene complements of species over relatively short evolutionary distances. These
        comparisons typically reveal dramatic expansions and contractions of gene families that can
        be related to underlying biological differences. For example, humans and mice differ in
        their sensory reliance on sight and smell respectively; colour vision in humans has been
        significantly enhanced by the duplication of an Opsin gene that allows us to distinguish
        light at three different wavelengths, while mice can distinguish only two. By contrast, a
        much higher proportion of the large gene family of olfactory receptors have retained their
        functionality in mice, as compared to humans.
        Given the apparent importance of gene duplication for the evolution of new biological
        functions over all evolutionary timescales, it is of great interest to be able to
        comprehensively document the duplicative differences that exist between our own species and
        our closest relatives, the great apes. The study by Fortna et al. (2004) in this issue of 
        PLoS Biology identifies over 3% of around 30,000 genes as having
        undergone lineage-specific copy number changes among five hominoid (humans plus the great
        apes) species. This is the first time that copy number changes among apes have been assayed
        for the vast majority of human genes, and we can expect that the biological consequences of
        the 140 human-specific copy number changes identified in this study will be heavily
        investigated over the coming years.
      
      
        How Do Duplications Arise?
        The various mechanisms by which genes become duplicated are often classified on the
        basis of the size of duplication generated, and whether they involve an RNA intermediate
        (Figure 1).
        ‘Retrotransposition’ describes the integration of reverse transcribed mature RNAs at
        random sites in a genome. The resultant duplicated genes (retrogenes) lack introns and have
        poly-A tails. Separated from their regulatory elements, these integrated sequences rarely
        give rise to expressed full-length coding sequences, although functional retrogenes have
        been identified in most genomes.
        Tandem duplication of a genomic segment (segmental duplication) is one of the possible
        outcomes of ‘unequal crossing over’, which results from homologous recombination between
        paralogous sequences. These recombination events can also give rise to the deletion or
        inversion of intervening sequences. Recent evidence suggests that the explosion of
        segmental duplications in recent primate evolution has been caused in part by the rapid
        proliferation of Alu elements about 40 MYA. Alu elements are derived from the 7SL RNA gene
        and represent the most frequent dispersed repeat in the human genome, with the
        approximately 1 million copies of the 300-bp Alu element representing around 10% of the
        entire genome. The striking enrichment of Alu elements at the junctions between duplicated
        and single copy sequences implicates unequal crossing over between these repeats in the
        generation of segmental duplications (Bailey et al. 2003).
        The observation of segmental duplication events with no evidence for homology-driven
        unequal crossing over suggests that segmental duplications can also arise through
        non-homologous mechanisms. A recent screen for spontaneous duplications in yeast suggests
        that replication-dependent chromosome breakages also play a significant role in generating
        tandem duplications, because duplication breakpoints are enriched at replication
        termination sites (Koszul et al. 2004).
        Genome duplication events generate a duplicate for every gene in the genome,
        representing a huge opportunity for a step-change in organismal complexity. However, genome
        duplication presents significant problems for the faithful transmission of a genome from
        one generation to the next, and is consequently a rare event, at least in Metazoa. In
        principle, genome duplications should be easily identified through the coincident emergence
        within a phylogeny of many gene families. Unfortunately, this signal is complicated by
        subsequent piecemeal loss and gain of gene family members. Consequently, there is heated
        debate over possible ancient genome duplication events in early vertebrate evolution and
        more recently in teleost fish, both of which must have occurred hundreds of millions of
        years ago (McLysaght et al. 2002; Van de Peer et al. 2003).
        So what are the relative contributions of these different mechanisms? Not all
        interspersed duplicate genes are generated by retrotransposition. The initially tandem
        arrangement of segmental duplications can be broken up by subsequent rearrangements. In
        keeping with this hypothesis, duplicated genes in a tandem arrangement typically represent
        more recent duplication events (Friedman and Hughes 2003). Recent analyses suggest that 70%
        of non-functional duplicated genes (pseudogenes) in the human genome result from
        retrotransposition rather than any DNA-based process (Torrents et al. 2003).
      
      
        What Fates Befall a Recently Duplicated Gene?
        A duplicated gene newly arisen in a single genome must overcome substantial hurdles
        before it can be observed in evolutionary comparisons. First, it must become fixed in the
        population, and second, it must be preserved over time. Population genetics tells us that
        for new alleles, fixation is a rare event, even for new mutations that confer an immediate
        selective advantage. Nevertheless, it has been estimated that one in a hundred genes is
        duplicated and fixed every million years (Lynch and Conery 2000), although it should be
        clear from the duplication mechanisms described above that it is highly unlikely that
        duplication rates are constant over time. However, once fixed, three possible fates are
        typically envisaged for our gene duplication.
        Despite the slackened selective constraints, mutations can still destroy the incipient
        functionality of a duplicated gene: for example, by introducing a premature stop codon or a
        mutation that destroys the structure of a major protein domain. These degenerative
        mutations result in the creation of a pseudogene (nonfunctionalization). Over time, the
        likelihood of such a mutation being introduced increases. Recent studies suggest that there
        is a relatively narrow time window for evolutionary exploration before degradation becomes
        the most likely outcome, typically of the order of 4 million years (Lynch and Conery
        2000).
        During the relatively brief period of relaxed selection following gene duplication, a
        new, advantageous allele may arise as a result of one of the gene copies gaining a new
        function (neofunctionalization). This can be revealed by an accelerated rate of amino-acid
        change after duplication in one of the gene copies. This burst of selection is necessarily
        episodic—once a new function is attained by one of the duplicates, selective constraints on
        this gene are reasserted. These patterns of selection can be observed in real data: most
        recently duplicated gene pairs in the human genome have diverged at different rates from
        their ancestral amino-acid sequence (Zhang et al. 2003). A convincing instance of
        neofunctionalization is the evolution of antibacterial activity in the 
        ECP gene in Old World Monkeys and hominoids after a burst of amino-acid
        changes following the tandem duplication of the progenitor gene 
        EDN (a ribonuclease) some 30 MYA (Zhang et al. 1998). The divergence of
        duplicated genes over time can be also monitored in genome-wide functional studies. In both
        yeast and nematodes, the ability of a gene to buffer the loss of its duplicate declines
        over time as their functional overlap decreases.
        Rather than one gene duplicate retaining the original function, while the other either
        degrades or evolves a new function, the original functions of the single-copy gene may be
        partitioned between the duplicates (subfunctionalization). Many genes perform a
        multiplicity of subtly distinct functions, and selective pressures have resulted in a
        compromise between optimal sequences for each role. Partitioning these functions between
        the duplicates may increase the fitness of the organism by removing the conflict between
        two or more functions. This outcome has become associated with a population genetic model
        known as the Duplication–Degeneration–Complementation (DDC) model, which focuses attention
        on the regulatory changes after duplication (Force et al. 1999). In this model,
        degenerative changes occur in regulatory sequences of both duplicates, such that these
        changes complement each other, and the union of the expression patterns of the two
        duplicates reconstitutes the expression pattern of the original (Figure 2).
        A recent study by Dorus and colleagues (Dorus et al. 2003) investigated the
        retrotransposition (since the existence of a human–mouse common ancestor) of one of the two
        autosomal copies of the 
        CDYL gene to Y chromosome (forming 
        CDY ). In the mouse, both 
        Cdyl genes produce two distinct transcripts, one of which is expressed
        ubiquitously while the other is testis-specific. By contrast, in humans both 
        CDYL genes produce a single ubiquitously expressed transcript, and 
        CDY exhibits testis-specific expression. As 
        CDY is a retrogene (see above) that has not been duplicated together with
        its ancestral regulatory sequences, it is clear that the DDC model is not the only route by
        which to achieve spatial partitioning of ancestral expression patterns.
        Subfunctionalization can also lead to the partitioning of temporal as well as spatial
        expression patterns. In humans, the β-globin cluster of duplicated genes contains three
        genes with coordinated but distinct developmental expression patterns. One gene is
        expressed in embryos, another in foetuses, and the third from neonates onwards. In
        addition, coding sequence changes have co-evolved with the regulatory changes so that the O
        2 binding affinity of haemoglobin is optimised for each developmental
        stage. This coupling between coding and regulatory change is similarly noted at a genomic
        level when expression differences between many duplicated genes pairs are correlated with
        their coding sequence divergence (Makova and Li 2003).
      
      
        Other Evolutionary Consequences of Gene Duplication
        If duplication results in the formation of a novel function as a result of interaction
        between the two diverged duplicates, which of the above categories of evolutionary outcome
        does this innovation fall into? Not all new biological functions resulting from gene
        duplications can be ascribed to individual genes. Protein–protein interactions often occur
        between diverged gene duplicates. This is especially true for ligand–receptor pairs, which
        are often supposed to coevolve after a gene duplication event, and thus progress from
        homophilic to heterophilic interactions. This emergent function of the new gene pair does
        not fit comfortably into any of the scenarios outlined above: both genes are functional yet
        neither retains the original function, nor has the original function been partitioned. This
        mode of ‘duplicate co-evolution’ is likely to be especially prevalent in signalling
        pathways.
        Earlier, we saw that homologous recombination between paralogous sequences can result in
        rearrangements, including tandem duplications. Such recombination events need not cause
        rearrangements, but can also result in the nonreciprocal transfer of sequence from one
        paralogue to the other—a process known as gene conversion. Gene conversion homogenizes
        paralogous sequences, retarding their divergence, and consequently obscuring their
        antiquity. This leads to the observation of ‘concerted evolution’ whereby duplicates within
        a species can be highly similar and yet continue to diverge between species (Figure 3).
        Once gene duplicates have diverged sufficiently so that they differ in their functionality
        (or non-functionality), gene conversion events can become deleterious—for example, by
        introducing disrupting mutations from a pseudogene into its functional duplicate. A
        substantial proportion of disease alleles in Gaucher disease result from the introduction
        of mutations into the glucocerebrosidase gene from a tandemly repeated pseudogene (Tayebi
        et al. 2003). These kinds of recombinatorial interactions only occur between paralogues
        that are minimally diverged. Thus, while selective interactions and functional overlap
        between duplicates declines relatively slowly over evolutionary time, the potential for
        recombinatorial interactions between paralogues is relatively short-lived.
        For some genes, duplication confers an immediate selective advantage by facilitating
        elevated expression, or as Ohno put it, ‘duplication for the sake of producing more of the
        same’. This has clearly been the case for histones and ribosomal RNA genes. In this
        scenario, gene conversion is of potential benefit in maintaining homogeneity between
        copies. Certainly both histone and rDNA genes are commonly found in arrays of duplicates:
        structures that facilitate array homogenization by both gene conversion and repeated
        unequal crossing over.
        Mechanisms of segmental duplication are oblivious to where genes begin and end, and so
        are additionally capable of duplicating parts of genes or several contiguous genes. The
        intragenic duplication of individual exons or enhancer elements also presents new
        opportunities for the evolution of new functions or greater regulatory complexity.
      
      
        Conclusions
        The likelihood that newly duplicated genes will both remain functional clearly relates
        to their inherent potential to undergo subfunctionalization or neofunctionalization. Under
        the DDC model, greater regulatory complexity bestows greater potential for
        subfunctionalization (Force et al. 1999), whereas neofunctionalization is more likely to
        occur in genes that are necessarily rapidly evolving, such as those involved in
        reproduction, immunity, and host defence (Emes et al. 2003). This is not to say that these
        biases are deterministic, there are plenty of ‘successful’ gene family clusters that
        contain associated pseudogenes.
        Duplicate gene evolution has most likely played a substantial role in both the rapid
        changes in organismal complexity apparent in deep evolutionary splits and the
        diversification of more closely related species. The rapid growth in the number of
        available genome sequences presents diverse opportunities to address important outstanding
        questions in duplicate gene evolution. For those interested in patterns of selection
        following duplication, the transient nature of the evolutionary window of opportunity
        following duplication will focus attention on recently duplicated genes. In this regard it
        will be important to document copy number variation not only among species, as Fortna et
        al. have, but within species as well. In addition, it has been, and will continue to be, a
        lot easier to identify copy number changes between genomes than it is to identify their
        biological consequences (if any). Extensive functional studies targeted at duplicated genes
        are required if we are to more fully understand the range of evolutionary outcomes.
        Moreover, collaborations between the proteomics and evolutionary genetics communities would
        facilitate investigation of the potential role of gene duplication during the evolution of
        the protein–protein and cell–cell interactions that are fundamental to the biology of
        multicellular organisms.
      
    
  

  
    
      
        
        In July 2000, the finger of blame for a mysterious mass killer of Californian oak trees
        came to rest on a previously undescribed plant pathogen. From the initial identification of
        
        Phytophthora ramorum , it took less than four years to produce a
        draft sequence of its genome, one of the fastest-ever discovery-to-sequence stories for a
        complex pathogen. This achievement was a United States initiative, facilitated by the
        injection of federal and state funding into 
        Phytophthora research. But the US is not alone in the battle
        against this genus. 
        Phytophthora species cause thousands of millions of dollars of
        damage to the world's commercial crops every year: they blight potatoes and tomatoes,
        devastate the lucrative soybean, and rot cacao, threatening the world's supply of chocolate
        (Figure 1). But for Sophien Kamoun, Associate Professor of Plant Pathology at Ohio State
        University (Wooster, Ohio, United States), these destructive organisms present an exciting
        opportunity. There are some 60 species of 
        Phytophthora , but so little is known about the genus, he says,
        that there are things about its species we didn't even know we didn't know. ‘What I find
        really exciting,’ he says, ‘is discovering these unknown unknowns.’
        The most infamous of the 
        Phytophthora pathogens is the potato late blight, 
        P. infestans . It was this species that led to the Irish potato
        famine in the mid-1840s, which resulted in the death or displacement of millions. Today, 
        P. infestans is estimated to cost potato and tomato farmers
        US$5,000,000,000 a year in lost revenue. The story for the soybean pathogen 
        P. sojae is similar, causing loss of more than US$1,000,000,000
        a year to soybean growers. In addition to the direct economic impact of these pathogens,
        introduced 
        Phytophthora can cause severe damage to native flora. The most
        recent 
        Phytophthora on the scene is 
        P. ramorum , which has caused ‘sudden oak death’ (SOD) in tens
        of thousands of oak trees across the coastal counties of California, is now present in at
        least three other US states and is threatening to take on the native flora of the entire
        North American continent. It is also lurking in Europe, although apparently with less
        devastating consequences.
      
      
        Molecular Machinery
        With this kind of impact, it's no surprise that money has poured into research on 
        Phytophthora . This year, the US federal government will channel
        US$7,400,000 toward research into SOD. A major focus of this funding is genomics.
        Sequencing the genomes of several 
        Phytophthora will help clarify the phylogeny and evolution of
        these enigmatic organisms (Box 1) and improve methods of detection and identification.
        Ultimately, however, sequencing should reveal the molecular tricks that this genus uses to
        subvert the defences of its plant hosts, allowing scientists to come up with new ways to
        combat these troublesome organisms.
        The 
        P. infestans sequencing initiative, coordinated by Kamoun, has
        recently completed a survey sequence of the genome that gives an initial understanding of
        how this organism is structured. Perhaps most striking is its size. ‘It's a huge genome,’
        says Kamoun. At about 250 megabases (Mb), it's about twice the size of the 
        Arabidopsis genome. His latest research, published in the 
        Journal of
        Biological Chemistry (Tian et al. 2004), describes a 
        P. infestans protease inhibitor—extracellular protease inhibitor
        1 (EPI1)—that could be one of a unique class of suppressor proteins that 
        Phytophthora deploy to infect and counteract host defences. The
        pathogen seems to upregulate the 
        epi1 gene during colonisation of its host. EPI1 inhibits plant apoplastic
        proteases—extracellular enzymes that are part of the host's defensive armoury that have
        evolved to prevent foreign proteins entering cells.
        ‘Based on its biological activity and expression pattern, EPI1 may function as a disease
        effector molecule and may play an important role in 
        P. infestans colonisation of host apoplast,’ Kamoun and his
        colleagues report (Tian et al. 2004). If further research confirms this function for EPI1,
        then it will become one of just a handful of pathogen molecules that have been shown to
        suppress host plant defenses. A search in sequence databases for matching motifs reveals
        just one similar sequence in the entire bacterial and fungal kingdoms. However,
        apicomplexans like 
        Toxoplasma gondii that transit through the mammalian digestive
        tract also appear to secrete protease inhibitors allied to EPI1. This similarity suggests
        an analogy between plant apoplasts and mammalian digestive tracts. Both environments are
        rich in proteases, but nevertheless are colonised by a variety of microbial pathogens. In
        the case of an apoplast, the pathogen is 
        P. infestans , whilst in the mammalian gut, it's 
        T. gondii —and although they are phylogenetically distant, these
        pathogens seem to have independently recruited similar secreted proteins to inhibit the
        defensive proteases produced by their hosts. Interestingly, whilst 
        T. gondii inhibitors inhibit gut enzymes trypsin and
        chymotrypsin, EPI1 does not, suggesting that coevolution between the inhibitors and their
        target proteases may shape the specificity of these pathogenic enzymes.
        Armed with this new insight into the molecular cunning of 
        P. infestans , Kamoun hopes that it will be possible to come up
        with ways of slowing disease progression. Importantly, it looks likely that protease
        inhibitors such as EPI1 are present in other 
        Phytophthora species. There are significant matches between the 
        epi1 gene sequence and motifs from at least five other closely related
        species. So any methods of blocking the action of protease inhibitors in 
        P. infestans might also work against other 
        Phytophthora . The protease inhibitors are one of Kamoun's
        ‘unknown unknowns’. ‘It's an example of something that we had absolutely no idea was in the
        genome,’ he says.
        Even more advanced than the 
        P. infestans genome project is an ongoing collaboration between
        the Virginia Bioinformatics Institute (Blacksburg, Virginia, United States) and the US
        Department of Energy's Joint Genome Project based in Walnut Creek, California. The focus
        here is the soybean pathogen 
        P. sojae and the SOD pathogen 
        P. ramorum , for which draft sequences are now complete
        (www.jgi.doe.gov/). The genomes are much smaller than that of the 250-Mb 
        P. infestans —
        P. sojae is about 90 Mb and 
        P. ramorum is just 55 Mb. This, in part, explains the
        comparative speed of these sequencing efforts. But another factor is undeniably the fear of
        the unknown 
        P. ramorum , which in 2002 netted the Virginia–California
        initiative US$3,800,000 in federal funding to describe its genome. Ultimately, however, the
        sequence of one species will help to inform on the sequence of other related species.
        Brett Tyler, Research Professor at the Virginia Bioinformatics Institute, is focusing on
        the molecular tools used by 
        P. sojae to infect its host. He agrees with Kamoun that
        understanding this machinery is the way to devise new control measures that could give
        plants the upper hand in the evolutionary arms race against their 
        Phytophthora pests. At present, most strategies to limit the
        damage caused by species like 
        P. infestans and 
        P. sojae rely on developing resistant cultivars by selective
        breeding of varieties with major resistance genes—single genes that can block a pathogen.
        However, 
        Phytophthora seem able to find ways to overcome these efforts. ‘
        P. infestans is absolutely notorious for its ability to
        genetically change in response to a major resistance gene,’ says Tyler. ‘Typically major
        resistance genes in potato barely last a single season.’
        Things look better for the soybean. New cultivars containing major resistance genes show
        resilience to 
        P. sojae for five to ten years. However, this resistance is
        starting to break down, and breeders are running out of major resistance genes with which
        to conjure new varieties. The alternative, says Tyler, is quantitative or multigenic
        resistance, which relies on getting plants to express several resistance genes at once,
        each of which makes a small contribution to the plant's overall resistance. It should be
        much harder for 
        P. sojae to evolve a new attack against this kind of robust
        defence. The search is also on for new genes that could be used to encourage quantitative
        resistance to host species. One particularly promising approach is to pit pathogen against
        host to see which genes are switched on. The host should upregulate genes that defend it
        against infection and the pathogen should upregulate genes that it needs to attack. The
        discovery that plants produce proteases and that 
        Phytophthora have responded by secreting protease inhibitors to
        disable them is important if long-lasting solutions are to be found. ‘We just need to find
        a way to introduce new protein-degrading enzymes into the plant that the pathogen doesn't
        know how to block,’ says Tyler. ‘With these genomic tools we can really accelerate the pace
        at which we can evaluate different possible protective measures.’
      
      
        Epidemiology: Identifying the Culprit
        An additional benefit of the abundance of genetic information is that species
        identification is becoming increasingly sophisticated. At a glance, 
        Phytophthora can be mistaken for a fungus, so DNA profiling of
        isolates is crucial if species and strains of species are to be identified correctly so
        that action appropriate to each infection can be taken. Since 2000, Matteo Garbelotto,
        Adjunct Professor of Mycology and Forest Pathology at the University of California at
        Berkeley (Berkeley, California, United States), has spent a significant part of his working
        life tracking the spread of 
        P. ramorum , the 
        Phytophthora that has killed off vast tracts of oak trees in
        native Californian forest (Box 2).
        DNA analysis has been crucial to confirm suspected cases of 
        P. ramorum , and has now revealed that infections have reached
        at least three other US states. This spread is probably due to the movement of infected
        ornamentals like rhododendron (
        Rhododendron spp.) and viburnum (
        Viburnum spp.), which seem to act as carriers for the pathogen.
        ‘What we're seeing is a parallel to what has been happening throughout Europe, where the
        infection has basically moved using the commercial routes of the ornamental plant
        industry,’ Garbelotto says.
      
      
        P. ramorum in Europe
        
        P. ramorum does not appear to have the same devastating
        consequences in Europe as it does in the US—at least not yet. It's not entirely clear why,
        but it could have something to do with the structure of the bark of different host species,
        suggests Garbelotto. ‘We normally see more infection where we have more corrugation, and
        that's because water accumulates in the fissures … where the zoospores have a chance to
        infect the bark.’ However, he notes, European beech (
        Fagus sylvatica ) appears to be extremely susceptible to 
        P. ramorum . ‘If it reaches areas where there are a lot of
        beeches, it could potentially mirror what's happening in California,’ he warns.
        In Europe, symptoms characteristic of 
        P. ramorum infection were first described on rhododendrons in
        The Netherlands in 1993. Once this was confirmed to be the same species as the pathogen
        responsible for Californian SOD, there was speculation that 
        P. ramorum had either been introduced to the US from Europe or
        vice versa. However, in December 2002, it emerged that these two populations are of
        different mating types—A1 in Europe and A2 in the US. The latest research from Garbelotto
        and his colleagues, due to be published in 
        Mycological Research (Ivors et al. 2004), supports this
        interpretation, demonstrating that although they belong to the same species, A1 and A2 are
        distinct lineages and have not exchanged genes for a long time. But last year, a batch of
        isolates from infected camellias (
        Camellia spp.) and rhododendrons in a nursery in Washington
        state showed that the A1 and A2 strains were living side by side. ‘That was a big surprise
        for us,’ recalls Garbelotto. ‘We had no knowledge at that point that both the European and
        the North American type could be present in the same nursery.’
      
      
        The Threat of Recombination
        Hybridisation between different species of 
        Phytophthora can produce a new species with different properties
        from those of either parent. One of the best cases comes from Europe, where a new 
        Phytophthora emerged in 1993 that began to attack alder trees (
        Alnus spp.). Research carried out by scientists in the United
        Kingdom demonstrated that the alder 
        Phytophthora was a product of a hybridisation event between 
        P. cambivora and an unknown species similar to 
        P. fragariae , neither of which attacks alder.
        Given this propensity for 
        Phytophthora species to hybridise and new phenotypes to emerge,
        there is legitimate concern that sexual recombination between the A1 and A2 mating types
        could produce something more devastating than either form. Within the controlled confines
        of his laboratory, Garbelotto has been exploring whether the two types of 
        P. ramorum can get it together. Initial findings are that
        oospores—the product of sexual recombination—are being produced, although most of them
        abort before they reach maturity. However, 30% progress further, and (microscopically, at
        least) look like they could be functional. ‘They'll germinate,’ he predicts.
        The threat that hybridisation could create a novel strain with a different host range is
        a concern that the UK is also taking seriously. In December 2003, an entirely new 
        Phytophthora was isolated from two sites in England. Although
        the new species—currently referred to as 
        Phytophthora taxon C sp. nov. (
        P . taxon C)— appears to cause relatively mild damage to its
        beech and rhododendron hosts, the UK's Department for Environment, Food and Rural Affairs
        (DEFRA) acknowledges that hybridisation of 
        P . taxon C with 
        P. ramorum could have serious consequences. ‘The potential for
        the pathogen to adapt further to its putative new environment intrinsically or via
        hybridisation is not known,’ note the authors of a DEFRA report on the mystery species
        (www.defra.gov.uk/planth/pra/forest.pdf). ‘Long-distance spread could easily occur through
        the movement of infected stock of rhododendron or beech and possibly other (as yet unknown)
        hosts,’ they warn.
        DEFRA is monitoring the situation closely (Figure 4). However, on the basis of a
        preliminary DNA analysis, 
        P . taxon C and 
        P. ramorum are only distantly related, making hybridisation
        unlikely, says Joan Webber, Head of Pathology at the UK government's Forestry Commission.
        The closest known relative of 
        P . taxon C is 
        P. boehmeriae , a pathogen that has been recorded on several
        species of tree in China and Australia, and on cotton in China and Greece, suggesting
        possible origins for the newly described species. But, says Webber, the sequence match
        between 
        P . taxon C and 
        P. boehmeriae is only 92%—not especially close. Much more
        evidence is needed to build a strong case for the origin of this new 
        Phytophthora , she says.
      
      
        Origins
        Indeed, it has taken more than 150 years to track down the geographical origin of the 
        P. infestans strain that caused the Irish potato famine. Jean
        Beagle Ristaino of North Carolina State University (Raleigh, North Carolina, United States)
        is due to publish in 
        Mycological Research an analysis of DNA extracted from diseased
        potato plants preserved from the nineteenth-century Irish epidemic (May and Ristaino 2004).
        It had long been suspected that the famine was caused by the Ib strain of 
        P. infestans , which is presumed to have originated in Mexico.
        However, Ristaino's molecular evidence spotlights the Ia strain and traces its probable
        roots to the Andes. The infection could have found its way from South America to Europe and
        the US via exports of potato seed on steamships, she speculates.
        This kind of forensic treatment is more than just interesting. Tracing a 
        Phytophthora species to its site of origin could reveal what
        keeps them at bay in the areas where they are native, and might suggest new ways to manage
        them when they are introduced to a different ecosystem, says Garbelotto: ‘There's a huge
        amount of information that can be learned from understanding where they're coming from.’ So
        where do pathogens like 
        P. ramorum originate? The best lead, Garbelotto says, is the
        ease with which it infects rhododendron. These ornamentals are natives of Asia, but there
        are only a few places where the climate would suit 
        P. ramorum . The most promising, he suggests, are the Southern
        Himalayas, the Tibetan plateau, or Yunnan province in China. But these are big places, and
        Garbelotto has plenty on his plate in his battle against the Californian SOD. ‘I am not
        very hopeful that we'll ever be able to find out where it comes from,’ he says.
      
    
  

  
    
      
        
        A scientist living in Russia is often asked two questions: “Why haven't you left?” and
        “Is it still possible to work there?” The best response to the first question is, “Why
        should I?”—which either terminates the conversation or leads to a stimulating discussion
        about the fate of the world. The second question, however, deserves a serious answer. In
        fact, this is the question that every one of us keeps asking ourselves.
        There is no simple answer. The biggest problems we face are brain drain, inadequate
        infrastructure, and lack of money (or perhaps, lack of money, lack of money, and lack of
        money). In the Soviet Union, fundamental science was supported to a great extent by
        military expenditure. Thus, it is not surprising that Soviet physics and mathematics were
        more successful than other fields, such as biology. In the 1990s, military spending on
        science declined sharply, although the exact numbers are hard to estimate. This year, the
        direct funding of science constitutes only 1.78% of Russia's national budget (an additional
        0.46% is allocated to the space program), although the law stipulates that this figure
        should be at least 4%. Still, this funding amounts to 46.2 billion rubles (approximately
        US$1.6 billion), more than twice the amount spent in 2000. Although this figure looks
        negligible compared with spending on science in the United States and many European
        countries, it could still be sufficient to support existing actively working groups at a
        reasonable level.
      
      
        Funding
        There are several mechanisms for distributing funds for research. The major share comes
        via Russia's Department of Science and the Russian Academy of Sciences. The Academy, unlike
        its Western analogs, not only acts as a consulting body of experts, but also has the
        authority to distribute money (Figure 1). The funds come both as long-term support for
        scientific institutes and as National or Academy research programs. The former covers base
        salaries, which are small even by local standards (about US$200 per month for a laboratory
        chief), and basic infrastructure (water, electricity, etc.). This system of long-term
        support inherited all the old Soviet ills, such as the lack of correlation between
        scientific output and the level of funding. As a result, the available resources are spread
        thinly over hundreds of labs, most of which are just barely alive. The National or Academy
        research programs can provide funding at a higher level, sometimes even enough to do
        experimental research. However, the procedure of establishing such programs, though
        formally competitive, is often not transparent, and a major role is played by the so-called
        “administrative resource” (Allakhverdov and Pokrovsky 2003).
        Money is also distributed through the Russian Foundation for Basic Research (RFBR). The
        decisionmaking mechanism used by RFBR is closer to Western standards, and involves
        anonymous refereeing followed by board discussions. Although its grants are rather small
        (at most, several thousand dollars per year for a maximum of three years), they provide
        important additional support for many small and mediumsized groups that may receive several
        such grants for different projects. In addition, RFBR supports the publication and
        translation of books, travel to international conferences, the organization of conferences
        in Russia, and similar activities. Unfortunately, several programs (in particular, support
        for young scientists) have recently been transferred from RFBR to a newly established
        government office, and have thus become less independent.
      
      
        Collaboration
        International collaboration and research grants are a major source of support for many
        active research groups (Table 1). Several agencies and foundations have programs for
        Eastern Europe, Russia, and/or former Soviet republics. Some of them, such as the Howard
        Hughes Medical Institute (Chevy Chase, Maryland, United States), fund individual groups;
        others—for example the International Science and Technology Center (Moscow,
        Russia)—stipulate that projects should be submitted jointly by academic and military
        researchers; and some agencies—in particular, European INTAS (Brussels, Belgium) and the
        American John E. Fogarty International Center (Bethesda, Maryland, United States)—support
        collaboration between Russian and Western laboratories.
        Another source of financial support is direct collaboration between Western and Russian
        laboratories. Even after a relatively short visit, the salary of a visiting researcher
        abroad can be stretched for several months back home in Russia; even more importantly,
        experimental biologists visiting foreign labs have access to modern instruments and
        chemicals, which allows them to do modern research. The hosts of such visits are often
        (although definitely not always) recent immigrants from Russia, and in such cases the
        collaborations may have roots in older days (Box 1).
        As well as supporting Russian science directly, international collaboration plays an
        important indirect role because it is less influenced by local politics. In fact, one of
        the main positive impacts of the New York–based International Science Foundation set up by
        George Soros in early 1990s was that it demonstrated the possibility of open competition
        with clearly defined rules—something unheard of in Soviet times—and thus served as a model
        for the RFBR, which was organized at approximately the same time.
        Unfortunately, international ties, especially with the United States, have been
        adversely affected by recent changes in visa procedures, which have become lengthy (leading
        to many missed conferences) and, even worse, completely unpredictable (e.g., Brumfiel
        2004). The grapevine distributes stories of “bad words” that should be avoided when
        describing one's research area during an interview at the consulate. Examples of such words
        include the adjective “nuclear” (even within a innocuous terms like “nuclear magnetic
        resonance”) or, more recently, anything that involves “bacteria.”
        The demand for fundamental and even for applied biological research from Russian
        industry is almost nonexistent. The pharmaceutical industry is content to produce generics,
        while Russian biotech companies are still exploiting old strains developed in the Soviet
        Union. However, some laboratories are conducting outsourced research, and there are now
        research outposts of Western and Japanese companies in Russia organized as standard
        industrial labs. On one hand, this work is a dead end for Russian scientists, because the
        results of such research normally cannot be published. This is a serious problem,
        especially for young scientists who want to establish themselves. On the other hand,
        royalties from patents or commercialization of the products can be used to support further
        research. One group that has followed this path successfully is Sergey Lukyanov's lab at
        the Shemyakin Institute of Bioorganic Chemistry in Moscow, Russia. They have developed the
        subtractive hybridization technique for enrichment of clone libraries by rare transcripts
        or specific genomic fragments (Rebrikov et al. 2004), and are distributing it via a company
        called Evrogen (http://www.evrogen.com/about.shtml).
      
      
        Infrastructure and Bureaucracy
        Another major problem is the degradation of infrastructure. Only a few labs can afford
        modern equipment and instruments, and for many others, even standard chemicals are too
        expensive. This leads to a vicious circle: without equipment, a lab cannot conduct
        experiments at the level demanded by highimpact journals—and without such publications, it
        cannot compete for large grants. Smaller RFBR grants, while simpler to obtain, are
        insufficient to purchase large pieces of equipment, and funds from several grants or
        several years cannot be combined due to bureaucratic restrictions. Thus, the only hope for
        these labs, apart from international collaboration, is a personal connection with senior
        bureaucrats that might result in an (un)expected windfall.
        Having the funds to purchase modern equipment abroad is only the first hurdle; the many
        conflicting rules and restrictions, inefficiency, and corruption within the system can
        subsequently hold up the process. Some items, such as tissue samples or animals, are
        virtually impossible to import legally. The process of clearing the shipments through
        customs is a difficult, timeconsuming job. Grigory Kopelevich, the Howard Hughes Medical
        Institute's Russian representative, recalls a story of a grantee whose microscope was
        stopped at customs because the box contained two screwdrivers not specified in the order.
        Fortunately, to resolve the issue, it was sufficient to present one of the screwdrivers to
        a customs officer as a gift.
        Even basic access to journals is a problem, especially outside the main research
        centers. Indeed, out of a random sample of ten major universities where electronic library
        catalogs were available via the Internet, only six had subscribed to 
        Nature , and only two to 
        Science . More specialized journals are available only in Moscow and
        perhaps St. Petersburg. This is partially offset by the proliferating open-access journals
        from the Public Library of Science and BioMed Central, free electronic versions of older
        issues provided by some journals, free subscriptions for Russian academic institutes
        granted by some publishers or purchased by international foundations (e.g., the
        e-library.ru project organized by the RFBR and supported by the Open Society Institute [the
        Soros Foundation, based in New York] and the Department of Education) (Table 2), reprints
        at authors' Web pages, and last but not least, colleagues abroad who break copyright laws
        by e-mailing PDF files; there is even a popular bulletin board coordinating this activity.
        However, these are only partial solutions. Russia is not considered a developing country,
        and thus is excluded from many international efforts that provide free access to journals
        (such as HINARI). Moreover, many journals have page charges, but no Russian grants cover
        these, and the cost of publication may be prohibitively high for many groups.
      
      
        Brain Drain
        These problems, along with low salaries, have naturally led to a huge brain drain.
        Entire generations have been decimated (Box 1); the dearth of researchers at the postdoc
        level, has caused a gap in the teaching and maintenance of scientific traditions. Many labs
        now consist of older chiefs and senior researchers, and graduate students who plan to leave
        immediately after getting the candidate degree (the equivalent of a Western doctorate).
        “Leaving” does not necessarily mean leaving the country; many capable young people go into
        business. While that might be good for the country in general, it is bad for science, at
        least in the short term. However, even emigration is not a completely negative thing; it
        creates a network of collaborators, and in many cases enhances ties with the international
        community.
        Despite all this, science in Russia is very much alive. Not-yet-Nobel-prizewinner Alexei
        Abrikosov's repeated exhorations to the scientific community “to help all the talented
        scientists leave Russia and to ignore the rest” were met by universal disgust (Hoffman
        1993; Leskov 1993; Migdal 1993). There are several competitive Russian labs doing
        first-rate research and publishing in the top-tier journals. Old habits die hard; even in
        these days, very decent results are often published in Russianlanguage journals, the best
        of which have impact factors that are around 1. Each year, many intelligent and capable
        students enroll in universities, and competition for admission is steadily increasing from
        the lows of the mid-1990s. There are also well-attended international conferences in
        several Russian cities.
      
      
        Prospects
        What can be done by the international community to support what is left of Russian
        science? Of course, direct support in the form of competitive grants is important,
        especially if there are few restrictions on spending; even the most carefully considered
        procedure cannot foresee all possible situations. But even more useful is the creation of
        joint research centers, such as the one opened by the international Ludwig Institute for
        Cancer Research (LICR) based jointly in Zurich, New York, and London, and the Belozersky
        Institute of Physico-Chemical Biology of Moscow State University in Moscow, Russia. This
        research center began with limited support for several stronger groups, and is gradually
        moving toward integration of a research program in Moscow with other LICR projects, and
        real collaboration between Moscow groups and LICR labs elsewhere.
        One of the most essential elements of successful research is access to up-to-date
        information. Consequently, any initiative that provides open access to scientific
        literature and databases is extremely useful. Seminars, lecture courses (such as the Moscow
        University [Moscow, Russia] cycle on oncology and immunology sponsored by LICR;
        www.oncoimmunology.ru/index_e.htm), and the participation of Western scientists in
        scientific conferences in Russia are important not only because they provide a fresh
        understanding of emerging trends, but also because they create personal contacts between
        Russian and Western scientists that often lead to fruitful collaboration.
        By contrast, some other types of joint project may be less successful. Artificial
        programs aimed at creating various participant “networks” usually do not work as expected,
        and training programs in Western universities often attract potential emigrants rather that
        those willing to continue active research inside Russia.
        The contribution of the international community cannot be the sole decisive factor in
        the future growth of Russian science. Important as it is in this transition period, it is
        no substitute for a systemic change. The ills of Russian science are not unique; the same
        issues have been raised by scientists from other Eastern European countries (e.g., Wojcik
        2004). Even the existing funds could go much further if scientific policies were more open,
        better structured, and more competitive. Large grants should be provided, on the basis of
        well-defined criteria, to only the strongest labs doing the best research. An often-heard
        opinion that no independent review is possible in a small, well-entrenched community is
        irrelevant, since international boards of experts can be formed—the example of the Soros
        foundation clearly demonstrates that this is feasible. However, smaller pilot grants are
        also needed to support young scientists and labs contemplating new projects. This would
        create competition at all levels and provide doctoral students and postdocs with an
        incentive to stay in Russia and enroll in a strong lab. But again, the procedure for
        awarding such grants should be well defined, transparent, and independent from
        administrative influences.
        Thus, the traditional model of top-down distribution of funds must be changed, and this
        may be difficult. The current system of decision making by Russian funding agencies is
        clearly inadequate. Moreover, the problems of Russian science mirror the problems of
        Russian society in general, and it would be naive to expect that they will be solved
        overnight, even given the political will. Still, if successful, this combination should
        provide both high-level research in established fields and sufficient flexibility to find
        new directions.
      
    
  

  
    
      
        
        During development, cells need to communicate with each other to establish properly
        organised and functional tissues. Cells communicate with each other in various ways, such
        as by secreting and receiving diffusible molecules (morphogens, hormones, and
        neurotransmitters) or by establishing intercellular connections (gap junctions and cell
        protrusions) to allow a direct exchange of instructive factors. A recent paper has shown
        that communication via tiny cell protrusions might be a more common mechanism than
        previously expected (Rustom et al. 2004).
        Many different types of cell extensions have been described in a variety of
        developmental processes and organisms (Miller et al. 1995; Bryant 1999; Chou and Chien
        2002; Rorth 2003), and for most of them a role in cell-to-cell communication has been
        hypothesized. For example, in the mouse, Salas-Vidal and Lomeli (2004) have described long
        processes (filopodia) that connect tissues in early embryos. Because these protrusions
        contain receptors for some well-known signalling molecules, it is thought that they might
        be responsible for receiving signals from neighbouring cells. Similarly, it has been
        proposed in the development of the 
        Drosophila wing and eye imaginal discs (precursors of adult
        structures) that signals modulating the growth and patterning of one epithelial layer of
        cells are received through microtubule-based cell extensions arising from the apposing
        epithelium (Cho et al. 2000; Gibson and Schubiger 2000). Furthermore, in the wing imaginal
        disc, planar extensions called cytonemes arise from the periphery of the epithelium and
        grow towards a central area in the wing disc that produces the signalling molecule
        Decapentaplegic. This directionality of growth, and the observation of vesicles inside
        cytonemes, led Ramirez-Weber and Kornberg (1999) to propose that cells meant to receive a
        signal were searching actively for it, extending long cell protrusions towards the region
        from which signals were emanating.
        Although cell protrusions have been described in different developmental processes,
        tissues, and organisms, their potential role in cell signalling has been difficult to pin
        down. Most cell processes are very fragile, and their study is mainly limited to live
        tissues; in these conditions it is technically challenging to define how the signalling is
        mediated via protrusions. Possibly, it could occur through the release of free molecules,
        in a similar manner to synaptic neurotransmission, or shedding of vesicles as exosomes
        followed by endocytosis by the recipient cell. Alternatively, membranetethered ligands
        (such as Delta) on the protrusion could bind and activate receptors displayed on the
        surface of the receiving cell (De Joussineau et al. 2003).
        The paper by Rustom and colleagues has provided a new outlook on the role of cell
        protrusions, by reporting a novel mechanism employed to transmit signals between cells
        connected by a protrusion. Surprisingly, they did not observe any of the mechanisms
        described above. Rather, transfer of molecules and organelles occurred directly from the
        cytoplasm of one cell to the other, passing through a protrusion that established membrane
        continuity between the connected cells.
        Using rat PC12 cells, Rustom and colleagues observed ultrafine protrusions (with a
        diameter of only 50–200 nm and a length spanning several cell diameters) connecting sparse
        cells in culture (Figure 1). Similar to other cell protrusions, these structures, termed
        tunnelling nanotubes (TNTs), displayed a pronounced sensitivity to both mechanical stress
        and chemical fixation and even to prolonged light excitation, resulting in the rupture of
        many of them. TNTs are actin-based and devoid of microtubules: interestingly most other
        types of cell protrusions also contain actin (Condeelis 1993; Rorth 2003). The researchers
        also confirmed the existence of TNTs in a human cell line (human embryonic kidney cells)
        and rat primary cells (normal rat kidney cells), suggesting that TNTs are not a peculiarity
        of PC12 cells.
        Aiming to investigate how TNTs were linking cells, the authors performed scanning and
        transmission electron microscopy of TNTs. They observed a seamless transition between TNTs
        and the cells they were connected to, suggesting that indeed there was continuity between
        the membranes of the two connected cells. Rustom and colleagues then went on to test
        whether TNTs could be used to transmit signals between cells. The experimental approach
        used was to mark two populations of cells in a distinct way, either by introducing genes
        that encoded proteins tagged with green fluorescent proteins or by using dyes. The two
        different cell populations were then mixed, cocultured, and analysed for transfer of marked
        proteins or dye-stained organelles from one cell to another, between cells that were
        differently marked and connected by a TNT. Strikingly, soluble cytoplasmic molecules could
        not pass freely along the TNTs (with actin tagged with green fluorescent protein being the
        only exception), whereas membrane-bound proteins were transferred along TNTs and detected
        in the receiving cells, further supporting the likelihood of membrane continuity between
        connected cells. Rustom and colleagues also observed transport of vesicles, which seemed to
        be unidirectional. Finally, in transfer experiments performed at close to 0 °C, where
        endo-, exo-, and phagocytosis would be blocked, vesicle exchange still occurred, suggesting
        that these events are not required for vesicle transfer and further supporting the idea
        that membrane continuity exists between connected cells. By contrast, interfering with
        actin polymerization, using the drug latrunculin-B, led to protrusion removal and arrest in
        organelle transfer, indicating that actin is required both for protrusion biogenesis and
        organelle transport.
        Taken together, the experiments performed by Rustom and colleagues strongly suggest a
        role for cell protrusions in cell-to-cell communication. They also provide evidence, in
        culture, for a novel mechanism used by cell protrusions to transport molecules and
        organelles. It will be interesting to test whether TNTs also exist in living tissues and,
        if so, what molecules they transport. TNTs could be distinct from the protrusions known so
        far and could be responsible for establishing another type of connection between cells.
        They could connect all cells in a tissue, directly or indirectly, establishing a global
        interaction network potentially important in exchanging basic survival information as well
        as positional cues (Milan et al. 2001).
        Another interesting question is how connections such as TNTs are established. Rustom et
        al. have shown that, initially, many filopodial extensions arise from one cell and are
        directed toward a neighbour. As soon as one of them reaches the target, it is stabilised,
        while the others degenerate. It is possible that membrane fusion occurs between the tip of
        the protrusion and the planar plasma membrane of the target cell. However, membrane fusion
        can be more easily achieved if the tips of two cell protrusions fuse with each other, thus
        suggesting the participation, in the process of membrane fusion, of microvilli or other
        tiny protrusions belonging to the target cell. Fusion between two protrusions is reported
        to rely on the cylindrical shape and narrow diameter of cell protrusions and also on the
        localised concentration of adhesion/fusion molecules at the tips of the cell protrusions,
        such as microvilli, that display particular tip-specific membrane microdomains (Monck and
        Fernandez 1996; Wilson and Snell 1998; Roper et al. 2000). The work performed by Rustom and
        colleagues suggests that cell protrusions are a general mechanism for cell-to-cell
        communication and that information exchange is occurring through the direct membrane
        continuity of connected cells, independently of exo- and endocytosis. It is important to
        determine whether events similar to these seen in cell culture are occurring in tissues and
        what functions cell protrusions perform during tissue morphogenesis.
        In my work as a graduate student, I am trying to address this question. We need to
        identify the types of cell protrusions that are present in tissues and the molecular
        complexes localizing on them as well as their functions. To then prove that cell
        protrusions are important in cell-to-cell communication in tissues, we would need to remove
        the protrusions and see how this affects tissue architecture and function. However, the
        necessary tools are still missing, given the lack of knowledge of the specific molecules
        important for the biogenesis of these protrusions. Thus far, the function of cell
        protrusions has been hypothesized mainly on the basis of their location in tissues and on
        crude attempts to remove them, for example by altering the actin cytoskeleton or even by
        removing the entire epithelium they belong to.
        The paper by Rustom et al. has shed some new light on these still mysterious cellular
        arms and has further boosted my interest in this emerging field of cell and developmental
        biology.
      
    
  

  
    
      
        
        Experimental psychologists working with humans have a fundamental advantage over
        scientists studying the behaviour of other animals. This is because human subjects can give
        a verbal account of their experience. For example, they can report: ‘These two lights of
        different colour look equally bright’ or ‘This object looks further away than that one’.
        Such direct reports facilitate studying how information from the sensory periphery, that
        is, the sense organs that actually interface with the environment, is processed in the
        brain.
        The perceptual world of animals is often very different from that of humans. Many
        animals have sensory facilities that we humans lack; for example, insects can see
        ultraviolet and polarised light. But how they actually perceive the world, based on
        information from their sensory periphery, is often beyond our grasp. Because animals cannot
        describe their sensations, our access to them is often based on indirect psychophysical
        tests, where animal performance depends fundamentally on motivation and training method
        (Chittka et al. 2003). However, some animals do in fact describe the world around them, but
        not necessarily in ways that we might intuitively understand. Perhaps the best example of
        this are the honeybees (genus 
        Apis ), which have a symbolic ‘language’ that nestmates use to
        communicate with each other about profitable food sources. By eavesdropping on this
        communication, scientists have recently obtained a unique perspective into the perceptual
        world of insects.
        How does the dance language work? A triumphant scout bee returns from the field, and
        advertises the location of a newly discovered food source to nestmates. To do this, the
        forager performs a repetitive sequence of movements, the so-called waggle dance, which is
        one of the most intriguing examples of complex animal behaviour. The successful forager
        wiggles her abdomen provocatively from side to side, moving forward in a straight line.
        Then she runs in a half circle to the left, back to her starting point, performs another
        straight wiggle run along the path of her first, and then circles to the right (Figure 1).
        This pattern is repeated multiple times, and is eagerly attended by unemployed bees in the
        hive. Shortly after such dances commence, dozens of newly recruited foragers arrive at the
        food source being advertised.
        In the 1940s, Nobel laureate Karl von Frisch deciphered the code hidden in this
        seemingly senseless choreography performed on vertical honeycombs in the darkness of the
        hive (reviewed in von Frisch 1967). He found that the angle of the waggle run from the
        vertical is equal to the angle between the sun's azimuth and the indicated food source
        outside the hive. For example, if a food source is found in the direction of the sun, the
        dancer will waggle ‘straight up’ the vertical comb. If food is found 45° to the right of
        the sun's direction, the waggle run will be oriented 45° to the right of vertical on the
        comb (Figure 1). The distance to the target, a flower patch with abundant nectar or pollen,
        is encoded in the duration of the waggle run: the longer the bee waggles, the larger the
        distance of the food from the hive. No other species (besides humans) uses a similarly
        symbolic representation to communicate information from the real world.
        But how do bees measure the flight distance that they communicate so precisely? It was
        previously thought they do this by measuring the energy used as they fly (Heran 1956).
        However, doubts emerged when it was found that distance estimation by bees could be
        manipulated by altering the number of landmarks between the hive and a food source,
        suggesting bees were counting landmarks encountered en route (Chittka and Geiger 1995). In
        an elegant experiment, Esch and Burns (1995) tapped into the bees' dance language to access
        their subjective assessment of flight distance. They let bees forage from a food source 70
        m from the hive and recorded the dance distance code of the returning foragers.
        Subsequently, the feeder was attached to a weather balloon, and slowly lifted to an
        altitude of 90 m—so that the distance between the hive and the food now increased from 70 m
        to 114 m. Correspondingly, foragers should have indicated a longer distance, by stretching
        their waggle run duration. But, in fact, the perceived distance (as indicated in the dance)
        
        decreased by more than 50%! This clearly shows that bee perception of
        distance cannot solely be based on energy expenditure, since a longer flight that cost more
        energy was danced as a shorter ‘distance’ in the waggle run.
        So what actually drives the bee odometer? Because the landscape bees pass in flight
        moves more slowly when bees fly at higher altitudes, Esch and Burns (1995) conjectured that
        foragers process the speed with which visual contours move across the eye (optic flow), and
        integrate this with travel time. To confirm this hypothesis, Srinivasan et al. (2000)
        further exaggerated the experienced image flow, by training bees to fly through narrow
        chequered tunnels. These bees grossly overestimated actual travel distance, bragging to
        their nestmates that they had flown 195 m when in fact they had flown 6 m. Attendees of
        these dances promptly believed the high-class swindle, and searched for food at remote
        locations that the dancers had never even visited (Esch et al. 2001).
        The quality of information available about the velocity of the passing landscape will
        depend, of course, on the sensitivity of the eyes. The eyes of bees contain three types of
        colour receptors, with maximum sensitivity in the ultraviolet, blue, and green domains of
        the spectrum (Autrum and von Zwehl 1964). Their excellent colour vision is optimal for
        flower identification (Chittka 1996), but do they also use it to measure the image velocity
        of the passing landscape? Surprisingly, the answer is no—bee odometry is in fact totally
        colour blind. Chittka and Tautz (2003) found that bees use exclusively the signal from
        their green receptors for measuring image velocity (Figure 2), confirming earlier reports
        that motion vision in bees is mediated only by this receptor type (Giurfa and Lehrer 2001;
        Spaethe et al. 2001). Thus, the level of intensity contrast present in the scene strongly
        influences the bees' subjective experience of flight distance (Chittka and Tautz 2003; Si
        et al. 2003).
        With so many external variables influencing distance estimation, it seems unlikely that
        the honeybee odometer would be very robust in natural conditions. Now, as reported in this
        issue of 
        PLoS Biology , Tautz et al. (2004) have quantified the bees' subjective
        experience of distance travelled when they fly over natural terrain with varying levels of
        contrast. Specifically, they compared the dances of bees flying over water (scenery with
        low visual contrast) with those of bees flying over land (scenery with relatively high
        contrast). They trained bees to forage at a feeder on a boat, which was paddled increasing
        distances from the hive, until it reached an island. All the while, observers at the hive
        deciphered the dances of the bees returning from the feeder. Interestingly, bees flying 200
        m over water hardly appeared to register an increase in travel distance, whereas the same
        increase in distance flown over land resulted in a substantial increase in perceived flight
        distance. This is consistent with the hypothesis that the bees' odometer is largely based
        on visual, external cues and demonstrates that this system is sensitive to visual
        contrast.
        But there must be something else beside visual cues. Navigation over water, in the near
        absence of visible ground features, is extremely difficult without a reliable internal
        instrument measuring travel speed. This is the case even for us humans with sophisticated
        measuring devices: malfunctioning air speed indicators have been responsible for several
        airplane crashes into water, for example Birgenair Flight 301 and AeroPeru Flight 603 in
        1996. Heran and Lindauer (1963) likewise observed that honeybees flying over lakes
        sometimes lost altitude and plunged into the water. However, the new study by Tautz et al.
        (2004) also shows that most bees will reliably fly over prolonged stretches of water
        without accident. Furthermore, even though bees experience only a small increase in
        subjective travel distance when flying over water, it is not zero. This indicates that bees
        do perhaps resort to an internal measure of flight distance when other cues fail. For
        example, bumblebees walking to a food source in absolute darkness, that is, in the complete
        absence of visual cues, are able to correctly gauge travel distance (Chittka et al. 1999),
        indicating that an internal odometer, possibly based on energy consumption, also exists. It
        appears that animal navigation, just like aviation, relies on multiple backup systems that
        support each other and can compensate if one system fails in a certain context.
        Spying on honeybee dances can not only tell us about the cues they use for navigation,
        but also allows insights into the cognitive architecture that governs other aspects of bee
        behaviour, such as the assessment of flower quality. We've learned that bees prefer high
        over low nectar concentrations because this is reflected in their dances. When bees find
        better nectar, they dance more enthusiastically, that is, the number of dance circuits per
        minute increases (Seeley et al. 2000; Waddington 2001). However, Waddington (2001) found
        that the relationship between actual and perceived nectar quality is nonlinear. In fact, it
        is a positive but decelerating relationship, so that an increase in sucrose concentration
        from 10% to 20% results in twice the difference in dance rate that an increase from 50% to
        60% does. Interestingly, the perceived change in quality is stronger when there is a 
        decrease than when there is an 
        increase in nectar quality of the same magnitude. Such asymmetric
        perception of gains and losses is well known in humans, where it has been linked to
        risk-aversive behaviour (Tversky and Kahnemann 1981). Unfortunately, animal subjects often
        do not yield this type of information very readily. Only in their own language do they
        reveal many of their perceptual peculiarities. Using the bee language as a window into
        insect visual perception has been a wonderful tool and is a promising avenue for further
        research into the question of how miniature brains encode the world around them.
      
    
  

  
    
      
        
        At some time almost 4 billion years ago, nature likely was faced with a chemical
        dilemma. Nucleic acids had emerged as replicable information carriers and primitive
        catalysts (Joyce 2002), yet their functional potential was constrained by their structural
        homogeneity and lack of reactive groups. These properties rendered nucleic acids well
        suited for storing information, but flawed for mediating the diverse chemistries required
        to sustain and improve increasingly complex biological systems. It is tempting to speculate
        that translation emerged as the solution to this dilemma. Translation, defined here as the
        conversion of an informationcarrying molecule into a corresponding encoded structure,
        enabled the expanded functional potential of proteins to be explored using powerful
        evolutionary methods that depend on the unique ability of nucleic acids to replicate.
        A small but growing number of researchers have begun to tackle a modern version of this
        dilemma. While proteins and nucleic acids can be manipulated using powerful molecular
        biology techniques that enable their directed evolution, the size, fragility, and
        relatively limited functional group diversity of biological macromolecules make them poorly
        suited for solving many problems in the chemical sciences. Ideally, researchers would like
        to apply evolution-based approaches to the discovery of functional synthetic, rather than
        biological, molecules. A solution analogous to nature's translation of mRNA into protein
        could, in principle, address this contemporary problem (Orgel 1995; Gartner and Liu 2001).
        If a laboratory system were developed that could translate amplifiable information carriers
        such as DNA into arbitrary synthetic molecules, the evolution of synthetic molecules using
        iterated cycles of translation, selection, amplification, and diversification would be
        possible.
        The translation of DNA into synthetic molecules is conceptually distinct from the use of
        DNA simply as a tag during the solidphase synthesis of a molecule that is part of a
        combinatorial library (Brenner and Lerner 1992). The latter process uses DNA to record the
        history of a series of chemical reactions by cosynthesizing a portion of a DNA
        oligonucleotide during each step of a molecule's solidphase synthesis. As a result, the
        identity of compounds that pass screening can be inferred by PCR amplification and
        sequencing of the DNA associated with a given bead (Needels et al. 1993). The resulting
        DNA, however, cannot redirect the synthesis of active compounds. In contrast, the
        translation of DNA into synthetic molecules uses the sequence of nucleotides in a strand of
        DNA to direct the synthesis of a nascent molecule. As a result, a complete cycle of
        translation, selection, and amplification can be applied to the discovery of synthetic
        molecules in a manner that is analogous to the processes that take place during biological
        evolution.
        DNA-templated organic synthesis (DTS) has emerged as one way to translate DNA sequences
        into a variety of complex synthetic small molecules (Gartner and Liu 2001; Gartner et al.
        2002; Li and Liu 2004). In this approach, starting materials covalently linked to DNA
        templates approximately 20–50 nucleotides in length are combined in very dilute solutions
        with reagents that are covalently linked to complementary DNA oligonucleotides. Upon
        Watson-Crick base pairing, the proximity of the synthetic reactive groups elevates their
        effective molarity by several orders of magnitude, inducing a chemical reaction. Because
        reactions do not take place between reactants linked to mismatched (noncomplementary) DNA,
        DTS generates synthetic products in a manner that is programmed by the sequence of bases in
        the template strand.
        In a series of three papers in this issue of 
        PLoS Biology , Harbury and co-workers describe an elegant new approach to
        translating DNA into synthetic peptides called “DNA display.” Their approach uses DNA
        hybridization to separate mixtures of DNA sequences into spatially distinct locations. The
        first paper (Halpin and Harbury 2004a) reports the development of resin-linked
        oligonucleotides that efficiently and sequence-specifically capture DNA containing
        complementary subsequences. This immobilization process is efficient enough to be iterated,
        so that DNA sequences specifying multiple amino acids can be routed to the appropriate
        miniature resin-filled columns during each step.
        In the second paper (Halpin and Harbury 2004b), Harbury and coworkers detail solid-phase
        peptide synthesis performed on unprotected DNA 340mers bound to DEAE Sepharose.
        Optimization of amino acid side-chain-protecting groups and peptide coupling conditions
        enabled a variety of amino acids to undergo efficient peptide coupling to bound
        oligonucleotides containing an amine group.
        The third paper (Halpin et al. 2004) integrates the routing and peptide synthesis
        described above into the translation of a library of 10
        6 DNA 340mers into a corresponding library of up to 10
        6 synthetic pentapeptides. To achieve chemical translation, the DNA
        library was subjected to iterated cycles of routing and solidphase peptide synthesis. After
        each routing step, the appropriate amino acid was coupled to each DNA-linked subpopulation.
        DNA routing was therefore used to achieve the splitting step of “split-and-pool”
        combinatorial peptide synthesis. The completed library of peptide–DNA conjugates was then
        subjected to in vitro selection based on the ability to bind an antibody with known
        affinity for the [Leu]enkephalin pentapeptide Tyr-Gly-Gly-Phe-Leu. After two rounds of
        routing, synthesis, and selection, followed by DNA sequencing, the remaining
        oligonucleotides predominantly encoded the Tyr-Gly-Gly-Phe-Leu sequence or close variants
        thereof. This result demonstrates that the DNA display method is capable of facilitating
        the discovery of functional molecules by enabling in vitro selection methods to be applied
        to molecules generated by split-and-pool combinatorial synthesis.
        The fundamental distinctions between DTS and DNA display approaches to chemical
        translation imply that these two strategies will be applicable to different types of
        synthetic structures. Because the DNA display approach separates the DNA hybridization step
        from the chemical synthesis step, it does not require the coupling of synthetic reagents to
        oligonucleotides (beyond the starting material), and can use reaction conditions such as
        high temperatures or high pH that may not be compatible with DNA hybridization. These
        features suggest that DNA display may be able to access structures that cannot be created
        by DTS. Likewise, because DTS approaches use effective molarity rather than intermolecular
        reactivity to direct organic synthesis, they enable modes of controlling reactivity (such
        as using otherwise incompatible reactions in a single solution [Calderone et al. 2002]) and
        classes of chemical reactions (such as heterocoupling of substrates that preferentially
        homocouple) that cannot be accessed using split-and-pool synthesis. In principle, these two
        approaches are complementary, and it is tantalizing to envision the use of both DNA display
        and DTS to direct different steps during a single chemical translation.
        In order for either approach to fully realize its potential of truly 
        evolving libraries of diverse synthetic molecules, rather than simply
        enriching libraries that already contain at the outset the “most fit” molecule, researchers
        must develop sophisticated library syntheses that generate remarkable complexity (vast
        numbers of different compounds) in a relatively modest number of DNA-compatible synthetic
        steps. True evolution takes place when the theoretical complexity of a population exceeds
        the number of different molecules that can be created in a single library translation step,
        and when diversification is required to access compounds in later generations that are more
        fit than any member of the starting pool.
        To my knowledge, no synthetic library to date contains this degree of complexity
        (indeed, the total size of the Chemical Abstracts Service database of known chemical
        substances is presently less than 10
        8 compounds). However, because so few copies of a DNA-linked synthetic
        molecule are required for in vitro selection (Doyon et al. 2003)—compared with the
        relatively large quantities of material that are required for conventional screening
        approaches—these chemical translation methods offer the first hope of achieving such
        synthetic complexity without requiring an impractical amount of material or storage space.
        For comparison, a conventional-format synthetic library containing 100 µg of each of 10
        8 different structures would represent 10 kg of material, not including
        the mass of beads or plates associated with the library, while a chemically translated
        library containing 10,000 copies of 10
        8 different species represents less than 1 µg of total material.
        While significant remaining challenges face efforts to develop and apply chemical
        translation, the promise of marrying evolution and organic synthesis is an irresistible
        combination for some researchers. The work of Harbury and co-workers described in this
        issue represents the latest approach to the very ancient problem of translating replicable
        information into functional structures.
      
    
  

  
    
      
        
        Grafting is an ancient technique used by farmers and gardeners to combine desired
        attributes of the rootstock with those of the donor plant shoot, or scion. Grafting
        essentially saved European wine making: when the insect 
        Dactylosphera vitifoliae devastated European grapewine varieties
        over the course of the late 1800s and early 1900s, the varieties were saved by grafting
        them onto resistant rootstocks from the New World. Since then, these rootstocks have been
        used to maintain the susceptible Old World cultivars. But grafting is also an excellent
        tool for scientists studying systemic signals traveling between the rootstock and distal
        parts of the plants, and vice versa. For example, two important studies (Palauqui et al.
        1997; Voinnet et al. 1998) used grafting to demonstrate the spreading of RNA silencing in
        plants. However, it was a subsequent paper (Crete et al. 2001) that followed up on certain
        inconsistencies in the grafting results that pointed to subtleties important for both
        experimental design and understanding systemic signaling in plants.
        RNA silencing (termed posttranscriptional gene silencing in plants, quelling in fungi,
        and RNA interference in animals) refers to the phenomenon whereby specific gene transcript
        levels are reduced in the presence of a related RNA. From studies of RNA silencing in
        several systems, much is now known about the mechanisms involved (Matzke 2002; Mlotshwa et
        al. 2002), but the systemic spreading in plants is still a bit of a mystery.
        Posttranscriptional gene silencing spreads systemically throughout the individual plants in
        a very characteristic manner reminiscent of viral spread. This has led to the hypothesis of
        a systemic silencing signal that is produced in the tissues where silencing is initiated
        and is then transmitted to the distant parts of the plant where it can initiate silencing
        in a sequence-specific manner. The sequence specificity of the silencing strongly implies
        that the signal is a nucleic acid, most likely an RNA, but the identity of the signal
        remains unknown. Silencing spreads mainly in the direction from carbon source to carbon
        sink, that is, from tissues such as leaves that export the sugar products of
        photosynthesis, to tissues such as roots that import these products, and it can take up to
        several weeks until it is established in the whole plant (Palauqui et al. 1997; Palauqui
        and Vaucheret 1998; Voinnet et al. 1998; Sonoda and Nishiguchi 2000).
        As expected, the discovery of this process triggered a quest for the “systemic inducer”
        of the process: a signal that travels through the plant and is able to initiate silencing
        in a remote location within the plant. Grafting was an obvious tool to use in the quest for
        this signal, as it allowed silencing source and sink tissues to be of different origin.
        Palauqui et al. (1997) were the first to unambiguously demonstrate that silencing
        spreads from a silenced rootstock to a nonsilenced scion. They used as a stock a transgenic
        tobacco carrying an additional copy of the endogenous nitrate reductase gene, 
        Nia . Some of the transgenic lines generated always showed higher levels
        of 
        Nia transcripts than the wild type—as expected from the presence of an
        additional gene—and were termed class I lines. However, other transgenic lines had
        undergone silencing for both the endogenous and exogenous 
        Nia genes and those were termed class II lines. Pallaqui et al. (1997)
        found that silenced class II rootstocks were able to silence class I scions. This was true
        even in a “sandwich graft,” where a wild-type (nontransgenic) segment was grafted between
        the silenced stock and the nonsilenced scion. Spreading of silencing was unidirectional
        from stock to scion. Though not explicitly stated, it was implied that it took more than 3
        wk after grafting for systemic silencing to occur in the scion. The reported rate of
        transmission was 100%.
        Related experiments by Voinnet et al. (1998) used scions that transgenically expressed
        green fluorescent protein grafted onto plants with established silencing of the same
        transgene. There, too, silencing spread through the graft to the nonsilenced scion, even
        when a wild-type section was grafted between transgenic rootstock and scion. Unlike the 
        Nia transgene, which has an endogenous counterpart in the wildtype plant,
        green fluorescent protein has no homolog in nontransgenic lines. Therefore, silencing
        spreading in the wild-type “spacer” in the sandwich grafts could not be assisted by an
        endogenous sequence. Rather, the systemic signal must have traveled all the way to the
        scion and induced gene silencing there. The establishment of systemic silencing took 4 wk
        in the “direct” grafts and 6 wk in the sandwich grafts. However, silencing spread to the
        scion only in some of the grafts: in ten out of 16 direct grafts and five out of 11
        sandwich grafts.
        I found these papers were very important not only for what they proved—the existence of
        a systemic signal of silencing—but also because they gave an unequivocal answer to the
        scientific questions they posed, using relatively simple methodology. Although excited by
        these successful examples of the transmission of silencing, I kept coming back to two
        questions: (1) what prevents transmission in some of grafts, and (2) why does it take
        longer to transmit silencing to the scion than it takes systemic silencing to reach the
        most remote parts of an intact plant?
        When we started working on the silencing signal ourselves, we repeated some of the above
        experiments but found somehow lower efficiencies in the initiation of silencing in the
        scions. We soon realized that our results were influenced by the developmental stage of our
        scions. A paper from Jeff Meins's laboratory in Switzerland shed light on some aspects of
        the grafting puzzle. Researchers there introduced additional chitinase genes using
        bolistics, in sense or antisense orientation under the control of a strong promoter (35S)
        into chitinase transformant lines of tobacco that never exhibited spontaneous gene
        silencing (Crete et al. 2001). In lines bombarded late in plant development, triggering of
        silencing was rarely observed. However, when the transgene was introduced earlier in
        development, a large portion of the lines showed a substantial decrease and eventually full
        suppression of the chitinase mRNA levels. Lines that showed silencing were used as
        rootstocks and nonsilencing lines were used as scions in three types of grafting
        experiments. In the first type of grafting experiment, called top grafting, a 5-cm scion
        cut into a wedge at the bottom was inserted into the vascular ring at the cut surface of a
        50-cm-high rootstock (Figure 1A). In the second type of experiment, reciprocal transverse
        grafts of 50-cm-tall plants were exchanged between class I and class II plants (Figure 1B).
        Finally, the third type of experiment involved plug grafts, which were made by exchanging
        transverse plugs of stems cut with a 5-mm-diameter cork borer from an internode
        approximately in the middle of a 50-cm plant (Figure 1C).
        Surprisingly, only top grafting resulted in scions that were systemically silenced by a
        rootstock signal. Furthermore, even transmission after top grafting was less effective than
        expected; in one stock/scion combination only 27 out of 71 grafts exhibited transmission of
        the silencing signal. The authors also found that antisense-induced silencing was never
        transmitted to the scion. These findings do not answer the many questions about the
        mechanisms underlying systemic silencing, but they point us in certain directions.
        The individual parts of a whole plant are, in terms of import and export, in an
        equilibrium that changes with development. When grafting takes place, how this equilibrium
        is altered depends on the individual “parts” that contribute to the “new” whole plant. In
        addition, there is now indirect evidence (Fagard and Vaucheret 2000) that what is source
        tissue and what is sink tissue in terms of sugar transport affects what is source and what
        is sink in terms of the systemic silencing signal. Taking into account the above findings
        and choosing the right combination of stock/scion, we have managed to significantly
        increase the efficiency of graft-transmitted silencing, a prerequisite for continuing the
        search for the systemic signal.
        From the grafting experiments to date, it is now evident that the transporting capacity
        of the vascular tissue bypass that is formed at the graft junction does not fully reach the
        level of the original vascular tissue. The basis for these restrictions is not known. In a
        way, the graft interface functions as an unintentional filter. If the specificities of this
        “filter” were known, it would help us comprehend some transmission inconsistencies. Keeping
        in mind these limitations, grafting remains an invaluable tool in the search for the
        systemic silencing signal.
      
    
  

  
    
      
        
        
          
            
              This is the third installment in a series of editorials on the
              implications of open-access publishing for established publishing practices and
              stakeholders in scientific and medical research.
            
          
        
        The questions, tensions, and social concerns surrounding copyright and the Internet are
        very different for scientific and medical literature than for other kinds of easily
        reproducible digital works. Peer-reviewed publications are often the sole tangible products
        of the tremendously time-consuming and expensive process of conducting primary research in
        biology and medicine. Who should own the primary research articles that are the culmination
        of years of work by scientists and staggering financial investments by governments,
        universities, and tax-exempt foundations? What uses should the documents' owners
        permit?
        In practice, academic authors typically assign full copyrights to their articles to the
        publishers of the journals in which the works appear. Scientists do not benefit financially
        from the transaction; indeed, they often subsidize the cost of their articles' publication
        in the form of page charges, color charges, and other fees levied by publishers. The
        prestige associated with publishing in a selective journal is sufficiently valuable that
        scientists are generally willing to abdicate the legal rights to their own work without
        remuneration.
        In recent years, however, several technological and legal innovations have led a growing
        number of scientists to begin to question the sagacity of this arrangement. The advent of
        electronic publishing and the Internet itself have made technically possible a slew of
        novel uses of primary research papers. Simultaneously, the traditional “all rights
        reserved” copyright license has been supplemented by a variety of alternative licenses—of
        equal legal validity and available at no charge to anyone who wants them—that allow
        copyright holders to prevent some uses of a work without permission, but to authorize
        others. Different licenses created by the nonprofit organization Creative Commons
        (www.creativecommons.org), for example, allow copyright holders to mark their work with
        freedoms—to permit a work's reproduction for any noncommercial purpose (the Noncommercial
        License) or for any purpose at all provided that the original authorship is properly
        attributed (the Attribution License).
        The upshot of these developments is that copyright holders can now permit a spectrum of
        uses of a paper by prospective researchers, anthologizers, archivists, teachers, patients,
        policy makers, journalists, and other interested parties. Precisely which uses are
        permitted and which are not is far from a trivial matter. The particular copyright license
        under which an article is published largely determines how the document can be stored,
        searched, and built upon by other scientists.
      
      
        Authors' Rights and Users' Rights
        One implication of the variety of copyright licenses now widely available is that the
        right to use an article in one way or another is largely independent of its accessibility
        online. A paper that is touted as “freely available” or “free access” is very different
        from one that is “open access.” (See http://www.plos.org/openaccess for the formal
        definition of an open-access article, drafted at the 11 April 2003 Bethesda Meeting on Open
        Access Publishing.) When a document is “freely available,” someone who comes across it may
        be permitted to do nothing more than read it online on a publisher's Web site; the right to
        use the article in any other way is typically granted only at the publisher's discretion.
        When a document is open access, however, a wide range of additional uses are perpetually
        and irrevocably allowed— from the reproduction and distribution of the paper by a professor
        for his or her students to the archiving of the paper in a searchable online repository
        available to anyone in the world with an Internet connection, and more.
        The Creative Commons Attribution License (CCAL), which governs this editorial and all
        other content in 
        PLoS
        Biology , permits a number of uses of articles that are typically
        restricted and for which there is an immediate demand. All articles published by PLoS can
        be included in coursepacks—a use-right that most authors would want to allow without
        exception, but that most traditional publishers grant only for a substantial fee (which
        they rarely share with authors). The CCAL also ensures that institutions are permitted to
        archive not only articles written by their own faculty, but all other works published under
        the same legal terms as well, thereby facilitating their permanent accessibility and
        preservation. For example, the LOCKSS (“Lots of Copies Keep Stuff Safe”) program
        (http://lockss.stanford.edu/projectdescbrief.htm), an ongoing project to support libraries'
        efforts to “create, preserve, and archive local electronic collections,” is viable only
        insofar as institutions are permitted to store information themselves, rather than access
        it exclusively via publishers' Web sites. Many collaborative projects between libraries and
        publishers have been complicated by legal constraints, including the stipulation that
        archives remain “dark,” or inaccessible to users, until any commercial incentive for
        restricting access to articles has been exhausted—clearly a suboptimal arrangement for
        researchers, and one that is unnecessary for collections of works governed by the CCAL.
        One of the truly revolutionary implications of open-access articles, however, is that we
        simply do not know the full range of their potential applications. They are available for
        any use that any entrepreneur can envision, so long as the authors of the papers are
        properly credited. The only certainty, then, is that the utility of open-access research
        articles will be limited solely by the imagination of those that are inspired by the
        possibilities—rather than by legal constraints.
      
      
        Authors' Protections
        Authors retain the copyright to all articles in 
        PLoS Biology and license their works under nonexclusive terms that
        reserve only some—rather than all—rights. There are several common objections, generally
        leveled by publishers, against this practice. For example, it is sometimes argued that the
        traditional copyright arrangement in scientific publishing protects against uses of
        articles that authors would object to—while the CCAL permits such uses and renders authors
        helpless to prevent them.
        To the extent that the uses in question are for academic or archival purposes, such as
        those discussed above, it is certainly true that the CCAL permits practices that “all
        rights reserved” licenses do not. Indeed, the expanded range of legitimate academic uses of
        articles is among the primary selling points of the CCAL in the context of scientific
        publishing. To the extent that the ostensibly objectionable uses are commercial, the
        problem is easily remedied with the Creative Commons Noncommercial License, which prohibits
        commercial reuse of a work without the copyright holder's consent.
        PLoS has chosen, for reasons both philosophical and pragmatic, to permit the commercial
        use of works we publish. As a matter of principle, all of our policies reflect the view
        that scientific publishers are service providers and should not themselves restrict the
        potential applications of the largely publicly funded work in their journals. More
        concretely, if a commercial enterprise is interested in repackaging the articles that PLoS
        has published, we are loath to prevent an author's work from wider distribution. Any risk
        that a company will use an article for a purpose its author would be uncomfortable with is,
        in our view, substantially outweighed by the benefits of allowing—not on a case-by-case
        basis, but across the board—the reproduction of the article for inclusion in online
        encyclopedias, or for distribution in countries in which Internet access is unreliable, or,
        indeed, for creative uses we hope to inspire by making primary research articles legally
        available to commercial interests.
        Another recurring objection to the copyright arrangement that PLoS employs is that
        authors are inappropriate copyright holders because they are ill-equipped to protect their
        own works against plagiarism, misattribution, and other misuse. Most scientists, however,
        have enough familiarity with cases of plagiarism in their own field to know that their
        strongest protection against mis- or nonattribution is derived not from the threat of
        prosecution for copyright infringement, but from community standards of conduct.
        Furthermore, among the benefits of open-access articles is the fact that their full texts,
        rather than just their abstracts, are searchable—which, as any teacher knows, makes
        plagiarism much easier to detect.
        Beyond plagiarism and misattribution, it is not clear what uses of primary research
        articles authors would actually want to prevent (other than, perhaps, the commercial uses
        that their work is already susceptible to, in many cases, when publishers hold copyrights).
        Scientists do not receive royalties for their published work. The more widely their
        articles are read and cited, the more their professional reputations are bolstered.
        Certainly, research articles have a wide range of uses that publishers typically object to—
        and indeed often file suit over—such as their compilation in coursepacks by copy shops.
        Those applications, however, tend not to constitute “misuse” in many authors' eyes.
      
      
        Authors' Voice
        There is no question that the licensing arrangement PLoS employs is relatively novel—and
        therefore untested over the long term—in biomedical publishing. However, it hardly takes a
        radical understanding of the interests of authors and users of primary research articles to
        conclude that the open-access terms of copyright promise substantial benefits for both
        groups.
        What, then, can scientists do to encourage other publishers to follow suit and strike
        similar legal arrangements with authors as a matter of course? One answer is to “vote with
        your submissions;” that is, authors should submit their work preferentially to journals
        with copyright and licensing practices that genuinely serve their interests. Another
        equally important action for scientists is to raise the issue with their professional
        societies. Scholarly associations exist, among other reasons, to serve the needs of their
        members—and society members should actively urge their society journals to employ the CCAL
        or a similar license for their research articles.
        Scientific and medical literature is different from fiction or movies or music. The
        United States government invests more than $28 billion per year in the National Institutes
        of Health alone to fund research in biology and medicine. The scientists who conduct that
        research and the research paid for by other public-minded institutions in the United States
        and abroad have an affirmative moral obligation to share the knowledge they create—not just
        with students and faculty at elite Western universities, but with everyone who could use it
        and build upon it. When authors publish their work in journals with restrictive copyright
        practices, it becomes illegal (often for even the authors themselves) to store primary
        research articles in many archives or include them in coursepacks or use them for other
        responsible purposes. Those obstacles to sharing knowledge can be avoided without
        legislative intervention, however, if scientists and publishers alike embrace a legal
        paradigm for disseminating new discoveries that maximizes their utility.
      
    
  

  
    
      
        
        The regeneration of lost body parts and injured organs has captured the human
        imagination since the time of the ancient Greeks. The deep-seated roots of this early
        fascination can be seen in Greek mythology. The many-headed Hydra nearly defeated the hero
        Heracles by growing two new heads for every one that Heracles cut off, and the liver of
        Prometheus, devoured by a ravenous eagle each night, regenerated every morning. Aristotle,
        who lived from 384–322 BC, noted that the tails of lizards and snakes, as well as the eyes
        of swallow-chicks, could regenerate (Aristotle 1965). This fascination became a legitimate
        area of scientific inquiry in 1712, when the French scientist René-Antoine Ferchault de
        Réaumur published his seminal work on crayfish limb and claw regeneration (Réaumur 1712).
        Soon thereafter, several other prominent scientists of the eighteenth century, including
        Abraham Trembley, Charles Bonnet, Peter Simon Pallas, and Lazzaro Spallanzani, discovered
        remarkable regenerative abilities in a variety of organisms. Hydra, earthworms, and
        planarians could regenerate their heads and tails (Pallas 1766; Lenhoff and Lenhoff 1986);
        salamanders could regenerate their limbs, tails, and jaws; premetamorphic frogs and toads
        could regenerate their tails and legs; slugs could regenerate their horns; and snails could
        regenerate their heads (Spallanzani 1769). This last discovery caused quite a stir in
        eighteenth-century France, leading to an “unprecedented assault” on snails as both
        naturalists and the general public participated in the quest for scientific knowledge by
        reproducing Spallanzani's intriguing results (Newth 1958).
      
      
        Stem Cells Versus Dedifferentiation
        During the nineteenth century and for most of the twentieth century, regeneration
        research primarily focused on the phenomenology of regeneration and its cellular basis.
        Many important discoveries were made during this period, which led in part to the general
        conclusion that progenitor cells are required for most regenerative processes. However, the
        origin of these progenitor cells varies between regenerating systems. In some cases, such
        as the regeneration of skin, blood, muscle, and bone in mammals and the replacement of lost
        tissues in the flatworm planarian, the progenitor cells pre-exist as reserve cells or stem
        cells that only need to be activated in response to injury or tissue depletion. In other
        cases, the progenitor cells can be created de novo through a process in which fully
        differentiated cells reverse their normal developmental processes and revert to
        proliferating progenitor cells. This latter process, known as cellular dedifferentiation,
        is especially prominent in vertebrates with exceptional regenerative abilities, such as
        salamanders. For example, during salamander limb regeneration, cells from muscle, bone,
        cartilage, nerve sheath, and connective tissues participate in the dedifferentiation
        process to form a pool of proliferating progenitor cells known as the regeneration blastema
        (Figure 1) (Chalkley 1954; Bodemer and Everett 1959; Hay and Fischman 1961; Wallace et al.
        1974; Lo et al. 1993; Kumar et al. 2000). It has not yet been determined whether
        pre-existing stem cells or reserve cells also contribute to the pool of progenitor
        cells—nor whether the blastemal cells are multipotent (capable of differentiating into
        multiple cell types), are committed to a particular cell lineage, or are a mix of
        multipotent and committed progenitor cells. Regardless, these blastemal cells will later
        redifferentiate to form all the internal tissues of the regenerated limb other than the
        peripheral nerve axons. This extraordinary degree of cellular plasticity distinguishes
        those vertebrates that can replace entire anatomical structures, such as limbs, from
        vertebrates with more limited regenerative abilities.
        The public has recently exhibited a renewed interest in regeneration research, due in
        large part to stem cell research, which has provided promising avenues for the field of
        regenerative medicine. In addition, celebrities such as Christopher Reeve and Michael J.
        Fox have given a human face to the many people who could benefit from effective
        regenerative therapies. The political, ethical, and religious controversies surrounding the
        use of human embryonic stem cells for therapeutic purposes have only served to increase the
        public's awareness of the promising potential of regenerative medicine. But this interest
        in using scientific knowledge to enhance the regenerative capacity in humans is not new.
        Spallanzani closed his 1768 monograph on regeneration, 
        An Essay on Animal Reproductions , with a series of questions— which,
        except for the antiquated language, could be asked by citizens of the twenty-first century:
        But if the abovementioned animals, either aquatic or amphibious, recover their legs, even
        when kept on dry ground, how comes it to pass, that other land animals, at least such as
        are commonly accounted perfect, and are better known to us, are not endued with the same
        power? Is it to be hoped they may acquire them by some useful dispositions? [A]nd should
        the flattering expectation of obtaining this advantage for ourselves be considered entirely
        as chimerical?
        Although most of the current interest in regenerative medicine focuses on the potential
        benefits of either embryonic or adult stem cells, there are several investigators who are
        now taking an entirely different approach to the problem. These researchers think that
        although stem cells may offer some benefits in the relatively near future, a more
        comprehensive approach will be required to meet all of our regenerative needs. To achieve
        this goal, they must first learn how nature has already solved the problem of regeneration
        and then use this information to enhance the regenerative capacity in mammals. These
        studies seek to understand the biology of regeneration, especially the cellular and
        molecular mechanisms that govern regenerative processes. The experimental systems range
        from the unicellular protozoa to complex vertebrates, such as salamanders and mice.
      
      
        The Molecular Biology of Regeneration
        With the technological advances that followed the advent of molecular biology,
        researchers acquired the basic tools to begin to unravel the molecular basis for cellular
        plasticity and regeneration. However, progress in this arena has been slow, given that most
        organisms with marked regenerative abilities are not yet amenable to routine genetic
        manipulation. Recent advances, such as the application of mutagenic screens to study fin
        regeneration in zebrafish (Johnson and Weston 1995; Poss et al. 2002b) and the application
        of RNAi knockdown technology to study regeneration in planarians (Sanchez Alvarado and
        Newmark 1999; Newmark et al. 2003), are quite promising and could largely ameliorate this
        deficiency. Nevertheless, results from several recent studies have converged on a set of
        genes that appear to play an important role in regeneration, and evidence is accumulating
        that suggests some of these genes may function to control regenerative cellular plasticity.
        Three such genes are 
        Msx1, BMP4 , and 
        Notch1 . These genes encode, respectively, a transcriptional repressor, a
        signaling ligand, and a signaling cell surface receptor.
        Numerous studies over the past three decades have shown that mammals, including humans,
        can regenerate their digit tips provided the amputation plane is distal to the terminal
        phalangeal joint (Douglas 1972; Illingworth 1974; Borgens 1982; Singer et al. 1987).
        However, 
        Msx1 -deficient mice exhibit impaired fetal digit-tip regeneration, a
        phenotype that can be rescued in ex vivo cultures in a dose-dependent manner by application
        of exogenous BMP4 (Han et al. 2003). Recently, it has been demonstrated that 
        Xenopus tadpoles are unable to regenerate their tails during a
        refractory period of development between stages 45 and 47 (Beck et al. 2003). If tails are
        amputated during this refractory period, genes that are normally expressed during the early
        stages of tadpole tail regeneration, such as 
        BMP4, Msx1 , and 
        Notch-1 , are not expressed. However, transgenic frogs carrying a
        hyperactive form of 
        Msx1 or constitutively active 
        ALK3 (a receptor for BMP4) are able to regenerate their tails during the
        refractory period. Transgenic frogs carrying a constitutively active 
        Notch-1 receptor will regenerate their notochords and spinal cords but
        exhibit little or no muscle regeneration, suggesting that Notch-1 signaling alone cannot
        rescue complete regenerative capacity in frog tadpoles (Beck et al. 2003). Results from
        expression studies in a variety of organisms are consistent with these in vivo gene
        function studies. 
        Msx genes are upregulated in regenerating salamander limbs and
        regenerating zebrafish fins and hearts (Simon et al. 1995; Poss et al. 2002a; Raya et al.
        2003), while 
        notch-1b and its ligand, 
        deltaC , are upregulated during zebrafish heart and fin regeneration
        (Raya et al. 2003).
      
      
        Msx1 and Cellular Plasticity
        Although these functional and expression studies indicate that 
        Msx1, Bmp4 , and 
        Notch-1 are important for a variety of regenerative processes, they do
        not address the mechanism by which these genes exert their effects. However, several in
        vitro studies suggest that 
        Msx1 may be involved in regulating cellular plasticity. Ectopic
        expression of 
        Msx1 can inhibit the differentiation of a variety of mesenchymal and
        epithelial progenitor cell types (Song et al. 1992; Hu et al. 2001), suggesting that this
        gene may play a role in maintaining cells in an undifferentiated state. Furthermore, 
        Msx1 may be functioning not only to prevent differentiation of progenitor
        cells but also to induce dedifferentiation of cells that have already differentiated.
        Ectopic expression of 
        Msx1 in mouse myotubes (differentiated muscle cells that are
        multinucleated and are able to contract), coupled with serum stimulation, can induce these
        multinucleated cells to reduce their levels of myogenic proteins and undergo a cell
        cleavage process that produces proliferating mononucleated cells (a process known as
        cellularization) (Odelberg et al. 2000). Clonal populations of these dedifferentiated cells
        can redifferentiate into cells expressing markers for cartilage, fat, and bone cells, as
        well as myotubes. These results suggest that the combination of ectopic 
        Msx1 expression and serum stimulation can induce differentiated muscle
        cells to dedifferentiate into proliferating multipotent progenitor cells. Given this degree
        of cellular plasticity, it is not surprising that 
        Msx1 can also induce muscle progenitor cells, known as myoblasts, to
        dedifferentiate to multipotent progenitor cells (Odelberg et al. 2000).
        Cellularization of myotubes and myoblast dedifferentiation can also be induced by at
        least two synthetic trisubstituted purines. Myoseverin is a trisubstituted purine that
        binds to and disassembles microtubules, leading to the cellularization of multinucleated
        myotubes (Rosania et al. 2000). The resulting mononucleated cells proliferate when
        stimulated with serum and redifferentiate into myotubes following serum starvation. A
        second trisubstituted purine, reversine, induces myoblasts to dedifferentiate into
        progenitor cells with adipogenic and osteogenic potential (Chen et al. 2004). Therefore,
        reversine and 
        Msx1 appear to have a similar effect on mouse myoblasts, although no
        reports have yet addressed whether reversine might induce dedifferentiation of
        multinucleated myotubes.
        In this issue of 
        PLoS Biology , Kumar et al. (2004) present data linking 
        Msx1 function to microtubule disassembly during the process of salamander
        myofiber cellularization and fragmentation (myofibers are formed from myotubes and
        represent the completely mature form of the differentiated skeletal muscle cell). Their
        results suggest that 
        Msx1 expression induces microtubule disassembly, which then leads to
        myofiber cellularization or fragmentation. If 
        Msx1 function is markedly reduced in salamander myofibers by preventing
        the efficient synthesis of the Msx1 protein, cellularization or fragmentation of the
        myofiber is inhibited, suggesting that Msx1 is required for this process. Thus, this study
        complements previous work (Odelberg et al. 2000) showing that ectopic 
        Msx1 expression, coupled with serum stimulation, is sufficient to induce
        cleavage, cellularization, and dedifferentiation of mouse myotubes. The two studies point
        to an essential role for 
        Msx1 in regenerative cellular plasticity and when combined with previous
        in vivo studies, raise the possibility that BMP or Notch signaling might also play a role
        in this process.
        Results from these and other similar studies are beginning to give researchers a glimpse
        into the molecular mechanisms that control regeneration and cellular plasticity. With the
        new tools available to identify candidate genes and assess their function, the next few
        decades appear promising for scientists engaged in regeneration research. Elucidating the
        molecular basis for regeneration may prove to be an essential step in devising effective
        methods for enhancing regeneration in mammals and may well usher in a golden era for
        regenerative medicine.
      
      
        Accession Numbers
        The Mouse Genome Informatics (http://www.informatics.jax.org/) accession numbers of the
        genes discussed in this paper are 
        ALK3 (MGI: 1338938), 
        BMP4 (MGI: 88180), 
        Msx1 (MGI: 97168), and 
        Notch1 (MGI: 97363). The GenBank (http://www.ncbi.nih.gov/GenBank/)
        accession numbers of the genes discussed in this paper are 
        deltaC (NM 130944), 
        notch1b (Y10352), 
        Ambystoma mexicanum
        Msx1 (AY525844), 
        Danio rerio msxb (U16311; partial sequence), 
        D. rerio msxc (NM 131272), 
        Homo sapiens
        ALK3 (Z22535), and 
        Mus musculus
        ALK3 (Z23154).
      
    
  

  
    
      
        
        Autoimmune diseases afflict a large segment of the population in Western countries. Many
        of them have been described, and rheumatoid arthritis, type I diabetes (also called
        insulin-dependent diabetes mellitus), multiple sclerosis, and systemic lupus erythematosus
        (SLE) are among the most common. Although tremendous progress has been made in disease
        management over the last decade, cures for these diseases have not yet been found.
        Consequently, a large research effort is sustained in this field. In addition, autoimmunity
        has intrigued basic immunologists since the early realization that the ability to
        discriminate self from non-self was at the core of the immune system's ability to protect
        an organism from pathogens while avoiding self-destruction. A failure of this mechanism
        results in autoimmune reactions that often lead to clinical disease. In spite of massive
        research efforts, the mechanisms by which autoimmune diseases develop are not clearly
        understood. Genetic predisposition as well as environmental triggers plays a role, but the
        identity of these factors has been largely elusive. The identification of the most common
        genetic and environmental factors that set off autoimmunity may lead to a better
        understanding of the ensuing pathogenesis, and offers the best hope for improved therapies,
        and ultimately, cures.
        So far, animal models have proved the best way to probe the mechanisms of disease in
        general, and autoimmune diseases in particular. In the past few decades, the mouse has
        become the model of choice for experimental medicine, and the rat is following close
        behind. Starting in the early twentieth century at the Jackson Laboratory (Bar Harbor,
        Maine, United States), the production of inbred strains of mice and the systematic
        collection and characterization of naturally occurring mutants have created the building
        blocks on which much of the research using animal models is now based. Inbred strains are
        collections of genetically identical animals obtained through selective breeding. These
        strains have provided homogenous experimental groups, with interindividual variability
        reduced to environmental (and stochastic) factors. In addition, inbred strains have an
        assortment of distinct phenotypes that have then been exploited as models of human
        diseases.
        Since the 1980s, techniques have been developed to manipulate the mouse genome. Specific
        genes now can be routinely over-expressed as transgenes, or eliminated by gene targeting,
        which creates a “knockout” (Smithies 1993; Wassarman and DePamphilis 1993). This approach,
        known as reverse genetics, has shown itself to be a powerful tool with which to evaluate
        the role of individual genes in various biological processes. The use of genetically
        engineered mouse models will likely play a major role in deciphering the function of a
        multitude of new genes revealed in the recently completed sequence of the mouse genome
        (Mouse Genome Sequencing Consortium 2002).
        Interestingly, the percentage of mouse genes without any homolog currently detectable in
        the human genome (and vice versa) has been estimated to be less than 1%, a fact that
        strengthens the validity of mouse models of human diseases. However, one has to be careful
        in directly applying data obtained from animal models to human diseases. Most human
        autoimmune diseases show an extremely heterogeneous clinical presentation, which animal
        models present as simplified versions. A mouse model, as in any reductionist approach, is
        both inconvenient, because it provides only a partial representation of the real biological
        complexity underlying the human disease, and advantageous, because it is a more tractable
        tool with which to probe mechanistic issues. In addition, a number of differences exist
        between the human and rodent immune systems (Mestas and Hughes 2004). Since immune
        dysfunctions are at the root of autoimmune diseases, such differences may limit
        extrapolations from animal models to autoimmune patients.
        Nonetheless, animal models are at the core of autoimmune research, and a large body of
        literature reflects the many advances brought by these models in terms of deciphering
        disease mechanisms. The relative lack of progress in certain human autoimmune diseases for
        which an animal model does not exist, such as neuropsychiatric lupus, corroborates the
        indispensable role played by animal models. Three basic types of animal models have been
        used in autoimmune research: spontaneous models, induced models, and genetically engineered
        models. The latter category has been further subdivided into transgenic and knockout
        strains (Table 1).
        Spontaneous models were produced through fortuitous observations of clinical symptoms
        reminiscent of a given human autoimmune disease developing in a given mouse strain, or in
        crosses between mouse strains. This happened, for example, with the nonobese diabetic (NOD)
        mouse, which developed type 1 diabetes, and the hybrid between the New Zealand Black (NZB)
        and the New Zealand White (NZW) mouse, (NZB × NZW)F1, which developed a lupus-like
        disease.
        Unfortunately, spontaneous models are not available for all human autoimmune diseases.
        Therefore, scientists have created induced models, often by exposing the animals to high
        doses of a suspected autoantigen at the same time as stimulating the immune system.
        Interestingly, marked differences exist between strains in their responses to these
        autoimmune inductions that reflect genetic variation associated with susceptibility to
        autoimmune diseases. For this reason, induced models have been limited to a small number of
        strains. For example, experimental autoimmune encephalomyelitis has been widely used as a
        model of multiple sclerosis. In this model, spinal cord homogenate or a protein derivative
        such as myelin basic protein is injected with a mixture of potent immunostimulants, most
        commonly in mice from the SJL strain. Another example of an induced model is
        collagen-induced arthritis, which has been used to study rheumatoid arthritis. In this
        model, type II collagen, a joint component, is injected also with immunostimulants, most
        commonly into mice from the DBA/1 strain.
        Finally, the ability to turn specific genes on or off, in specific cell types and/or at
        specific times, has created a plethora of mouse models limited only by the immunologist's
        imagination. Genes suspected to play a role in the pathogenesis of various autoimmune
        diseases have been evaluated this way, and those studies have been very informative in
        mapping out functional pathways that are targeted in these diseases. However, these models
        have intrinsic problems that have become more apparent in the past few years, and require
        careful controls to avoid possible misinterpretation. These problems are a result of the
        fundamental way in which transgenic and knockout strains are produced. When a piece of DNA
        carrying the gene of interest is injected as a “transgene” into fertilized eggs, it
        integrates randomly into the genome, and in doing so, potentially modifies the expression
        of the gene it integrates into. Since all genetic studies have recognized that a large
        number of genes are involved in autoimmune disease susceptibility, the potential for a
        transgene to hit one of those susceptibility genes is not negligible. This potentially
        confounding factor is usually controlled by producing and comparing several independent
        transgenic lines.
        More problematic is the interpretation of results obtained with knockout models. A gene
        is “knocked out” (KO) by homologous recombination of a disrupting piece of DNA within that
        gene. This genetic manipulation takes place in embryonic stem (ES) cells, which once
        mutated, are introduced into the inner cavity of a blastocyst (very early mouse embryo),
        creating chimeric embryos that are put back into female mice that carry the pregnancy to
        term. After multiple trials (and errors), ES cell lines from the 129 strain and blastocysts
        from the C57BL/6 (B6) strain have shown themselves to be superior in terms of efficiency
        and reliability. Consequently, this strain combination is at the origin of the overwhelming
        majority of knockout strains. Although the chimeric mice are usually backcrossed to B6 to
        dilute the contribution of the 129 genome, the knockout strains are always a mixture of the
        two genomes (Figure 1). Most importantly, a large region flanking the KO gene remains of
        129 origin, unless extreme measures are taken to select for recombination between tightly
        linked markers. There have been sporadic reports of phenotypes initially attributed to
        deficiency in the expression of a given gene that disappeared with additional backcrosses
        to B6. The only possible interpretation was that these phenotypes were in fact due to 129
        alleles that were replaced by B6 alleles with further backcrossing.
        The production of autoantibodies and mild antibody-related renal pathology, highly
        relevant to autoimmune diseases, especially SLE, has been reported independently in four
        mouse models using the (129 × B6) genetic background (Obara et al. 1979; Botto et al 1998;
        Bickerstaff et al. 1999; Santiago-Raber et al. 2001). It is generally accepted that genetic
        susceptibility to autoimmune diseases is conferred by multiple highly interactive genes
        that have small individual effects. In this context, the autoimmune phenotypes resulting
        from the combination of the 129 and B6 genomes may might therefore provide a primed
        background upon which the effects of deficiency in the target gene can be amplified. On the
        other hand, the autoimmune phenotype may be overwhelmingly contributed by the (129 × B6)
        genomic combination, with little if any effect of the deficiency of the KO gene.
        In this issue of 
        PLoS Biology , Marina Botto and her colleagues have tested this
        hypothesis by taking one of their knockout models for SLE, the serum amyloid P component
        deficient mouse (
        Apcs
        −/− ) (Bygrave et al. 2004). This group has published that 
        Apcs
        −/− mice on a (129 × B6) genetic background develop a lupus-like
        disease, even after repeated backcrosses to B6 (Bickerstaff et al. 1999). Serum amyloid P
        component binds to debris generated from dying cells. Efficient removal of this debris has
        been shown to be critical to the prevention of the production of autoantibodies against
        intracellular material. 
        Apcs was therefore an interesting candidate gene to evaluate. 
        Apcs , however, is located in a region near the tip of mouse Chromosome 1
        that is rich in SLE susceptibility loci and in genes that have been directly associated
        with SLE in humans (Wakeland et al. 1999). As mentioned above, the 
        Apcs
        −/− mouse, although on a mostly B6 genomic background, has its entire 
        Apcs flanking region replaced with 129 alleles. In a critical experiment,
        Botto and colleagues compared the autoimmune phenotypes of 
        Apcs
        −/− mice to congenic mice (i.e., genetically identical, except for the
        gene of interest), carrying the same 129 region on Chromosome 1, but expressing 
        Apcs . Amazingly, no difference was found between the two strains
        regarding the production of autoantibodies, clearly eliminating 
        Apcs deficiency as a mechanism for this autoimmune process. 
        Apcs deficiency was however associated with markedly increased renal
        damage, suggesting that this gene may be involved in preventing pathological consequences
        of autoantibody production.
        It has been reported anecdotally that the most common outcome of a genetic knockout is a
        lupus-like disease. Botto and her colleagues may have identified the reason behind this
        somewhat surprising observation as a spurious consequence of the gene-targeting process.
        Gene targeting has been an invaluable tool in understanding the mechanisms of immunological
        diseases, and has still a very important role to play with increasingly sophisticated
        techniques of selective targeting. The immediate consequence of this work should be an
        increased scrutiny for appropriate controls, which may include congenic mice carrying the
        same 129 flanking region, but expressing the targeted gene (Figure 2).
        The past few years have shown that genetic susceptibility to autoimmune diseases
        involves a large number of genes with small individual contributions. In spite of this
        great complexity, advances have been made, and a small but growing number of susceptibility
        genes have been identified (Morahan and Morel 2002). A common trait shared by successful
        studies has been the use of mouse models, either directly or indirectly. As the pace of
        genetic analysis increases in autoimmune diseases, and powerful tools have been created to
        navigate between the mouse and human genomes, the use of mouse models has been reaffirmed
        at multiple levels. Mouse models are used to discover new susceptibility genes that can
        then be assessed in patient populations, as well as to validate genes that have been
        directly identified in human genetic studies. Mouse models are also used to perform
        detailed functional and physiological analyses that cannot be conducted in humans. Finally,
        mouse models have been invaluable to screen disease-specific therapeutic agents.
        Using mouse models has its pitfalls; many differences, both obvious and subtle, exist
        between mice and humans. Those differences are, however, outweighed by the power of the
        experimental system offered by the mouse. What the new study of Botto and colleagues
        reminds us is that the appropriate control is still crucial to meaningful data
        interpretation. Keeping that in mind, one can predict that many of the keys to human
        autoimmune diseases are still in the mouse room.
      
    
  

  
    
      
        
        My graduate adviser, Sydney Brenner, used to exclaim ‘Revenons à nos mutants!’ as he sat
        down at the bench to search for yet more genetic variants of 
        C. elegans . Those mutants won him a Nobel Prize, some thirty
        years later. Armand Leroi is another aficionado of 
        C. elegans mutants, but he decided to write a book—and then to
        make a television series—on mutants of humanity, not of worms. He says at the beginning and
        end of the series: “We are all mutants, but some of us are more mutant than others.” This
        is a good slogan, and very proper for embracing humanity as a whole. He backs it up with an
        aphorism from Etienne Geoffroy Saint-Hilaire, pioneer of teratology, who proclaimed: “There
        are no monsters and Nature is one.” What Geoffroy meant was that abnormalities provide
        clues to normal processes, and hence are invaluable to science, if they can be properly
        understood. But monsters and mutants are, undeniably, fascinating in their own right.
        
        Mutants , the book, is excellent: impressively researched and illustrated
        and extremely well written. The resulting television series on Channel 4 in the United
        Kingdom has a distinctly different impact. The series covers only a few of the subjects
        dealt with in the book, and handles the material in a different way. Inevitably, television
        can't include much science or scholarly detail, but it compensates with human images that
        are wholly gripping—both the preserved specimens and the living subjects who talk about the
        strange conditions that they live with. It's a freak show, but a freak show with thoughtful
        scientific commentary.
        Each of the three programmes in the series has a particular theme. The first, ‘The
        Mystery of Growth’, is devoted mainly to skeletal disorders. We meet Carole Ozel, who copes
        with extraordinary courage with a terrible disease called fibrodysplasia ossificans
        progressiva (FOP), in which bony tissue forms throughout the body, gradually immobilizing
        the body in a second skeleton. Later we encounter a crew of charming and articulate dwarfs
        taking time out from a disco at the Reno convention of the Little People of America. They
        are happy to be called dwarfs or little people, but midget is no longer an acceptable term.
        Being a dwarf, in fact, is sometimes a ticket to fame and fortune, as in the case of Joseph
        Boruwlawski, last of the court dwarfs, who enchanted European royalty, married a beautiful
        woman, and lived happily to the age of 98.
        The second part, ‘The Dangerous Womb’, is about birth defects, conjoined twins, and
        basic embryology. The makers of the piece went to the trouble of getting the developmental
        biologist Eddie de Robertis to reconstruct the classic experiment of Mangold and Spemann
        that revealed the underlying basis of some of these defects. The programme goes into some
        detail about what is now known about the molecular basis of normal and abnormal
        development, and how we can begin to explain such extraordinary forms as that of the Parodi
        twins, who had distinct heads and shoulders, but merged into a single torso and a single
        pair of legs.
        The third programme, ‘The Meaning of Beauty’, deals with lesser but still striking
        abnormalities such as albinism and hypertrichosis (excess hair). It also moves into
        contentious areas, in a frank discussion of race genetics. Leroi makes the essential point
        loudly and clearly: there is much more genetic variation within any village on earth than
        there is between different human populations. We are extraordinarily unified, from a
        genetic standpoint. But, as he notes, the idea of race persists, and about 7% of global
        genetic variation in human DNA includes AIMs, ancestry informative markers. Some of these
        distinguish, for example, Africans and Europeans and provide objective information about
        the ancestry of different human populations.
        This being television, the series closes with a discussion of beauty, which Leroi
        proposes to be simply the absence of visible mutant defects, using Saira Mohan, 
        Newsweek 's idea of human facial perfection, as an exemplar. Yes, she
        looks nice, but nice can be boring. Beauty, above all other human attributes, is profoundly
        influenced by culture, and it is hard to take this interpretation of beauty as an adequate
        explanation, rather than just a pretty way to finish the series.
        For the most part, Leroi makes an agreeable and humane commentator, though he is not
        immune to the slight self-satisfaction that seems to overcome all scientists on television.
        The camera also spends an excessive amount of time dwelling on him, to a point where it
        becomes irritating to see him walking—frequently in slow motion— into yet another museum or
        laboratory. Sometimes the focus on the presenter pays off, as when we see the six-foot
        scientist looking like a small child beside Chris Greener, the tallest man in Britain, or
        witness Leroi's faint chagrin at discovering that his DNA is mostly European, despite his
        cosmopolitan family history. More questionable are the bits where he paddles casually
        through a tub of preserved viscera from some long-gone sufferer from situs inversus
        (mirror-reversed organs) and succumbs to laughter at the sight of Ditto, the amazing
        two-faced pig. This may be an honest attempt at portraying the conflicted reactions we all
        have to abnormality, but it seems bound to cause trouble.
        Arty camerawork is a running feature of the programmes, and there is a great deal of
        smoke and mirrors about the whole production. Perhaps this is deliberate, reminding us that
        it is hard to look directly at extreme deformity, but there is an air of ‘I wants to make
        your flesh creep’ about the many sidelong shots of mutant babies in bottles in the Vrolik
        Museum, to which we return again and again throughout the series. Viewing the already
        distorted fetuses through further distorting camera angles and under green lighting doesn't
        really achieve anything. The treatment begins to resemble the first 
        Alien movie, in which the audience was never allowed to see the monster
        directly.
        In the end, what is most memorable about these programmes is the living people
        themselves, and how they have coped with their various genetic disorders. It is very
        touching to see the home movies of Tiffany York, born with mermaid syndrome, or sirenomelia
        (in which the legs are fused together), taking her first tottery steps after corrective
        surgery, and to listen to her talking philosophically about her life as she floats in a
        Florida swimming pool. It is similarly cheering to hear from the dwarfs and albinos, or
        from Chuy Aceves, who has hypertrichosis and looks like the original Hollywood wolfman but
        suffers no ill effects and is proud of his rare condition. For these sections alone, let
        alone the serious and well-explained scientific background, the series is well worth
        seeing. It makes one feel surprisingly good about the human race and the human spirit.
      
    
  

  
    
      
        
        
        Can “race” be gleaned from our genes? Concern over the emerging trajectories of genetics
        research has led to ongoing debates about how to characterize and interpret genetic
        variation. Despite the mantra that humans share 99.9% of our genetic makeup, there is
        increasing interest in identifying the relatively small percentage difference that
        distinguishes individuals. Therein lies the paradox: if we are all the same, why do we
        continue to search for the ways in which we differ from one another? We can take an
        essential first step toward addressing this paradox by acknowledging the often conflicting
        stakes for individuals and groups in debates that center around genes, science, and race.
        Whose genes will be studied? For what purpose? And who has the authority to decide?
        These stakes are laid bare by Cassandra Medley in her play “Relativity,” which ran at
        the Magic Theater in San Francisco this spring, in a production directed by Edris
        Cooper-Anifowoshe. The play focuses on the inner conflict of a young scientist, an African
        American woman who tries unsuccessfully to straddle the opposing world views of her
        profession and her family. In “Relativity,” Kalima Davis is a postdoctoral fellow in a
        prestigious stem cell laboratory on the East Coast. Her impeccable academic pedigree
        distinguishes Kalima as a rising researcher, and she is one of the few women and African
        American scientists working in the field. Kalima is also the devoted daughter of the
        charismatic psychotherapist Claire Reid, who directs the “Leon Davis Foundation,” which
        Kalima's late father dedicated to the belief that neuropeptide melanin, found in “people of
        color,” enhances intelligence, athleticism, and emotional sensitivity. This theory also
        points to the lack of melanin among lighter skinned individuals as a cause of “white
        racism.” Kalima, who has inherited co-directorship of her father's foundation, is asked by
        her mother to offer “scientific proof” of the melanin theories and to discount the
        assertion that all groups are genetically similar.
        At issue in “Relativity” is the struggle over what constitutes a valid belief. Reid
        suggests to Kalima that “science is not the sole province of what the ‘West’ defines it to
        be,” and refers to Chinese acupuncture, Hindu Chakra, and tribal African shamanism as
        examples of legitimate “sciences.” But the power of Western science to trump other
        interpretations of lived experience has become all too clear in the genomic era. Truth is
        excavated from the human body, where genes emerge as the iconographic oracles of our past,
        present, and future. As Kalima recounts, “We can't get around it. DNA is fact.”
        Nonetheless, Kalima naively attempts to find a way to retain these opposing epistemologies.
        The futility of this dual position is made apparent by the arrival of Iris Preston, an
        African American senior scientist who has taken over as the new head of Kalima's lab.
        Preston, a highly vocal critic of Claire Reid and melanin theories, serves as a formidable
        apostle of the scientific method. Shortly after her arrival, Preston convenes members of
        her new lab to filter those seeking financial and other derivative rewards from the truly
        devoted, who are motivated solely by their “sense of wonder and amazement” and their desire
        to “cultivate what Einstein referred to as ‘holy curiosity.’” She makes plain that science,
        like all ideologies, demands consummate faith and unwavering piety.
        Kalima's struggle to claim her “true lineage” is much more than a simple choice between
        her biological mother (Reid) and her intellectual mentor (Preston). Her plight forces her
        to explore the meaning of justice. The contrast between melanin theories and genomic
        research, initially stark, blurs as it becomes increasingly clear that both Reid and
        Preston seek to use their “science” to redress race-related disparities. Citing a history
        of racism, including the historically well-documented Tuskegee Syphilis Study, Reid asks
        rhetorically whether genetics research will result in a “genetically modified” white upper
        class and a lower, dark-skinned “natural birth class”? At stake are issues of power and
        trust, and the question of whether new genetic technologies will close the gaps between
        groups or make them wider. Some postulate that the “new genetics” will render conventional
        notions of race obsolete. However, it is doubtful that such a color-blind utopia will be
        won through the sequencing of genes without a serious engagement with the differences that
        lie outside cell walls. A social infrastructure of inequality that mediates race through
        nutritional deficits, exposure to pollutants, and the use of the emergency room as the sole
        venue for healthcare will not be rehabilitated through gene therapy or
        pharmacogenomics.
        Preston seeks to use her stature in science to focus on the inequities within the
        academy. Encouraging Kalima to appear with her on a television program about stem-cell
        research, Preston urges her to imagine the milestone of “not just one, but two black women
        scientists, holding forth among the usual cadre of white males.” As one of the rare, highly
        scrutinized, “minority” scientists, Kalima embodies a “double jeopardy.” She must prove
        that she is worthy of her position—that she is as good, if not better, than her “white”
        peers—yet she must always be “remembering from whence she came.” When Dan, a white
        colleague in Kalima's lab who is also her boyfriend, hears that Preston has chosen Kalima
        to appear with her on television, he jealously accuses her of benefiting from preferential
        treatment. The play's depiction of this assertion of “reverse racism” questions the
        legitimate use of race in evaluating promotion and achievement in science. What is apparent
        is that race, while an ever-present subject, is often presented as the antithesis of
        conventional notions of a color-blind meritocracy. Left to linger is the critical question:
        does merely identifying differences among groups constitute an act of racism? Can
        statements of racial differences be neutral, unfettered by a relative hierarchy?
        Medley throws into stark relief the politics around race and gender in science. She
        illuminates how an individual's dilemma transcends the private realm; personal decisions
        are never truly “personal,” but are inherently public because they always have wider social
        repercussions. In the fateful confrontation between Kalima and her mother, Reid challenges
        her daughter to “grow up” and to risk her disapproval. A similar challenge can be issued to
        those who have inherited the “new genetics”—to vanquish the continuing paradox of reciting
        a “mantra of sameness” all the while searching for meaningful differences. In “Relativity,”
        Medley serves us a cautionary tale of the costs of our chronic ambivalence about the
        critical issues of race and justice in science. A good first step is to recognize that the
        search for meaning in human difference is inseparable from the struggle over the moral
        order in which we live.
        To underestimate the power of science to define our social agenda is to lose an
        opportunity to determine our future course. We only need to look to history as our
        proof.
      
    
  

  
    
      
        
        At first glance, the preschool classroom on the other side of the two-way mirror looks
        like any other—brightly colored rugs, scattered toys, and tiny chairs. But almost
        immediately an observer notices differences in the Team Toddle students here at the
        Neuropsychiatric Institute of the University of California at Los Angeles (UCLA) (Los
        Angeles, California, United States).
        A therapist instructs a toddler on his colors, flashing a rapid sequence of blocks at
        him. When the toddler starts rocking in his chair and repeatedly touching his forehead, the
        therapist physically restrains his hands, placing them back on the tabletop until he stops
        the repetitive behaviors and focuses once again on her face and the blocks. During
        playtime, a two-year-old girl sits by herself in the corner, fixated on some picture cards,
        oblivious to a group of other children playing with a racetrack and to the therapist who
        tries to draw her out to join the group.
        These children lack some of the key social skills that normal toddlers pick up
        naturally—looking to others for reassurance or cues, focusing on faces, and playing
        together. Social and communication impairment is a hallmark of autism and can show up as
        early as 12–18 months of age. But with an unknown cause, and genetic linkages still hazy,
        there is little consensus among researchers on how the disorder develops in children and
        how it causes a broad spectrum of social, language, and behavioral deficits.
        Following one line of research, David Amaral's laboratory at the M.I.N.D. Institute at
        the University of California at Davis Medical Center in Sacramento (California, United
        States) has recorded, in autistic brains, a brain volume increase in a specific structure,
        the amygdala, which is thought to be important for social behavior. A similar study at the
        University of Washington in Seattle (UW) (Seattle, Washington, United States) has reached
        the same conclusion. “There are so few facts about autism, to have two labs come up with
        the same data is phenomenal,” says Amaral. “We feel confident this is a real finding, but
        what does it mean to these kids?”
        On another research track, using functional imaging, Ralph-Axel Müller, a cognitive
        neuroscientist at San Diego State University (San Diego, California, United States) sees a
        scattering of brain activation in autistic brains that he views as an indication of a more
        general brain development problem underlying the disorder (Figure 1). He has hypothesized
        that the early-developing basic functions may require more brain area in autism, pushing
        out and disturbing the later specialization for more complex functions. “I'm sure this is
        wrong,” he says, “but it will allow us to look in a more hypothesis-driven way at animal
        studies of how the cerebral cortex develops specialization.” Animal models may, in turn,
        yield clues about normal and abnormal brain development in humans.
        “Since there is no major hypothesis as to cause [of autism], there are many plausible
        ideas,” says Amaral. “If we go after all of them, we will waste all of our resources. [We
        have to] come to some consensus about which are most plausible.” At least two levels of
        pursuit exist for tracing brain problems associated with autism—the exploration of the
        general developmental disruptions that result in an autistic brain, and the examination of
        more specific problems in particular brain structures that produce symptoms. Although
        scientists still debate how autism evolves in a patient, the field has begun in the last
        decade to replicate findings and make science-based arguments for interventions. Progress
        has come in small steps, with advances in neuroimaging and more rigorous experimental
        designs.
        Research focuses have shifted from “curing” autism to finding better diagnostics for
        early intervention, improving behavioral therapies, and gaining insight into the
        development and function of the autistic brain. Both advocacy groups and government
        programs have started to bring together neuroscience and genetics experts, clinicians, and
        families to sharpen the focus of studies and ensure progress in what has often been a messy
        field.
      
      
        A World Apart
        Autism spectrum disorder strikes between one and six out of every 1,000 children around
        the world, but diagnosis and treatment are currently limited to developed countries. Autism
        is four times more prevalent in boys than girls, but makes no racial, ethnic, or
        socioeconomic distinctions. It is characterized by three main symptoms: impaired language,
        social and communicative deficits, and repetitive and stereotyped behaviors, such as hand
        flapping, rocking, and unusual responses to sensory stimuli. Autism spectrum disorders can
        be broken down into other categories, such as low-functioning autism (IQ below 70),
        high-functioning autism (IQ above 70), and Asperger syndrome (similar to high-functioning
        autism but with no language deficit).
        Researchers suspect that there are even more distinct subsets of autism patients. For
        example, some patients also have epilepsy, and it has been suggested that there is a
        regressive form of autism—children who, at two or three years of age, appear to regress and
        lose developmental milestones they had already achieved. Researchers say that sorting out
        these different profiles—or phenotypes—of autism will be especially important in sorting
        out which genes or which brain abnormalities are implicated for particular deficits. This
        sorting should also help clarify the mounds of contradictory data that have dogged the
        field, by tamping down the experimental “noise” in studies. Boosting the number of children
        studied and following them from early infancy through adolescence and beyond will also be
        key components of future studies.
        “There is not going to be rapid progress in autism research unless we subtype,” Amaral
        says. He predicts that “brain differences in kids with a regressive form of autism will be
        different than those of kids with the more congenital type of autism.” He and others are
        teaming up in an autism phenotyping project that will characterize 600 children into
        categories of autism (comparing them to 600 children with mental retardation and 600
        controls). Splitting autism into subtypes will boost both neurobiology and genetics studies
        (Box 1) to find real effects related to specific traits.
      
      
        Facing Up to Autism
        A key area of research explores the brain's response to human faces at a young age.
        Studies at the UW Autism Center have shown that unlike typically developing
        three-year-olds, autistic children do not show a differential brain response to their
        mother's face compared to that of a stranger. While dysfunctional face recognition may be
        one of the more devastating symptoms for caregivers, it is also one of the most promising
        avenues for research to determine how autistic brains process their world differently.
        Sara Webb, a child psychologist at UW, has followed about 70 autistic children since the
        age of three for a longitudinal study that will test many parameters until they reach age
        nine. Her work has already shown that autistic three-year-olds process seeing a strange toy
        differently from seeing a favorite toy, in the same way a normal child does. But activity
        in their brains—measured through a network of electrodes placed on the scalp—is similar
        whether the face is familiar (for example, mom) or strange. This, Webb says, led to two
        hypotheses: either the brain area for face processing is not set up correctly in autistic
        children, or the way these children incorporate experiences from their environment is so
        different that the brain area develops improperly.
        “We think the latter is a more likely explanation at this point,” says Webb. “By the
        time they are adolescents or adults, they are showing the [proper] response for familiar
        faces.” Indeed, a functional MRI (fMRI) study by UW neuroimaging researcher Elizabeth
        Aylward showed that the brains of high-functioning adolescents and adults did activate the
        face-recognition center, the fusiform gyrus, when shown a very familiar face. However, the
        same subjects did not activate the center when viewing strange faces. This points to the
        possibility that greater experience seeing the familiar face (i.e., on a daily basis for
        many years) can eventually influence the appropriate brain areas.
        “You need the biological wiring set up properly, but you also need experience for it to
        function normally,” says Aylward. “We're guessing what is missing is the experience.” To
        test that idea, one of her graduate students will “train” half of the autistic patients in
        face recognition—something most children pick up on their own—by having them study,
        manipulate, and match faces using computer games. Then fMRI scans will be done again to see
        if the fusiform gyrus might now be activated when viewing strange faces, as it is in
        control subjects. Intense training of a similar type for reading has already been shown to
        effect change in brain activation in as little as three weeks for children with
        dyslexia.
        In their model, it is as if “all the parts are there, ready to go, but somehow they
        haven't gotten the ignition turned on,” says Aylward. At the 2004 annual meeting of the
        American Association for the Advancement of Science (Washington DC, United States), the UW
        center director Geraldine Dawson explained that this tackling of specific deficits will
        help researchers attach them to particular “mind modules” in the brain and will ultimately
        lead to the genes that control the development or function of those modules. That modular
        view, however, is not shared by many of her colleagues elsewhere, who argue that autistic
        behaviors are the result of a system-wide perturbation of early brain development and
        connectivity.
      
      
        Structural Support
        For example, Müller points to structural studies that seem to uphold his theory of
        overall disorganization of the brain's cortex. Work by Manuel Casanova and colleagues at
        the University of Louisville (Louisville, Kentucky, United States) shows that the
        “minicolumns” of neurons that make up the cortex are narrower and more numerous in autistic
        brains. Normally, these organized bundles appear very early in the developing fetal brain.
        In postmortem studies of autistic brains, Casanova found that the minicolumns had the same
        number of neurons, but smaller margins between the bundles. The margins, Casanova says, may
        act like “a shower curtain of inhibition that prevents information from flooding adjacent
        minicolumns.”
        Reducing those margins, he hypothesizes, could mean that an autistic brain has too much
        positive feedback, acting like a noisy amplifier. “For an autistic individual who is trying
        to piece together too much information from a face, maybe it's like looking at the sun,” he
        says.
        More general studies of adult autistic neuroanatomy have given conflicting results—most
        likely from diversity in the study populations—that make functional inferences difficult,
        if not impossible. But recent studies that focus on developing autistic brains earlier in
        life have revealed intriguing differences from normally developing children.
        Several studies have shown that from ages two to four, autistic children have larger
        overall brain volumes (and correspondingly larger head circumferences) than normal
        children, but that the difference had disappeared by about age six or seven. Since autism
        is usually diagnosed around age two or three, when the brain is already abnormally large,
        Eric Courchesne and colleagues at University of California, San Diego (San Diego,
        California, United States) hypothesized that brain overgrowth must occur earlier, before
        signs of autism appear.
        In an elegant retrospective study, the team analyzed head circumference and brain volume
        measurements of autistic children that started at birth and continued until 14 months of
        age. The study revealed that at birth, autistic children's head size is much smaller than
        healthy children, in the 25th percentile, but by 6–14 months, their head size had increased
        to the 84th percentile, an excessive growth rate. The increase correlated with increased
        brain volumes of both gray and white matter regions measured by structural imaging between
        ages two to five.
        The Courchesne study strongly suggests that with autism, significant unregulated brain
        growth occurs in the first year of life. The team also found an association between greater
        increases in brain size in infancy and a later age for first word, worse repetitive
        behavior, and a trend toward more severe autistic symptoms later, at diagnosis. The rapid
        growth of autistic brains may produce too many connections too quickly, without the
        opportunity to be shaped by the experience and input that a typically developing child
        accumulates over many years. At age six or later, when the growth slows, the already
        derailed connections may no longer be able to incorporate experiences. “By that time,”
        write Courchesne et al., “the period of plasticity that allows the exquisite and graceful
        complexity of the human brain to emerge will have passed.”
      
      
        Playing Well with Others
        This idea that autistic brains are developing at warp speed, to their detriment, fits
        intriguingly well with what is known about treatment of autism—the earlier and more intense
        behavioral therapy an autistic child receives, the better the outcome will be. That's why
        the toddlers at UCLA get one-on-one training by therapists, who fire rapid questions and
        physically repeat tasks until they sink in.
        Stephanny Freeman, co-director of the Early Childhood Partial Hospitalization program at
        UCLA (Los Angeles, California, United States), says these methods would be alien to, and
        lost on, typically developing two-year-olds, who would be bewildered by such a highly
        structured environment. Her colleague and co-director, Tanya Paparella, chimes in, “It as
        if we are opening a window or door to the autistic brain.” Keeping that door open as long
        as possible in very young autistic patients seems to give them a better prognosis than
        older children, who are more difficult to treat.
        But while most agree that early and intense therapy is good for autistic children, until
        recently, little research on intervention methods existed. Connie Kasari, an educational
        psychologist at UCLA, along with Freeman and Paparella, has run one of the first
        randomized, controlled trials on therapies designed to teach autistic kids social skills.
        The group tested two skills in particular—sharing attention with others and pretend playing
        (Figure 2). The team hypothesizes that these skills, which normal children pick up easily
        and early, lay important groundwork for language development.
        The team's results show that autistic children can learn these skills from intense
        training. At least anecdotally, some of these children have gone on to function in normal
        school classrooms, even making a few friends, although they are still a bit socially
        awkward. Whether or not improvements in those skills will correlate with language
        improvements will require further testing. But Kasari notes that this work is not
        universally accepted in the autism therapy community, and that many more controlled studies
        will have to be published before a system-wide change in autism preschool education can
        occur.
      
      
        Funding the Search
        In the last decade, National Institutes of Health funding for autism research has
        increased from $10 million to $80 million, and much of that has been funneled into large,
        multidisciplinary research projects. Advocacy groups such as Cure Autism Now (Los Angeles,
        California, United States) and the National Alliance for Autism Research (Princeton, New
        Jersey, United States) greatly influence which autism research projects get funded, both
        through their own grant programs and also by lobbying Congress for increased federal
        grants. Some question whether it is wise to let emotions and the desire to find a cure
        drive research agendas. In the past, tensions between government programs and advocacy
        programs have run high.
        Casanova, for one, criticizes the disproportionate flow of money to what he calls
        imaging and genetic “fishing expeditions” and says more should go to neuropathology
        studies. He points out that only about 40 postmortem, mostly adult, autistic brains have
        been studied so far, a tiny fraction compared to those studied in other neuropathological
        disorders like Alzheimer's disease or schizophrenia.
        But Daniel Geschwind, a neurogeneticist at UCLA, defends this approach, saying that a
        well-planned fishing expedition that uses the right technology and looks in the appropriate
        places can result in a “freezer full of fish.” He also says that parent organizations keep
        the field honest by “constantly reminding us to keep an eye on the ball and don't get
        distracted.” Geschwind, Amaral, and other top experts have recently been recruited by
        advocacy groups or by friends with autistic children to shift some of their research
        questions to examining autism.
        As more researchers in genetics and neuroscience have become involved, Amaral says, the
        tensions between the parent groups and the National Institutes of Health have eased. “The
        parents communicated to the scientists the tremendous need for research and the scientists
        convey back to them which [research projects] make sense to fund,” he says. He adds that
        advocacy groups have been indispensable to research, setting up large genetic and brain
        tissue banks and enlisting families to participate in those efforts.
        So, researchers say, the goals of the National Institutes of Health programs and the
        advocacy programs have started to come together to focus on well-executed studies that
        might lead to better diagnostics and earlier, proven interventions. The work of Courchesne
        et al. suggests that children at risk for autism might easily be diagnosed by head
        circumference measurements as early as the first few months of life. Imaging studies
        combined with training programs, such as the work at UW on face recognition, may one day be
        able to verify that behavioral interventions are effective at activating target brain
        areas. As researchers work to untangle the causes and effects of brain dysfunctions in
        autism, Aylward notes, there is good reason to be hopeful: “Although this is a genetic
        disorder, we know there is plasticity in the young brain.”
      
    
  

  
    
      
        
        There is an old Chinese curse: ‘May you live in interesting times.’ According to those
        who know about such things, we live in a momentous time, the time of the Sixth Mass
        Extinction! But most of us do not feel at all cursed. Because, in fact, the Sixth is quite
        different to the previous Big Five—no-one would notice this one if we were not repeatedly
        reminded of it by ecologists. Previous mass extinctions were not so bashful, so discreet.
        The fossil record reveals the disappearance of pollen during previous ones, replaced by an
        abundance of fungus spores, telling us of a world of devastated forests rotting away. The
        earliest sediments after the mass extinction that did away with the dinosaurs are barren of
        fossils: so it is not just that species were going extinct, conditions for life itself were
        bad. Not only did species diversity drop, the abundance of life did as well.
        But conditions for life itself have never been better than today. In the history of the
        planet, there has never been anything as productive of life as a wheat field in Kansas. It
        may not have a large diversity of species, but that is a different matter. In fact, one of
        the reasons for the ongoing loss of plant diversity from grasslands is the very reason the
        wheat field is so productive—fertilisation. We are pouring nitrogen fertiliser into the
        environment and, through the wellstudied ‘paradox of enrichment’, this reduces species
        diversity while increasing actual biomass. Now, there is no question that if current trends
        of habitat alteration and climate change continue then we will ultimately lose large
        numbers of species—diversity will drop—but this does not necessarily translate into a loss
        of abundance of life, and that is a big difference between now and previous mass
        extinctions. Looking at specific groups of organisms tells the same story. So, for example,
        many island bird species are threatened, like the kagu of New Caledonia, but British
        seabird populations, like puffins, are booming. Worldwide amphibian diversity is
        threatened, but cane toads are a pest in Australia. Introduced species pose a threat to
        diversity—the ‘McDonald-isation’ of nature—precisely because they achieve enormous
        abundances.
        Actually, all six mass extinctions may have one very important thing in common: from the
        point of view of the vast bulk of life on the planet they are probably not mass extinctions
        at all. By any criterion—number of individuals or total biomass—the vast majority of life
        on earth is invisible—microbial. So, for example, at least 10% of the living biomass on
        earth consists of bacteria living deep in the oceans' sediments: it would take more than an
        asteroid impact to disturb them. And microbial life is extraordinarily robust: microbes can
        be found living happily in pressurised water hotter than your boiling kettle, in
        concentrated acid, and in rock, and their spores can survive for years in the rigours of
        outer space.
        In talks and lectures, the renowned oceanographer and paleontologist Jeremy Jackson
        paints a vivid picture of what is currently happening to coastal ecosystems, talking about
        a wall of slime emanating from populated areas and growing outwards inexorably towards the
        open oceans, replacing beloved ecologies like coral reef systems. What he means is that the
        visible life that we find attractive and useful—pretty fish, turtles, and so on—is being
        replaced by microbes in splendid profusion. It is taken completely for granted that this is
        disastrous. From a utilitarian point of view indeed it is disastrous, since we like eating
        fish and turtles, and don't like snorkling in slime. But from the point of view of life per
        se, again things have never been better. Life is so abundant that in some places all the
        oxygen in the water is completely used up. These are called ‘dead zones’, but they are no
        more ‘dead’ than the Dead Sea, which is actually teeming with life—just not fish. But,
        nonetheless, we consider what is occurring to be a disaster not just from a utilitarian
        point of view, but at some deeper level giving us an emotional reaction to the word
        ‘slime’—somehow it is just plain wrong.
        But this reflects nothing other than our evolutionary origins. Evolution has programmed
        us to be positively interested in plants and animals, our food, and to be repelled by
        slimes and oozes, teeming with potentially harmful microbes. These emotional responses
        colour our view of ecology, for example, in a way that has no parallel in other sciences:
        physicists do not just study particles that they find pretty. No ecologist wants to study
        the rich ecosystem that each of us carries around inside our gut, because evolution has
        programmed our brains to find bottom-related matters disgusting. I think it likely that
        naturalists from a different planet, silicon entities evolved under very different
        circumstances, would find tropical forests uninteresting (mainly primary producers with
        some herbivory and mutualisms) and animal guts fascinating, with their complex metabolic
        networks in which each node is manned by different species with wildly varying means of
        energy production.
        Our guts should be an ecological scientist's dream come true, ecological theatres that
        are replicated billions of times, which operate on a fast time scale and are easy to get
        to! (If aliens are ecologists, this would explain why they always ‘probe’ their abductees.)
        Many natural experiments are going on all the time as antibiotics and probiotics are
        administered and people find all sorts of different ways, voluntary or otherwise, to
        establish migration links between their gut ecologies. Microbiologists are increasingly
        interested in our guts from an ecological point of view but, unlike ecologists, they are
        used to faeces from their work in sewage plants.
        Two thousand years ago the Roman senator Cicero noted the creation of barren desert-like
        land in North Africa after the forests were felled for their timber, providing the earliest
        record of an ecosystem ‘service’ provided by forests—the stabilisation of soils. Other
        services provided by biodiversity readily come to mind, like pollination and carrion
        clean-up, and there may be many more. But perhaps the clearest example of an ecosystem
        service provided by biodiversity comes from our gut. Throughout our history, until very
        recently, we all had worms. In rich countries we have quite happily eradicated them from
        our inner ecosystems with none of the handwringing we expend on rhinos. But it is
        increasingly believed that the loss of worms from our internal ecology is responsible for
        the upsurge in inflammatory bowel disorders such as Crohn's disease and colitis. In fact,
        there are clinical trials underway in the United States testing the efficacy of worms as
        treatment for these diseases. The mechanism is clear: worms trigger one arm of the immune
        system which down-regulates another, inflammatory arm. Our immune system has evolved to
        expect a certain constellation of species in our gut: in that context, worms provide an
        ecosystem service of balancing the immune system.
        Our perception of our impact on the planet as equivalent to a mass extinction simply
        reflects the evolutionary prism through which we view life. Of course, we may yet live up
        to our own publicity and pull off something apocalyptic like a runaway greenhouse that
        sterilises the Earth. But it is at least as likely that the microbial world, resentful at
        being either ignored or exterminated, will come up with something to consign us to a
        footnote in the history of life when it is ultimately written by the silicon entities. The
        Spanish flu, SARS, and HIV have just been early experiments.
      
    
  

  
    
      
        
        Organisms of vastly differing morphologies, ecologies, and behaviors—such as fruit flies
        and humans—are now known to share a multitude of molecular, cellular, and developmental
        processes. Not only is there extensive similarity in the sequences of fly and human genes,
        but in addition, almost all of the proteins and major signal transduction pathways that
        control cell division and differentiation in mammals are also found in the fruitfly 
        Drosophila melanogaster (Rubin et al. 2000;
        http://flybase.bio.indiana.edu/). Components in these pathways perform the same biochemical
        functions and act in the same order in both fruitfly and mammalian cells.
        Evolutionary conservation is of considerable practical and theoretical importance to
        biologists. First, it provides a valuable source of data for the reconstruction of
        phylogeny (Salemi and Vandamme 2003). Evolutionary connections between organisms that were
        once hidden by morphology have now been exposed in genomic analyses. Second, the
        conservation of evolutionary processes or traits is a prime area of investigation in
        theoretical evolutionary biology (Gould 2002). What can, and cannot, be changed
        evolutionarily? In a constantly evolving world, how can any biological system or trait
        survive unchanged (Van Valen 1973)? Finally, conservation provides fundamental insights
        into how complex biological systems, such as immunity, are assembled, maintained, and
        altered in evolution.
      
      
        Elements of Immunity
        “Innate” immunity refers to the variety of physical, cellular, and molecular features
        that provide the first lines of defense against infections. The relatively quick innate
        immune responses operate along with slower but more targeted adaptive immune responses that
        generate antigen-specific mechanisms that eventually lead to the destruction and
        elimination of the pathogen.
        In mammals, the skin and the epithelial lining of the mucosal tissues act as the primary
        nonspecific barriers, impeding infectious agents from entering the body. The mucous
        membrane barrier traps microorganisms, and the cilia present on the epithelial cells assist
        in sweeping the microbes towards the external openings of the respiratory and
        gastrointestinal tracts. If infectious agents gain entry into the body, internal innate
        immune responses become activated and rapidly eliminate the infection. Internal innate
        immune agents and responses include (amongst others) low pH of the stomach and vagina,
        proteolytic enzymes and bile in the small intestine, and phagocytosis.
        Phagocytosis is a fundamental innate immune mechanism carried out by a number of
        different cell types, including macrophages. Specific macrophage subpopulations are
        associated with different tissues (alveolar macrophages in the lung, microglial cells in
        the central nervous system, etc.). Their main function is to consume microorganisms, other
        foreign substances, and old, dying cells.
        Innate immunity is present from birth, and the information for innate immune responses
        is inherited. Cells in the mammalian innate immune system (e.g., macrophages) detect
        “microbial nonself” by recognizing pathogen-associated molecular patterns (PAMPs; Janeway
        1989). PAMPs are products of microbial metabolism that are conserved over evolution,
        distributed in a wide variety of pathogens, and not found in host cells. Lipopolysaccharide
        is an example of a PAMP and is found in bacteria, viruses, and fungi. Receptors, called
        pattern recognition receptors, are present on surfaces of host cells and recognize PAMPs.
        When activated, pattern recognition receptors induce intracellular signaling via the
        transcription factor NF-κB, resulting in the activation of genes involved in host
        defense.
        Adaptive immunity is characterized by greater specificity than innate immunity, as the
        adaptive immune response can not only distinguish foreign cells from self, but can also
        distinguish one foreign antigen from another. Another hallmark of adaptive immunity is
        memory, which enables the body to remember specific adaptive responses in response to
        specific antigens. Immunological memory allows the body to make a greater and more rapid
        second response when the body is reinfected by the same pathogen. Immunological memory
        underlies both immunization and resistance to reinfection, conferring a tremendous
        evolutionary advantage to vertebrates. The adaptive immune response has nearly infinite
        flexibility: the T and B lymphocytes of the acquired immune system can rearrange the
        elements of their immunoglobulin and T-cell receptor genes to create billions of clones
        with distinct antigen receptors. In organisms where both innate and acquired immune systems
        are present, there is a clear interdependence between the two systems. For a fully
        functional immune system, these components must act in synergy.
      
      
        Innate Immunity in Drosophila
        Because it lacks an adaptive immune response, 
        Drosophila melanogaster serves as a wonderful model for studying
        aspects of the innate immune system that might otherwise be obscured by the actions of the
        adaptive immune response. Insects defend themselves against parasites and pathogens by
        invoking a multitude of innate immune responses (Figure 1; for more details, see recent
        reviews by Hoffmann and Reichhart [2002], Hultmark [2003], Brennan and Anderson [2004],
        Meister [2004], and Theopold et al. [2004]). Like humans, 
        Drosophila protects itself against microbes and parasites via
        epithelial barriers: for example, epithelial cells of the trachea, gut, genital tract, and
        Malpighian tubules produce antimicrobial peptides (local response).
        Once within the body cavity, microbes may be consumed by the phagocytic blood cells
        called plasmatocytes (Figure 1). Larger pathogens (such as eggs of parasitic wasps) are
        inactivated by encapsulation, an immune response carried out by specialized cells called
        lamellocytes (Figure 1). Lamellocytes differentiate in response to macroscopic pathogens,
        and their precursors are thought to reside in the larval lymph gland. The transcription
        factors (GATA, Friend-of-GATA, and Runx family proteins) and signal transduction pathways
        (Toll/NF-κB, Serrate/Notch, and JAK/STAT) that are required for specification and
        proliferation of blood cells during normal hematopoiesis, as well as during the
        hematopoietic proliferation that accompanies immune challenge, are conserved (Evans et al.
        2003; Meister 2004). In this issue of 
        PLoS Biology , Crozatier et al. (2004) identify the transcription factor
        Collier as being critical for the differentiation of lamellocytes in 
        Drosophila . The mammalian ortholog of Collier (Early B-cell
        Factor) is involved in B-cell differentiation in mice.
        In addition to triggering cellular immune responses, invading pathogens also activate
        humoral reactions. Microbes induce the rapid secretion of antimicrobial peptides from the
        cells of the fat body into the larval or adult body cavity (systemic response; Figure 1). A
        microbial infection initiates a zymogen cascade that plays a crucial role in the activation
        of the antimicrobial genes in the fat body. Infection or wounding also triggers a
        protein-cleaving cascade that results in the production of toxic intermediates and melanin
        around microbes or wound sites. This proteolytic cascade is similar to the vertebrate
        clotting cascade. 
        Drosophila hemolymph also coagulates and participates in host
        defense and wound healing (Figure 1; Theopold et al. 2004). Given the evolutionary success
        of insects, this combination of defense mechanisms has proven to be extremely effective,
        allowing insects to thrive in septic environments.
      
      
        NF-κB Activation: The Toll and Imd Pathways of Drosophila
        The 
        Drosophila genome encodes several members of the multifunctional
        Toll family of receptors (Beutler and Rehli 2002). Mutations in the 
        Drosophila Toll gene (as well as in other components in the
        pathway) make the fly susceptible to fungal or gram-positive bacterial infections. However,
        Toll does not act as a pattern recognition receptor in the fly; instead its activation
        depends on the presence of the processed (active) form of the growth-factor-like
        polypeptide Spätzle. Processing of Spätzle depends on a serpin-controlled proteolytic
        cascade (Figure 2).
        While components of the 
        Drosophila Toll pathway were identified in earlier genetic
        screens for developmental mutants, those in the Imd pathway have been the focus of more
        recent studies, mainly in the context of 
        Drosophila immunity (Hoffmann and Reichhart 2002; Hultmark
        2003). The effector NF-κB transcription factor of the Imd pathway is Relish, which upon
        immune activation is cleaved by the Dredd caspase (Figure 2). Using a combination of the
        RNA interference approach of silencing gene function and a high-throughput cell culture
        assay, Foley and O'Farrell (2004) report the identification of two new conserved members of
        this Imd pathway: Sickie is a novel protein required for Relish activation, and Defense
        repressor 1 is a novel inhibitor of the Dredd caspase.
        The impressive progress in our understanding of innate immunity in 
        Drosophila is now guiding scientists to explore the immune
        system of other insects such as the mosquito, 
        Anopheles gambiae , that spreads human malaria. Immune responses
        in this mosquito are linked to the elimination of the malarial parasites (Osta et al.
        2004). A comparison of the immunity-related genes in 
        Anopheles and 
        Drosophila reveals the presence of the Toll signaling pathway in
        the mosquito genome, even though there are some differences in genes encoding pathogen
        recognition and signal transduction molecules (Christophides et al. 2002). A detailed and
        comparative view of the genetic mechanisms underlying their host defense will contribute to
        the identification of new targets for insecticide development, and provide opportunities
        for controlling the transmission of pathogens.
      
      
        Concluding Remarks
        The homologs of many genes involved in innate immune responses in flies and humans have
        also been found in mice, sharks, nematodes, and plants (e.g., Pujol et al. 2001; Nurnberger
        and Brunner 2002). In species studied to date, host defense appears to be mediated by
        homologous proteins. Taken together, these findings suggest that the regulatory mechanisms
        of host defense may be hard-wired in the genome much as DNA replication and cell division
        are. Protein motifs, domains, and signaling elements have, for millions of years, not only
        retained their ancestral biochemical features but have also continued to participate in
        similar physiological responses. It is crucial that our evolving knowledge of “genomic
        recycling” be used to enhance our understanding of the evolution of humans, not only in the
        context of “descendants of ancient apes,” but in the larger context of our fundamental
        unity and shared genetic history with all other species. This simple but fundamental idea
        has yet to be adopted by the majority of our students and teachers. Unless we do more to
        overcome resistance to the idea that humans share deep evolutionary connections with all
        animal life, students will become increasingly isolated from an understanding of, and
        participation in, the genomics and bioinformatics revolution that is transforming the
        biological and biomedical sciences.
      
    
  

  
    
      
        
        
          
            
              “It was not only difficult for him to understand that the generic term
              dog embraced so many unlike specimens of differing sizes and different forms; he was
              disturbed by the fact that a dog at three-fourteen (seen in profile) should have the
              same name as the dog at three-fifteen (seen from the front).…Without effort, he had
              learned English, French, Portuguese, Latin. I suspect, nevertheless, that he was not
              very capable of thought. To think is to forget a difference, to generalize, to
              abstract. In the overly replete world of Funes there were nothing but details, almost
              contiguous details.” —Jorge Luis Borges, “Funes the Memorius”
            
          
        
        We are told scientists are divided into experimentalists and theoreticians. The
        dialectic description of the dynamics of science, with one tribe gathering data and
        collecting evidence and another tribe providing form to these observations, has striking
        examples that argue for the importance of synthesis. The 16th century revolution, which
        settled the way in which we see the sky today, is probably one of the best examples of how
        comparatively ineffective each of these tribes can be in isolation. Tycho Brahe, the
        exquisite observer, who built, calibrated, and refined instruments to see in the sky what
        no one else could, collected the evidence to prove a theory that Copernicus had already
        stated years before (in a book he dedicated to the Pope). It was only many years later that
        Galileo established the bridge between theory and observation; he understood the data in
        terms of the theory and thereby cemented the revolution. Copernicus's statements, showed
        Galileo, were not only figments of his imagination; they were an adequate description of
        the universe as he and Brahe had observed it.
        Since my first steps in biology, after a prompt departure from physics and mathematics,
        I have looked for such encounters between theory and experiment. I began studying the
        visual system and the series of fundamental works by Atneave (1954), Barlow (1960), and
        Atick (1992) on the relationship between our visual world and the way the brain processes
        it. Their research was based on a simple hypothesis: (1) the images that we see are highly
        redundant and (2) the optic nerve is a limited channel, thus the retina has to compress
        information. Compression, redundancy? How do such concepts relate to the biology of the
        brain?
        In the middle of the last century, working on the problem of communications, languages,
        codes, and channels, Claude Shannon proposed a very elegant theory that formalized
        intuitive ideas on the essence (and the limits) of communications (Weaver and Shannon
        1949). One of its key aspects was that, depending on the code and on the input, channels
        are not used optimally and thus do not carry all the information they potentially could.
        When we compress (zip) a file, we are actually rewriting it in a more efficient (though not
        necessarily more intelligible) language in which we can store the same amount of
        information in less space. This compression has, of course, a limit (we cannot convey the
        universe in a single point), and the mere existence of an optimal code is central to
        Shannon's idea. Attneave was probably the first to think that these ideas could help
        unravel how the brain worked, and in a long series of works relating these ideas to
        experimental data, it was shown that the retina's business is mainly to get rid of the
        redundancies in the visual world.
        About four years ago, Jacob Feldman published a paper, similar in spirit, proposing a
        simple explanation for a long-standing problem in cognitive science about why some concepts
        are inherently more difficult to learn than others (Feldman 2000). An article whose first
        reference is to work carried out 50 years previously makes us suspect that an important gap
        is being filled. As in the previous experiments, Feldman borrowed a theory—he did not
        invent it—to explain longstanding and previously unexplained research in this area.
        Feldman's idea was based on a theory developed by Kolmogorov that established formal
        mathematical grounds to define and measure complexity. Kolmogorov's theory was focused on
        categories, which are just subsets of a universe, a bunch of exemplars that constitute a
        group. “Dogs” is a category in the universe of animals. Different statements can define the
        same category, for example, “big dogs” and “dogs that are not small” are the same group,
        and some information in the statement may be irrelevant because it does not aid in
        determining which elements belong or not. In the same way that Shannon spoke of a
        non-redundant code, Kolmogorov showed that categories could be defined with an optimal
        (non-redundant) statement. The length of this statement defines a measure of complexity
        termed Kolmogorov complexity.
        To visualize the intuitive nature of his theory it helps to do a thought experiment.
        Imagine a set of objects defined by, say, three features: form, color, and size. And
        imagine, for simplicity, that each feature is binary, that is, there are only two possible
        cases. Size is big or small, color is yellow or red, and shape can be either a triangle or
        a circle. This defines, of course, a universe of eight objects. We can now define
        categories within this universe: for example, all the circles (a category of four
        exemplars), or all the big and yellow objects (two exemplars), or all the triangles that
        are not red (again two) (see Figure 1). We can also define a category by enumeration, for
        example, the small red triangle, the big yellow circle, and the small yellow circle (three
        exemplars). Some rules (and thus the groups defined by these rules) are intuitively easier
        to define than others. “All the circles” is an easier statement to make (and probably to
        remember) than “small circles and yellow big objects.” This notion of difficulty is what
        Kolmogorov's theory formalized, stating that complexity was the length of the shortest
        definition (among all the possible definitions) of a given set. From this thought
        experiment, we can understand the logic of Feldman's paper, which showed that. Kolmogrov
        complexity is very closely related to our intuitive notion of conceptual difficulty.
        Feldman presented subjects with all possible categories (of a fixed number of exemplars) in
        different universes and showed that the critical parameter to rank the difficulty of a
        given subset was its Kolmogrov complexity. Moreover by explicitly presenting all the
        members and the nonmembers of a category to naïve subjects, he showed that we can
        spontaneously reduce a category to its minimal form and remember it without any explicit
        instruction. Thus, what Feldman found, following the original ideas of Shepard, was that
        our psychological measure of complexity—our difficulty in defining and remembering a
        category or concept—is also determined by the Kolmogorov complexity that describes it.
        This essay is, in a way, about how we avoid becoming Borges's character Funes, who could
        not understand repeated observations as exemplars of a common rule and thus could not
        synthesize and categorize. Simply, he could not think. Probably the most disappointing
        moment of Feldman's paper comes at the very end, where it deals with its (somehow
        unavoidable) recursive quest. Understanding why some concepts are difficult to learn may
        itself be difficult to learn. Modern mathematics, together with Kolmogorov complexity and
        information theory, has taught us another fundamental concept that may be relevant when
        trying to understand the logic of the mind. In a long series of paradoxes enumerated by
        Bertrand Russell, Kurt Goedel, and others, we learn that a formal system that looks at
        itself is bound to fail. At the very end of his paper, Feldman writes, “In a sense, this
        final conclusion [that psychological complexity is Boolean complexity] may seem negative:
        human conceptual difficulty reflects intrinsic mathematical complexity after all, rather
        than some idiosyncratic and uniquely human bias.” Who invented mathematics? The Martians?
        On the contrary, I believe this result supports a more naturalistic and less Platonic
        conception of mathematics. Formal ideas in mathematics are not arbitrary constructions of
        an arbitrary architecture; rather, they reflect the workings of the brain like a massive
        collective cognitive experiment. Mathematics does not only serve to help us understand
        biology; mathematics is biology. We are not less original if our thoughts resemble our
        mental constructions, we are just consistent. It is within this loop, this unavoidable
        recursion—mathematics understanding the logic of the brain—that we will have an opportunity
        to test, as some conspire, whether among all the wonders evolution has come out with, the
        ultimate might be a brain good enough to avoid the risk of understanding itself.
      
    
  

  
    
      
        
        I need a “tricorder”—the convenient, hand-held device featured on 
        Star Trek that can detect life forms even from orbit. Unfortunately, we
        don't have a clue how a tricorder might work, since life forms don't seem to have any
        observable property that distinguishes them from inanimate matter. Furthermore, we lack a
        definition of life that can guide a search for life outside Earth. How can we find what we
        can't define? An answer may lie in the observation that life uses a small, discrete set of
        organic molecules as basic building blocks. On the surface of Europa and in the subsurface
        of Mars, we can search for alien but analogous patterns in the organics.
      
      
        Life As We Know It
        The obvious diversity of life on Earth overlies a fundamental biochemical and genetic
        similarity. The three main polymers of biology—the nucleic acids, the proteins, and the
        polysaccarides—are built from 20 amino acids, five nucleotide bases, and a few sugars,
        respectively. Together with lipids and fatty acids, these are the main constituents of
        biomass: the hardware of life (Lehninger 1975, p 21). The DNA and RNA software of life is
        also common, indicating shared descent (Woese 1987). But with only one example of life—life
        on Earth—it is not all that surprising that we do not have a fundamental understanding of
        what life is. We don't know which features of Earth life are essential and which are just
        accidents of history.
        Our lack of data is reflected in our attempts to define life. Koshland (2002) lists
        seven features of life: (1) program (DNA), (2) improvisation (response to environment), (3)
        compartmentalization, (4) energy, (5) regeneration, (6) adaptability, and (7) seclusion
        (chemical control and selectivity). A simpler definition is that life is a material system
        that undergoes reproduction, mutation, and natural selection (McKay 1991). Cleland and
        Chyba (2002) have suggested that life might be like water, hard to define
        phenomenologically, but easy to define at the fundamental level. But life is like fire, not
        water—it is a process, not a pure substance. Such definitions are grist for philosophical
        discussion, but they neither inform biological research nor provide a basis for the search
        for life on other worlds.
        The simplest, but not the only, proof of life is to find something that is alive. There
        are only two properties that can determine if an object is alive: metabolism and motion.
        (Metabolism is used here to include an organism's life functions, biomass increase, and
        reproduction.) All living things require some level of metabolism to remain viable against
        entropy. Movement (either microscopic or macroscopic) in response to stimuli or in the
        presence of food can be a convincing indicator of a living thing. But both metabolism
        (fire) and motion (wind) occur in nature in the absence of biology.
        The practical approach to the search for life is to determine what life needs. The
        simplest list is probably: energy, carbon, liquid water, and a few other elements such as
        nitrogen, sulfur, and phosphorus (McKay 1991). Life requires energy to maintain itself
        against entropy, as does any self-organizing open system. In the memorable words of Erwin
        Schrödinger (1945), “It feeds on negative entropy.” On Earth, the vast majority of life
        forms ultimately derive their energy from sunlight. The only other source of primary
        productivity known is chemical energy, and there are only two ecosystems known, both
        methanogen-based (Stevens and McKinley 1995; Chapelle et al. 2002), that rely exclusively
        on chemical energy (that is, they do not use sunlight or its product, oxygen).
        Photosynthetic organisms can use sunlight at levels below the level of sunlight at the
        orbit of Pluto (Ravens et al. 2000); therefore, energy is not the limitation for life.
        Carbon, nitrogen, sulfur, and phosphorus are the elements of life, and they are abundant in
        the Solar System. Indeed, the Sun and the outer Solar System have more than 10,000 times
        the carbon content of the bulk of Earth (McKay 1991). When we scan the other worlds of our
        Solar System, the missing ecological ingredient for life is liquid water. It makes sense,
        then, that the search for liquid water is currently the first step in the search for life
        on other worlds. The presence of liquid water is a powerful indication that the ecological
        prerequisites for life are satisfied.
        Orbital images, such as the canyon in Figure 1, show clear evidence of the stable and
        repeated, if not persistent, flow of a low-viscosity fluid on Mars at certain times in its
        past history. The fluid was probably water, but the images could also suggest wind, ice,
        lava, even carbon dioxide or sulfur dioxide. Recently, results from the Mars Exploration
        Rover missions have shown that this liquid carried salts and precipitated hematite in
        concretions. The case for water, we could say, is tight.
        On Jupiter's moon Europa, the cracks and icebergs on the surface of the ice indicate
        water beneath the ice, but not necessarily at the present time. Present water on Europa is
        indicated by the magnetic disturbance Europa makes as it moves through Jupiter's magnetic
        field, not unlike the way coins in the pocket of a passenger will set off an airport metal
        detector. Europa has a large conductor, and this is most likely a global, salty layer of
        water.
      
      
        Viking on Mars: Been There, Tried That
        The Viking missions to Mars in the late 1970s were the first (and as yet, the only)
        search for life outside Earth. Each Viking conducted three incubation experiments to detect
        the presence of metabolism in the Martian soil. Each lander also carried a sophisticated
        Gas Chromatograph Mass Spectrometer for characterizing organic molecules. The results were
        unexpected (Klein 1978, 1999). There was a detectable reaction in two of the incubation
        experiments. In the “Gas Exchange” experiment, a burst of oxygen was released when the soil
        was exposed to water. The “Labeled Release” experiment showed that organic material was
        consumed, and that carbon dioxide was released concomitantly. In the Labeled Release
        experiment, this reaction ceased if the soil was first heated to sterilizing temperatures,
        but the reaction of the Gas Exchange Experiment persisted.
        If considered alone, the Labeled Release results would be a plausible indication for
        life on Mars. However, the Gas Chromatograph Mass Spectrometer did not detect the presence
        of any organic molecules in the soil at level of one part per billion (Biemann 1979). It is
        difficult to imagine life without associated organic material, and this is the main
        argument against a biological interpretation of the Viking results (Klein 1999; but cf.
        Levin and Straat 1981). It is also unlikely that the oxygen release in the Gas Exchange
        experiment had a biological explanation, because the reaction was so rapid and persisted
        after heating. It is generally thought that the reactivity observed by the Viking biology
        experiments was caused by one or more inorganic oxidants present in the soil, and was
        ultimately produced by ultraviolet light in the atmosphere. Consistent with the apparently
        negative results of the Viking biology experiments, the surface of Mars also appears to be
        too dry for life. Indeed, conditions are such that liquid water is rare and transient, if
        it occurs at all (e.g., Hecht 2002).
      
      
        It's Life, Jim, but Not As We Know It
        Table 1 shows a categorization of life as we have observed it. Using this diagram, we
        can speculate about how life might be different on Mars or Europa. At the bottom of the
        table, life is composed of matter—a reasonable assumption for now. Carbon and liquid water
        are the next level; this makes Mars and Europa likely candidates, because they have carbon
        and have, or have had, liquid water. Other worlds may have a different chemical baseline
        for life. The usual speculation in this area is that the presence of ammonia and silicon,
        rather than water and carbon, might be preconditions for life on other planets. Such
        speculation has yet to lead to any specific suggestions for experiments, or to new ways to
        search for such life, but this may just reflect a failure of human imagination rather than
        a fundamental limitation on the nature of life.
        Life on Mars is also likely to be the same at the top of the table: at the ecological
        level. Primary production in a Martian ecosystem is likely to be phototrophic, using carbon
        dioxide and water. Heterotrophs are likely to be present to consume the phototrophs and in
        turn to be consumed by predators. Darwinian evolution would result in many of the same
        patterns we see in ecosystems on Earth. While it may be similar at the top (ecological) and
        bottom (chemical) levels, life on Mars could be quite alien in the middle, in the realm of
        biochemistry. Pace (2001) has argued that alien biochemistry will turn out to be the same
        as biochemistry on Earth, because there is one best way to do things and that natural
        selection will ensure that life everywhere discovers that way. Only observation will tell
        if there is one possible biochemistry, or many.
        Future missions to Mars might find microfossils in sedimentary rocks such as those at
        Meridiani Planum. Microbes don't readily form convincing fossils; the one exception may be
        the strings of magnetite formed by magnetotactic bacteria (Friedmann et al. 2001). As
        interesting as fossils might be, we could not be sure that a fossil found on Mars was not
        merely another example of Earth life. We know that rocks have come to Earth from Mars, and
        it is possible that such rocks could have carried life between the planets (Mileikowsky et
        al. 2000; Weiss et al. 2000). Finding fossil evidence for life on Mars does not demonstrate
        a second genesis in our Solar System.
      
      
        Finding a Way to Search for Alien Life
        If we were to find organic material in the subsurface of Mars or on the ice of Europa,
        how could we determine whether it was the product of a system of biology or merely abiotic,
        organic material from meteorites or photochemistry? If this life were in fact related to
        Earth life, this should be easy to determine. We now have very sensitive methods, such as
        PCR and fluorescent antibody markers, for detecting life like us. This case would be the
        simplest to determine, but it would also be the least interesting. If the life turned out
        to be truly alien, then the probes specific to our biology would be unlikely to work. What,
        then, could we do to determine a biological origin?
        The question is open and possibly urgent. On space missions already being planned, we
        may have the opportunity to analyze the remains of alien organics on the surface of Europa
        or frozen below ground on Mars. The instruments that will make this analysis must be
        designed in the next couple of years.
        One approach appears promising. I call it the “Lego Principle.” It is based on the
        patterns of the molecules of life. Biological processes, in contrast to abiotic mechanisms,
        do not make use of the range of possible organic molecules. Instead, biology is built from
        a selected set. Thus, organic molecules that are chemically very similar to each other may
        have widely different concentrations in a sample of biological organics. An example of this
        on Earth is the 20 amino acids used in proteins and the use of the left-handed version of
        these amino acids. The selectivity of biological processes is shown schematically in Figure
        2 by the distribution of spikes in contrast to a smooth, nonbiological distribution.
        General arguments of thermodynamic efficiency and specificity of enzymatic reactions
        suggest that this selectivity is required for biological function and is a general result
        of natural selection. Different life forms are likely to have different patterns, and at
        the very least we might find the mirror symmetry of life on Earth, with 
        d - instead of 
        l -amino acids.
        This approach has immediate practical benefit in the search for biochemistry in the
        Solar System. Samples of organic material collected from Mars and Europa can be easily
        tested for the prevalence of one chirality of amino acid over the other. More generally, a
        complete analysis of the relative concentration of different types of organic molecules
        might reveal a pattern that is biological even if that pattern does not involve any of the
        familiar biomolecules. Interestingly, if a sample of organics from Mars or Europa shows a
        preponderance of D-amino acids, this would be evidence of life, and at the same time would
        show that this life was distinct from Earth life. This same conclusion would apply to any
        clearly biological pattern that was distinct from that of Earth life.
        Organic material of biological origin will eventually lose its distinctive pattern when
        exposed to heat and other types of radiation, (examples of this include the thermal
        racemization of amino acids), but at the low temperatures in the Martian permafrost,
        calculations suggest that there has been no thermal alteration (Kanavarioti and Mancinelli
        1990). An interesting question, as yet unanswered, is how long organic material frozen into
        the surface ice of Europa would retain a biological signature in the strong radiation
        environment.
        On Europa, the organic material for our tests might be collected right from the dark
        regions on the surface. On Mars, there is ice-rich ground in the cratered southern polar
        regions (Feldmann et al. 2002), which presumably overlies deeper, older ice. The surprise
        discovery of strong magnetic fields in the southern hemisphere of Mars (Acuña et al. 1999;
        Connerney et al. 1999) indicates that the area may be the oldest undisturbed permafrost on
        that planet. Like the mammoths extracted from the ice in Siberia, any Martian microbes
        found in this ice would be dead, but their biochemistry would be preserved. From these
        biological remains, it would then be possible to determine the biochemical composition of,
        and the phylogenetic relationship between, Earth life and Martian life. We may then have,
        for the first time, a second example of life.
      
    
  

  
    
      
        
        Diatoms, unicellular algae with ornate silica shells, have fascinated amateur and
        professional biologists ever since the invention of the microscope. But these days, diatoms
        and their exquisite shells are also attracting the attention of nanotechnologists who hope
        that diatoms will teach them how to make minute structures currently beyond the
        capabilities of materials scientists. And now these nanotechnologists, together with
        ecologists interested in the global carbon cycle—in which diatoms play a central role—have
        a genomic blueprint to help them in their studies: the annotated genome sequence of 
        Thalassiosira
        pseudonana (http://genome.jgi-psf.org/diatom/).
      
      
        What Are Diatoms?
        Diatoms, microalgae that are found in all aquatic and moist environments, first appeared
        more than 180 million years ago. Since then, diatom diversity has literally exploded; no
        one is sure how many living species there are—probably about 100,000—or why there are so
        many different types. Plant molecular biologist Chris Bowler (Ecole Normale Supérieure,
        Paris, France and Stazione Zoologica, Napoli, Italy) explains that molecular phylogeny and
        morphological studies suggest that diatoms originated ‘probably as the result of a
        eukaryote being invaded or engulfed by a photosynthetic eukaryote, most probably a red
        alga’.
        The basic structure of all diatoms is similar: a single cell, often with a large
        vacuole, contained within a silica shell or frustule made of two overlapping halves or
        valves joined by girdle bands, which are also made of silica. The girdle bands form the
        rims of the two valves and allow unidirectional growth of the diatom during vegetative
        division. ‘The shell is rather like a Camembert cheese box or a petri dish’, explains
        marine ecologist Christian Hamm (Alfred Wegener Institute for Polar and Marine Research,
        Bremerhaven, Germany).
        There are only two main types of diatom: centric diatoms, which often have a circular
        symmetry, and pennate diatoms, which are usually bilaterally symmetrical. Nevertheless,
        diatom shells come in a dazzling array of forms and sizes (Figure 1; Box 1). ‘They can be
        circular, oval, stick-shaped, you name it, and range from several micrometres large to
        about a millimetre’, says ecologist Mary Ann Tiffany (San Diego State University,
        California, United States), who is using scanning electron microscopy to examine diatom
        valve formation as part of her graduate studies. ‘When a diatom divides, each daughter cell
        makes a new half shell’, explains Tiffany. The first stage of construction is the
        generation and deposition of silica nanospheres; the more ornate structures are built up
        from there. Both the finished shells, with their precise and reproducible nanometre-scale
        features, and the intermediate structures that lead up to the finished product, could be of
        interest to nanotechnologists, suggests Tiffany.
      
      
        Turning to Nature for Engineering Solutions
        Richard Gordon, Professor of Radiology at the University of Manitoba in Winnipeg,
        Canada, somewhat accidentally laid the foundations of ‘diatom nanotechnology’ in 1988 when
        he was invited to give a lecture at an engineering conference. ‘I'm not an engineer’,
        explains Gordon, ‘but I knew engineers were interested in what was then called
        microfabrication so I told them about diatoms because they are so good at making small
        things’. Gordon, a keen diatom hobbyist, explained to his audience how diatoms could make a
        three-dimensional micro- or nanoscale structure for them without them lifting a finger. By
        contrast, says Gordon, ‘nanotechnology techniques then and now are tedious, involving
        painstakingly building three-dimensional structures up layer by layer’.
        Such tedious techniques are currently used in the semiconductor industry. At present,
        explains Michael Sussman, Director of the Biotechnology Center at the University of
        Wisconsin-Madison (Madison, Wisconsin, United States), ‘features are etched onto circuit
        boards using light. However, the wavelength of light limits the smallest size that can be
        achieved, and for the next generation of faster computers, engineers need to get denser
        features onto computer chips than is possible with light etching’. Diatoms, says Sussman,
        ‘are natural-born lithographers in the nanometre range. If we could work out how diatoms
        lay down micro lines of silica, then we may be able to simulate it’. The proteins that
        diatoms use to direct silica deposition could be very useful to the semiconductor industry,
        says Sussman.
        There are other ways in which diatoms could help us clumsy humans build nanoscale
        ‘widgets’. Molecular biologist Mark Hildebrand (Scripps Institution of Oceanography, San
        Diego, California, United States) is a member of a collaborative project trying to develop
        genetically engineered micro/nanodevices (also called GEMs). Already, engineers are using
        diatoms to help them build extremely sensitive sensors based on microfluidic devices, he
        explains. Hildebrand is also interested in the optical properties of diatoms. ‘Information
        processing technology is moving from electronically to optically based hardware, which
        allows more information to be carried and stored. Optical systems need materials with
        regularly repeating structures with features below the micrometre size range. These are
        very difficult to make by standard manufacturing techniques, but diatoms make structures
        like this all the time’.
        It might also be possible to use diatom shells as delivery vehicles for drugs, suggests
        chemical engineer Tony Rogers, an assistant professor at Michigan Technological University
        (Houghton, Michigan, United States). ‘They have a uniform nanoscale pore structure and are
        chemically inert and biocompatible’. Rogers envisages loading diatoms with a drug that
        would then leach out into the blood stream at a rate dependent on the diatom species used.
        By incorporating ferromagnetic particles within the diatom structure, it might be possible
        to use a magnet to guide the drug to the right organ, he suggests.
        Diatom structures are not just of interest to people interested in tiny objects. As Hamm
        comments, ‘in diatoms, Nature has solved many of the problems that engineers want to solve.
        For example, diatoms are particularly good at making lightweight but strong structures.
        Because it is possible to scale static structures like shells, diatoms can teach us how to
        make lightweight constructions for the aerospace and car industry’.
        Some of the potential applications of diatoms can be investigated right now, using
        naturally occurring diatoms. In addition, subtle but important changes can be induced in
        diatoms by varying the amount of silica in their environment or changing the water flow.
        Gordon also envisages a device he calls a compustat, which would be used to select diatoms
        for a specific purpose. Diatoms taken from the sea, for example, would be individually
        examined using a computer-controlled microscope. ‘We would tell the computer what
        characteristics we were looking for, and it would go through the culture, zapping those
        diatoms furthest from the ideal with a laser beam. The culture would then be allowed to
        grow up again and the process repeated until we got the sort of diatoms we wanted’, says
        Gordon.
        Gordon has not built a compustat yet—it may not work, he says, because we don't know how
        far we can push diatoms by forced evolution. And even if the compustat does work, to make
        the most of the nanotechnological potential of diatoms, we need to know exactly how diatoms
        make their shells. At present, all we know is that silicon transporters and a group of
        long-chain, polyamine-containing proteins called silaffins, which act as nucleation points
        for silica deposition, are involved. This is where the diatom sequencing project at the
        United States Department of the Environment's Joint Genome Initiative (JGI) at Walnut
        Creek, California, comes in.
      
      
        The First Diatom Is Sequenced
        Daniel Rokhsar, Department Head for Computational Genomics at JGI, explains why his
        institute undertook the sequencing and computer annotation of the genome of 
        T. pseudonana , a marine centric diatom ‘We believe that knowing
        this genome will help us to figure out how to mimic the processes that diatoms use to
        construct their very precise structures, and that we can then learn how to create similarly
        precise structures ourselves’. Also, he adds, diatoms are extremely important on an
        ecological level.
        Oceanographer Ginger Armbrust (University of Washington, Seattle, Washington, United
        States), Principal Investigator on the sequencing project for 
        T. pseudonana , explains further. Diatoms are responsible for
        between 25% and 40% of all the primary productivity of the oceans, she says. ‘They also
        keep the biological pump going. By fixing carbon dioxide and then sinking, diatoms draw
        carbon dioxide out of the atmosphere and take it into the deeper waters of the ocean, where
        it is retained for longer than it would be if the diatoms stayed near the surface’.
        
        T. pseudonana , she continues, was chosen as the first diatom to
        sequence in part because it has a small genome, but mainly because it represents a
        cosmopolitan genus of diatoms and its physiology has been well studied. Once the primary
        sequence of the genome had been determined, molecular biologists, oceanographers, and
        ecologists from around the world gathered at JGI for a ‘genome jamboree’. ‘The first of
        these was in October 2002, a massive brainstorming session at which we all dug around in
        the genome for our favourite genes and tried to get a feel for what was there’, explains
        Bowler. ‘It was really refreshing to get the insights of oceanographers and ecologists into
        what this genome was telling us’.
        Among other things, Armbrust and her collaborators are interested in finding out what
        the 
        T. pseudonana genome can tell them about the difference between
        photosynthesis on land and in the sea. They also want to investigate how these organisms
        adapt to their environment. ‘Now that we have the genome’, says Armbrust, ‘we can
        investigate how gene expression varies at different places in the water column, for
        example. This will be the first time a eukaryotic genome has been interpreted in this
        ecological sort of way’.
      
      
        What About Silicon Metabolism and the Nanotechnology Dream?
        ‘One of the striking things about the 
        T. pseudonana genome is that we can figure out quite a bit from
        it about how this diatom deals with organic materials, but it is hard to figure out what it
        is doing with silicon’, admits Rokhsar. ‘The only way we can really figure out what a gene
        is doing is by comparing it with known genes in other organisms, but because diatoms are so
        unique in their use of silicon, we don't have that option. We literally just have the parts
        list’.
        To get a hook on which of the 10,000 or so 
        T. pseudonana genes is important in silicon metabolism, Sussman
        is using microarrays to investigate how silicon concentrations affect gene expression
        patterns in the diatom. ‘There may be a few hundred genes whose expression changes in
        response to silicon stress’, he predicts, ‘and we can then focus on the role that these
        genes play in silicon metabolism’. In another approach, Hildebrand is purifying the
        proteins present in diatom shells. ‘Once we have isolated these proteins, we can get a
        little bit of protein sequence, and from there go back to the genome to pull the gene out’,
        he explains.
        In an ideal world, the next step would be to see what effect genetically altering the
        expression of the proteins identified by Sussman and Hildebrand has on the silica shell of 
        T. pseudonana . Unfortunately, this can't currently be done.
        ‘The only diatom we can genetically manipulate is 
        Phaeodactylum tricornutum , a pennate diatom’, explains Bowler. 
        P. tricornutum , he says, is the ‘lab rat’ of the diatom world
        but is much less important ecologically than 
        T. pseudonana (Figure 3). Bowler has previously determined the
        size of 
        P. tricornutum 's genome and is now leading a JGI project that
        is 75% of the way through sequencing the 
        P. tricornutum genome. ‘It will be critical to have this second
        genome’, notes Rokhsar, ‘because it will highlight what is unique to this group of
        organisms, and provide additional help in pulling out silicon metabolism genes’.
        Once the details of silicon metabolism have been revealed, the stage should be set for
        nanotechnologists to harness diatom proteins for the manufacture of nanodevices. ‘Whether
        we use those proteins inside the diatom or in test tubes remains to be seen, but one way or
        another, diatoms are harbouring a secret that engineers need to learn about’, says Sussman.
        Hildebrand agrees, noting how ‘important it is that materials scientists recognise the
        incredible ability of biology to make structures that could perhaps be incorporated in the
        design of nanotechnological widgets’.
        For Armbrust, it is the ecological insights are coming out of the 
        T. pseudonana genome sequencing project—which is part of a
        bigger JGI program on algal genomics—that are most exciting. ‘Already, multiple little
        insights are encouraging us to think differently about how diatoms perceive their
        environment and survive in it. We have also seen many things we can't figure out at all
        right now. My heart lies in the ecology of these organisms, but if we can generate
        information that leads to spinoffs for nanotechnology, that will be fantastic’, she
        concludes.
      
    
  

  
    
      
        
        The ongoing battle between hosts and pathogens has long been of interest to evolutionary
        biologists. Because hosts and pathogens act as environments for each other, their
        intertwined struggle for existence is both continual and rapid. At the molecular level,
        this cycle of environmental change and evolutionary response means that mutations are
        continually being tried out by natural selection. It is therefore little wonder that the
        host and pathogen genes that control infection and immunity frequently show high levels of
        genetic diversity and present some of the best examples of positive selection (adaptive
        evolution) reported to date (Yang and Bielawski 2000). In particular, rates of
        nonsynonymous substitution per site (resulting in an amino acid change; d
        N ) often greatly exceed those of synonymous substitution per site
        (silent change; d
        S ), as expected if most mutations are fixed because they increase
        fitness (Figure 1).
        At the host level, most studies of the selection pressures acting on immune system genes
        have concentrated on genes implicated in the adaptive immune response against microbial
        pathogens, particularly those producing antibodies (Sitnikova and Nei 1998; Sumiyama et al.
        2002), or genes encoding reconnaissance molecules known as the major histocompatibility
        complex (MHC), which control the action of T-cells (Hughes and Nei 1988; Yeager and Hughes
        1999) (Box 1). As its name suggests, the role of the adaptive immune response is to
        stimulate and ‘memorise’ immunity to specific pathogens. As microbial pathogens such as
        viruses are both abundant and rapidly evolving, positive selection on components of the
        adaptive immune response is often very strong (Yeager and Hughes 1999). Far less attention
        has been directed toward the less specific innate (‘nonadaptive’) immune response, even
        though this response requires a wide array of genes and acts as the front line of immune
        defence (Box 1). Would we expect the same strength of positive selection on a generalized
        pathogen control system? This is a question of fundamental importance because the luxury of
        adaptive immunity is not available to most organisms, having probably evolved along with
        the vertebrates (Bartl et al. 1994), whereas the more widespread innate immune system is
        often depicted as a primitive characteristic.
      
      
        Molecular Evolution of the Innate Immune System
        The genes involved in innate immunity have recently come under the molecular
        evolutionists' gaze. One important group are the defensins, a large class of short
        antimicrobial peptides that constitute an effective immune response team in organisms as
        diverse as plants and primates (Boman 1995). Because defensins are cationic (positively
        charged), they are able to interact with negatively charged molecules on the surface of
        microbes and permeate their membranes. Sequence analyses of defensins and similar
        antimicrobial peptides have revealed the telltale signatures of positive selection, with d
        N greater than d
        S in many comparisons (Hughes 1999; Duda et al. 2002; Maxwell et al.
        2003). Other genes of the innate immune system also seem to be subject to powerful positive
        selection. One dramatic example described in this issue of 
        PLoS Biology is the APOBEC3G gene of primates (Sawyer et al. 2004). This
        case is especially striking because rather than killing pathogens through protein or
        cellular interactions, like most immune genes, APOBEC3G works by manipulating the genome
        sequence of the invading microbe.
        The genomes of primates contain a family of nine APOBEC genes that encode enzymes
        involved in the editing of RNA and/or DNA through the deamination of cytosine (C), so that
        this nucleotide mutates to uracil (U). This is essential for various aspects of cellular
        function. APOBEC1, for example, is involved in the C→U editing of apolipoprotein B mRNA
        (therein christening the family), while another family member, the activation-induced
        deaminase, has a vital role in adaptive immunity in that it assists in the diversification
        of antibodies. Two more enzymes, APOBEC3G and APOBEC3F, form part of the innate immune
        system; they function as antiviral agents and are being intensively studied in the context
        of infection with the human immunodeficiency virus (HIV), the cause of AIDS. In particular,
        APOBEC3G targets the reverse transcription step of the HIV life cycle, in which the viral
        genomic RNA is converted into proviral DNA, which is then integrated into the host genome
        (Mangeat et al. 2003). APOBEC3G-induced deamination at this stage results in monotonous
        guanine-to-adenine (G→A) nucleotide changes, a phenomenon called G→A hypermutation that had
        long been noted by HIV researchers without a clear understanding of its cause. We now know
        that G→A hypermutation is part of the innate immune response to retroviral infections.
        Although there is still some debate over exactly how APOBEC3G leads to viral
        eradication, the most likely scenario is that G3A hypermutation results in the generation
        and incorporation of a multitude of deleterious mutations that fatally disrupt viral
        functions. This strategy is likely to work well for retroviruses like HIV because their
        genomes are so compact that individual sequence regions often perform multiple functions.
        Under these cramped conditions, most mutations are likely to severely disrupt some aspect
        of viral function and thereby reduce fitness (Holmes 2003). Indeed, it has been estimated
        that the deleterious mutation rate in viruses that replicate using RNA polymerases (either
        reverse transcriptase in the case of retroviruses or RNA-dependent RNA polymerase for other
        RNA viruses) is on the order of one error per replication cycle, so that many of the viral
        progeny produced by replication are defective (Elena and Moya 1999). HIV is normally able
        to overcome this burden of deleterious mutation because of its remarkable reproductive
        power; each day, on the order of 10
        10 virions are produced in a single infected individual (Perelson et al.
        1996), so that enough fit and able recruits will make it through to the next
        generation.
      
      
        Treating RNA Virus Infections Through Lethal Mutagenesis
        The high mutation rates of RNA viruses mean that adaptively useful genetic variation is
        produced frequently. The rub, however, is that fitness-enhancing mutations are a small
        minority, and the preponderance of deleterious mutations means that RNA viruses live on the
        edge of survival (Domingo 2000). By increasing the rate at which deleterious mutations
        appear, APOBEC3G pushes viruses over this edge, causing a form of ‘lethal mutagenesis’ that
        results in their destruction; the rate of mutation becomes so high that no genome can
        reproduce itself faithfully, and the population crashes. Intriguingly, researchers
        designing new antiviral drugs have also begun to realise that forcing viruses into this
        sort of ‘error catastrophe’ might be an effective way to treat them (Figure 2). There are a
        growing number of studies in which mutagens, such as ribavirin and 5-fluorouracil, are
        applied to viral infections in vitro and in vivo, including HIV, in the hope that these
        will induce so many deleterious mutations that the virus suffers an error catastrophe and
        is cleared (Loeb et al. 1999; Sierra et al. 2000; Crotty et al. 2001; Ruiz-Jarabo et al.
        2003). The results produced to date are highly encouraging, particularly when these
        error-inducing drugs are combined with more conventional treatment strategies that aim to
        reduce the rate of viral replication (Pariente et al. 2001). The discovery that a natural
        antiviral agent, APOBEC3G, probably works in much the same way should provide even more
        encouragement.
        Sadly, however, the pathogens have fought back. The anti-HIV properties of APOBEC3G were
        discovered because most viral strains escape its neutralising properties. Lentiviruses like
        HIV possess a gene that encodes an protein called Vif (‘viral infectivity factor’) that
        counters APOBEC3G (Sheehy et al. 2002). Hence, it is probably only in naturally occurring
        Vif-defective mutants that APOBEC3G is effective against HIV. Furthermore, because positive
        selection on APOBEC3G has operated for at least 30 million years and lentiviruses in
        general, and HIV in particular, are likely to be more recently evolved than this, it is
        clear that a broad range of retroviral pathogens have been responsible for the adaptive
        evolution of this particular immune gene (Sawyer et al. 2004). Given the frequency with
        which the remnants of past retroviral infections are found in the mammalian genome (Smit
        1999), in the form of usually defunct endogenous retroviruses (Box 1), it is likely that
        our genomes are continually bombarded with retroviruses like HIV but that the majority are
        cleared by innate immune mechanisms like APOBEC3G. It is possible that the retroviruses
        that successfully infect us are those, like HIV, that have managed to evolve strategies to
        avoid the destructive capacities of APOBEC3G.
        The intense selective pressure on the defensins and APOBEC3G illustrates that although
        the innate immune response is generalist in its action, it is as highly and intricately
        evolved as its better-studied ally, the adaptive immune system. Rather than being an
        evolutionary remnant, it is a dynamic and continually adapting system. Less clear is
        whether other host proteins act in the same manner as APOBEC3G. In particular, the most
        common and destructive pathogens faced by humans and other mammals are RNA viruses, such as
        influenza A, yellow fever, and hepatitis C. In most cases, our ability to survive these
        viral infections is simply a combination of good luck and good breeding; with the right
        combination of MHC alleles, itself a function of population history and what we by chance
        inherit from our parents, some individuals may be more able to fight off viral infections
        than others. The ubiquity of RNA viruses hints that our genomes might also contain an
        innate, yet highly adapted, defence system that targets this abundant class of pathogens by
        manipulating their mutation rate. Although utilizing lethal mutagenesis might one day be an
        important way to design new drugs against a variety of viral pathogens, it would come as no
        surprise if nature got there first.
      
    
  

  
    
      
        
        The language of conservation is changing: protecting biodiversity is no longer just
        about ethics and aesthetics; the latest buzzwords are commodities and consumers.
        Traditionally, conservation initiatives have talked up the benefits they will bring to the
        global community—saving species, habitats, ecosystems, and ultimately the planet. But
        conservation also has its costs, and these are usually borne by local people prevented from
        exploiting the resources around them in other ways. It is unfair to expect a localised
        minority to pick up costs that ultimately benefit a dispersed majority, argue conservation
        biologists. There has to be more money made available by concerned individuals,
        non-governmental organisations, national governments, and international bodies, and there
        need to be better ways to spend this money if conservation is to be effective, they say.
        Biodiversity is a commodity that can be bought and sold. We are consumers and must pay.
      
      
        Costs and Benefits
        Kenya boasts one of the world's most spectacular networks of national parks and reserves
        covering around 60,000 km
        2 of the country (Figure 1). But devoting such a vast area to
        conservation has its drawbacks. It has been estimated that were this land developed it
        would be worth around $270 million to the Kenyan people every year. Similarly, two national
        parks in Madagascar are estimated to have reduced the annual income of local villagers by
        around 10%. Of course, protected areas do bring some benefits to neighbouring communities,
        most notably through tourism. But in many cases the rewards are not great, they are rarely
        distributed evenly among individuals, and do not necessarily outweigh the costs.
        ‘The costs of conservation fall disproportionately on local people, whereas the benefits
        are dispersed,’ says Andrew Balmford, a conservation biologist at the University of
        Cambridge in the United Kingdom. National and global communities stand to benefit from
        conservation of tropical biodiversity, but they must pay if they want to realise that
        benefit, he says. Conservation expenditure in the developed world is only about a third of
        what is needed for effective protection of 15% of the earth's terrestrial habitats, an area
        just large enough to preserve a representative sample of species, habitats, and ecosystems
        in the medium to long term (Balmford et al. 2003). The developed world must make up this
        funding shortfall, argues Balmford. What's more, there need to be smarter ways to spend the
        money that's available, he says.
      
      
        Conservation by Distraction
        In recent years, many funding bodies have taken an indirect approach to conservation,
        investing in projects that encourage people to take up alternative practices that are
        compatible with conservation rather than investing in conservation itself. Perhaps the best
        example of this ‘conservation by distraction’ is ploughing money into community-based
        ecotourism projects. Such initiatives aim to bring the benefits of tourism to local people,
        thereby encouraging them to preserve the biodiversity they have.
        It's an attractive idea. In the mid 1990s, the United States Agency for International
        Development was investing more than $2 billion a year in 105 conservation projects with an
        ecotourism component. Similarly, between 1988 and 2003, the World Bank funded 55
        development projects that supported protected areas in Africa, 32 of which placed an
        emphasis on ecotourism.
        However, an absence of quantitative data and analysis has made it hard to judge whether
        these projects actually achieve their dual purpose of preserving biodiversity and
        simultaneously reducing rural poverty. ‘Much of the information about community-based
        ecotourism is anecdotal and subjective,’ says Agnes Kiss of the Environment and Social
        Development Unit at the World Bank. The real contribution of these initiatives to
        biodiversity conservation is debatable, she says. ‘Many community-based ecotourism projects
        cited as success stories actually involve little change in existing local land- and
        resource-use practices, provide only modest supplement to local livelihoods, and remain
        dependent on external support for long periods, if not indefinitely’ (Kiss 2004).
        For example, communities involved in the Infierno Community Ecotourism Project in Peru
        have received nearly $120,000 from their share in a tourist lodge and wages for providing
        services to visitors. This may have increased the income for a minority that are lodge
        employees, but only one family, whose adult members were all employed by the lodge, could
        afford to live solely on tourism. In the community as a whole, the average annual income
        from tourism was only $735 compared with nearly $2,000 earned elsewhere. Most of the
        community was still heavily dependent on other activities, and most of those activities are
        somewhat disruptive of conservation goals, says Kiss.
        Johan du Toit of the Mammal Research Institute at the University of Pretoria in South
        Africa is also critical of this kind of indirect approach to conservation. At the heart of
        the argument for community-based ecotourism is the idea of the ‘ecologically noble savage’,
        he says—the notion that those living closest to nature will know what's best for it. ‘It's
        a wonderful idea, but it just doesn't work. Nowhere in the history of evolution has
        sustainability ever been naturally selected for,’ says du Toit. ‘The AK47 automatic assault
        rifle has replaced the bow and arrow.…Every individual in a rural community that's out
        hunting will shoot what he sees when he sees it, because if he doesn't somebody else
        will.’
        Nowhere is this problem more evident than in the ecotourist paradise of the Galápagos
        Islands (Figure 2), where a small minority of fishermen is coming into conflict with
        conservation aims with increasing regularity (Box 1). ‘Things are going down very quickly,’
        says one Galápagos guide. ‘The iceberg is starting to tip over, and we are going to lose
        everything.’ If it still pays locals to exploit the environment rather than take part in
        one of the world's most buoyant ecotourism industries, it is clear that ecotourism alone
        cannot solve the world's conservation problems. Many think that ‘direct payment’ could be a
        useful tool. ‘Direct payment, very boldly speaking, is paying people in rural areas not to
        bugger up their environment,’ says du Toit. ‘It's just like if we want exclusive artworks
        to be looked after in the Louvre Gallery in Paris. Somebody's got to pay for it,’ he says.
        ‘You can't expect the Parisians who live in that arrondissement to cover the costs.’
      
      
        You Get What You Pay for—You Should Pay for What You Want to Get
        For people living in developing countries, where most of the world's biodiversity
        exists, the short-term rewards of exploiting these natural resources are significant.
        Replacing indirect conservation measures, such as community-based ecotourism, with payments
        directly into the pockets of local people could turn out to be a much more effective way to
        stem this exploitation, argues Paul Ferraro, an economist at Georgia State University in
        Atlanta (Ferraro and Kiss 2002). It could also bring far greater development benefits than
        indirect financial support, he says (Box 2). An additional spin-off is that direct payments
        force conservation biologists to quantify and hence clarify their objectives, says John
        Hough, principal technical advisor on biodiversity for the United Nations Development
        Programme. ‘We know what we don't want,’ he says, ‘but we're not very good at saying what
        we do want.’
        A hypothetical model simulating how Madagascar should distribute an annual conservation
        budget of $4 million reveals that direct payments would have protected some 80% of original
        forest compared with only 12% protected through a system of indirect incentives. What's
        more, the annual income of rural residents would have been twice that generated through
        indirect investment (Conrad and Ferraro 2001).
        For Ferraro, the logic of direct payment is simple. He draws an analogy with a car
        journey from A to B. There are two routes that will bring you to B, one circuitous and the
        other direct. If you only have a single tank of fuel, opting for the direct route improves
        the likelihood you will arrive at your destination. An indirect approach to conservation is
        like taking the circuitous route, he says, and the chances are that you will run out of
        fuel. But if it's that simple, why are governments, non-governmental organisations, private
        bodies, and international organisations not jumping at the chance to experiment with this
        approach?
      
      
        Paying in Perpetuity
        There are those that have reservations about direct payments. The distinction between
        indirect and direct interventions is artificial, says Thomas Lovejoy, president of the
        Heinz Center, a nonprofit institution dedicated to improving the scientific and economic
        foundation for environmental policy. ‘In some cases, direct payment is the only way
        conservation can happen,’ he says. ‘In others, the indirect is important to reinforce a
        situation where there already is conservation. In yet others both are needed.’
        Sjaak Swart of the Section of Science and Society at Groningen University in The
        Netherlands argues that if conservation is to succeed, it must be rooted in the hearts and
        minds of those involved. Direct payments create a vision of nature dominated by calculable,
        monetary concerns, he says. This approach can only work in the short term, he argues, and
        indirect tools like debate and education are needed to involve communities in the long
        term. ‘You need the commitment of the local people to save the biodiversity of our world,’
        he says.
        Marine biologist Steve Trott agrees. He is project coordinator for the Local Ocean
        Trust, a charity-based conservation organisation operating in the Watamu and Malindi Marine
        Parks and Reserve in Kenya (http://www.watamuturtles.com), and is using direct payments to
        help reduce the slaughter of turtles by local fishermen. The Watamu Turtle Watch Program is
        currently paying fishermen just over $3 a turtle to release the animals from their nets
        rather than kill them. Before the scheme started in 2000, only around 50 turtles were being
        released from nets each year. By 2003, more than 500 a year were making it back into the
        sea. Elsewhere along the Kenyan coast, where fishermen do not get these payments, turtles
        continue to be killed, says Trott. However, the financial incentives are only part of a
        grander program of education and support to sensitise people to the conservation message,
        he says. Eventually, the plan is to stop payments altogether. ‘Payment will be reduced as
        education and awareness is increased to the point where it's phased out,’ he says.
        Reducing or stopping the payment could work, says Ferraro, but it is more likely that
        the turtles will begin to suffer once more. ‘If I had to wager, I'd bet people would go
        back to their old patterns eventually.’ This means that direct payments require an ongoing
        financial commitment, and many people don't like this idea, he says.
      
      
        To the Test
        The idea of direct payments needs empirical testing before it can be embraced with
        confidence, admits Ferraro. Funding bodies should demand experimental and control data to
        allow the success of an intervention to be gauged. Conservation biologists must therefore
        be trained in the skills needed to collect and evaluate these data. ‘Without adequate data
        and controls you're only going to be left with guesses and vague anecdotes about the
        effects of a program intervention,’ he says. Decision makers should begin to design
        controlled experiments from which they can make inferences about the effectiveness of these
        different interventions, he suggests.
        There are other drawbacks of direct payments. One concern is that they might just shift
        the pressure from one site to another that was not previously being exploited. Furthermore,
        in developing countries, land tenure is often ambiguous, which can make investment an
        unattractive prospect for funding agencies—they want to be sure they know where their money
        is going. But, notes Ferraro, such objections also apply to indirect interventions. ‘I
        don't necessarily believe that conservation payments will be successful,’ he says. ‘It's
        more I believe that of all the ideas out there for protecting biodiversity, this is the
        least bad.’
        All this talk of cost, benefit, and efficiency is creeping into conservation speak. For
        some, these cold and calculating terms are an odd way to describe the world's wonderfully
        unpredictable wildlife. But, increasingly, there are calls for conservation biology to cast
        aside its sentimental demons: biodiversity is a commodity that can be bought and sold;
        conservation is business.
      
    
  

  
    
      
        Phytohormones: What Are they?
        Plant growth and development involves the integration of many environmental and
        endogenous signals that, together with the intrinsic genetic program, determine plant form.
        Fundamental to this process are several growth regulators collectively called the plant
        hormones or phytohormones. This group includes auxin, cytokinin, the gibberellins (GAs),
        abscisic acid (ABA), ethylene, the brassinosteroids (BRs), and jasmonic acid (JA), each of
        which acts at low concentrations to regulate many aspects of plant growth and
        development.
        With the notable exception of the steroidal hormones of the BR group, plant hormones
        bear little resemblance to their animal counterparts (Figure 1). Rather, they are
        relatively simple, small molecules such as ethylene gas and indole-3-acetic acid (IAA), the
        primary auxin in the majority of plant species. The concept of plant hormones originates
        from a classical experiment on phototropism, the bending of plants toward light, carried
        out by Charles Darwin and his son Francis in 1880. The Darwins were able to demonstrate
        that when oat seedlings were exposed to a lateral light source, a transported signal
        originating from the plant apex promoted differential cell elongation in the lower parts of
        the seedling that resulted in it bending toward the light source. This signal was
        subsequently shown to be IAA, the first known plant hormone.
      
      
        What Do They Do?
        Virtually every aspect of plant growth and development is under hormonal control to some
        degree. A single hormone can regulate an amazingly diverse array of cellular and
        developmental processes, while at the same time multiple hormones often influence a single
        process. Well-studied examples include the promotion of fruit ripening by ethylene,
        regulation of the cell cycle by auxin and cytokinin, induction of seed germination and stem
        elongation by GA, and the maintenance of seed dormancy by ABA. Historically, the effects of
        each hormone have been defined largely by the application of exogenous hormone. More
        recently, the isolation of hormone biosynthetic and response mutants has provided powerful
        new tools for painting a clearer picture of the roles of the various phytohormones in plant
        growth and development.
      
      
        How Do They Work?
        Plant biologists have been fascinated by the regulatory capacity of phytohormones since
        the time of their discovery, and the notion that hormone levels or responses could be
        manipulated to improve desired plant traits has long been an area of intense interest.
        Perhaps the best-known example of this is the isolation of dwarf varieties of wheat and
        rice that led to the “green revolution” in the second half of the 20th century, which is
        credited with saving millions of people around the globe from starvation. These dwarf
        varieties have shorter stems than wild-type, making these plants less susceptible to damage
        by wind and rain. The molecular isolation of these “dwarfing genes” has revealed that they
        encode components of the GA biosynthesis and response pathways (Peng et al. 1999; Sasaki et
        al. 2002).
        To elucidate the molecular mechanisms underlying phytohormone action, several
        researchers have utilized the genetically facile model plant 
        Arabidopsis thaliana to isolate mutations that confer altered
        response to applied hormone. Molecular and biochemical analysis of the gene products
        defined by these mutations, coupled with expression studies aimed at identifying the
        downstream target genes that mediate hormonal changes in growth and development, has begun
        to unlock some of the mysteries behind phytohormone action. While no hormone transduction
        pathway is completely understood, we now have a rudimentary understanding of many of the
        molecular events underlying hormone action. Several reviews covering the individual hormone
        pathways in greater detail have recently been published (Turner et al. 2002; Gomi and
        Matsuoka 2003; Himmelbach et al. 2003; Kakimoto 2003; Dharmasiri and Estelle 2004; Guo and
        Ecker 2004; Wang and He 2004).
      
      
        Common Themes
        Regulation by proteolysis has emerged as a resounding theme in plant hormone signaling.
        The ubiquitin-mediated degradation of key regulatory proteins has been demonstrated, or is
        at least likely, for all of the phytohormone response pathways (Smalle and Vierstra 2004).
        In the case of auxin, the response pathway is normally subject to repression by a large
        family of transcriptional regulators called the Aux/IAA proteins (Figure 2). These proteins
        dimerize with members of the auxin response factor (ARF) family of transcription factors,
        thus preventing ARFs from activating auxin-responsive genes (Tiwari et al. 2004). Upon an
        auxin stimulus, an SCF (SKP1/Cullin/F-box protein) ubiquitin ligase (Deshaies 1999)
        containing the TIR1 F-box protein ubiquitinates the Aux/IAA proteins, marking them for
        degradation by the 26S proteasome thereby de-repressing the response pathway (Gray et al.
        2001). The hormone promotes the Aux/IAA–TIR1 interaction; however, the molecular mechanisms
        behind this regulation are unclear. Most yeast and animal SCF substrates must be
        post-translationally modified, usually by phosphorylation, before they are recognized by
        their cognate F-box protein. Despite numerous efforts to identify auxin-induced
        modification of Aux/IAA proteins, no such signal has been discovered, raising the distinct
        possibility that auxin uses a novel mechanism to regulate SCF–substrate interactions.
        Ethylene and cytokinin are both perceived by receptors sharing similarity to bacterial
        two-component regulators. Common in prokaryotes, but apparently restricted to plants and
        fungi in eukaryotes, these modular signaling systems involve a membrane-bound receptor
        containing an intracellular histidine kinase (HK) domain (Wolanin et al. 2002). Ligand
        binding activates the kinase, resulting in autophosphorylation and initiation of a series
        of phosphotransfer reactions that culminates with the activation of a response regulator
        protein that functions as the effector component of the pathway. Cytokinin signaling
        appears to largely follow this paradigm (Kakimoto 2003). Ethylene response, however,
        appears more complex (Guo and Ecker 2004).
        Ethylene is perceived by a family of five receptors. ETR1 and ERS1 contain a consensus
        HK domain, however, the HK domains of ETR2, ERS2, and EIN4 are degenerate and lack elements
        necessary for catalytic activity. This fact, together with studies of “kinase-dead” mutants
        of 
        ETR1 , suggests that HK activity is not required for ethylene response.
        Mutations that abolish ethylene binding in any of the five receptor genes are dominant and
        confer ethylene insensitivity, indicating that the receptors function as negative
        regulators of the ethylene pathway.
        Genetic and molecular studies have positioned these receptors upstream of the Raf-like
        MAP kinase kinase kinase, CTR1, which interacts with the receptors and also acts as a
        negative regulator (Figure 3). The integral membrane protein, EIN2, and the transcription
        factors EIN3 and EIL1 are positive regulators of ethylene signaling downstream of CTR1.
        Current models propose that hormone binding inactivates the receptors, thus resulting in
        down-regulation of CTR1 activity. Since the identification of CTR1, biologists have
        speculated that a MAP kinase cascade may be involved. Only recently, however, have putative
        MAP kinase kinase and MAP kinase components of the ethylene pathway been identified (Chang
        2003). Interestingly, these kinases appear to positively regulate ethylene response,
        suggesting that CTR1 must inhibit their function. If so, this would represent a novel twist
        on the traditional MAP kinase signaling paradigm. Precisely how the ethylene signal is
        transduced to the EIN3 and EIL1 transcription factors remains unclear. However, the recent
        finding that ethylene stabilizes these transcription factors, which are targeted for
        degradation by an SCF complex in the absence of ethylene, clearly indicates a role for the
        ubiquitin pathway (Guo and Ecker 2003; Potuschak et al. 2003). One of the known targets for
        EIN3 is the ERF1 transcription factor, which activates several genes involved in a subset
        of ethylene responses.
      
      
        Signal Integration and Combinatorial Control
        Long ago, plant physiologists noted the apparent antagonistic interactions between some
        of the phytohormones, such as between auxin and cytokinin in the regulation of root–shoot
        differentiation and between GA and ABA in germination. Other processes are synergistically
        regulated by multiple hormones. While it has long been obvious that hormones do not
        function in discrete pathways, but rather exhibit extensive cross-talk and signal
        integration with each other and with environmental and developmental signaling pathways,
        the molecular basis for such coordinated regulation has been unclear. Several recent
        findings have begun to elucidate the molecular details of some of these events.
        One example of such signal integration was recently described for the ethylene and JA
        pathways (Lorenzo et al. 2003). Genetic studies had previously implicated both hormones as
        important regulators of pathogen defense responses, as well as of the wounding response and
        other stress-related pathways. Additionally, microarray analysis has identified a large
        number of genes that are responsive to both hormones. The ERF1 transcription factor was
        recently found to be an intersection point for these two signaling pathways (Lorenzo et al.
        2003). Like ethylene, JA rapidly induces 
        ERF1 expression, and treatment with both hormones synergistically
        activates 
        ERF1 . Induction of 
        ERF1 by both hormones alone or in combination is dependent upon both
        signaling pathways, and constitutive overexpression of 
        ERF1 rescues the defense-response defects of both ethylene- and
        JA-insensitive mutants. These findings suggest that 
        ERF1 represents one of the first signaling nodes identified in the
        complex web of hormonal cross-talk.
        The auxin and BR pathways also appear to converge and mutually regulate some
        developmental processes. Both hormones promote cell expansion, and microarray studies have
        revealed that as many as 40% of all BR-induced genes are also up-regulated by auxin (Goda
        et al. 2004; Nemhauser et al. 2004). BR is perceived by the cell surface receptor kinase
        BRI1 (Wang and He 2004). The SHAGGY/GSK3-type kinase BIN2 acts as a negative regulator of
        the pathway downstream of the receptor. In the absence of a BR signal, BIN2 phosphorylates
        the transcription factors BES1 and BZR1, targeting them for proteolysis by the 26S
        proteasome. Upon a BR stimulus, BIN2 is inactivated, allowing BES1 and BZR1to accumulate in
        the nucleus, where they are presumably involved in regulating BR-responsive genes.
        Using combined genetic, physiological, and genomic approaches, Nemhauser and colleagues
        (2004) were able to demonstrate that auxin and BR regulate 
        Arabidopsis hypocotyl (embryonic stem) elongation in a
        synergistic and interdependent fashion. Elevating endogenous auxin levels rendered plants
        more sensitive to BR application in hypocotyl elongation assays, and this response was
        dependent upon both the auxin and BR signaling pathways. Genetic studies suggest that the
        convergence of these two pathways occurs at a late point in hormone signaling, perhaps at
        the promoters of the many genes responsive to both hormones. In support of this notion,
        bioinformatic analysis identified distinct sequence elements that were enriched
        specifically in the promoters of auxin-induced, BR-induced, and auxin/BR-induced genes.
      
      
        Many Unanswered Questions
        While great strides have been made in recent years in understanding the molecular basis
        of phytohormone action, many fundamental questions remain. Receptors and other upstream
        signaling components remain to be identified for the majority of the phytohormones. Equally
        important are the elucidation of hormonal networks and the integration of these networks
        with the morphogenetic program, such that our understanding of hormone action can be placed
        in a developmental context.
      
    
  

  
    
      
        
        Sitting in the enveloping quietness of an anechoic chamber, or other quiet spot, you
        soon become aware that the ear makes its own distinctive sounds. Whistling, buzzing,
        hissing, perhaps a chiming chorus of many tones—such continuous sounds seem remarkably
        nonbiological to my perception, more in the realm of the electronic.
        Even more remarkable, put a sensitive microphone in the ear canal and you will usually
        pick up an objective counterpart of that subjective experience. Now known in auditory
        science as spontaneous otoacoustic emission, the sound registered by the microphone is a
        clear message that the cochlea uses active processes to detect the phenomenally faint
        sounds—measured in micropascals—that our ears routinely hear. If the ear were more
        sensitive, we would need to contend with the sound of air molecules raining upon our
        eardrums.
        What is that process—the mechanical or electrical scheme that Hallowell Davis in 1983
        called the ‘cochlear amplifier’ (Davis 1983)—which energises the pea-sized hearing organ
        buried in the solid bone of our skull?
        That question has engaged my curiosity since the late 1970s, when English auditory
        physicist David Kemp first put a microphone to an ear and discovered the telltale sounds of
        the cochlea at work (Kemp 1978). Siren-like, the sounds have drawn me into the theory and
        experiment of cochlear mechanics, now as part of a PhD course at the Australian National
        University in Canberra. I am studying the micromechanics of this process from a theoretical
        point of view, and investigating whether a resonance picture of some kind can be applied to
        the faint but mysterious sounds most cochleas emit.
        Kemp's discoveries are rightly viewed as opening a fresh path to auditory science, and
        to the tools and techniques for diagnosing the functional status of the cochlea. But in
        terms of fundamental understanding, a key paper remains that of Thomas Gold more than half
        a century ago (Gold 1948). Still cited widely today, this paper deals with the basic
        question of how the cochlea works to analyse sound into its component frequencies. Two
        prominent theories—sympathetic resonance, proposed by Hermann Helmholtz (1885), and
        travelling waves, proposed by Georg von Békésy (1960)—need to be distinguished (Figure 1).
        In a nutshell, are there tiny, independently tuned elements in the cochlea, like the
        discrete strings of a piano, that are set into sympathetic vibration by incoming sound
        (Helmholtz), or is the continuously graded sensing surface of the cochlea hydrodynamically
        coupled so that, like flicking a rope, motion of the eardrum and middle ear bones causes a
        travelling wave to sweep from one end towards the other (von Békésy)?
        The first option, sympathetic resonance, has the advantage of allowing vanishingly small
        energies to build up, cycle by cycle, into an appreciable motion—like boosting a child on a
        swing. The second, travelling wave, has the weight of von Békésy's extensive experiments
        behind it. At the same time, one of the drawbacks of the travelling wave theory is the
        difficulty of accounting for the ear's exquisite fine tuning: trained musicians can easily
        detect tuning differences of less than 0.2%. Even von Békésy himself notes, on page 404 of
        his classic book, that ‘the resonance theory of hearing is probably the most elegant of all
        theories of hearing’.
        Gold's work, done in collaboration with RJ Pumphrey (Gold and Pumphrey 1948), was the
        first to consider that the ear cannot act passively, as both Helmholtz and von Békésy had
        thought, but must be an active detector. Gold was a physicist who had done wartime work on
        radar, and he brought his signal-processing knowledge to bear on how the cochlea works. He
        knew that, to preserve signal-to-noise ratio, a signal had to be amplified before the
        detector, and that ‘surely nature can't be as stupid as to go and put a nerve fibre—that is
        a detector—right at the front end of the sensitivity of the system’. He therefore proposed
        that the ear operated like a regenerative receiver, much like some radio receivers of the
        time that used positive feedback to amplify a signal before it was detected. Regenerative
        receivers were simple—one could be built with a single vacuum tube—and they provided high
        sensitivity and narrow bandwidth. A drawback, however, was that, if provoked, the circuit
        could ‘take off’, producing an unwanted whistle. Gold connected this with the perception of
        ringing in the ear (tinnitus), and daringly suggested that if a microphone were put next to
        the ear, a corresponding sound might be picked up. He experimented, placing a microphone in
        his ear after inducing temporary tinnitus with overly loud sound. The technology wasn't up
        to the job—in 1948 microphones weren't sensitive enough—and the experiment, sadly,
        failed.
        Gold's pioneering work is now acknowledged to be a harbinger of Kemp's discoveries. But
        there is one aspect of Gold's paper that is not so widely considered: Gold's experiments
        led him to favour a resonance theory of hearing. In fact, the abstract of his 1948 paper
        declares that ‘previous theories of hearing are considered, and it is shown that only the
        resonance theory of Helmholtz… is consistent with observation’.
        Gold and Pumphrey did psychophysical experiments in which hearing thresholds were
        determined for listeners first for continuous pure tones and then for increasingly briefer
        stimuli of the same frequency. Gold and Pumphrey showed that their results could only be
        accounted for by considering the cochlea as a set of resonators, each of which responds to
        a narrow frequency range.
        In a second neat experiment, listeners had to detect differences between the sound of
        repetitive tone pips (series one) and those same stimuli but with the phase of every second
        pip inverted (series two, in which compressions replaced rarefactions and vice versa).
        Out-of-phase pips should counteract the action of in-phase pips and, following the
        child-on-swing analogy, rapidly bring swinging to a halt. Therefore, the argument goes, the
        two series should sound different. By increasing the silent interval between pips until the
        difference disappeared, the experimenters could infer how long the vibrations (or swinging)
        appeared to persist and could then put a measure on the quality factor 
        (Q), or narrowness in frequency range, of the presumed underlying
        resonance.
        From the first experiment, Gold and Pumphrey derived values of 
        Q of 32 to 300, meaning that the range of response was as little as
        1/300-th of the imposed frequency—based on the picture of a broad travelling wave. The
        second experiment gave comparable results. However, their resonance interpretation has been
        dismissed because of a methodological flaw in the second experiment: the spectral
        signatures of the two series are not the same and provide additional cues. Nevertheless, it
        is not widely appreciated that the first experiment seems methodologically sound, and its
        results remain persuasive.
        I think the resonance theory deserves reconsideration. The evidence of my ears tells me
        that the cochlea is very highly tuned, and an active resonance theory of some sort seems to
        provide the most satisfying explanation. Furthermore, as well as Gold's neglected
        experiment, we now know from studies of acoustic emissions that the relative bandwidth of
        spontaneously emitted sound from the cochlea can be 1/1000 of the emission's frequency, or
        less. My research, guided by Professors M. V. Srinivasan and N. H. Fletcher, has centred on
        finding an answer to that most fundamental question: if the cochlea is resonating, what are
        the resonant elements?
        A point of inspiration for me is Gold's later discussion of cochlear function (Gold
        1987)—some nine years after Kemp's discoveries had been made. Gold draws a striking analogy
        for the problem confronting the cochlea, whose resonant elements—whatever they are—sit
        immersed in fluid (the aqueous lymph that fills the organ). To make these elements resonate
        is difficult, says Gold, because they are damped by surrounding fluid, just like the
        strings of a piano submerged in water would be. He concludes that, to make ‘an underwater
        piano’ work, we would have to add sensors and actuators to every string so that once a
        string is sounded the damping is counteracted by positive feedback. ‘If we now supplied
        each string with a correctly designed feedback circuit,’ he surmises, ‘then the underwater
        piano would work again.’
        My research is investigating what Gold's underwater piano strings might be. A suggestion
        put forward in a recent paper (Bell and Fletcher 2004) is that resonance might occur in the
        space between the cochlea's geometrically arranged rows of outer hair cells. These cells
        are both effectors (they change length when stimulated) and sensors (their stereocilia
        detect minute displacements), so a positive feedback network can form that sets up
        resonance between one row of cells and its neighbour. The key is to transmit the feedback
        with the correct phase delay, and the new paper describes how this can be done using
        ‘squirting waves’ in the gap occupied by the outer hair cell stereocilia. The paper
        suggests that the outer hair cells create a standing wave resonance, from which energy is
        delivered to inner hair cells (where neural transduction takes place). In this way, the
        input signal is amplified before it is detected—an active system functioning just like
        Gold's regenerative receiver.
        With a prime candidate in place for the resonating elements, this should, I think,
        prompt us to re-evaluate resonance theories of hearing, which were first put forward by the
        ancient Greeks and which, irrepressibly, keep resurfacing. The best-known resonance theory
        was that formulated by Helmholtz, but at that time no satisfactory resonating elements
        could be identified, and it lapsed until Gold's attempt to revive it. There are other
        difficulties in reviving a resonance theory of hearing, but I think they can be
        overcome.
        If there really are resonant elements in the ear, the outstanding question would be, how
        are they stimulated? It is conceivable that motion of the conventional travelling wave sets
        them off, in which case we have an interesting hybrid of travelling wave and resonance. The
        other possibility, which I favour, is that outer hair cells are stimulated by the fast
        pressure wave that sweeps through all of the cochlear fluid at the speed of sound in water
        (1,500 m/s). If that is the case, and outer hair cells are primarily pressure sensors, not
        displacement detectors, then the ear is a fully resonant, pressure-driven system. New life,
        perhaps, to that old resonance idea.
      
    
  

  
    
      
        
        A ban in the 1866s by the French Academy of Sciences on publications about the origin of
        human language must have been one of the strangest bans in the history of sciences. Yet it
        was highly effective. After the ban, scientists and interested laymen had to wait for more
        than a century to hold a textbook on language evolution in their hands. 
        Language Evolution, a compilation of essays by a diverse group of
        respected researchers, is amongst the first books that try to tackle what is arguably one
        of the hardest scientific problems. The editors set themselves the ambitious target of
        creating an up-to-date book about this emerging field, and they have to be congratulated
        for their efforts. Linguists, cognitive scientists, behavioural ecologists, and theoretical
        biologists all offer their view on the origin of human language and, refreshingly, do not
        shy from pointing out the real or assumed weaknesses of the other approaches.
        One of the main themes of the book is the evolutionary approach and the importance of
        biological structures and properties that were co-opted in the development of language
        (pre-adaptations). In one essay, Michael Studdert-Kenedy and Louis Goldstein propose that
        speech, as a motor function, draws on phylogenetically ancient mammalian oral capacities
        for sucking, licking, swallowing, and chewing. Thus, our hominid ancestors adopted an
        apparatus already divided neuroanatomically into discrete components. Complementing this
        evidence, Marc Hauser and Tecumseh Fitch compare human speech production and perception
        with that of nonhuman species. They conclude that many traits that were formerly thought to
        have evolved specifically for speech (such as having a descended larynx or categorical
        perception) are also present in other species.
        But perhaps the most interesting idea about pre-adaptation comes from the work of
        neuroscientist Michael Arbib on ‘mirror’ neurons in monkeys. These neurons are a subset of
        the grasp-related premotor neurons that discharge not only, as other premotor neurons do,
        when the monkey executes a certain class of actions, but also when the monkey observes more
        or less similarly meaningful hand movements made by the experimenter (or by another
        monkey). The area in which these grasp-related neurons are found is analogous with the
        Broca's area in human brains, which is involved in assessing the syntax of words. This
        observation serves as the basis for the mirror-system hypothesis, which postulates that
        Broca's area in humans evolved from a basic mechanism not originally related to
        communication but rather from the mirror system for grasping in the common ancestor of
        monkey and human. As a result, the mirror system provides a possible ‘neural link’ in the
        evolution of human language.
        There is still much debate about the selection pressures that led to the evolution of
        language. Observing the overabundance of potential selective scenarios for why language
        evolved, the linguist Derek Bickerton voices his scepticism: ‘The fact that these and
        similar explanations flourish side by side tells one immediately not enough constraints are
        being used to limit possible explanations.’ One frequent source of confusion, he notes, is
        equating language with speech by not distinguishing between modality, lexicon, and
        structure. Hauser and Fitch share Bickerton's scepticism and urge scientists to rely more
        on the traditional comparative approach, which was always the strength of Darwinian
        evolutionary theory.
        Primatologist Robin Dunbar, who originally proposed that grooming (group bonding) could
        have provided the stimulus for language, dismisses two other possible scenarios—hunting and
        tool-making—as potential ecological contexts for the evolution of human language. Gestural
        origins are also dismissed in his theory, because gestural languages do not seem to develop
        spontaneously and also require a line-of-sight contact making them useless at night.
        Interestingly, Steven Pinker rules out both Dunbar's theory of grooming and Geoffrey
        Miller's theory of sexual selection, whereas Bickerton rules out grooming, gossip, mating
        contract, and Machiavellian intelligence as likely contexts for the origin of human
        language.
        Also under fire in the book is the idea that the human brain is somehow equipped at
        birth with a ‘universal grammar’ out of which all human languages later develop. Several
        authors try to provide alternatives to innate predispositions, such as the importance of
        function to categorization (Michael Tomasello) and the importance of cultural transmission
        to the structure of language (Simon Kirby and Morton Christiansen). Arbib explicitly
        questions the traditional Chomskyan theory of innate linguistic predispositions and argues
        that what humans have and had in the past is ‘language readiness’ rather than a fixed
        universal grammar.
        Neuroscientist Terrence Deacon also puts an alternative theory forward. According to
        Deacon, many of the language universals reflect semiotic constraints inherent in the
        requirements for producing symbolic reference rather than innate predispositions. Thus,
        neither evolved innate predispositions nor culturally evolved and transmitted regularities
        can be considered as the ultimate source of language universals. He draws a parallel with
        mathematical operations (addition, subtraction, etc.) and with prime numbers. Symbolic
        reference, he argues, is constrained by the structure it refers to.
        The editors claim, in the light of this diversity, that ‘this book is intended to bring
        together, for the first time, all the major perspectives on language evolution’. We have
        two concerns with this aim. First, two books of the same organization and scope have been
        published in the past six years based on the material from language evolution conferences
        (Hurford et al. 1998; Knight et al. 2000). Although this first concern might be just
        splitting hairs, the second is more substantial: several crucial aspects of language
        evolution are not represented at all or are just touched superficially.
        One of these missing themes is the selective advantage of early language. As discussed,
        many of the contributors express their scepticism towards the selective scenarios found in
        the literature—and indeed towards such constructions in general—but there is no review and
        no balanced evaluation of these selective scenarios. Since one of the key questions of
        language evolution is the selective advantage of early language, the lack of such a review
        is a major weakness. A balanced account could have been presented even if the editors and
        most of the contributors are frustrated by the plethora of selective scenarios.
        Related to the possible selective advantage of language is the issue of genetic
        background. Although there is mention of the so-called FOX genes—some mutations of which
        are associated with language disorders—there is no detailed discussion of our current
        knowledge of genetics related to language.
        Another lightly treated theme is the neural basis of language and language evolution.
        Understandably it is one of the most difficult issues concerning human language, and no one
        expects the editors or any of the contributors to come up with an answer to all the
        questions. What is missing again is a good survey outlining the problems and the current
        findings of the field.
        The weaknesses of the book come from its structure and organization. The editors,
        instead of outlining a structure and asking specialists to contribute to that structure,
        appear to have let every contributor write freely about their current ideas and current
        research without regard to the bigger picture. This definitely shows the interests of the
        contributors and outlines the current state of the art; it leaves gaps, however, in the
        coverage of crucial topics related to the evolution of human language.
      
    
  

  
    
      
        
        
          
            
              “Some qualities nature carefully fixes and transmits, but some, and
              those the finer, she exhales with the breath of the individual as too costly to
              perpetuate. But I notice also that they may become fixed and permanent in any stock,
              by painting and repainting them on every individual, until at last nature adopts them
              and bakes them into her porcelain”—Ralph Waldo Emerson
            
          
        
        The history of domesticated plant form and function evolves along a two-tiered track
        that doubles back on itself, offering panoramic vistas of natural forces intertwined with
        the creative force of human endeavor (Figure 1). For approximately 10,000 years, human
        beings have modified the traits of plants and animals, giving rise to hundreds of thousands
        of domesticated breeds that today form the foundation of the world's food supply. Modern
        breeds are descendents of the wild species from which they were derived. The process of
        domestication dramatically changed the performance and genetic architecture of the
        ancestral species through the process of hybridization and selection as originally
        described by Charles Darwin (1859).
        Despite the low yields and poor eating quality of most wild ancestors and primitive crop
        varieties, these ancient sources of genetic variation continue to provide the basic
        building blocks from which all modern varieties are constructed. Breeders have discovered
        that genes hidden in these low-yielding ancestors can enhance the performance of some of
        the world's most productive crop varieties. In this essay, I will provide some historical
        context for the paper by Gur and Zamir in this issue of 
        PLoS Biology (Gur and Zamir 2004). I will discuss how “smart breeding”
        recycles “old genes” to develop highly productive, stress-resistant modern varieties and
        why this approach is particularly attractive to increase food security in regions of the
        world with high concentrations of genetic diversity.
        The job of the plant breeder is to create an improved variety. This may be accomplished
        simply by selecting a superior individual from among a range of existing possibilities, or
        it may require that a breeder know how to efficiently swap or replace parts, recombine
        components, and rebuild a biological system that will be capable of growing vigorously and
        productively in the context of an agricultural environment. How the breeding is done and
        what goals are achieved is largely a matter of biological feasibility, consumer demand, and
        production economics. What is clear is that the surest way to succeed in a reasonable
        amount of time is to have access to a large and diverse pool of genetic variation.
        The process of plant breeding is theoretically simple, but its power resides in the fact
        that it creates novelty. A breeder generally selects two individuals for crossing, each of
        which has specific traits or characteristics of interest. The cross provides the mechanism
        by which genes are exchanged between the parents so that a wide array of diverse
        individuals is observed in the progeny of future generations. From a breeding perspective,
        this provides the basis for selection so that individuals containing the best features of
        both parents can be identified and further bred. By selecting parents that are genetically
        similar, a breeder restricts the amount of variation that will be evaluated in the
        offspring. On the other hand, by crossing genetically divergent parents, the range of
        phenotypic variation will be much more extensive and can even be surprising, with many
        individuals presenting phenotypes that would not be expected based on the attributes of the
        parents. Thus, if a breeder is interested in innovation and wants to generate maximum
        variation from which to make selections, wide crosses are the most productive.
        Not all genetic variation is created equal. When Darwin first introduced the concept of
        evolution (Darwin 1859), he challenged the prevailing view that species were fixed entities
        with a single, invariable genetic identity. The concept of natural selection presupposed
        that species were comprised of genetically variable individuals such that selection could
        act on them. The genetic variants differ in the alleles (versions of genes) they carry.
        Alleles that are deleterious in terms of the survival and reproduction of the organism will
        eventually be eliminated while alleles that are favorable or neutral will be perpetuated in
        the population. Recombination in natural populations allows alleles that may be deleterious
        in one genetic background to be reassessed in a different genetic context. Over time, the
        alleles that are transmitted at high frequency across generations represent those with a
        substantial likelihood of contributing positively to an organism's long-term viability in a
        variable environment. For this reason, natural variation is a much more valuable and
        informative reservoir of genes for the purposes of plant improvement than would be an
        equivalent number of induced mutations generated in a laboratory.
      
      
        Domestication—The Winnowing of Natural Genetic Variation
        Cultivars (domesticated varieties) have been selected by humans in the last 10,000 years
        and inevitably represent a subset of the variation found in their wild ancestors. Cultivars
        are recognizable because they manifest characteristics that are associated with
        domestication in plants. Unusual or extreme phenotypes, such as large fruit or seed size,
        intense color, sweet flavor, or pleasing aroma are often selected by humans and maintained
        in their cultivars for aesthetic reasons, while synchronous ripening or inhibition of seed
        shattering (a dispersal mechanism) are selected to facilitate harvest. These phenotypes may
        occur in nature but they will frequently be eliminated by natural selection before they are
        fixed in a population. Because of human selection, cultivars may exemplify a range of
        exaggerated phenotypic attributes that give them the appearance of being, on the whole,
        more diverse than some of the wild populations from which they were derived, but in truth,
        domestication usually represents a kind of genetic bottleneck. Furthermore, cultivars are
        grown in agricultural environments that are generally more uniform than the environments in
        which wild species grow, and this tends to further narrow the gene pool. Thus, while
        cultivars may embody a high degree of obvious phenotypic variation, this may not always be
        a good predictor of the extent of their genetic variation.
        The landrace varieties are the earliest form of cultivar and represent the first step in
        the domestication process. Landraces are highly heterogeneous, having been selected for
        subsistence agricultural environments where low, but stable yields were important and
        natural environmental fluctuation required a broad genetic base (Figure 2). Landraces are
        closely related to the wild ancestors and embody a great deal more genetic variation than
        do modern, high-yielding varieties that are selected for optimal performance within a
        narrow range of highly managed environmental conditions. The value of both the wild species
        and the early landrace varieties in the context of modern plant breeding is that they
        provide a broad representation of the natural variation that is present in the species as a
        whole. The fact that natural selection has acted on such populations over the course of
        evolution makes them particularly valuable as materials for breeders. The value added by
        imposing a low intensity of human selection on the early landraces resides in the fact that
        some of these early varieties represent accumulations of alleles that produce phenotypes
        particularly favorable or attractive to the human eye, nose, palette, or other appetites.
        It is also noteworthy that some of these rare or unique alleles or allele combinations that
        were selected by humans might never survive in the wild.
        Wild relatives and early landrace varieties have long been recognized as the essential
        pool of genetic variation that will drive the future of plant improvement (Bessey 1906;
        Burbank 1914). Early plant collections made by people such as Nikolai Vavilov (1887–1943)
        or Jack Harlan (1917–1998) inspired the international community to establish long-term
        collections of plant genetic resources that provide modern plant breeders with the material
        they need to creatively address the challenges of today (Box 1). Many may question the
        emphasis on wild and primitive landraces that cannot compete with new, high-yielding
        varieties in terms of productivity or eating quality, particularly in an age when
        biotechnology and genetic engineering promise to provide an endless stream of genetic
        novelty. Indeed, if all forms of novelty were equally valuable, the old varieties would
        hardly be worth saving. But the security of the world's food supply depends on an exquisite
        balance between new ideas and the intelligent use of time-tested resources. In 1972, more
        than a decade before the age of automated sequencing, Jack Harlan commented that, “We are
        not really much interested in conserving the old varieties as varieties; it is the genes we
        are concerned about. The old land races can be considered as populations of genes and
        genetic variability is absolutely essential for further improvement. In fact, variability
        is absolutely essential to even hold onto what we already have” (Harlan 1972a).
      
      
        Combining Breeding with Molecular Genetics
        In today's world where automated sequencing and DNA synthesis are mundane activities, it
        may seem contradictory to be worrying about saving or using “old genes.” Can't new ones be
        synthesized to order? Can't we modify a plant at will by introducing a new gene or two into
        an existing variety? Why should we worry about saving populations of historically valuable
        genes in millions of living plant specimens at great cost to the tax-paying public?
        Perhaps it is not the genes themselves we are now in fear of losing. It is the
        information they encode in all their combinatorial complexity. After all, we are only at
        the very beginning of the endeavor to understand the way in which a genotype confers a
        particular set of attributes to a living organism. The subtleties of phenotypic plasticity
        in the face of a changing environment and the layers of genetic redundancy that
        characterize biological systems are largely mysterious. We have only just begun to consider
        the millions and billions of genetic trials and errors that have been evaluated by nature
        over evolutionary time. We cannot even begin to simulate the selective filters that have
        provided us with the diversity of form and function in the living world. We do know that
        living forms of natural diversity are needed to sustain life, and that it would be
        impossible to replace or recreate that diversity if it were lost at this time.
        As plant breeders, we know what to do with living forms of genetic diversity. If we keep
        our options open and learn to better utilize the reservoirs of natural variation that have
        been preserved in our gene banks and in the few remaining in situ populations of wild
        species and landrace varieties, an almost infinite array of novelty can be achieved using
        traditional, time-proven practices involving crossing and selection of genes that have
        withstood the test of evolutionary time (Burbank 1914; Hawkes 1958; Rick 1967; Harlan 1975,
        1976; Peloquin 1983). By restricting the gene pool, we can readily channel a phenotype into
        a constrained and predictable outcome. By expanding the gene pool, we can open up many new
        possibilities for consideration that have not been previously evaluated, would be unlikely
        to be generated in nature, and would not be readily predicted based on current
        knowledge.
        In crosses between wild and cultivated species of inbreeding plants, alleles that were
        “left behind” during the domestication process may be reintroduced into the cultivated gene
        pool. This infusion of “new blood” renews and invigorates modern cultivars in surprising
        and interesting ways. It is not uncommon for some of the inbred progenies derived from
        these crosses to perform better than the better parent (Frey et al. 1975; Rick 1976, 1983;
        Tanksley and McCouch 1997). This phenomenon is known as transgressive variation and results
        from positive interaction between the genotypes of the parents. Today, plant breeders can
        analyze populations derived from wide crosses using molecular markers to determine which
        portions of the chromosomes are associated with the transgressive variation of interest.
        This makes it possible to dissect a complex phenotype and to determine where individual
        genes or, more correctly, quantitative trait loci (QTLs) map along the chromosomes.
        Information about DNA markers linked to QTLs represents a powerful diagnostic tool that
        enables a breeder to select for specific introgressions of interest, a technique referred
        to as “marker-assisted selection.”
        This approach has proven to be extremely successful in several crop species (tomato
        [Bernacchi et al. 1998], hybrid rice [Xiao et al. 1998], inbred rice [Thomson et al. 2003],
        wheat [Huang et al. 2003], barley [Pillen et al. 2003], and pepper [Rao et al. 2003]). In
        China, two introgressions from a wild relative of rice have been associated with a 30%
        increase in the yields of the world's highest-yielding hybrid rice (Deng et al. 2004). In
        tomato, yield increases of greater than 50% resulted from introgressing three independent
        segments from a wild relative, as reported by Gur and Zamir (2004). The effect of these
        introgressions on yield was stable in different genetic backgrounds and in both irrigated
        and drought conditions. This work was facilitated by the availability of a library of
        chromosome segment substitution lines, called introgression lines when the donor is a wild
        species, that provided the foundation for exploring the interactions among the independent
        QTLs. Plant geneticists have long recognized the value of exotic libraries (Brassica
        [Ramsay et al. 1996; Cermankova et al. 1999], millet [Hash 1999], rice [Sobrizal et al.
        1996; Ghesquiere et al. 1997; Ahn et al. 2002], tomato [Monforte and Tanksley 2000; Zamir
        2001], wheat [Sears 1956; Pestsova et al. 2002, 2003], and Arabidopsis [Koumproglou et al.
        2002]). They represent a permanent genetic resource that greatly facilitates the
        utilization of wild and exotic germplasm in a breeding program, and they are also an
        efficient reagent for the discovery and isolation of genes underlying traits of
        agricultural importance.
      
      
        Uncovering the Genes That Underlie Agronomic Traits
        Several genes underlying traits of agricultural importance have been cloned using
        substitution lines derived from interspecific or intersubspecific crosses (Martin et al.
        1993; Song et al. 1995; Frary et al. 2000; Yano et al. 2000; Takahashi et al. 2001; Yano
        2001), including one of the yield QTLs targeted by Gur and Zamir (Fridman et al. 2000).
        While the identity of the yield gene conferring the phenotype was not critical to the
        success of the cultivar development scheme described by Zamir and Gur (2004), there is
        great curiosity to understand the gene(s) or genes and genetic mechanisms that underlie
        traits of interest to agriculture. In some cases, knowing the gene or the exact functional
        nucleotide polymorphism within the gene that determines the phenotype (Bryan et al. 2000;
        Robin et al. 2002) may dramatically improve the resolution of selection during the breeding
        process. It also may allow a breeder to make more informed decisions about which germplasm
        resources to use as parents in a crossing program and which genes within those resources to
        use in a pyramiding scheme.
        As more genes of interest are cloned and their contributions to complex biological
        systems are better understood, there will be many opportunities for creative synthesis of
        new varieties. It is likely that some of the opportunities will involve genetic engineering
        approaches, where new information about genes, gene regulation, and plant responses to the
        environment may be used in innovative ways to fine-tune existing plant varieties so that
        they utilize resources more efficiently, provide greater nutritional value, or simply taste
        better.
      
      
        Natural Variation and Food Security
        The scientific enterprise has always challenged beliefs about the way the world
        functions, its origins, and its possibilities. Deeply held beliefs are frequently resistant
        to the most carefully crafted scientific explanations. When belief systems are unconscious,
        they may prove particularly resilient to change. Occasionally, science provides an
        interpretation that fits cleanly into the framework of existing ideas, and then it is
        heralded with great applause, and often with a sense of relief. When this is not the case,
        public opinion tends to react fitfully, with many starts and stops. Public opinion has been
        on a roller coaster recently with respect to transgenic organisms in agriculture. This is
        in response to what is perceived to be a kind of scientific intrusion into the intimacy of
        the relationship between humans and their food supply. This relationship is inherently
        complex, representing a textured fabric of historical, cultural, geographic, economic,
        biological, and aesthetic concerns. Despite the fact that food is increasingly treated as a
        commodity in today's global economy, human culture the world over has always recognized
        that food represents more than a biological remedy for hunger. Food is a force that brings
        diverse people together, it provides a focal point for human discourse, and it enhances our
        enjoyment of life. Food also has a spiritual component. Harvesting other living organisms
        to support human life represents a powerful connection between different spheres of the
        natural world.
        At some level, the idea of using natural genetic variation found in wild species and
        early landrace varieties to revitalize modern crop varieties is both emotionally appealing
        and intellectually compelling. As a “smart breeding” strategy, it will facilitate the
        exploration and utilization of natural genetic variation, expanding the genetic base of our
        crop plants and providing more flexibility for the future. By using a marker-assisted
        approach, it will provide a noninvasive road map to expedite the selective introgression of
        useful traits in the years ahead. Because the approach is primarily useful for
        self-pollinating species (as opposed to cross-pollinators), variety development can go
        forward with the expectation that new varieties can be developed and distributed as inbred
        strains. This will come as very good news to people who are concerned about the
        infrastructural requirements needed to maintain a hybrid seed industry. Inbred variety seed
        can be saved from year to year without noticeable loss of vigor. Farmers are free to
        amplify the varieties and pass seed on to their neighbors if it proves valuable. Plant
        breeders living in parts of the world where germplasm diversity is highest are in the best
        position to explore its value. Until now, there have been few opportunities to make use of
        the wealth of natural diversity that abounds in many countries where people are the poorest
        and population is growing the fastest. This approach offers a way forward and can help
        people make good use of locally available resources to enhance the food security of their
        own nations.
        As we consider the implementation of smart breeding efforts in the future, we might ask,
        who will have access to nature's reserves of genetic diversity? How will knowledge about
        the patterns that govern the generation and selective elimination of that diversity help
        guide conservation efforts as well as current and future crop improvement efforts? What are
        the limits to biological variation? How far can we push those limits, and what will be the
        consequences of not pushing them? Who will participate in the endeavor? What will the rules
        of engagement be? What tools can we use to expedite the effort?
        What genetic characteristics will help us cope with climate change, global warming, the
        emergence of new pests and diseases, depleted soils, shortages of fresh water, and
        increasing levels of water and air pollution? What trace minerals, vitamins, and other
        metabolites will we need to breed into the crops of the future to fight the causes of
        hidden hunger, to prevent cancer, or to enhance the immune system? The combinatorial
        possibilities for crop improvement are almost infinite, as long as we maintain our options.
        Faced with a clear choice today, it is obvious that enhancing the potential for genetic
        flexibility in the future is a wise course of action and one we ignore at our peril.
      
    
  

  
    
      
        
        Skeletal muscle demonstrates a remarkable plasticity, adapting to a variety of external
        stimuli (Booth and Thomason 1991; Chibalin et al. 2000; Hawley 2002; Flück and Hoppeler
        2003), including habitual level of contractile activity (e.g., endurance exercise
        training), loading state (e.g., resistance exercise training), substrate availability
        (e.g., macronutrient supply), and the prevailing environmental conditions (e.g., thermal
        stress). This phenomenon of plasticity is common to all vertebrates (Schiaffino and
        Reggiani 1996). However, there exists a large variation in the magnitude of adaptability
        among species, and between individuals within a species. Such variability partly explains
        the marked differences in aspects of physical performance, such as endurance or strength,
        between individuals, as well as the relationship of skeletal muscle fiber type composition
        to certain chronic disease states, including obesity and insulin resistance.
        In most mammals, skeletal muscle comprises about 55% of individual body mass and plays
        vital roles in locomotion, heat production during periods of cold stress, and overall
        metabolism (Figure 1). Thus, knowledge of the molecular and cellular events that regulate
        skeletal muscle plasticity can define the potential for adaptation in performance and
        metabolism, as well as lead to the discovery of novel genes and pathways in common clinical
        disease states.
      
      
        How Is Skeletal Muscle Fiber Type Classified?
        Much of our early understanding of the plasticity of skeletal muscle has been derived
        from studies undertaken by exercise physiologists (e.g., Holloszy 1967). With the
        application of surgical techniques to exercise physiology in the late 1960s (Bergstrom and
        Hultman 1966), it became possible to obtain biopsy samples (∼150 mg) of human skeletal
        muscle, and by means of histological and biochemical analyses, specific morphological,
        contractile, and metabolic properties were identified. In 1873, the French anatomist Louis
        Antoine Ranvier had already observed that some muscles of the rabbit were redder in color,
        and contracted in a slower, more sustained manner, than paler muscles of the same animal.
        These early observations formed the basis of the classical terminology of red and white
        muscle fibers, which was subsequently found to be related to myoglobin (an iron-containing
        oxygen-transport protein in the red cells of the blood) content (Needham 1926). Based upon
        histochemical staining (Engel 1962), muscle fibers are now commonly distinguished as
        slow-twitch (ST), which stain dark or red, and fast-twitch (FT), which stain light or pale.
        In humans, a further subdivision of the FT fibers is made (Brooke and Kasier 1970), whereby
        the more aerobic (or oxidative) FT fiber is designated FT
        a , and the more anaerobic (glycolytic) fiber is termed FT
        b . Under aerobic conditions (sufficient oxygen supply to the working
        muscles), energy is produced without the production of lactate. Under anaerobic conditions
        (insufficient oxygen supply to the working muscles), energy is produced via the glycolytic
        pathway, which results in lactate accumulation and in turn limits anaerobic exercise. Thus,
        muscle fibers can be classified in terms of contractile and metabolic properties (Table
        1).
        All individuals have different capacities to perform aerobic or anaerobic exercise,
        partly depending on their muscle fiber composition. In untrained individuals, the
        proportion of ST fibers in the 
        vastus lateralis muscle (the largest of the quadriceps muscles and the
        most commonly studied muscle in humans), is typically around 55%, with FT
        a fibers being twice as common as FT
        b fibers (Saltin et al. 1977). While marked differences in the metabolic
        potentials between FT
        a and FT
        b fibers are observed in untrained humans, the absolute level for the
        activities of oxidative and glycolytic enzymes in all fiber types is large enough to
        accommodate substantial aerobic and anaerobic metabolism (Saltin et al. 1977). While there
        is a large degree of homogeneity within individual skeletal muscles from rodents (Delp and
        Duan 1996), this is not the case for humans (Saltin et al. 1977). The dramatic
        heterogeneity of fiber type composition between people may explain their remarkable
        variation in exercise performance.
      
      
        Does Muscle Fiber Type Composition Influence Athletic Performance?
        During the 1970s and 1980s, it was popular to determine the muscle fiber composition of
        athletes from different sports events. These studies revealed that successful endurance
        athletes have relatively more ST than FT fibers in the trained musculature (Costill et al.
        1976; Fink et al. 1977; Saltin et al. 1977). In contrast, sprinters have muscles that are
        composed predominantly of FT fibers (Costill et al. 1976). Accordingly, the belief that
        muscle fiber type can predict athletic success gained credibility. In particular, the
        notion that the proportion of ST fibers might be a factor governing success in endurance
        events was proposed (Gollnick et al. 1972; Costill et al. 1976).
        In this regard, the results of Fink et al. (1977) are important. These researchers
        determined the fiber composition from the 
        gastrocnemius muscle (the muscle of the calf of the leg) of 14 elite male
        long distance runners, 18 good (but not world-class) male long distance runners, and 19
        untrained men. The elite group included Olympic medal winners (Figure 2) and American
        record holders at the time. Muscle from the elite runners contained a larger proportion of
        ST fibers than either the good runners or the untrained men (79.0% ± 3.5% versus 61.8% ±
        2.9% versus 57.7% ± 2.5% respectively; 
        p < 0.05). The values found for several of the elite runners were the
        highest observed in human muscle (> 92% ST). Moreover, the ST fibers from the elite
        runners were 29% larger than FT fibers (
        p < 0.05), and both ST and FT fibers were larger in the good runners
        than in the untrained men. Because of the marked hypertrophy (bulk increase) of the ST
        fibers in the elite runners, the cross-sectional area composed of these fibers was greater
        than either the good runners or the untrained subjects (82.9% ± 3.1% versus 62.1% ± 2.6%
        versus 60.0% ± 2.7% respectively; 
        p < 0.05). When the data from the elite and good runners was combined,
        a positive correlation between the proportion of ST fibers and the best 6-mile performance
        time was noted (
        r = −0.62, 
        p < 0.05).
        However, fiber type alone did not determine the performances of the elite athletes. For
        example, two athletes with similar best times for the 42.2 km marathon distance
        (approximately 2 hr 18 min) had 50% versus 98% ST muscle fibers. Subsequent work (Foster et
        al. 1978) revealed that endurance running performance was better related to an athlete's
        maximal O
        2 uptake (VO
        2max ; 
        r = −0.84, −0.87, and −0.88 for 1-, 2-, and 6-mile times, respectively).
        Indeed, while an athlete's muscle fiber type is an important morphological component and is
        related to several contractile and metabolic properties (see Table 1), other physiological
        factors (e.g., VO
        2max , maximal cardiac output, and speed/power output at the lactate
        threshold) are more likely to determine the upper limits of endurance capacity (Coyle 1995;
        Hawley and Stepto 2001).
      
      
        Do Alterations in Skeletal Muscle Fiber Type Contribute to Metabolic Disease?
        The close coupling between muscle fiber type and associated morphological, metabolic,
        and functional properties is not confined to athletic ability. Insulin sensitivity also
        correlates with the proportion of ST oxidative fibers (Lillioja et al. 1987). Specifically,
        insulin-stimulated glucose transport is greater in skeletal muscle enriched with ST muscle
        fibers (Henriksen et al. 1990; Song et al. 1999; Daugaard et al. 2000), thus priming ST
        muscle for accelerated glucose uptake and metabolism. A shift in fiber distribution from ST
        to FT fibers gives rise to altered activities of key oxidative and glycolytic enzymes
        (Pette and Hofer 1980). Indeed, the ratio between glycolytic and oxidative enzyme
        activities in the skeletal muscle of non-insulin-dependent diabetic or obese individuals is
        related to insulin resistance (Simoneau et al. 1995; Simoneau and Kelley 1997). Similarly,
        with ageing and physical inactivity, two other conditions associated with ST-toFT
        fiber-type transformation, oxidative capacity and insulin sensitivity, are diminished (Papa
        1996).
      
      
        Genes That Define Skeletal Muscle Phenotype
        Skeletal muscle fiber-type phenotype is regulated by several independent signaling
        pathways (Figure 3). These include pathways involved with the Ras/mitogen-activated protein
        kinase (MAPK) (Murgia et al. 2000), calcineurin (Chin et al. 1998; Naya et al. 2000),
        calcium/calmodulin-dependent protein kinase IV (Wu et al. 2002), and the peroxisome
        proliferator γ coactivator 1 (PGC-1) (Lin et al. 2002). The Ras/MAPK signaling pathway
        links the motor neurons and signaling systems, coupling excitation and transcription
        regulation to promote the nerve-dependent induction of the slow program in regenerating
        muscle (Murgia et al. 2000). Calcineurin, a Ca
        2+ /calmodulin-activated phosphatase implicated in nerve
        activity-dependent fiber-type specification in skeletal muscle, directly controls the
        phosphorylation state of the transcription factor NFAT, allowing for its translocation to
        the nucleus and leading to the activation of slow-type muscle proteins in cooperation with
        myocyte enhancer factor 2 (MEF2) proteins and other regulatory proteins (Chin et al. 1998;
        Serrano et al. 2001). Calcium-dependent Ca
        2+ /calmodulin kinase activity is also upregulated by slow motor neuron
        activity, possibly because it amplifies the slow-type calcineurin-generated responses by
        promoting MEF2 transactivator functions and enhancing oxidative capacity through
        stimulation of mitochondrial biogenesis (Wu et al. 2002).
        PGC1-α, a transcriptional coactivator of nuclear receptors important to the regulation
        of a number of mitochondrial genes involved in oxidative metabolism, directly interacts
        with MEF2 to synergistically activate selective ST muscle genes and also serves as a target
        for calcineurin signaling (Lin et al. 2002; Wu et al. 2001). New data presented in this
        issue of 
        PLoS Biology (Wang et al. 2004) reveals that a peroxisome
        proliferator-activated receptor δ (PPARδ)-mediated transcriptional pathway is involved in
        the regulation of the skeletal musclefiber phenotype. Mice that harbor an activated form of
        PPARd display an “endurance” phenotype, with a coordinated increase in oxidative enzymes
        and mitochondrial biogenesis and an increased proportion of ST fibers. Thus—through
        functional genomics—calcineurin, calmodulin-dependent kinase, PGC-1α, and activated PPARδ
        form the basis of a signaling network that controls skeletal muscle fiber-type
        transformation and metabolic profiles that protect against insulin resistance and
        obesity.
        The transition from aerobic to anaerobic metabolism during intense work requires that
        several systems are rapidly activated to ensure a constant supply of ATP for the working
        muscles. These include a switch from fat-based to carbohydrate-based fuels, a
        redistribution of blood flow from nonworking to exercising muscles, and the removal of
        several of the byproducts of anaerobic metabolism, such as carbon dioxide and lactic acid.
        Some of these responses are governed by transcriptional control of the FT glycolytic
        phenotype. For example, skeletal muscle reprogramming from a ST glycolytic phenotype to a
        FT glycolytic phenotype involves the Six1/Eya1 complex, composed of members of the Six
        protein family (Grifone et al. 2004). Moreover, the Hypoxia Inducible Factor-1α (HIF-1α)
        has been identified as a master regulator for the expression of genes involved in essential
        hypoxic responses that maintain ATP levels in cells. In this issue of 
        PLoS Biology (Mason et al. 2004), a key role for HIF-1α in mediating
        exercise-induced gene regulatory responses of glycolytic enzymes is revealed. Ablation of
        HIF-1α in skeletal muscle was associated with an increase in the activity of rate-limiting
        enzymes of the mitochondria, indicating that the citric acid cycle and increased fatty acid
        oxidation may be compensating for decreased flow through the glycolytic pathway in these
        animals. However, hypoxia-mediated HIF-1α responses are also linked to the regulation of
        mitochondrial dysfunction through the formation of excessive reactive oxygen species in
        mitochondria.
      
      
        Can You Become a Slow-Twitcher?
        With the 2004 Olympics still fresh on our minds, many will ask: Who has the right stuff
        to go the distance? Athletes like Olympic champion Frank Shorter are clearly exceptional
        and represent an extreme in human skeletal muscle phenotype. Realistically, few of us can
        ever hope to run a marathon in world-class time. However, there may be cause for some
        optimism for the average mortal, since endurance exercise training in healthy humans leads
        to fiber-type specific increases in the abundance of PGC-1 and PPAR-α protein in skeletal
        muscle (Russell et al. 2003). Moreover, functional genomics support the concept that
        skeletal muscle remodeling to a ST phenotype, either through activated calcineurin or
        PPARδ, can protect against the development of dietary-induced insulin resistance (Ryder et
        al. 2003) and obesity (Wang et al. 2004). The results of these studies have clinical
        relevance since insulin-resistant elderly subjects and offspring of patients with type 2
        diabetes mellitus have skeletal muscle mitochondrial dysfunction (Petersen et al. 2003;
        Petersen et al. 2004). Clearly, further translational studies in humans are required to
        test the hypothesis that increasing the proportion of ST oxidative muscle fibers will
        overcome the mitochondrial dysfunction and metabolic defects associated with
        insulin-resistant states.
      
    
  

  
    
      
        
        Most bird-pollinated flowers are both red and rich in nectar. The traditional
        explanation for this association is that, since red is inconspicuous to bees, it evolved to
        prevent bees from depleting the nectar of bird-pollinated flowers without effecting
        pollination. But bees can see, and they actually visit red flowers. So why are most
        bird-pollinated flowers red? To help answer this question, we need to consider how the
        outcomes of foraging decisions are affected by the community in which individuals live, and
        by the foraging options of other individuals.
      
      
        The Mystery
        Plants face a trade-off between attracting pollinators and remaining hidden from flower
        parasites (such as nectar robbers and seed predators). Consequently, there is often strong
        selection pressure for highly specific communication channels that can advertise the
        presence of their flowers to effective pollinators but not to other individuals. Many
        aspects of pollinator syndromes are best understood in these terms (Proctor et al. 1996).
        For example, flowers that are pollinated by birds—bird flowers—produce nectar at much
        higher rates than those pollinated by bees (Stiles 1981). If a bee is attracted to such a
        flower, it might sometimes remove nectar and pollen without providing an outcrossing
        service (i.e., bringing pollen from a different plant of the same species) to the flower.
        Therefore, bird-pollinated flowers should advertise their presence to birds, but not to
        bees. Following this line of reasoning, Peter Raven (1972) suggested more than thirty years
        ago that bird-pollinated flowers were predominantly red because ‘red is the only color of
        the spectrum that is at once inconspicuous to most insects and also an excellent “signal”
        of a high caloric reward for birds’. Raven's interpretation of inconspicuousness was soon
        transformed into invisibility; it was assumed that bees did not visit red flowers because
        they couldn't detect them (Proctor et al. 1996; Vogel 1996).
        However, this interpretation no longer holds. Chittka and Waser (1997) have shown that
        red flowers are not actually invisible to bees. Indeed, typical bird flowers with no UV
        reflectance, such as the scarlet gilia (
        Ipomopsis aggregata ) and the scarlet monkeyflower (
        Mimulus cardinalis ) (Figure 1), are routinely visited and
        exploited by different bee species (reviewed by Chittka and Waser 1997). Moreover, when
        bees are extremely abundant, they can drive birds away from red flowers. 
        Echium wildpretii , an endemic of the Canary Islands, presents
        an entomophylous (‘insect-loving’) and an ornithophyllous (‘bird-loving’) subspecies
        (Figure 2) that differ in flower colour: 
        E. wildpretii trichosiphon , endemic to La Palma Island, has
        entomophylous, pink flowers, whereas 
        E. wildpretii wildpretii , endemic to Tenerife Island, has
        ornithophyllous, red flowers, pollinated by generalist native birds and insects. 
        E. wildpretii wildpretii is pollinated predominantly by birds
        early in the season until introduced honeybees (
        Apis mellifera , which have increased enormously in number
        because of apiculture) deplete the nectar and displace nectar-feeding birds (Valido et al.
        2002).
        So if red flowers are not invisible to bees, why are most bird-pollinated flowers red?
        Perhaps birds are particularly apt at detecting red objects (Chittka and Waser 1997)?
        Again, this is not strictly true. Although all birds detect red objects and some birds do
        have their greatest spectral sensitivity and finest hue discrimination towards the
        long-wavelength (red) end of the spectrum (Stiles 1981), they can also respond to
        ultraviolet light, and there is no evidence that, for example, hummingbirds have greater
        spectral sensitivity or greater spectral discrimination ability in the red part of the
        spectrum (Goldsmith and Goldsmith 1979). Feeding experiments, where hummingbirds are given
        nectar in artificial flowers of different colours, show no inherited colour preferences;
        hummingbirds have temporary preferences that can be modified by conditioning (Proctor et
        al. 1996). So are there other clues as to how this mystery might be solved?
      
      
        The Visual System of Bees
        One clue might come from the visual system of bees. Humans perceive light with a
        wavelength above approximately 600 nm as red (Buser and Imbert 1968). Most bees have three
        types of colour receptors, with sensitivity peaks at 340, 430, and 540 nm (Chittka 1996),
        although a very few bee species have sensitivity peaks at substantially longer wavelengths.
        For the majority, however, provided that the light source is sufficiently intense, red
        light (up to 650 nm) will stimulate the 540 nm receptor of bees (Chittka and Waser 1997).
        Bees will therefore perceive red objects. To discriminate red flowers from their green
        background, bees must rely essentially on the difference between the intensity of the
        signal that flower and foliage generate on the bees' ‘green’ (540 nm) receptor (Giurfa et
        al. 1996). Therefore, depending on the relative intensity of the green and red sources,
        bees may or may not be able to discriminate between red flowers and green foliage (Chittka
        and Waser 1997).
        Because of the structure of their visual system, bees trained to feed at artificial red
        flowers take longer to find their goals than bees trained to feed at other-coloured flowers
        (Spaethe et al. 2001). In a real environment, where red flowers would be more camouflaged
        against the different shades and intensities of the green foliage, the ability of bees to
        discriminate red flowers should be further reduced.
      
      
        Colour Vision and Niche Partition
        The fact that bees require more time to find red flowers than other-coloured flowers,
        together with some results from optimal-foraging theory, outlined here, could unlock the
        mystery and explain the association between red coloration and bird pollination in
        flowers.
        When different animals, either from the same or different species, are forced to share
        some resources, any degree of specialization tends to result in habitat selection
        (Rosenzweig 1981). In 1992, Possingham, developed a ‘habitat selection’ model that showed
        how two nectar-feeding pollinator species, which differed in their foraging efficiency,
        would forage on two types of flowers. Although an abstract model, we can use it to
        illustrate how birds might interact with bees at different-coloured flowers.
        Consider a community that includes bees and birds, and red and blue flowers. Let us
        assume that the flowers differ only in their colour, that there are only two patches of
        flowers (one of blue, the other of red flowers), and that the density of flowers is the
        same in both patches. (For a general analysis, with the same qualitative results, see
        Possingham 1992.)
        The question is: how many birds and bees should forage at the red and blue patches so
        that their intake of nectar is maximised? The expected intake rate is the average amount of
        nectar obtained per flower (or standing crop) divided by the time it takes to find and
        exploit a flower. If the flowers are the same distance apart and birds can detect red and
        blue flowers equally well, then travel time is independent of flower colour. Under these
        circumstances, an ecological equilibrium, with birds exploiting red and blue flowers
        equally, would indicate that the amount of nectar available from both flower colours was
        identical.
        Now add a few bees to this community of birds, sufficiently few that their intake of
        nectar is negligible. We know that the standing crop is the same at red and blue flowers.
        However, we also know that bees require more time to find red flowers than blue ones
        (Spaethe et al. 2001), so their intake rate of nectar will be higher at blue flowers, and
        they will all go to the blue patch.
        If we continue to add bees one at a time to this community, then sooner or later, the
        number of bees will no longer be sufficiently low for us to ignore their depleting effect
        on the nectar available. What will happen at that point? Will bees now start visiting red
        flowers? Not yet. For a bee to visit the red patch, the difference in standing crop between
        red and blue flowers would have to be large enough to compensate for the difference in
        detection time. Before that happens, some birds will shift to the red patch. Indeed, since
        birds require the same time to detect red or blue flowers, some birds will move from the
        blue to the red patch as soon as bees start to noticeably reduce the nectar available from
        the blue flowers.
        What Possingham's model predicts, therefore, is that when the number of bees is large
        enough, all birds will forage at the red patch. Only when the difference in standing crop
        between red and blue flowers is so large that it compensates for the reduced detectability
        of red flowers, will bees start visiting the red patch.
        To conclude, there will be an association between red flowers and birds. Birds will
        exploit red flowers, and bees blue flowers. In addition, depending on the relative
        abundance of bees and birds (and of red and blue flowers), either birds or bees, but never
        both simultaneously, can also exploit the other flower type (Figure 3).
      
      
        Niche Partition and the Evolution of Red Flowers
        Possingham's model (1992) helps to explain the ecological association between flower
        colour and pollinator type, provided that both flower colours and pollinator types are
        present—but why did the red colouration of these flowers evolve in the first place? We
        believe that the model can also help explain the evolution of red coloration in
        bird-pollinated flowers.
        To understand the evolutionary process, consider a community where bees and birds are
        present, and where two flower species coexist. One flower type, the generalist flower, is
        blue and is efficiently pollinated by bees and birds alike. The blue Rocky Mountain
        penstemon, 
        Penstemon strictus , provides a good example (Castellanos et al.
        2003). The other flower type, or bird flower, is yellow and is efficiently pollinated by
        birds, but not by bees—the red beardlip penstemon 
        P. barbatus provides an example of this type (Castellanos et al.
        2003). If bee visits were costly for the ancestral bird flowers, they would experience a
        selective pressure to become red. Bees could impose several costs on the ancestral bird
        flowers; for example, the number of hummingbird visits may depend on the amount of nectar
        available in the flowers.
        Throughout evolutionary history, there will be variability and heritability in flower
        colour (as documented for 
        Mimulus by Bradshaw et al. 1995). Since both bees and birds
        easily detect and efficiently pollinate generalist blue flowers, there is no particular
        reason to expect that their colour will evolve in one direction or another. Things are
        otherwise for bird flowers, which are more efficiently pollinated by birds. For simplicity,
        consider that, at any given time, this bird flower comes in only two shades of colour, one
        of them with a slightly longer wavelength (an orange morph). On an ecological timescale,
        yellow flowers will be visited mainly by bees and orange flowers mainly by birds. Orange
        flowers, being more efficiently pollinated by birds, will therefore have higher fitness
        than yellow flowers, and given enough time, there will be selection for bird flowers to
        become orange. In the absence of other costs, mutant flowers with higher wavelengths (i.e.,
        becoming redder) can invade a population of yellow flowers so long as bird flowers continue
        to be visited by bees (unpublished data). So bird flowers will continue to shift their
        colour until bees are completely excluded from the bird flowers or until further shifts
        deteriorate detectability by birds.
        This explanation for the evolution of red coloration in bird-pollinated flowers differs
        from the one proposed by Raven (1972) in a key respect. In our view, the main point is not
        that bees fly over red flowers without seeing them; it is not even that they are unable to
        exploit red flowers efficiently in absolute terms. It is rather a question of relative
        efficiency that makes bees avoid red flowers when birds are depleting their nectar; it
        would work just as well if birds were colourblind and perceived red flowers as badly as
        flowers of other colours. Of course, Possingham's model (1992) is not incompatible with
        birds being more efficient than bees at exploiting red flowers, and the results would be
        strengthened if, as has been suggested (Raven 1972; Chittka and Waser 1997), birds are
        better at detecting red flowers than blue ones.
      
      
        Toward a Solution
        Comparable problems can be found in other plant–pollinator systems. For example, when
        several species of bumblebees coexist, resource partitioning normally doesn't follow
        colour, but is dependent on different parameters: the corolla length of the plant and the
        proboscis length of the bee. Proboscis length affects the efficiency with which flowers of
        different depth are exploited (Inouye 1980); bumblebees with long proboscises
        preferentially exploit flowers with deep corollas, while bumblebees with short proboscises
        exploit shallow flowers (Heinrich 1976). But a bumblebee with a long proboscis can also
        exploit shallow flowers, and, to some extent, a bumblebee with a short proboscis can
        exploit deep flowers, if corollas are not too deep (although they will still leave some
        nectar behind). Indeed, when one bumblebee type is experimentally removed, the other one is
        seen to exploit both deep and shallow flowers (Inouye 1978). The same, we believe, should
        happen with flower colour: the experimental removal of birds should lead to the systematic
        exploitation of red flowers by bees, at least when corolla tube morphology does not prevent
        bees from accessing the nectar.
        In fact, there is even no need to perform experimental bird removals, because plants
        provide us with a ready-made design: bees visit flowers searching for both nectar and
        pollen, while most birds exploit only the nectar. Hence, bees should readily collect pollen
        at red bird flowers. There are numerous examples of this, although in most cases they are
        indirectly documented. For example, solitary bees and syrphid and muscoid flies visit the
        red, hummingbird-pollinated flowers of 
        Ipomopsis aggregata to collect pollen when hummingbirds visits
        are frequent, while bumblebee (
        Bombus appositus ) visits to collect nectar are only common when
        hummingbird visits are rare (Mayfield et al. 2001). Outside the native range of
        bird-pollinated plants, the same phenomenon can be observed: in Spanish gardens, the
        honeybee collects pollen from 
        Aloe arborescens plants. Bees cannot access the nectar,
        concealed at the bottom of the corolla tube. This is opportunistically collected by birds
        such as the Sardinian warbler 
        Sylvia melanocephala (unpublished data).
        Another comparison of interest concerns beetle-pollinated flowers, which in the
        Mediterranean region have open, bowl shapes and red coloration (Dafni et al. 1990). 
        Amphicoma beetles are more efficient pollinators of these
        flowers than commonly occurring bees (Dafni et al. 1990), so the red coloration of these
        flowers might help to keep other visitors (possibly bees and flies) at bay. Indeed, other
        bowl-shaped flowers of different colours (such as yellow, white, and purple, e.g., in the
        genera 
        Cistus and 
        Helianthemum ) are commonly visited by pollen-collecting bees
        and bumblebees. A particularly interesting test case is provided by the corn poppy 
        Papaver rhoeas ; in the eastern Mediterranean region, it is
        pollinated by beetles and does not reflect in the UV (Dafni et al. 1990), while in central
        and western Europe it reflects in the UV (Daumer 1958) and is pollinated by bees.
        Although refinements of Possingham's model, such as developing a prey-model version, or
        introducing stochasticity or several foraging constraints, might help us determine the
        extent to which we should expect resource partitioning along the colour dimension to take
        place, it is, in our view, far more pressing to determine the extent and conditions under
        which bees exploit red flowers (i.e., through comparisons of pollen vs. nectar
        exploitation, bird exclusion experiments, etc.), the detection time of red flowers against
        a natural background, and the effect of flower colour and size on flight mode in the field.
        Only then will we be able to fully unravel the factors that solve this fascinating
        mystery.
      
    
  

  
    
      
        
        The global debate over access to primary research literature heated up this summer,
        fueled by a slew of congressional and parliamentary recommendations, claims of political
        victory by critics and proponents of open access, and redoubled lobbying efforts on every
        side of the issue. After months of often dizzying rhetoric from virtually all camps, one
        concrete development has indisputably emerged from the fray: governments around the world
        have begun to take an interest in the question of who can and can't read the results of the
        scientific research they fund. “We are convinced,” concluded a recent report from the
        Science and Technology Committee of the United Kingdom's House of Commons, “that the amount
        of public money invested in scientific research and its outputs is sufficient to merit
        Government involvement in the publishing process” (House of Commons Science and Technology
        Committee 2004). United States National Institutes of Health (NIH) Director Elias Zerhouni
        echoed the British assessment, asserting that “the public needs to have access to what
        they've paid for,” in a July 28 meeting of stakeholders in scientific and medical
        publishing. “The status quo,” he added, “just can't stand” (Park 2004).
        While such pronouncements may sow fear in the hearts of some scientists and publishers,
        concerns that governments are poised to tell researchers where or how to publish seem
        largely unfounded. Both the UK report and rumblings from the US government suggest that any
        legislative dictates on access to scientific literature are likely to be structured to
        minimize potentially deleterious implications for established, subscription-based journals,
        for-profit and not-for-profit alike. Mandates for open access to articles summarizing the
        results of publicly funded research would not be mandates for scientists to submit work
        only to the handful of journals, like 
        PLoS Biology and 
        PLoS Medicine, that currently make their content immediately free online
        in centralized repositories. A US House of Representatives Committee on Appropriations, for
        example, recently passed language that would allow many, though not all, publishers six
        months between the date of publication of NIH-funded research articles and the date of
        their deposition in a free-to-use archive. (At the time of this writing, the bill is
        awaiting further discussion in the House and Senate.)
        In any case, it is a perfectly reasonable premise that governments should attach
        conditions to grants mandating public access to resulting peer-reviewed, published
        articles. Making funding for research contingent on the results of the work being
        disseminated as widely as possible is hardly a revolutionary proposition. All funders
        expect, of course, that scientists won't simply stash their findings in a desk drawer.
        Most, like NIH, include in their mission statements clauses about “fostering the
        communication of medical and health sciences information” (NIH 2004). The US National
        Library of Medicine, a division of NIH, goes so far as to provide the infrastructure for
        hosting and storing the full texts of journal articles online, in the form of PubMed
        Central. Actually requiring that publicly funded works be 
        included in publicly funded electronic archives like PubMed Central, as
        the US Congress might, would be less a paradigm shift or a radically interventionist
        mandate than a sensible extension of existing policy for most governments and their funding
        agencies.
        Increasingly, it seems, this is the view being adopted by policy makers—that it is the
        status quo, rather than prospective policy revision, that is anomalous or hard to justify.
        “We would be very surprised,” the Science and Technology Committee notes, “if Government
        did not itself feel the need to account for its investment [in research] in the publishing
        process. We… hope that this report will be a catalyst for change” (House of Commons Science
        and Technology Committee 2004).
        As a matter of sheer principle, it strikes many people as odd that “anyone can download
        medical nonsense from the Web for free, but citizens must pay to see the results of
        carefully conducted biomedical research that was financed by their taxes,” as Rick Weiss
        noted on the front page of the 
        Washington Post last year (Weiss 2003). While neither the US nor the UK
        has yet to legislate a remedy for this 
        prima facie paradoxical state of affairs, both appear ready to address
        the issue systematically, and—more significantly—with the input of a wide range of affected
        constituents: scientists, publishers, librarians, patient advocates, text-miners,
        entrepreneurs, and more. The Science and Technology Committee (2004) report was the product
        of a seven-month investigation, featuring some 127 submissions of written evidence and four
        days of oral testimony from the likes of Nature Publishing Group, Reed Elsevier, and
        indeed, the Public Library of Science. NIH has promised a period of public comment on its
        plan for implementing the Appropriations Committee's requirement before moving forward, in
        addition to the information-gathering meeting of publishers in July and subsequent meetings
        hosted by Dr. Zerhouni. All told, the current spate of government attention to the issue of
        public access to research results seems methodical, inclusive, and likely to prove
        productive for scientific communities and the public.
      
    
  

  
    
      
        
        In this issue of 
        PLoS Biology, Hebert et al. (2004) have set out to test the resolution
        and performance of “DNA barcoding,” using a single mtDNA gene, cytochrome 
        c oxidase I (COI), for a sample of North American birds. Before turning
        to details of this study, it is useful as context to consider the following questions: What
        is DNA barcoding, and what does it promise? What is new about it? Why is it controversial?
        What are the potential pitfalls?
        Put simply, the intent of DNA barcoding is to use large-scale screening of one or a few
        reference genes in order to (i) assign unknown individuals to species, and (ii) enhance
        discovery of new species (Hebert et al. 2003; Stoeckle 2003). Proponents envisage
        development of a comprehensive database of sequences, preferably associated with voucher
        specimens representing described species, against which sequences from sampled individuals
        can be compared. Given the long history of use of molecular markers (e.g., allozymes, rDNA,
        and mtDNA) for these purposes (Avise 2004), there is nothing fundamentally new in the DNA
        barcoding concept, except increased scale and proposed standardization. The former is
        inevitable. Standardization, i.e., the selection of one or more reference genes, is of
        proven value in the microbial community and in stimulating large-scale phylogenetic
        analyses, but whether “one gene fits all” is open to debate.
        Why, then, all the fuss? Initial reactions to the DNA barcoding concept have ranged from
        unbridled enthusiasm, especially from ecologists (Janzen 2004), to outright condemnation,
        largely from taxonomists (e.g., see the February 2003 issue of 
        Trends in Ecology and Evolution ). The former view reflects a real need
        to connect different life history stages and to increase the precision and efficiency of
        field studies involving diverse and difficult-to-identify taxa. The criticisms are mainly
        in response to the view that single-gene sequences should be the primary identifier for
        species (“DNA taxonomy”; Tautz et al. 2002; see also Blaxter 2004). At least for the
        macrobiota, the DNA barcoding community has moved away from this to emphasize the
        importance of embedding any large-scale sequence database within the existing framework and
        practice of systematics, including the importance of voucher specimens and of integrating
        molecular with morphological characters. Another point of contention—that DNA barcodes have
        limited phylogenetic resolution—arises from confusion about the scope of inference. At
        best, single-gene assays can hope to identify an individual to species or reveal
        inconsistencies between molecular variation and current perceptions of species boundaries.
        DNA barcoding should not be confused with efforts to resolve the “tree of life.” It should
        connect with and benefit from such projects, but resolving phylogeny at scales from species
        to major eukaryotic clades requires a very different strategy for selecting genes. Indeed,
        the very characteristic that makes the COI gene a candidate for high-throughput DNA
        barcoding—highly constrained amino acid sequence and thus broad applicability of primers
        (Hebert et al. 2003)—also limits its information content at deeper phylogenetic levels
        (e.g., Russo et al. 1996; Zardoya and Meyer 1996; Naylor and Brown 1997). Finally, while
        superficially appealing, the very term DNA barcoding is unfortunate, as it implies that
        each species has a fixed and invariant characteristic—like a barcode on a supermarket
        product. As evolutionary biologists, we should question this analogy.
        In evaluating the promise and pitfalls of DNA barcoding, we need to separate the two
        areas of application: molecular diagnostics of individuals relative to described taxa, and
        DNA-led discovery of new species. Both are inherently phylogenetic and rely on a solid
        taxonomic foundation, including adequate sampling of variation within species and inclusion
        of all previously described extant species within a given genus. Accurate diagnosis depends
        on low intraspecific variation compared with that between species, such that a short DNA
        sequence will allow precise allocation of an individual to a described taxon. The extensive
        literature on mtDNA phylogeography (Avise 2000) indicates that this condition often holds,
        although there are exceptions. Furthermore, within many species there is sufficient
        structure that it will be possible to allocate an individual to a particular geographic
        population. Such identifications should be accompanied by a statement of confidence—e.g.,
        node support in a phylogenetic analysis and caveats in relation to the breath of sampling
        in the reference database (e.g., whale forensics; Palumbi and Cipriano 1998).
        DNA-led species discovery is more contentious, but again is not new. In animals,
        inclusion of mtDNA evidence in biogeographic and systematic analyses often reveals
        unexpected diversity or discordance with morphology, which then prompts re-evaluation of
        morphological and ecological characteristics and, if warranted, taxonomic revision. But,
        despite recent proposals (Wiens and Penkrot 2002; Hebert et al. 2004), it does not follow
        that mtDNA divergence should be a primary criterion for recognizing species boundaries (see
        also Sites and Marshall 2003). Potential limitations of using mtDNA to infer species
        boundaries include retention of ancestral polymorphism, male-biased gene flow, selection on
        any mtDNA nucleotide (as the whole genome is one linkage group), introgression following
        hybridization, and paralogy resulting from transfer of mtDNA gene copies to the nucleus.
        These are acknowledged by Hebert et al. (2004) and well documented in the literature
        (Bensasson et al. 2001; Ballard and Whitlock 2004), including that on birds (Degnan 1993;
        Quinn and White 1987; Lovette and Bermingham 2001; Weckstein et al. 2001). More
        specifically, using some level of mtDNA divergence as a yardstick for species boundaries
        ignores the low precision with which coalescence of mtDNA predicts phylogenetic divergence
        at nuclear genes (Hudson and Turelli 2003). An additional problem with focusing on mtDNA
        (or any other molecular) divergence as a primary criterion for recognizing species is that
        it will lead us to overlook new or rapidly diverged species, such as might arise through
        divergent selection or polyploidy, and thus to conclude that speciation requires long-term
        isolation. For example, a recent mtDNA analysis of North American birds (Johnson and Cicero
        2004) showed that numerous avian species have low divergences and that speciation can occur
        relatively rapidly under certain circumstances. We contend, therefore, that whereas
        divergent or discordant mtDNA sequences might stimulate taxonomic reassessment based on
        nuclear genes as well as morphology, ecology, or behavior, mtDNA divergence is neither
        necessary nor sufficient as a criterion for delineating species. This view accords with
        existing practice: taxonomic splits in North American birds typically are based on multiple
        lines of biological evidence, e.g., morphological and vocal differences as well as genetic
        data (American Ornithologists' Union 1998).
        We turn now to the core of Hebert et al.'s paper—COI sequencing of a substantial sample
        of North American birds (260 of 667 species) and its validity as a test of the barcoding
        concept. Their aim is to test “the correspondence between species boundaries signaled by
        COI barcodes and those established by prior taxonomic research.” North American birds are
        an interesting choice because their species-level taxonomy is relatively well resolved and
        there has been extensive previous analysis of levels of mtDNA sequence divergence within
        and among described species (Klicka and Zink 1997; Avise and Walker 1998; Johnson and
        Cicero 2004). Herbert et al. (2004) found differences in COI sequences “between closely
        related species” that were 19–24 times greater in magnitude than the differences within
        species (7.05%–7.93% versus 0.27%–0.43%, respectively). From these data, they conclude that
        most North American bird species can be discriminated via molecular diagnosis of
        individuals and propose a “standard sequence threshold” of ten times the mean intraspecific
        variation (yielding a 2.7% threshold in birds) to flag genetically divergent taxa as
        “provisional species.” Thus, their analysis seeks to address both potential applications of
        DNA barcoding.
        Although Herbert et al. sampled a large number of species, a true test of the precision
        of mtDNA barcodes to assign individuals to species would include comparisons with sister
        species—the most closely related extant relatives. This would require that all members of a
        genus be examined, rather than a random sample of imprecisely defined close relatives, and
        that taxa be included from more than one geographic region. Johnson and Cicero (2004)
        showed the importance of comparing sister species when examining genetic divergence values
        in North American birds, with results that contrast strongly with those of Hebert et al. as
        well as previous studies (e.g., Klicka and Zink 1997). For 39 pairs of avian sister
        species, mtDNA sequence divergences ranged from 0.0% to 8.2%, with an average of 1.9% (cf.
        7% to 8% among closely related species in Hebert et al.). Of these, 29 pairs (74%) are at
        or below the 2.7% threshold proposed by Herbert et al. and thus would not be recognized as
        species despite biological differences. Moreover, although only a few of these 39 pairs
        (see Table 1 in Johnson and Cicero [2004]) had sufficient sampling to assess intraspecific
        variation in mtDNA sequences, these typically showed paraphyly in mtDNA haplotypes.
        Therefore, there are still too few cases with adequate sampling of intraspecific diversity
        for sister species pairs to know how common paraphyly is, although a recent meta-analysis
        found that 17% of bird species deviated from mtDNA monophyly (Funk and Omland 2003).
        Collectively, these observations cast doubt on the precision of DNA barcoding for
        allocating individuals to previously described avian species.
        
        Empidonax flycatchers, which are renowned for their
        morphological similarity and could thereby benefit from DNA-based identification tools,
        provide an example of the importance of a more detailed analysis. A complete molecular
        phylogeny for this group (Johnson and Cicero 2002) yielded distances between four pairs of
        sister species that ranged from 0.7% (
        E. difficilis versus 
        E. occidentalis ) to 4.6% (
        E. traillii versus 
        E. alnorum ); notably, the genetic distance between mainland and
        island populations of 
        E. difficilis (
        E. d. difficilis and 
        E. d. insulicola , 0.9%) was greater than that between sister
        species (Johnson and Cicero 2002). Herbert et al.'s analysis included only two species of 
        Empidonax (
        E. traillii and 
        E. virescens ), which are not sisters but members of divergent
        clades. Because 
        E. virescens is genetically distant from all other species of 
        Empidonax (10.3% to 12.5% uncorrected distance; Johnson and
        Cicero 2002), its comparison with 
        E. trailli therefore inflates estimates of interspecific
        distances within the genus.
        Another key point of Hebert et al.'s analysis was to estimate levels of intraspecific
        diversity. For 130 species of the 260 examined, more than two individuals were sequenced (
        n = 2 to 12 individuals per species, mean = 2.4), and pooled pairwise
        genetic distances were found to be uncorrelated with geographic distances, leading Hebert
        et al. to conclude that “high levels of intraspecific divergence in COI in North American
        birds appear uncommon.” However, this makes the assumption that there is a common
        underlying pattern of phylogeographic structure, which is unlikely for North American birds
        (Zink 1996, Zink et al. 2001). If there is significant variation, assessment of
        intraspecific diversity can be based on a small sample of individuals only if individuals
        are sampled across existing population subdivisions for which geography and phenotypic
        variation are reasonable initial surrogates.
        The analyses presented by Hebert et al. will certainly stimulate further debate (a reply
        by Hebert et al. to the present letter is posted at http://www.barcodinglife.com), but, for
        the reasons outlined here, they are not yet a definitive test of the utility of DNA
        barcoding for either diagnosis of individuals or discovery of species. We also question
        whether the results for North American birds can be extrapolated to the tropics, where DNA
        barcoding could have maximum value. In general, among-population sequence divergence
        increases with decreasing latitude, even excluding previously glaciated regions (Martin and
        MacKay 2004), and studies of intraspecific genetic diversity in Neotropical birds have
        revealed a higher level of phylogeographic subdivision compared to temperate species
        (Remsen 1997, Lovette and Bermingham 2001). Thus, the general utility of mtDNA barcoding
        across different biogeographic regions—and between resident versus migratory taxa—requires
        further scrutiny.
        There is little doubt that large-scale and standardized sequencing, when integrated with
        existing taxonomic practice, can contribute significantly to the challenges of identifying
        individuals and increasing the rate of discovering biological diversity. But to determine
        when and where this approach is applicable, we now need to discover the boundary
        conditions. The real challenge lies with tropical taxa and those with limited dispersal and
        thus substantial phylogeographic structure. Such analyses need to be taxonomically broad
        and need to extend beyond the focal geographic region to ensure that potential sister taxa
        are evaluated and can be discriminated. There is also the need to examine groups with
        frequent (possibly cryptic) hybridization, recent radiations, and high rates of gene
        transfer from mtDNA to the nucleus. Only then will the skeptics be satisfied.
      
    
  

  
    
      
        
        Is Michael Moore liberal America's Rush Limbaugh? If so, is he filling a much needed, or
        a much lamented, gap in turning issues that are really cast in pastel shades into Day-Glo
        relief? In this hale monograph, Jeff Hawkins (rendered by Sandra Blakeslee) plays exactly
        this role for theoretical neuroscience. As a pastel practitioner myself, but furtively
        sharing many of Hawkins' prejudices and hunches about computational modelling in
        neuroscience, I am caught between commendation and consternation.
        Hawkins is an engineer, entrepreneur, and scientist who founded and led the companies
        Palm and then Handspring. He created, against what must have been considerable obstacles,
        the first widely successful PDA, and continued the development of this platform. He has
        thus amply earned a bully pulpit. The autobiographical segments of this book detail that,
        throughout his career, he has been interested in understanding how the brain works, using
        his substantial knowledge and intuition about the architecture and design of conventional
        computers as a counterpoint.
        More recently, Hawkins has generously put his money where his ideas about mentation
        dictate, founding the Redwood Neuroscience Institute and also funding various conferences
        and workshops. The institute is dedicated to ‘studying and promoting biologically accurate
        mathematical models of memory and cognition.’Despite its youth, the Institute already has
        attracted notable attention as a centre for theoretical neuroscience. Hawkins' quest,
        and-depending on which statements of the book you read-its endpoint (‘… a comprehensive
        theory of how the brain works … describ[ing] what intelligence is and how your brain
        creates it’) or just its tipping point (‘join me, along with others who take up the
        challenge’), are the subject here.
        There are really three books jostling inside the covers. One is the (highly abbreviated)
        autobiography. The history of modern computing is very brief and (at least judging by the
        sales) very glorious, and this story is most entertaining. Don't miss the wonderfully faux
        naive letter from Hawkins to Gordon Moore asking, in 1980, to set up a research group
        within Intel devoted to the brain. That Hawkins prospered in clear opposition to accepted
        wisdom is perhaps one of the key subtexts of the book.
        The second, and rather less satisfying, book is about the philosophy of mind and the
        history of artificial intelligence and neural network approaches to understanding the brain
        and replicating cognition. With respect to the fields of artificial intelligence and neural
        nets, the text seems rather to be fighting yesterday's battles. The importance of learning,
        flexibility in representation and inference, and even decentralisation of control has been
        more than amply recognised in the inexorable rise of probabilistic approaches in both
        fields.
        With respect to the philosophy of mind, there seems to be something of an enthusiast's
        disdain for the niceties of philosophical pettifogging, even arguing by assertion. The
        discussions at the end on creativity and consciousness all seem a bit gossamer. The book is
        somewhat careless about functionalism, a key doctrine for computational theorists about how
        brains give rise to minds. According to this doctrine, at least roughly, it is the
        functional roles of, and functional interactions among, the physical elements of brain that
        matter, and not their precise physical nature. If you can capture those functional aspects
        correctly, for instance, in a computer program, then you can (re-)create what's important
        about mental states. Functionalism licenses a form of inquiry into the computational jobs
        played by structures in the brain. However, although formally agreeing that ‘there's
        nothing inherently special or magical about the brain that allows it to be intelligent,’the
        book slips into statements such as ‘brains and computers do fundamentally different
        things,’which are, at best, unfortunate shorthand.
        The book is a little apt to sneak plausible, but misleading, claims under the radar.
        Just to give one instance, it compellingly compares a six year old hopping from rock to
        rock in a streambed with a lumbering robot failing to do the same task. However, this is a
        bit unfair. One of Hawkins' self-denying ordinances is to consider the cortex pretty much
        by itself. As aficionados of the cerebellum (an evolutionarily ancient brain region with a
        special role in the organisation of smooth, precise, well-timed, and task-sensitive motor
        output) would be quick to point out, the singular role for the cortex in such graceful
        behaviour is rather questionable.
        The third book is what I think is intended to be the real contribution. This contains a
        (not wholly convincing) attempt to conceptualise the definition of intelligence in terms of
        prediction rather than behaviour, and then to describe its possible instantiation in the
        anatomy (and mostly only the anatomy) of the cortex.
      
      
        Unsupervised Learning
        To situate Hawkins' suggestions, it is instructive to consider current models of how the
        cerebral cortex represents, and learns to represent, information about the world without
        being explicitly taught. Being a popular account, the book fairly breezes by these
        so-called unsupervised learning models (see Hinton and Ghahramani 1997; Rao et al. 2002),
        in which the neocortex is treated as a general device for finding relationships or
        structure in its input. The algorithms are called unsupervised since they have to work
        without detailed information from a teacher or a supervisor about the actual structure in
        each input. Rather, they must rely on general, statistical characteristics.
        First, where does the structure in the inputs come from? For the sake of concreteness,
        think of the input as being something like movies on a television screen. Movies don't look
        like white noise, or ‘snow’, because of their statistical structure. For instance, in
        movies, pixel activities tend to change rather slowly over time, and pixels that are close
        to each other on the screen tend to have relatively similar activities at any given time.
        Neither of these is true of white noise. More technically, movies constitute only a tiny
        fraction of the space of all possible activations of all the pixels on your screen. They
        (and indeed real visual scenes) have a particular statistical structure that the cortex is
        supposed to extract.
        What is the cortex supposed to do with this structure? The idea is that the cortex
        learns to model, or ‘parameterize’, it. Then, the activities of cortical cells over time
        for a particular input, for example, a particular face in a movie, indicate the values of
        the parameters associated with that face. Thereby the cortical activities represent the
        input. The parameters for a face might include one set for its physical structure (e.g.,
        the separation between the eyes and whether it is more round or more square), another set
        for the expression, and yet others, too.
        Cortical representations are thus intended to reflect directly the statistical structure
        in the input. Importantly, for inputs such as movies, this structure is thought to be
        hierarchical and, concomitantly, to provide an account of the observed hierarchical
        structure of sensory cortical areas. One source of hierarchical structure in movies is the
        simple fact that objects (such as the faces) have parts (such as eyes and cheeks) whose
        form and changes in form over time are interdependent. Another source of hierarchical
        structure is that the same face can appear in many different poses, under many different
        forms of illumination, and so on. Pattern theory (Grenander 1995), one of the parent
        disciplines of the field, calls these dimensions of variation deformations. Loosely, the
        deformations are independent of the objects themselves, and we might expect this
        independence to be reflected in the cortical representations. Indeed, there is
        neurophysiological evidence for just such invariant neural responses to deformations of a
        stimulus.
        How does the cortex do all this? Of course, some fraction of this structure was built in
        over evolution. However, the unsupervised learning tradition concentrates on ontogenic
        adaptation, based on multiple presented input movies. An additional facet of the lack of
        supervision is that this adaptation is taken as not depending on any particular behavioural
        task.
        Finally, what does this process allow the cortex to do? The whole representational
        structure is intended to support inference. Crudely, this involves turning partial or noisy
        inputs into the completed, cleaned-up patterns they imply, using connections between areas
        in the cortical hierarchy. Construed this way, probabilistic inference actually
        instantiates a very general form of computation. Crucially, over the course of the
        development of unsupervised learning methods, it has been realised that the best way to
        approach the extraction of input structure, and inference with it, is through the language
        and tools of probability theory and statistics. The same realisation has driven substantial
        developments in artificial intelligence, machine learning, computer vision, and a host of
        other disciplines.
      
      
        Predictive Auto-Association
        We can now return to the book. Hawkins compactly sums up his thesis in the following
        way. ‘To make predictions of future events, your neocortex has to store sequences of
        patterns. To recall appropriate memories, it has to retrieve patterns by their similarity
        to past patterns (auto-associative recall). And finally, memories have to be stored in an
        invariant form so that the knowledge of past events can be applied to new situations that
        are similar but not identical to the past.’In fact, to take the latter points first, the
        sort of auto-associative storage and recall to which Hawkins refers is a theoretically and
        practically hobbled version of unsupervised learning's probabilistic inference. Invariance
        is closely related to the deformations we described above in the context of pattern
        theory.
        Unsupervised learning has certainly paid substantial attention to sequences of inputs
        and prediction, and to some good effect. For instance, (artificial) speech recognition
        programs are based on a probabilistic device called a hidden Markov model, which is a key
        element in a wealth of unsupervised learning approaches to prediction. However, despite
        heroic efforts, these modelling methods are incapable of capturing the sort of complex
        structure seen in inputs such as natural languages. They fail on phenomena like
        long-distance dependencies, for example, the agreement between the cases of subjects and
        verbs, which are rife. This does tend to offer a vaccine against Hawkins' otherwise
        infectious optimism.
        Once place in which Hawkins goes beyond existing unsupervised learning models is in an
        extension to actions and control, and in an ascription of parts of the model to cortical
        anatomy. The hierarchical conception of cortex here goes all the way down to primary motor
        cortex (the neocortical area most directly associated with motor output). This allows
        auto-associative recall of sequences of past inputs and outputs to be used to specify
        actions that have formerly been successful. The discussion of this possibility is,
        unfortunately, rather brief. Central issues are omitted, such as the way that planning over
        multiple actions might happen. Also, the way that value is assigned to outcomes to
        determine success or failure is not discussed. The latter is widely believed to involve the
        neuromodulatory systems that lie below the cortex and that the book's cortical chauvinism
        leads it cheerfully to ignore.
        By contrast, the book has a rather detailed description of how the model should map onto
        the anatomy of the cerebral cortex. Like many unsupervised learning modellers, Hawkins is a
        self-confessed ‘lumper’. He ignores huge swathes of complexity and specificity in cortical
        structure and connections in favour of a scheme of crystalline regularity. Though this will
        doubtless irk many readers (as will the lack of citations to some influential prior
        proponents such as Douglas and Martin [1991]), some (though not necessarily this) strong
        form of abstraction and omission is necessary to get to clear functional ideas. This part
        has interesting suggestions, such as a neat solution for a persistent dilemma for
        proponents of hierarchical models. The battle comes between cases in which information in a
        higher cortical area, acting as prior information, boosts activities in a lower cortical
        area, and cases of predictive coding, in which the higher cortical area informs the lower
        cortical area about what it already knows and therefore suppresses the information that the
        lower area would otherwise just repeat up the hierarchy. The proposed solution involves the
        invention (or rather prediction) of two different sorts of neurons in a particular layer of
        cortex.
        Unsupervised learning models of cortex are without doubt very elegant. However, if
        pushed, purveyors of this approach will often admit to being kept awake at night by a
        number of critical concerns even apart from the difficulty of getting the models to work in
        interestingly rich sensory domains. Does the book provide computational Halcyon? First, the
        representations acquired by unsupervised learning are intended to be used for
        something-such as accomplishing more specific learning tasks, for example, making
        predictions of reward. However, most aspects of the statistical structure of inputs are
        irrelevant. This might be called the ‘carpet’problem: there is a wealth of statistical
        structure in the visual texture of carpets; however, this structure is irrelevant for
        almost any task. Capturing it might therefore (a) constitute a terrible waste of cortical
        representational power, or, worse, (b) interfere with, or warp, the parameterization of the
        aspects of the input that are important, making it harder to extract critical distinctions.
        The book does not address this issue, relying on there being enough predictive power to
        capture any and all predictions, including predictive characterisation of motor
        control.
        Second, although our subjective sense is that we build a sophisticated predictive model
        of the entire sensory input, experiments into such phenomena as change blindness (Rensink
        2002) show this probably isn't true. A classic example involves alternating the
        presentation of two pictures, which differ in some significant way (e.g., the colour of the
        trousers of one of the main protagonists). Subjects have great difficulty in identifying
        the difference between the pictures, even though (a) they are explicitly told to look for
        it, (b) they have the subjective sense that they have represented all the information in
        each picture, and (c) if the location of the change is pointed out, they see it as
        blindingly obvious. This, and other attentional phenomena, suggests that substantially less
        is actually represented than we might naively think. In fact, elaborate computations go
        into selecting aspects of the input to which the models might be applied, and sophisticated
        models of these computations, such as Li's salience circuit (2002), involve aspects of
        cortical anatomy and physiology ignored in the book.
        As a final example of a spur to insomnia, unsupervised learners worry that Damasio
        (1994) might be somewhat right. That is, cool logic and hot emotion may be tightly coupled
        in a way that a model such as this that is rigidly confined to cortical processing,
        ignoring key subcortical contributions to practical decision making, will find hard to
        capture.
        To sum up, in terms of the adage that genius is 1% inspiration and 99% perspiration, the
        book's enthymematic nature suggests that not quite enough sweat has been broken. Were it 1%
        inspiration and 99% aspiration, though, then the appealing call to arms for a new
        generation of modellers should more than suffice.
      
    
  

  
    
      
        
        As the premier biological electron acceptor, molecular oxygen (O
        2 ) serves a vital role in fundamental cellular functions, including the
        process of aerobic respiration. Nevertheless, with the beneficial properties of O
        2 comes the inadvertent formation of reactive oxygen species, including
        superoxide (O
        −
        2 ), hydrogen peroxide (H
        2 O
        2 ), and hydroxyl radical (•OH); these differ from O
        2 in having one, two, and three additional electrons, respectively
        (Figure 1). Cells also encounter elevated levels of these reactive oxygen species when they
        are released by animals, plants, and insects as a defense against detrimental organisms
        such as microbial pathogens. Reactive oxygen species can damage cells in many ways: by
        inactivating proteins, damaging nucleic acids, and altering the fatty acids of lipids,
        which leads in turn to perturbations in membrane structure and function. The accumulation
        of this oxidative damage underlies the formation of many disease states in humans. It is
        postulated that tissue injury by these reactive oxygen species accumulates over a long
        period of time and plays roles in the aging process and the development of heart disease,
        diabetes, chronic inflammatory diseases, cancer, and several neurodegenerative diseases
        (Halliwell 1999).
        Many organisms have evolved strategies to remove reactive oxygen species and repair
        damage, which have enabled them to prosper from the tremendous oxidizing potential of O
        2 without succumbing to oxidative damage. Bacteria, yeast, and mammalian
        cells all induce the synthesis of global regulatory responses to survive oxidative insults.
        The consequences of oxidative stress and the corresponding defense responses have been
        extensively studied in 
        Escherichia coli . For ease of study in the laboratory, the
        stress responses are often provoked by the external addition of chemical oxidants that
        specifically elevate the levels of reactive oxygen species within cells, or by the use of
        mutant strains that disrupt the normal “homeostatic mechanisms” for removing reactive
        oxygen species or the damage they do. While this primer focuses on a particular set of
        protective and regulatory protein modifications induced by oxidative stress in 
        E. coli , it should be noted that many of the same mechanisms
        are present in other organisms; some specific examples from other species will also be
        described.
        The major target of O
        2
        − damage identified in bacteria is a class of dehydratase enzymes that
        utilize [4Fe–4S] clusters to bind their substrate (Imlay 2003; Djaman et al. 2004). Since
        some of these enzymes function in the citric acid cycle (also called the Krebs cycle) and
        in amino acid biosynthesis, high levels of O
        2
        − lead to a requirement for certain amino acids in growth media (Imlay
        and is well known Fridovich 1991). H
        2 O
        2 for its role in oxidizing thiol (SH) groups of cysteinyl amino acid
        residues in proteins. Elevated levels of H
        2 O
        2 also are associated with the oxidation of other amino acids, leading
        to the formation of methionine sulfoxide and a variety of carbonyls. Lastly, because of its
        extreme reactivity, •OH targets all of the major macromolecules of cells: RNA, DNA,
        protein, and lipids. The extent to which membrane lipids are targets appears to depend on
        the presence of polyunsaturated fatty acids in lipids, which are not as prevalent in
        bacteria as they are in mammals.
        Many enzymes that protect against oxidative damage have been identified in 
        E. coli (Imlay 2002, 2003). Three superoxide dismutases, each of
        which contain a different metal center and show different expression patterns and
        subcellular localization, catalyze the dismutation of O
        2
        − to H
        2 O
        2 . While the superoxide dismutases eliminate O
        2
        − , they also are a source of endogenously produced H
        2 O
        2 in 
        E. coli . The major enzymes involved in reducing H
        2 O
        2 to H
        2 O and O
        2 in 
        E. coli are catalase and alkyl hydroperoxide reductase. There is
        no enzymatic mechanism for decreasing levels of •OH, produced from H
        2 O
        2 . Thus, levels of •OH will be directly proportional to levels of H
        2 O
        2 , and accordingly, catalase and alkyl hydroperoxide reductase
        activities are critical to oxidative stress survival.
        Another component to the oxidative stress response is the reduction of oxidized thiols
        that arises through one of the mechanisms described below. The tripeptide glutathione and
        the thiol reductants glutaredoxin and thioredoxin are key to the restoration of thiols to
        their reduced state (SH) (Fernandes and Holmgren 2004). 
        E. coli contains three glutaredoxins that utilize the reducing
        power of glutathione to catalyze the reduction of disulfide bonds (–S–S–) in the presence
        of NADPH and glutathione reductase. There are two thioredoxins in 
        E. coli that also function to reduce disulfide bonds. Reduced
        thioredoxin is regenerated by thioredoxin reductase and NADPH. The fact that NADPH is
        required to maintain the reduced state of glutathione and thioredoxin indicates that the
        response to oxidative stress is coupled to the physiological status of core pathways that
        generate NADPH.
      
      
        Regulatory Roles of Thiol Modifications
        As mentioned above, proteins—in particular, the thiols of cysteines—are the major
        targets of H
        2 O
        2 . The reaction of cysteinyl thiolates with H
        2 O
        2 can lead to the formation of different modifications, such as sulfenic
        acid (–SOH), sulfinic acid (–SO
        2 H), and sulfonic acid (–SO3H), as well as disulfide bond formation
        (–S–S–) and glutathione conjugation (–S–GSH) (Jacob et al. 2004; Poole et al. 2004) (Figure
        2). These modifications often alter the structure and function of the protein. Recent
        progress in this field points to a common chemistry in the reaction of H
        2 O
        2 with thiolates through the initial formation of sulfenic acid. In the
        case of proteins that have a nearby cysteinyl residue, a disulfide bond forms between the
        two sulfur atoms. The sulfenated cysteinyl residue also can react with a cysteinyl residue
        on another protein or with glutathione, leading to a mixed disulfide. If no cysteinyl
        residue is nearby, the sulfenated cysteine can be further oxidized to sulfinic or sulfonic
        acid, or it can remain in the sulfenic acid state. All but the sulfinic and sulfonic acid
        modifications are readily reversible by reduction, using proteins such as thioredoxin or
        glutaredoxin; though sulfinic acid reductase activities have recently been identified in
        yeast and mammalian cells (denoted sulfiredoxin and sestrin, respectively) (Biteau et al.
        2003; Budanov et al. 2004).
        Given the reversible nature of most forms of thiol oxidation, it has been suggested that
        thiol modifications can play roles in signal transduction that are similar to protein
        phosphorylation/dephosphorylation (Sitia and Molteni 2004). In support of this model, there
        are several examples of proteins whose activities are modulated by thiol oxidation and
        reduction.
        The first of these examples is the OxyR transcription factor, which upregulates peroxide
        defenses in 
        E. coli and a variety of other bacteria. OxyR contains two
        critical cysteines that are oxidized to form an intramolecular disulfide bond when cells
        encounter peroxide stress (Zheng et al. 1998; Aslund et al. 1999). Disulfide bond formation
        is associated with a conformational change that alters OxyR binding to DNA and allows the
        protein to activate the transcription of genes encoding enzymes, such as catalase and the
        alkylhydroperoxide reductase, that destroy H
        2 O
        2 . Once the H
        2 O
        2 concentration is decreased, OxyR is reduced and the system is reset.
        The unusually reactive cysteine in OxyR that is oxidized by H
        2 O
        2 to form the sulfenic acid intermediate can clearly be nitrosylated and
        glutathionylated in vitro (Hausladen et al. 1996; Kim et al. 2002), but the in vivo
        relevance of these other modifications is questionable (Mukhopadhyay et al. 2004).
        Two other examples of redox-regulated proteins are the 
        E. coli chaperone protein Hsp33 (Jakob et al. 2000) and the 
        Streptomyces coelicolor anti-sigma factor, RsrA (Li et al. 2003;
        Paget and Buttner 2003; Bae et al. 2004). For these proteins, the cysteine residues, which
        form intramolecular disulfide bonds, are in a reduced state when coordinated to a zinc ion
        (Zn
        2+ ), and zinc is released upon oxidation of the thiols. For both
        proteins, oxidation and zinc release are associated with an opening of the protein
        structure. For Hsp33, this structural change allows for dimerization and activates its
        chaperone activity (Graf et al. 2004). RsrA, on the other hand, dissociates from a promoter
        specificity factor of RNA polymerase (an extracytoplasmic-function-type alternative sigma
        factor) allowing the transcription of genes that permit recovery from the stress (Li et al.
        2003; Bae et al. 2004). Among the target gene products is a thioredoxin, which reduces the
        disulfide bonds that form within oxidized RsrA. Presumably, reduction of the disulfide
        restores the binding of zinc and its inhibitory association with the sigma factor. Thus,
        the RsrA regulatory circuit provides another example, comparable to OxyR, in which the
        modification of a regulatory protein thiol group can be linked to a change in the
        transcriptional output of genes that remediate stress.
        The peroxide-sensing repressor OhrR from 
        Xanthomonas campestris pv. 
        phaseoli (Panmanee et al. 2002) and 
        Bacillus subtilus (Fuangthong and Helmann 2002) can be
        inactivated by H
        2 O
        2 or by organic peroxides (ROOH) formed by the oxidation of a variety of
        organic molecules in the cell or in the environment. The 
        B. subtilis OhrR transcription regulator contains only a single
        cysteine that forms a relatively stable sulfenic acid upon its reaction with H
        2 O
        2 or organic peroxides (Fuangthong and Helmann 2002). Oxidation of the
        single cysteine leads to the dissociation of OhrR from its DNA binding site and the
        derepression of the gene encoding an organic hydroperoxidase that eliminates the initial
        oxidizing insult.
        In this issue, Hondorp and Matthews (2004) provide an example of a thiol modification
        that protects an enzyme activity during oxidative stress. Their data suggest that when
        cells encounter oxidative stress, a key cysteinyl residue near the active site of
        methionine synthase (MetE) is glutathionylated. This modification blocks access of the
        substrate and prevents further synthesis of methionine. This finding is significant in that
        it presents a mechanism to reversibly preserve the function of a protein during oxidative
        challenge. By glutathionylating a single cysteinyl residue, the protein is protected from
        further oxidation of that cysteinyl residue to the irreversible sulfinic and sulfonic acid
        forms. Once the stress is removed, the mixed disulfide bond will be readily reduced, and
        access to the substrate restored.
      
      
        Prevalence of Regulatory Thiol Modifications?
        As illustrated by the examples above, an array of chemical modifications obtained by
        oxidizing cysteinyl residues has been exploited in combating oxidative stress. Yet it is
        important to note that not all cysteinyl residues of proteins are readily oxidized by
        oxidants such as H
        2 O
        2 . We do not currently understand all of the features that determine
        the reactivity of a particular thiol to H
        2 O
        2 (Poole et al. 2004). The pKa of the thiolates clearly plays an
        important role, as thiolates are more reactive than their protonated counterparts. In
        addition, the contribution of protein environment to the stability of the oxidized products
        is also known to be a factor, but is not well understood. Given that many of the thiol
        modifications do not appear to be in equilibrium with the redox state of the cell, the
        features of the protein that determine the rate at which the modifications are formed are
        another important parameter.
        The added complexity of the cysteine targets that compose part of a Zn binding site
        found for Hsp33 and RsrA raises questions about the function of the zinc. Perhaps Zn
        binding provides some additional control over the reactivity of the cysteine thiols, or
        perhaps the loss of the zinc facilitates conformational changes. Recently, the oxidative,
        stress-induced thioredoxin-2 from 
        E. coli has also been shown to contain a H
        2 O
        2 -labile zinc site, although the loss of zinc does not change its
        reductase activity (Collet et al. 2003). Thus, the way this oxidatively labile Zn site
        affects thioredoxin function has yet to be established.
        The extent of thiol oxidation within the cell remains another open question. The variety
        of modifications that arise from treatment with H
        2 O
        2 and the experimental challenges associated with their detection has
        made it difficult to catalog all the proteins that are modified and all the types of
        modifications that exist. In this issue, Leichert and Jakob (2004) report a general method
        for detecting cellular proteins whose cysteinyl residues were modified after imposing an
        oxidative stress. Such an approach will greatly enhance our understanding of targets of
        oxidative stress. The method described by Leichert and Jakob also will be useful in
        detecting transient cysteine modifications.
        The importance of monitoring transient changes in cysteines is highlighted by the recent
        finding that oxidation of the Yap1 activator of antioxidant genes in the yeast 
        Saccharomyces cerevisiae requires a peroxidase denoted Gpx3 or
        Orp1 (Delaunay et al. 2002). In this case, H
        2 O
        2 reacts with a cysteine in Orp1, forming an unstable sulfenic acid
        intermediate that then reacts with a cysteinyl residue of Yap1 to form an intermolecular
        disulfide. The disulfide undergoes an exchange with a second cysteine within Yap1 to form
        an intramolecular disulfide that locks Yap1 in a confirmation that masks the nuclear export
        signal (Wood et al. 2004). Thus, methods that allow the appearance of thiol modifications
        in cells to be monitored kinetically will greatly enhance our understanding of how cysteine
        residues become oxidized.
        The examples mentioned here illustrate the versatile potential of thiol modifications.
        Given the reversibility of thiol oxidations and the wide range of structural constraints
        that can be imposed by the formation of a sulfenic or sulfinic acid or a disulfide bond, we
        predict there will be many more examples of regulation by thiol modification.
      
    
  

  
    
      
        
        The consequence of Parkinson's disease (PD) is well described: a progressive movement
        disorder that, whilst responding to symptomatic therapy, chronically disables its sufferers
        and adds an enormous economic burden in an aging society. We have some clues to the process
        underlying the disease from the snapshot provided by postmortem studies of diseased brains.
        Groups of neurons in specific brain regions are lost, notably those that produce dopamine
        in a part of the midbrain called the substantia nigra. Those neurons that do survive to the
        end of the disease course contain accumulations of proteins and lipids within their
        cytoplasm. Named after their discoverer, these “Lewy bodies” are one piece of evidence that
        protein aggregation is related to the ongoing disease process.
        In contrast, the causes of PD are poorly defined except in those rare variant forms that
        are clearly genetic. Several families have been described where PD-like syndromes are
        inherited in either a dominant or recessive fashion, and four of the underlying genes have
        been identified. The precise relationships between these different syndromes are complex
        and are the subject of some controversy. For example, it is not clear whether all the
        genetic diseases given PARK nomenclature have Lewy bodies and should be considered “true”
        PD—the term parkinsonism is preferred for these syndromes (Hardy and Langston 2004). For
        the purposes of this primer, I will concentrate on the molecular biology of the genes
        linked to PD rather than disease etiology. However, my assumption is that symptoms of the
        disease are a reflection of neuronal dysfunction, and that in the disease state the balance
        between damage and survival tips in the direction of cell loss. Whilst dominant mutations
        overwhelm the ability of cells to survive, recessive mutations result in the absence of
        protective proteins and make the neuron grow weaker.
      
      
        Aggregation of α-Synuclein in Neurodegeneration
        On the detrimental side of the cell survival equation is the PD gene that was discovered
        first, α
        -synuclein . The synaptic protein encoded by this gene, α-synuclein, is
        prone to aggregation, and, as is the case for other aggregating proteins, mutations in α-
        synuclein are associated with dominantly inherited disease. Related to
        this, α-synuclein is a major protein component of Lewy bodies. The phenotype of patients
        with α-
        synuclein mutations varies from PD to a more diffuse Lewy body disease in
        which pathology is detected in the cerebral cortex and other areas of the brain. Mutations
        in α-
        synuclein include three point mutations (A30P, E46K, and A53T) and
        multiplication of the wild-type (normal) gene. All of these mutations increase the tendency
        of α-synuclein to aggregate, suggesting that disease is a consequence of protein
        aggregation. An interesting example is the triplication of the wild-type gene: toxicity and
        aggregation can both be driven by increased expression and are thus qualitative, not
        quantitative, effects (Singleton et al. 2003). The fact that the wild-type protein can
        aggregate suggests that the process is fundamentally similar for both inherited and
        sporadic PD in which wild-type α-synuclein is also present in Lewy bodies. Several
        commentators have suggested that non-genetic risk factors may also promote damage via their
        effects on (wild-type) α-synuclein conformation or aggregation (e.g., Di Monte 2003). This
        reinforces the notion that α-synuclein is central to the pathogenesis of both sporadic and
        familial PD.
        There is some controversy about the exact nature of the toxic species produced by
        α-synuclein, as one point mutation (A30P) behaves differently from the others. Instead of
        forming fibrils, which are insoluble, high-molecular-weight species, A30P forms relatively
        soluble, partially aggregated species (Conway et al. 2000). These intermediate-sized
        protein aggregates are referred to as oligomers or protofibrils. Some authors have argued
        that since A30P causes disease, oligomers/protofibrils are the authentic toxic species. It
        is generally assumed that fibrils are the form of α-synuclein deposited into Lewy bodies,
        but whether Lewy bodies damage cells is controversial. One possibility is that by
        sequestering α-synuclein into this insoluble body and compartmentalizing the potentially
        toxic species away from possible targets in the cytoplasm, the Lewy body represents an
        attempt of the cell to protect itself (Olanow et al. 2004).
        Whether the Lewy body is damaging or neuroprotective, there are clearly several possible
        targets for toxic α-synuclein within the cell. For example, aggregated α-synuclein can
        permeabilize cellular membranes and thus might damage organelles (Volles and Lansbury
        2003). Mitochondrial function and synaptic transmission may be especially affected, and
        both of these can secondarily increase oxidative stress within the cytosol (Greenamyre and
        Hastings 2004). When overexpressed, mutant α-synuclein can inhibit the proteasome
        (Petrucelli et al. 2002), a multiprotein complex that degrades many unwanted or
        inappropriate proteins in cells. Mutant forms of α-synuclein also inhibit
        chaperone-mediated autophagy, another important protein turnover pathway that involves
        lysosomes (Cuervo et al. 2004). Between these two effects, it is likely that cells with
        aggregated α-synuclein will become less able to handle damaged or misfolded proteins. It is
        also possible that other cellular processes that we have not yet identified are affected by
        the presence of this protein that has such an innate tendency to aggregate.
        Presumably, neurons require α-synuclein for their normal function and thus cannot simply
        dispense with this protein that has toxic properties, although mice in which the α-
        synuclein gene is knocked out have no obvious deficits (see Dauer and
        Przedborski [2003] for discussion).
      
      
        Parkin, DJ-1, and PINK1 in Neuroprotection
        Evolution has provided cells with many ways to protect themselves. As we will see,
        mutations that cause recessive diseases result in the loss of these neuroprotective
        functions. The genes involved in recessive parkinsonism are, in order of discovery, 
        parkin, DJ-1, and 
        PINK1 . The three protein products of these genes all have different
        functions, thus implicating several different cellular functions in neuroprotection. Parkin
        is an E3 ubiquitin–protein ligase, promoting the addition of ubiquitin to target proteins
        prior to their degradation by the proteasome. The identification of parkin's function was
        facilitated by the observation that the protein contains a RING finger (Zhang et al. 2000),
        a common motif amongst this class of E3 enzymes. Several parkin substrates have been
        proposed, and at least two are damaging to neurons if they are allowed to accumulate (Dong
        et al. 2003; Yang et al. 2003). Therefore, our best evidence to date indicates that parkin
        benefits neurons by removing proteins that might otherwise damage the cell. In fact,
        expression of parkin is neuroprotective in a number of contexts, and there is even evidence
        for a beneficial effect of this E3 ligase on mitochondrial function (Shen and Cookson
        2004).
        Data on PINK1 are limited, but the protein contains two motifs that indicate its likely
        cellular role. At the amino-terminus of PINK1 is a mitochondrial-targeting sequence, and
        mitochondrial localization has been confirmed in the one study published to date (Valente
        et al. 2004). Most of the rest of PINK1 is a Serine/Threonine protein kinase domain,
        followed by a short carboxy-terminal region of unclear significance. The substrates of
        PINK1 have not yet been identified, but presumably phosphorylation of these substrates
        controls some critical function for neuronal survival. In their paper, Valente and
        colleagues show that PINK1 decreases damage to mitochondria induced by proteasome
        inhibition, but a recessive mutant PINK1 is unable to protect cells.
        The discussion of protein functions gets more complicated in the case of DJ-1. Unlike
        parkin or PINK1, there are no motifs within DJ-1 that hint strongly at a single function.
        Instead, 
        DJ-1 is a member of a large superfamily of genes with several different
        functions across species (Bandyopadhyay and Cookson 2004). These include proteases in
        thermophilic bacteria, transcription factors, and chaperones that promote protein
        refolding. Several research groups have published data in support of DJ-1 having one or
        more of these activities, including the report, published in this issue of 
        PLoS Biology , that DJ-1 is a molecular chaperone that regulates
        α-synuclein, among other molecules (Shendelman et al. 2004). It is not yet firmly
        established which activity of DJ-1 is most relevant to recessive parkinsonism. The
        important function of DJ-1 might be unrelated to any of the above activities. For example,
        there are several roles of this protein in modulation of transcriptional responses, which
        may be critical in maintaining neuronal viability (Bonifati et al. 2003 and references
        therein)
        DJ-1 is also known to be responsive to oxidative conditions, under which cysteine
        residues are oxidized to form cysteine-sulfinic acids. There is some discussion about which
        cysteine residue is oxidized; the most likely is cysteine 106, which is present in a
        nucleophile elbow in the protein. We have suggested that modifying this residue precludes
        DJ-1 oxidation under mild conditions and also blocks the neuroprotective activity of DJ-1
        against mitochondrial toxicity (Canet-Aviles et al. 2004). Therefore, whatever the function
        of DJ-1, it seems to be related to oxidation. In support of this idea, cells with 
        DJ-1 knocked out show increased sensitivity to oxidative stress (Yokota
        et al. 2003). Another study published in this issue of 
        PLoS Biology shows that dopamine neurons differentiated from embryonic
        stem cells lacking functional DJ-1 are especially sensitive to oxidative stress (Martinat
        et al. 2004).
        This discussion indicates that the genes responsible for recessive parkinsonism all have
        different functions but are all, in a broad sense, neuroprotective. A very difficult
        question to answer is whether this has anything to do with α-synuclein. We have shown that
        parkin can mitigate the toxicity of mutant α-synuclein (Petrucelli et al. 2002). Although
        there are reports that a proportion of α-synuclein is a parkin substrate (Shimura et al.
        2001), most of the protein is not degraded by the ubiquitin-proteasome system. Recent
        evidence points, instead, to an important role of the lysosome, the other major pathway
        within cells for degrading unwanted proteins, in clearing α-synuclein (Cuervo et al. 2004).
        On balance, therefore, there is no direct evidence that parkin controls α-synuclein
        toxicity by an effect on protein levels within the cell. Furthermore, parkin does not just
        prevent α-synuclein toxicity: it is beneficial against several other stresses (discussed in
        Shen and Cookson 2004), leading to the possibility that this protein protects neurons
        against more than just the processes implicated in PD.
        It has also been suggested that DJ-1 can prevent the accumulation of aggregated
        α-synuclein and that cysteine 53 is critical for this activity (Shendelman et al. 2004).
        However, DJ-1 is not just a chaperone for α-synuclein; it can also promote refolding of
        citrate synthase, glutathione transferase, and neurofilament light. Other research groups
        have reported similar findings (Olzmann et al. 2004), although there are differences
        between these studies in which cysteine residues are thought to be required for DJ-1
        function. Given that there are some differences in these results, further clarification of
        the role for DJ-1 in α-synuclein-mediated toxicity is needed. More generally, we have to
        bear in mind that whether recessive parkinsonism has anything to do with α-synuclein is
        still an open question. What is clear is that some neurons rely on 
        parkin, DJ-1, or 
        PINK1 to protect themselves against the many stresses that they face.
        However, mutations in these genes do not cause generalized neurodegeneration; in fact, they
        tend to be more restricted and less progressive than, for example, α-synuclein mutations.
        This suggests, at least to my mind, that recessive mutations indicate something about the
        neurons that are damaged in these disorders. Why is this of more than academic importance?
        Perhaps by identifying the proximal events that are sufficient to cause a specific set of
        neurons to degenerate, we might begin to design therapies that address the underlying
        degeneration in PD and not just the consequences.
      
    
  

  
    
      
        
        There's a cheap, common, and mostly safe drug, in daily use for centuries by hundreds of
        millions of people, that only lately has been investigated for its therapeutic potential
        for a long list of common ills. The list includes Alzheimer disease, Parkinson disease,
        depression and anxiety, schizophrenia, attention deficit hyperactivity disorder (ADHD), and
        even pain and obesity. Why has interest in this potential cure-all been slow to develop?
        One reason: in its current forms the drug offers pharmaceutical companies no possibility of
        substantial profit. Another, perhaps more important: the drug is reviled as the world's
        most addictive. The drug, of course, is nicotine.
        Nicotine is an alkaloid in the tobacco plant 
        Nicotiana tabacum , which was smoked or chewed in the Americas
        for thousands of years before European invaders also succumbed to its pleasures and shipped
        it back to the Old World. Nicotine has always been regarded as medicinal and enjoyable at
        its usual low doses. Native Americans chewed tobacco to treat intestinal symptoms, and in
        1560, Jean Nicot de Villemain sent tobacco seeds to the French court, claiming tobacco had
        medicinal properties and describing it as a panacea for many ailments. Higher doses are
        toxic, even lethal—which is why nicotine is used around the world as an insecticide. Yet
        few of the horrendous health effects of smoking are traceable to nicotine itself—cigarettes
        contain nearly 4,000 other compounds that play a role. Until recently, nicotine research
        has been driven primarily by nicotine's unparalleled power to keep people smoking, rather
        than its potential therapeutic uses.
        Nicotine locks on to one group of receptors that are normally targeted by the
        neurotransmitter acetylcholine. Nicotinic acetylcholine receptors (nAChRs) are ion channels
        threaded through cell membranes. When activated, either by acetylcholine or by nicotine,
        they allow selected ions to flow across the cell membrane. In vertebrates nAChRs are all
        over the autonomic and central nervous sytems and the neuromuscular junction. A nAChR is
        composed of five polypeptide subunits (Figure 1), but there are many nAChR subtypes made of
        different subunit combinations, a diversity that helps explain why nicotine can have so
        many different physiological and cognitive effects.
        It is now conventional wisdom that acetylcholine and nicotine act at these receptors to
        alter electrochemical properties at a variety of synapses, which can in turn affect the
        release of several other neurotransmitters. This wisdom exists thanks in part to work by
        Lorna Role and her colleagues at Columbia University in New York City. “In 1995, we turned
        people's attention to how nicotine works as a modulator, tuning synapses and increasing the
        gain on transmitter release,” Role recalls. Although all nAChRs are activated by nicotine,
        other drugs could be found or designed that affect only a subset of these receptor types.
        “If you can dissect out the important players with respect to which nicotine receptors are
        tuning [a] particular set of synapses, then that provides another way to potentially target
        the therapeutics.”
      
      
        Nicotine and the Brain
        People with depressive-spectrum disorders, schizophrenia, and adult ADHD tend to smoke
        heavily, which suggested to researchers that nicotine may soothe their symptoms. Common to
        all these disorders is a failure of attention, an inability to concentrate on particular
        stimuli and screen out the rest. Nicotine helps. Researchers at the National Institute on
        Drug Abuse have shown via functional magnetic resonance imaging that nicotine activates
        specific brain areas during tasks that demand attention (Box 1). This may be because of its
        effects, shared with many other addictive drugs, on the release of the neurotransmitter
        dopamine. “Schizophrenia is a disorder largely of the dopamine system,” says John Dani of
        the Baylor College of Medicine in Houston, Texas. Dopamine signals in the brain occur in
        two modes—a kind of background trickle, punctuated by brief bursts. “It's thought that
        schizophrenics have a hard time separating that background information from important
        bursts. We've shown that nicotine helps to normalize that signaling by depressing the
        background but letting the bursts through well,” he says. “I'll be surprised if there's not
        a co-therapy [to help schizophrenics] that takes advantage of nicotine systems in less than
        a decade.”
        Nicotine may be the link between two genes that appear to figure in schizophrenia.
        Sherry Leonard and Robert Freedman of the University of Colorado in Denver, Colorado, have
        shown that expression of the gene for the alpha 7 neuronal nicotinic receptor is reduced in
        schizophrenics, and have argued that alpha 7 abnormalities lead to attention problems.
        Researchers in Iceland and elsewhere have shown that a different gene, for the growth
        factor neuregulin, also appears to figure in the disease. Neuregulin, Role and her
        colleagues have shown, governs the expression of nAChRs in neurons and helps to stabilize
        the synapses where they are found. The researchers are currently studying interactions
        between neuregulin and alpha 7, which Role thinks will prove important.
        Smokers also have lower rates of neurodegenerative disorders, and nicotine improves
        cognitive and motor functioning in people with Alzheimer disease and Parkinson disease. The
        prevailing hypothesis is that nicotine increases release of neurotransmitters depleted in
        those diseases. Dani and his colleagues have recently shown that acetylcholinesterase
        inhibitors—which block the degradation of acetylcholine and hence prolong its action—used
        to treat Alzheimer disease also stimulate dopamine release. They suspect that
        malfunctioning of the dopamine system may be affecting noncognitive aspects of dementia
        such as depressed mood, and that this might be alleviated by nicotine.
        Paul Newhouse and his colleagues at the University of Vermont in Burlington, Vermont,
        are studying nicotine drugs as potential therapeutic agents for cognitive dysfunction.
        Newhouse, a long-time nicotine researcher, is heading the first study ever to examine the
        efficacy and safety of nicotine patches for treating mild cognitive impairment, thought to
        be a precursor of Alzheimer disease. The researchers hope to see a positive effect on
        attention and learning. Newhouse also heads two studies of nicotinic stimulation in ADHD,
        using the patch, nicotine blockers, and some novel drugs that activate nicotine
        receptors.
      
      
        Nicotine and Pain
        Nicotine's salutary effects in patients with neurodegenerative and mental disorders have
        been studied a lot and are fairly well known. Two much newer topics of academic research
        are nicotine's potential for pain relief and for treating obesity.Nicotine itself has
        provided modest pain relief in animal studies. Although the analgesic effect of drugs that
        mimic acetylcholine were originally attributed to a different class of receptors, it is now
        clear that nAChRs play an important role in the control of pain. For instance, epibatidine,
        a drug that is extracted from the skin of an Ecuadorian frog and that acts at nAChRs, has
        been shown to be 200 times more potent than morphine at blocking pain in animals. Current
        animal research is aimed at discovering just where, how, and which classes of nAChRs work
        against pain, with the aim of developing more selective drugs.
        Meanwhile, nicotine is also being investigated as an analgesic in humans. For example,
        Pamela Flood, an anesthesiologist at Columbia, is investigating nicotine's future as a
        postoperative analgesic. She recently completed a pilot study of 20 women undergoing
        gynecological surgery. All the women had access to unlimited morphine and also got either a
        single 3-mg dose of nicotine nasal spray or a placebo. The placebo group had peak pain
        scores of eight out of a possible ten in the first hour after surgery. Women who got
        nicotine averaged a pain score of five. Despite the small sample size, Flood says, the
        results were highly significant. “As far as I know this is the first clinical study to use
        nicotine for analgesia, and it was much more successful than I ever would have
        imagined.”
        “The nice thing about nicotine and drugs like nicotine is that they have opposite side
        effects to anesthetics. Instead of being respiratory depressants, they are respiratory
        stimulants. Instead of being sedating, they increase alertness. So theoretically this class
        of drugs is actually the perfect thing to add to an opioid regimen. The fact that they're
        synergistic was a fortuitous thing that we had never looked at, and neither had anybody
        else.”
      
      
        Nicotine and Weight Gain
        Nicotine may be the most effective drug around for weight control. As ex-smokers know,
        to their rue, one of the worst things about quitting cigarettes is putting on pounds—as
        much as 10% of body weight. “Something about being addicted to nicotine and then going off
        it causes massive increase in weight,” Role points out.
        Young-Hwan Jo in Role's lab is looking at a particular brain circuit involved in
        motivational behavior, especially feeding behavior. It is lodged primarily in the lateral
        hypothalamus but has projections all over the cortex, especially the nucleus accumbens,
        which is the center of reinforcement. “This is where information that has come in to the
        thalamus and the hypothalamus is relayed to cortical areas with some sense of salience or
        remembrance. It presumably is involved in changing perception and motivation for eating.
        It's not, ‘I have to eat this,’ it's, ‘I want to eat this,’” says Role.
        Jo has been comparing the synaptic effects of nicotine, which reduces appetite, to those
        of cannabinoids, which stimulate it. “Control of these projection neurons seems to be
        oppositely regulated by these two,” Role notes. “It doesn't necessarily mean we've found
        the root of the munchies, but it at least points to pathways that these things have in
        common.” Jo is also examining how nicotine and cannabinoids modulate these pathways in
        genetically obese mice, and also their interactions with leptins. Role says tuning these
        pathways up or down might be a reasonable aim. “If that could be done in a selective
        fashion, maybe that could be introduced in appetite control. Certainly I see…antagonism of
        some of these pathways that nicotine activates or the complementary activation of the
        cannabinoid pathways as very important targets for therapeutics with respect to the
        anorexia that's associated with chemotherapy.”
        Ming Li and his colleagues at the University of Texas in San Antonio, Texas, are
        studying nicotine's effects on weight and on expression of genes that nicotine upregulates
        orexin and neuropeptide Y and, more recently, that it also regulates leptin signaling. All
        three molecules regulate feeding behavior controlled by the hypothalamus. In the weight
        study, nicotine-treated rats not only lost weight, they lost about 20% of their body fat
        compared to saline-treated controls. The researchers suggest that, among its other effects,
        nicotine alters fat storage.
        The University of Texas researchers have scoured the literature for genes related to
        nicotine, and they are developing microarrays to study the expression of these genes
        (Figure 3). While nicotine seems to affect all the molecules known to influence weight, Li
        says it's clear the story is even more complex. “That's the reason we keep looking at
        different molecules, to find key targets involved in this regulation.” The ultimate hope is
        to develop new drug applications.
        Dani predicts that weight control is likely to be one of the earliest nicotine-based
        therapies. “There's a very good chance that the first drug is unlikely to be…nicotine
        itself, but will take advantage of nicotinic receptors in the therapy,” he says. “I know
        there are drugs now being tested by drug companies just for that purpose.”
      
      
        Nicotine's Future
        Developing new drugs that selectively target specific subtypes of nicotine receptors is
        an expensive, albeit potentially lucrative, proposition. And therein lies a question. Will
        nicotine-based therapy consist mostly of costly new drugs from the pharmaceutical industry?
        Or can less expensive nicotine products like the patch, chewing gum, and nasal spray—which
        are generally intended for smoking cessation but widely available, usually without
        prescription—find their way into the world's medicine cabinets?
        “It's a little early to call whether nicotine will be used itself as a therapeutic agent
        or whether these more specific drugs that are being produced or maybe even used in
        combination with other drugs may be the most important way to go,” says Dani. But he
        doesn't see the medicinal use of plain nicotine as very likely. Dani points out that the
        body's own agent, acetylcholine, acts over milliseconds to activate nicotinic receptors,
        whereas nicotine itself stimulates these receptors for hours. That lengthy action means
        that, although nicotine activates the receptors, it then often turns particular receptor
        subtypes off again, a process called desensitization. “It's hard to predict inside of a
        body what you're getting. Am I getting an activation or am I turning the receptors
        off?”
        Yet much of the work to date showing nicotine's effectiveness on a huge range of
        disorders has involved products available at any drugstore and intended to help people quit
        smoking. Newhouse is using patches for mild cognitive impairment. Flood has demonstrated
        pain relief with nasal spray and will use patches in her next study. And Role feels that
        gum hasn't been adequately explored for its therapeutic potential. Nicotine gum, she notes,
        is a better imitator of smoking than the patch because it delivers brief hits rather than a
        steady supply. She's also uncertain whether natural nicotine has been studied enough. But
        Role also points out that nicotine has its serious problems—addictive potential,
        cardiovascular damage, and (especially when delivered through the mucosa) cancer.
        Dani says, “People are probably going to have to find creative ways to understand which
        subtypes of nicotinic receptors they're turning on and which ones they're desensitizing.
        Maybe drug delivery methods will matter. Maybe subtype specificity will matter. It's less
        than a decade that we've known how important nicotinic receptors are. Now we have to move
        forward from there.”
        “We've made an enormous amount of progress on understanding the biology of these
        receptor systems and how to target them. What has been trickier has been to develop an
        appropriate pharmacology that allows one to selectively target agents for particular
        therapeutic purposes with an adequate safety index,” Newhouse says. “But some of the drugs
        that are coming on in human trials now are very promising. So I'm cautiously optimistic
        that we're on the road to developing some useful nicotinic therapies.”
      
    
  

  
    
      
        
        Penguins have been receiving a lot of bad press lately. They are considered somehow
        counter, spare, strange. Unlike most plant and animal groups, they do not show a peak of
        species richness towards the equator and a decline towards the poles. This more
        conventional spatial pattern is conveniently known as the latitudinal diversity gradient
        because of the strong covariance of richness and other measures of biodiversity that it
        describes. It is one of the most venerable, well-documented, and controversial large-scale
        patterns in macroecology (Willig et al. 2003). Equatorial peaks in species richness have
        characterised the planet since the Devonian (408–362 million years ago) (Crame 2001) and
        are typical of a wide range of both terrestrial and marine plants and animals (Gaston 1996;
        Willig et al. 2003). Despite the fact that this pattern has been documented since the late
        1700s, sustained interest in both the regularity of the pattern and its likely underlying
        mechanisms is relatively modern. The realisation that human activity is posing substantial
        threats to biodiversity has quickened the pace of this interest (Willig et al. 2003). Where
        the peaks in richness lie (biodiversity hotspots), how these peaks relate to centres of
        endemism (areas that support large numbers of species that occur nowhere else), and how
        these patterns are likely to change through time, especially in the face of major
        environmental change, are major concerns. Without such knowledge, conservation is unlikely
        to succeed.
        Although spatial patterns in biodiversity, and particularly the latitudinal gradient,
        are increasingly well documented for a range of taxa, the proposed mechanisms underlying
        these gradients remain controversial. In essence, the multitude of mechanisms proposed to
        explain diversity gradients can be reduced to three categories: historical, ecological, or
        null. Most significant in raising the temperature of recent discussions is the question of
        the relative importance of each of these major categories. Historical mechanisms are those
        that suggest that earth history (e.g., the opening of the Drake Passage and the cooling of
        Antarctica) and phylogenetic history have played major roles in generating current patterns
        in diversity, and tend to emphasise regional (and especially longitudinal) differences
        therein (Qian and Ricklefs 2004; Ricklefs 2004). Explanations involving ecological
        mechanisms often downplay the significance of such regional differences and give most
        attention to covariation between current diversity and variables such as energy and water
        availability, and to the ultimate mechanisms underlying this covariation (Hawkins et al.
        2003; Currie and Francis 2004). By contrast, null models, and specifically the geometric
        constraints model, argue that the expected pattern of latitudinal variation in richness is
        not a uniform one, but rather a mid-domain peak, which is almost inevitably the outcome of
        the random placement of a set of variable species ranges within a bounded domain (Colwell
        et al. 2004; but see also Zapata et al. 2003). It is deviation from the mid-domain
        expectation that is then argued to be of most interest. In many cases the historical and
        ecological mechanisms might be difficult to disentangle, such as the historical effects of
        the establishment of the Antarctic Circumpolar Current, and its consequences for energy
        availability in the region today (Clarke 2003).
        Nonetheless, juxtaposing these three major mechanisms raises several questions that
        could substantially inform the debate in many ways, but have enjoyed far less attention
        than debating the relative merits of each of them. The geometric constraints model suggests
        that, to the extent that there is symmetry in the continuity of land (or water) about the
        equator, declines in richness from the tropical peak should also be symmetrical, with any
        asymmetries in the latter matching those in the former. Indeed, most texts and reviews
        dealing with latitudinal diversity gradients only briefly mention hemisphere-related
        differences and focus instead on the general decline of diversity away from the tropics in
        both directions (e.g., Brown and Lomolino 1998; Willig et al. 2003). However, that
        diversity gradients in the two hemispheres might in many cases be highly asymmetric has
        long been appreciated (Gaston 1996). Although several historical hypotheses suggest reasons
        why this asymmetry should exist (reviewed in Brown and Lomolino 1998), differences in
        present ecological factors, such as temperature gradients and rainfall variation, might
        also explain such asymmetry. If ecological factors are important, then these asymmetries
        should show up not only in diversity patterns, but also at other levels in the ecological
        and genealogical hierarchies. From the perspective of ecological explanations for such
        spatial variation, the questions, then, are how common and strong are such asymmetries, how
        common are they in patterns of diversity, and what, if any, might be the ecological, rather
        than null or historical, mechanisms responsible for them?
      
      
        Continents and Climates
        The last 100 million years have seen both a substantial steepening in latitudinal
        diversity gradients and the fragmentation of continental land masses (Crame 2001). By 15
        million years ago the continents had largely assumed their current positions and a
        latitudinal temperature gradient very similar to the present one had been established.
        Today, 70% of all land is in the northern hemisphere, and between latitudes 30° and 60°
        north, the ratio of water to land is about 1:1, whereas between 30° and 60° south, it is
        approximately 16:1. The continentality of the north and oceanicity of the south have
        considerable effects on the climates of the hemispheres, as has long been appreciated
        (Bonan 2002). Although there is obviously much local and mesoscale variation, terrestrial
        temperatures in the south (excluding Antarctica) are usually warmer, and much less extreme
        in terms of their absolute range, than those in the north (Figure 1A), especially over the
        winter months. Southern sites between 30° and 60° typically have mean July temperatures
        between 0 and 10 °C, whereas at similar latitudes in the north, mean January temperatures
        vary from −40 to 0 °C. In winter the smaller range of variation in the south is around a
        physically and biologically significant threshold: the freezing point of water. In the
        north, winter temperatures are more variable, but generally well below this point. Ocean
        water temperatures are much less variable than those on land, although variability in the
        ocean surrounding Antarctica is much reduced compared with that of the Arctic (Figure
        1B).
        Mean annual precipitation is spatially more complex. Overall, precipitation is slightly
        higher in the south than in the north, at least below 60° latitude. However, much of this
        precipitation falls over the ocean in the south, leaving the more temperate parts of the
        southern continents as dry as their northern counterparts (Bonan 2002). Spatial patterns in
        rainfall variability are also complex, but variability tends to be higher and
        predictability lower in southern areas. From a biological perspective the significant
        factor is not necessarily just the magnitude of the variance, but also the mean about which
        it occurs (Guernier et al. 2004).
        Clearly, the spatial complexity of climatic variation is much greater than the present
        overview would suggest. However, these broad brush strokes capture the hemisphere-related
        variation that might be most significant from a biological perspective.
      
      
        Ecological Consequences
        If differences in climates do cascade upwards to influence individuals, species, and
        broader scale patterns in diversity, their influence should be readily detectable at the
        level of species' life histories and distributions. In birds, large-scale geographic
        variation in life history variables, such as the incidence of cooperative breeding, extent
        of parental care, survival, and the timing of reproduction, has been studied for at least
        the past 50 years, and the mechanisms underlying this variation have been much debated.
        Taking phylogeny and the idiosyncrasies of the Australian avifauna into account, southern
        species typically lay small clutches and have long fledging periods, and it is often
        difficult to predict their date of first laying or, indeed, whether they will lay in a
        particular year at all (Covas et al. 1999; Russell et al. 2004). By contrast, northern
        species lay larger clutches and have shorter fledging periods, and laying date is more
        readily predicted, making investigations of phenological shifts associated with modern
        climatic change more straightforward (e.g., Crick et al. 1997).
        These kinds of differences extend to other taxa. Thus, although the variation of
        metabolic rate with latitude is becoming increasingly well known for a variety of groups,
        Lovegrove (2000) has recently suggested, based on comparative work taking both species body
        mass and phylogeny into account, that unpredictability of resources associated with
        considerable inter-annual unpredictability in rainfall (in turn partly a consequence of El
        Niño–associated variability) has been responsible for the evolution of generally low
        metabolic rates in terrestrial mammals of most of the southern continents. Although El Niño
        effects are by no means restricted to these regions, it is perhaps low resource
        availability to start off with, associated with considerable unpredictability, that is of
        most significance (see also Guernier et al. 2004).
        Insect life histories also show hemisphere-related variation. Low-temperature-related
        diapause is virtually absent in southern species (e.g., Convey 1996), and metabolic
        rate–temperature relationships are much shallower in the south than the north (Addo-Bediako
        et al. 2002). The latter is a consequence of relatively cool growing seasons and lack of
        pronounced seasonality in the south. However, the clearest example of a hemispheric
        asymmetry is that of cold hardiness strategies (Sinclair et al. 2003). Insects can survive
        sub-zero temperatures either by tolerating internal ice formation or by reducing their
        freezing points to avoid ice formation altogether. Although there is further variation
        within each of these strategies, in general, freeze-avoiding species need to undergo
        substantial preparation for winter cold and consequently can take some time to emerge from
        the cold hardy state. This also seems to be true of strongly freeze-tolerant species that
        can survive freezing far below the point at which they actually freeze. By contrast,
        moderately freeze-tolerant species—those that can survive only a few degrees of
        freezing—appear to need little preparation for a freezing event and seem perfectly prepared
        to continue with their routine activities immediately after thawing. In northern cold
        climate areas, with the exception of the Arctic, where extremely low temperatures constrain
        insects to being strongly freeze tolerant, most cold hardy species avoid freezing, whereas
        in the south most are moderately freeze tolerant (Figure 2). Microclimates reveal why this
        is the case. As might be expected from macroclimatic variation, southern temperate insects
        are faced with regular freeze–thaw cycles (i.e., variation about 0 °C), including
        pronounced summer cold snaps, whereas the continental climates of many areas in the north
        mean that once temperatures decline below freezing for winter, they stay below this
        threshold.
        North–south asymmetries also show up in snowlines, treelines, the frost tolerance of
        trees, and the proportion of winter deciduous species (Woodward 1987; Körner 1998; Körner
        and Paulsen 2004). Indeed, such differences have long been appreciated for vegetation. In
        marine systems, one of the best-known asymmetries is the low upper thermal limit to
        performance and survival in Antarctic compared with Arctic ectotherms. This difference in
        limits to survival and performance is characteristic of fish, invertebrates, and macroalgae
        (e.g., Wiencke et al. 1994) (Figure 3). Asymmetries are also apparent in the geographic
        ranges of a wide variety of animals and plants. Rapoport's rule proposes that species
        ranges will be larger at high than at low latitudes (Stevens 1989). The pattern is thought
        to be a consequence of considerably greater temporal climatic (and especially temperature)
        variation at high latitudes, and the resulting need for broader physiological tolerances of
        individuals. These broad tolerances enable the species to which these individuals belong to
        occur across a wider range of sites than species at lower latitudes. However, if there is
        much less temporal temperature variation in the south than in the north, evidence for the
        rule should be less forthcoming in the southern hemisphere. This is indeed the case.
        Consistent increases in latitudinal extents with latitude are uncommon in the south, and
        Rapoport's rule is now largely considered to be a northern phenomenon (Gaston et al.
        1998).
      
      
        Large-Scale Asymmetries in Biodiversity
        In the years since Platnick (1992) suggested that the world is pear-shaped from a
        biodiversity perspective, with more rapid declines in richness from the equator in the
        northern than in the southern hemisphere, evidence that there are large-scale asymmetries
        in the latitudinal diversity gradient has been accumulating. Seed plant and mammalian
        family richness per unit area declines more steeply in the northern hemisphere than in the
        south (Woodward 1987; Gaston et al. 1995), and similar asymmetries, mostly at the species
        level, have been noted for other groups such as New World birds, several groups of insects,
        spiders, foraminiferans, and a variety of benthic marine taxa (Platnick 1992; Rex et al.
        1993; Eggleton 1994; Blackburn and Gaston 1996; Culver and Buzas 2000; Rodriguero and Gorla
        2004). Nonetheless, not all groups show these trends, and a recent meta-analysis, albeit
        one on a relatively coarse scale, failed to find consistent north–south differences in
        latitudinal gradients (Hillebrand 2004). Recent reviews, particularly of marine diversity,
        have pointed out the difficulty of making comparisons of this kind owing to sampling
        constraints (Clarke and Johnston 2003). However, it remains remarkable that even simple
        exercises—such as plotting richness values for different latitudes or latitudinal bands
        against each other for the hemispheres and examining the resulting relationship, or
        overlaying them on the same range of latitudes—rarely appear in the literature. Thus, it is
        not yet clear how common or strong hemisphere-related asymmetry is.
        By contrast, it appears that proximate ecological correlates of diversity gradients
        differ considerably between north and south. Although both historical and ecological
        factors have led to variation in the numbers and identity of species across the globe
        (Ricklefs 2004), climate, and particularly energy and water availability, is a strong
        predictor of broad-scale patterns in species richness for both plants and animals. However,
        the extent to which energy and water availability constrain species richness varies. In a
        recent comparative analysis, Hawkins et al. (2003) showed that water availability is the
        key limiting component of richness for the southern hemisphere, but for temperate regions
        of the north, energy availability is more important (Figure 4). They ascribe this
        difference to the warmer and less thermally variable conditions of the southern hemisphere,
        which, as we have already noted, have considerable effects on species life histories.
        Of course, biodiversity is not just species richness, but also encompasses the
        ecological complexes of which species are a part. Although potential north–south
        asymmetries in interactions have not been widely explored, recent work is providing
        tantalising glimpses of such variation. Thus, it appears that on the basis of a
        straightforward (not phylogenetically corrected) comparative analysis, specialisation in
        plant–pollinator relationships is much greater in the south than in the north. European and
        North American orchids are typically visited by five species of insects, whereas in
        southern Africa the median is a single pollinator species per species of orchid (Johnson
        and Steiner 2003) (Figure 5). Insect–plant interactions might also vary in other ways
        between the hemispheres, as the rarity of showy autumn colours and the paucity of aphid
        species—which are thought by some to be a driver of these displays (Archetti and Brown
        2004)—in south temperate areas suggests. Asymmetries in patterns of human disease point to
        similar hemisphere-related variation in interactions between organisms (Guernier et al.
        2004).
      
      
        A World in Flux
        Despite considerable spatial complexity, there do seem to be regular north–south
        differences in species life histories and patterns of range size variation that are
        consistent with disparities in the climates of the two hemispheres (Figure 6). These
        differences extend to the proximate ecological mechanisms underlying spatial variation in
        species richness, and, in some cases, apparently to ecological interactions. However, what
        is less clear is the regularity and strength of north–south differences in spatial
        diversity patterns, and especially the latitudinal gradient in diversity, as well as the
        ways in which abiotic variation between the hemispheres might extend through the
        genealogical and ecological hierarchies to effect such differences. Indeed, if the extent
        to which abiotic differences between the hemispheres influence biodiversity patterns is to
        be better comprehended, several key issues deserve attention.
        First, both phylogenetically independent and non-independent comparisons of life history
        traits and physiological variables across a variety of groups are required. Contrasting
        these approaches will provide considerable insight into how much of the signal is based on
        phylogenetic patterns, and how much on current ecological responses. Whilst in some cases
        data may be obtained from the literature, it is likely that new work will have to be
        undertaken, especially in the southern hemisphere, where the number of past investigations
        of such traits is generally much lower than in the north. Moreover, replicated studies
        using similar methods might substantially improve the signal-to-noise ratio, which can be
        weakened in “macrophysiological” or large-scale life history and physiological comparisons
        by the fact that different methods often lead to different outcomes.
        Second, there is much to be said for the application of similar methods to
        investigations of large-scale, hemisphere-related patterns of species interactions.
        Differences like those in plant–pollinator systems discussed here might extend to other
        interactions in marine and terrestrial systems. Contrasting phylogenetically independent
        and non-independent comparisons are likely to provide much insight into the reasons for
        those asymmetries that are found.
        Finally, comparisons of latitudinal gradients and their underlying correlates in the two
        hemispheres for the same taxon, sampled using similar methods, and investigated with
        methods that take cognisance of likely confounding effects are required. This approach will
        provide a means of determining whether asymmetries in the climates of the two hemispheres
        really do translate into differences in biodiversity patterns. Such an approach goes to the
        heart of the question of the processes underlying the latitudinal gradient in species
        richness, and could go a considerable way to teasing apart the importance of historical,
        ecological, and null explanations, and identifying the mechanisms that underlie them.
        In our view, clarifying these issues is of considerable importance. What is at stake is
        not a set of arcane ecological questions, but rather questions that are central to
        determining whether ecological and conservation lessons learnt in one area can be applied
        more broadly. For example, it has been suggested that climate change will cause substantial
        extinctions in the near future (Thomas et al. 2004). Indeed, responses by species to such
        change, via phenological shifts and northward movement of species range margins, are well
        documented for northern hemisphere species (Parmesan and Yohe 2003). However, if there are
        substantial differences in abiotic environments such that patterns in diversity and their
        responses to change differ between hemispheres, then such shifts may not be of similar
        consequence in the south. To date, southern hemisphere studies represent less than 1% of
        the total in this field (Root et al. 2003), suggesting that it is not at all clear how the
        considerable biodiversity in the south will respond to future change. We find such a
        situation extraordinary. Thus, whilst penguins might at first appear counter, spare, and
        strange, they serve as a reminder that differences between the north and south might not be
        so much strange, as remarkable and worthy of closer attention.
      
    
  

  
    
      
        
        
          
            
              ‘You’, your joys and your sorrows, your memories and your ambitions,
              your sense of personal identity and free will, are in fact no more than the behavior
              of a vast assembly of nerve cells…” —Crick (1994, p. 3)
            
          
        
        Francis Crick was an evangelical atheist. He believed that scientific understanding
        removed the need for religious explanations of natural phenomena. From James Watson's and
        his early work, the structure of DNA explained the α, the origins of life. This was a
        starting point; from the elucidation of the structure of DNA, there was an explosion, a
        massive diversity of science that in part removed the need to postulate a creator or a
        creation myth. Francis still felt that life was no less astonishing just because it was
        biological and natural in origin. He had a consistent and completely rational world view
        without a need to invoke vitalism, or any non-material force (M. Crick 2004). And in the
        last decades of his life, he applied this philosophy to the Ω, consciousness.
        Once the structure of DNA was known, the physicist George Gamow formed the RNA Tie Club,
        with Francis and eighteen others including his close friends Leslie Orgel and Sydney
        Brenner (2001); it was an ingathering that sowed seeds for future molecular biologists
        (Judson 1996). DNA had become the “α,” the beginning (Bronowski 1978), not just of
        Francis's career, but of a whole new culture of scientific life and understanding (Crick
        1966).
        Ten years later, the secrets of DNA transcription and translation unmasked, Francis
        turned to consciousness. He admitted he knew little at first, only that the structure of
        consciousness was as tough a problem as DNA's structure. DNA was certainly not played out,
        but the Ferrier Lectures in the Proceedings of the Royal Society of London by David Hubel
        and Torsten Wiesel were just available, tempting Francis with an almost physicist's view of
        neurons in action. Hubel and Wiesel wrote of functional architectures, embedded in
        beautiful, almost crystalline structure. The comprehension of mind invoked by a biological
        mechanism appeared ripe for the sort of thoughtful, theoretical science he had applied to
        DNA. Francis was now sixty years old and moved from Cambridge to the Salk Institute in La
        Jolla, California. Francis began with the brightest young minds he could find.
        David Marr was a young mathematician and physiologist whose doctoral thesis on a theory
        of mammalian brain function at Cambridge had brought him into some contact with Brenner and
        Francis. A professor at the Massachusetts Institute of Technology, he began working with
        Tomasio Poggio of the Max Plank Institute in Tübingen on a computational theory of
        neuroscience. Following an invitation from Francis, Poggio and Marr spent the month of
        April, 1979 extending their intense examination of the core problems of visual perception.
        They spent hours sitting at the most western end of the Salk Institute, at the cafeteria or
        in Francis's office, gazing into the Pacific Ocean with all its daily changes, discussing
        not only architecture of visual cortex and visual perception, but the ramifications of a
        good theory of brain function. We know of these conversations, as the probing of Marr by
        Francis is captured in the final chapter of Marr's now classic book “Vision” (Marr 1982).
        (Although Marr speaks of a three-way conversation, judging from our own experiences as
        Francis's younger colleagues, the interlocutor simply seems to be Francis.)
        Marr had been diagnosed with acute leukemia in the winter of 1978 (Marr and Vaina 1991).
        The one-month visit to the Salk Institute was an intellectual gift, for eighteen months
        later, Marr died. Francis had simultaneously lost a young friend and colleague who had
        brought an “incisive mind and creative energy” (Crick 1994, p. 77) and his best new ideas
        of a theoretical neurology to the brain (Marr 1969, 1970). And he saw the tragedy of Marr
        being cut off from solving the big problems for which he was so clearly destined.
        During those early years, Francis must have thought that consciousness was tractable—if
        only the right way of thinking was brought to bear on it. Francis's brain was capable of
        collecting and filing away many disparate data, which he could then combine uniquely and
        imaginatively, leading to that “dramatic moment of sudden enlightenment that floods the
        minds when the right idea clicks into place” (Crick 1990, p. 141). Whatever his initial
        thoughts about the nature of the problem, Francis soon came to realize that the problem of
        consciousness was even tougher than he imagined, that the “click” was not happening with
        consciousness. In 1988, he wrote, “I have yet to produce any theory that is both novel and
        also explains many disconnected facts in a convincing way” (Crick 1990, p. 162).
        Over the quarter century he was at the Salk Institute, Francis did propose solutions to
        some smaller problems in neuroscience (Sejnowski 2004) and brought consciousness into the
        scientific fold (Rich and Stevens 2004). But something else was going on quietly and behind
        the scenes. Francis was building an army to help him take on consciousness. This was not
        empire building with Francis as the head of a group of directed scientists in the Cambridge
        or German model. Francis continually encouraged and assisted young scientists to approach
        the hardest problems of the brain. Marr and Poggio were just the first recruits he helped
        embolden. He started his long-time collaboration with Christof Koch, once a post-doctoral
        trainee with Poggio, on “The Problem of Consciousness” (Crick and Koch 1990, 1992). His
        door was always open to graduate students, postdoctoral trainees, faculty who wanted to
        discuss those problems as many others and we can attest. Francis could be found daily at
        tea time, an ingathering of the Salk Institute computational and vision laboratories of
        Simon LeVay, Terry Sejnowski and Thomas Albright, surrounded by graduate students and
        post-doctoral trainees, with conversation ranging across science—Francis listening to their
        stories of their explorations and encouraging them to reach beyond their horizons. Francis
        had a “love of the truth and helped others to move to the truth” (Watson 2004).
        When Francis worked on the structure of DNA, he had some simple facts, such as
        Chargaff's Laws, and means to make point mutations from which it could be determined how
        function followed structure. But not a single neuroanatomist knew how many neurons actually
        converged in their input to a particular single cell. No one knew how to eliminate a
        specific cell type from a circuit— to make a point mutation, so to speak, in the structure
        of consciousness. His 1979 article in Scientific American, “Thinking about the Brain,” did
        not have much impact at the time, even when it explicitly described three needed methods:
        first, a method by which all the connections to a single neuron could be stained; second, a
        method by which “all neurons of just one type could be inactivated, leaving the others more
        or less unaltered”; and third, a means to differentially stain each cortical area, “…so
        that we could see exactly how many there are, how big each one is and exactly how it is
        connected to other areas.” By the mid-1980s, Francis had realized that these massive holes
        in our understanding of the most simple brain facts were not being filled. Something needed
        to be done.
        Over the twenty years since the RNA Tie Club, molecular biology had matured. Francis
        actively began encouraging the inclusion of the critical tools of molecular biology in the
        study of neural circuits and perception; in his thinking, molecular biology was critical to
        understand how the brain worked because it provided tools. He would encourage junior
        scientists, postdoctoral trainees, and faculty—all those who had visited him over the
        years—to think about using these tools. He would give short homilies about the plethora of
        sub-types of neurons in the retina; would not the cortex be at least as rich in
        possibilities? Molecular tools could unravel this knot.
        As we reminisced after Francis's death, we discovered that Francis had spoken with each
        of us on these molecular methods, across a twenty-year interval. In the mid-1980s, Francis
        spoke with Ralph, pressing him to consider how he might do highly specific lesions of
        single neuron types in motion cortex using molecular identifiers. At the time, the only
        tools imaginable were some sort of killer antibody approach. Twenty years later, Ed recalls
        Francis continuing to encourage this cross-disciplinary molecular and systems approach. It
        was absolutely imperative to Francis's vision of the maturation of neuroscience that there
        would be a conjoining of molecular biology and systems neuroscience. We are sure we were
        not unique in hearing this call; with how many others had he shared his vision?
        The science of the mind is a thinker's game. It is chess against the grandest masters,
        biological evolution and natural selection—and we are just learning to move the pieces. Our
        viewpoint is often myopic, with our noses pressed against the back row of the chessboard.
        It is hard to see the pieces, let alone their arrangement or the strategies they are
        forming. Francis may not have had the overview needed to reveal evolution's gambit, but he
        knew the moves needed to clear the “tangle of difficulties” (Crick 1994, p. 77) that
        prevented an unfogged view of his opponent's pieces.
        Francis hoped for simplicity. He wrote, “Curiously enough, in biology it is sometimes
        those basic problems that look impossibly difficult to solve which yield most easily…. The
        biological problems that are really difficult to unscramble are those where there is almost
        infinity of plausible answers and one has to painstakingly attempt to distinguish between
        them.” (Crick 1990, p. 157–158). Watson and Crick had picked the right pieces of
        information to construct their model. Francis early on had had the same hopes to open the
        doors of consciousness (to paraphrase Huxley 1963). Watson and Crick used their intuition
        to fill in the gaps. But Francis found that there were just too many possibilities, and the
        gaps in knowledge were still just too big for consciousness.
        In 1999, Francis felt that gentle and informal direction was not enough. Thus, he
        convened a meeting of molecular biologists and neuroscientists at the Salk Institute to
        encourage them to work together. He brought scientists including Tom Albright, Ursula
        Bellugi, Ed Callaway, Rusty Gage, Steve Heinemann, Terry Sejnowski, Chuck Stevens, and
        Inder Verma into one room and said it was time to get serious. He reminded them of the
        advantages of genetic methods for targeting specific cell types within complex neural
        circuits, and he reiterated the need for methods that could be used to identify,
        manipulate, and observe neural circuits in action. Not only were methods to be used in
        transgenic mice, but also methods based on viral vectors were needed to study the visual
        system of monkeys. From this, a number of initiatives moved forward, with studies ranging
        from the molecular biology of Williams syndrome to basic molecular tool building (Naldini
        et al. 1996; Blomer et al. 1997; Bellugi et al. 1999; Pfeifer et al. 2001; Zhao et al.
        2001; Kaspar et al. 2002a, 2002b, 2003; Lechner et al. 2002; Lein et al. 2004).
        Today the tools are emerging at an ever faster pace, at least in part due to Francis's
        maneuvers behind the scenes and his encouragement of junior scientists. Time is curing
        Francis's bout of scientific prematurity (Stent 1972). Individual cell types will soon be
        reversibly inactivated (Johns et al. 1999; Lechner et al. 2002; Slimko et al. 2002;
        Ibanez-Tallon et al. 2004); viral methods of tracing connections will start to fill in the
        gaps; new sensor methods for simultaneously recording from hundreds and thousands of
        identified neurons are coming (Guerrero and Isacoff 2001; Zemelman and Miesenbock 2001;
        Tsien 2003). There is a new field Francis termed “molecular psychology” or “molecular
        biology of systems neuroscience”; Albright simply calls it Neuroscience.
        In 2001, Francis was diagnosed with colon cancer. He realized that the problem of the
        neural correlates of consciousness might outlast him. Francis was walking with a cane,
        still not waiting for anyone, nor allowing anyone to wait for him. He continued to find
        time for new faces in the field and continued to work on consciousness. While he had made
        many strides forward, he saw the race for him was winding down. He had had his hope for
        understanding the structure of consciousness. He had laid the groundwork. He decided to
        encapsulate his ideas in a “Framework” paper with Koch (Crick and Koch 2003). For many of
        us it was clear that he was laying out where he would go, had he enough time.
        Each of the points of the Framework could form a major research initiative. Perhaps they
        should. But the central point is the 
        approach to understanding consciousness; it is both structural and
        functional, peering forward into the future into what the shape might be. It was clear to
        his friends and colleagues that Francis was leaving a last testament.
        As the cancer finally caught up with Francis, he focused on the role of the rarely
        studied claustrum (Sherk 1986). He wrote internal memos, brought friends and colleagues to
        working lunches at home with Odile, his wife of fifty-five years. Why do this? Why all this
        focus on another part of the brain, when only months remained? (Indeed it turned out to be
        weeks.) Was it his way of saying goodbye, of bringing his extended family close again? We
        think not. Francis wanted to make sure his plan went forward. He stressed to his visitors
        queries about the origins of the claustrum, its molecular biology, its role in
        consciousness. He was using his framework, pointing out the route to understanding the Ω of
        his career.
        Francis was doing what he truly loved to his last moments. He needed to be doing
        science, perhaps more than ever, to take him away from the physical pain that he surely
        felt. He had built his army. Perhaps none of us even knew we had enlisted, but we had. And
        he was setting us off on the long march forward into a time that soon would not be for him.
        Francis died on the cusp of a new age of molecular systems neuroscience. Soon, we will have
        the tools and the data, but we will not have Francis. Francis had existed between the α of
        DNA and the Ω of consciousness. And for a man who never believed in the afterlife, he had
        indeed achieved immortality.
      
    
  

  
    
      
        
        Reinforcement, like sympatric speciation (see Box 1), has charisma. Evolutionary
        biologists are still deeply uncertain about how often these processes take place, and hence
        how important they are in explaining the biological diversity we see today. Empirical and
        theoretical support for both ideas has waxed and waned over recent decades. Yet both ideas
        have consistently garnered an unusual amount of attention.
        Much of the appeal of both reinforcement and sympatric speciation lies in the way they
        unite micro- and macroevolution. Reinforcement, a concept popularized by Dobzhansky (1937),
        is a process by which speciation, a macroevolutionary process, can be driven directly by
        natural selection, one of the primary microevolutionary forces. Sympatric speciation can
        make the same claim. Because of this close linkage between the concepts, the study of one
        can tell us a great deal about the other (see Kirkpatrick and Ravigné 2002). Such studies
        can also reveal a lot about the general role of microevolution in species divergence.
        Reinforcement provides a pathway toward the completion of the speciation process.
        Imagine that two divergent populations (potentially even classified as separate species)
        come into contact after a period of allopatry (Figure 1). If the populations have been
        apart for a long time, evolved differences between them will cause a certain degree of
        incompatibility when the populations come together. Often, this incompatibility comes in
        the form of low hybrid fitness (postzygotic isolation) or mismatched mating characteristics
        (premating isolation). The degree of the development of these isolating mechanisms is
        roughly proportional to the genetic distance between the populations, reflecting the fact
        that incompatibilities accumulate over time (Coyne and Orr 1989).
        If the isolating mechanisms between these populations are only partially complete,
        extensive hybridization may occur. This can result in fusion back into a single population,
        or in the swamping of one population's gene pool by the genes of the other (extinction).
        But there is another possibility, one that can cause the speciation between the two
        populations to proceed. Remember that if the populations have been separated for long
        enough, it is likely that hybrids between them will have relatively low fitness.
        Individuals who mate with members of the opposing population will therefore produce
        offspring of poor quality, and hence have lower fitness than individuals that mate within
        their own population. This favors the evolution (or further divergence) of characteristics
        that cause mating within, rather than between, populations (Figure 1C). Speciation between
        the populations is driven further towards completion through this increase in premating
        isolation.
        This process, the evolution of premating isolation after secondary contact due to
        selection against hybrids, is reinforcement sensu Dobzhansky (1937). Recent authors have
        broadened the definition of reinforcement to include as a driving force any form of
        selection against mating between populations (e.g., Servedio and Noor 2003). This could
        include, for example, lower fertility, or higher mortality of females that mate with
        members of other populations. In all definitions, however, the microevolutionary process of
        selection is essential for reinforcement. In fact, in reinforcement, speciation itself can
        be thought of as an adaptive response to selection. It is little wonder that this causal
        linking of micro- and macroevolution has appeal for many evolutionary biologists.
      
      
        Reinforcement in the 21st Century
        Despite the substantial progress in our understanding of reinforcement that has been
        achieved over the last few decades, many questions remain about the process. These
        questions lend themselves to exploration by a broad variety of disciplines (evolution,
        ecology, behavior, phylogenetics, phylogeography, genetics), approaches (experimental,
        observational, comparative, theoretical) and taxonomic systems.
        Doubtless, the most important unanswered question about reinforcement is how often it
        occurs. It is very difficult to prove that reinforcement is occurring, or has occurred,
        between two species. Reinforcement occasionally leaves a signature, called reproductive
        character displacement, in which mating characteristics have diverged between populations
        in areas of sympatry but not areas of allopatry (Figure 2) (the relationship between
        reinforcement and reproductive character displacement, and controversy over the definition
        of the latter, is reviewed in Howard 1993). In sympatric areas, populations are capable of
        producing hybrids, which drives reinforcement, while in allopatry hybrid production, and
        hence the selection for reinforcement, is absent. Reproductive character displacement has
        been found to be common, suggesting to some that reinforcement may be common as well
        (Howard 1993). It is universally acknowledged, however, both that reproductive character
        displacement can be caused by processes other than reinforcement, and that reinforcement
        can occur without leaving this signature (e.g., when population ranges are completely
        sympatric). Proving that reinforcement has occurred requires the ruling out of several
        alternative hypotheses, which are themselves difficult to assess (Noor 1999; Coyne and Orr
        2004).
        Several isolated examples of reinforcement between specific pairs of species have been
        demonstrated, fairly conclusively, in a variety of taxa including 
        Drosophila pseudoobscura and 
        D. persimilis (Noor 1995), flycatchers (Sætre et al. 1997),
        sticklebacks (e.g., Rundle and Schluter 1998), spadefoot toads (Pfennig 2003), and
        walking-stick insects (Nosil et al. 2003) (see also reviews of Noor 1999; Coyne and Orr
        2004). These studies involve a variety of behavioral tests of mate choice, analyses of
        hybrid fitness and the production of hybrids in the wild, and controls for alternative
        explanations.
        While examples such as these provide essential information about reinforcement, their
        slow rate of compilation and biased reporting do not provide efficient ways to assess how
        often reinforcement occurs in general. Comparative approaches, which examine patterns
        across a broader taxonomic group, can also provide support for reinforcement without these
        detailed mechanistic analyses (review in Coyne and Orr 2004). The revival of reinforcement
        in the late 1980s began with one such study in the genus 
        Drosophila (Coyne and Orr 1989). By comparing patterns across a
        wide number of species, such studies can give a better assessment of the potential
        frequency with which reinforcement occurs—without, however, providing conclusive evidence
        for reinforcement between specific species pairs.
        Another area where further research is essential is the determination of which
        biological factors promote reinforcement, as opposed to population fusion. Theoretical
        studies, using mathematical models and computer simulations, are proving useful in
        pinpointing the effects of many factors such as migration rates and patterns, the type of
        selection against interspecific mating, and the genetic basis of premating isolation
        (reviews in Turelli et al. 2001; Servedio and Noor 2003). Fortunately, some of the cases of
        reinforcement in specific species pairs are now being developed to the point where they can
        address similar questions (e.g., sex linkage of mating genes; Sætre et al. 2003). Both
        theoretical studies and these well developed empirical systems are also starting to address
        a third important area of research: how reinforcement interacts with other forces, such as
        ecological selection pressures, that promote speciation (e.g., Servedio 2004; Nosil et al.
        2003). These integrated studies are essential to the correct placement of reinforcement
        within the bigger context of speciation processes.
        In recent years, exciting developments have started to take place in the analysis of the
        genetics of reinforcement (reviewed in Servedio and Noor 2003). These developments both
        parallel and overlap with progress made on the genetics of speciation and species
        differences in general. For example, significant progress has recently been made in
        identifying the genetic control of hybrid incompatibilities (e.g., Presgraves et al. 2003;
        Barbash et al. 2003). This progress has been accompanied by a new understanding of how
        chromosomal rearrangements may allow these incompatibilities to be maintained despite
        hybridization in sympatry (Rieseberg 2001; Navarro and Barton 2003; Brown et al. 2004).
        Sympatric maintenance of incompatibilities, of course, has profound implications for
        reinforcement, which requires these incompatibilities as the force driving divergence (Noor
        et al. 2001).
        Genetic analysis is also allowing a new understanding of the mechanisms by which
        reinforcement might be taking place in specific cases. Work by Ortiz-Barrientos et al.
        (2004) in this issue of 
        PLoS Biology illustrates the extent of the insights that can be made with
        this approach. Using high-resolution genetic mapping the authors have identified the
        locations of genes that cause increased discrimination against 
        Drosophila persimilis males by 
        D. pseudoobscura females, due to reinforcement in sympatry.
        Surprisingly, these genes map to very different areas of the chromosomes than do genes that
        cause a basal level of mating discrimination between the species in allopatry. Among other
        insights, the position of these genes suggests that the reinforced discrimination is based
        on odor, not on the mechanism used in allopatry, male song. This leads to the novel
        conclusion that reinforcement is not just increasing the strength of an already existing
        mechanism of species discrimination, but is occurring through the development of a new
        discrimination system. These kinds of developments can also motivate more realistic
        theoretical models of the reinforcement process.
      
      
        Implications and Extensions of Reinforcement
        What if, when our assessment of the frequency of reinforcement is improved, it turns out
        to have been a rare occurrence in the generation of current biological diversity? The study
        of reinforcement is broad and varied enough that many of our findings about the process
        would still have wide-reaching implications.
        First, recall the claim, at the start of this article, that studying reinforcement
        reveals much about the role of microevolution in the macroevolutionary process of
        speciation. Knowledge gained about this relationship is not only directly applicable to the
        very similar process of sympatric speciation, but can also tell us a great deal about
        speciation caused by ecological adaptation and sexual selection, which are critical
        components of reinforcement in many systems (e.g., Nosil et al. 2003; Haavie et al. 2004).
        Studies looking for reinforcement have also led to insights into the formation and
        maintenance of hybrid zones (e.g., Butlin 1998; Britch et al. 2001). Situations where
        reinforcement fails to occur likewise teach a lesson, elucidating possible mechanisms of
        extinction when secondary contact occurs between species.
        Second, analysis of reinforcement clarifies the interactions between levels of
        reproductive isolation that occur at different stages in the life cycle. Reinforcement,
        broadly defined, can be driven by isolation at the postzygotic level or by
        incompatibilities that occur between mating and zygote production (postmating-prezygotic
        incompatibilities; Servedio 2001). Postzygotic isolation can likewise cause divergence at
        the premating stage (reinforcement) or potentially at the postmating-prezygotic stage,
        through the evolution of conspecific sperm precedence (Marshall et al. 2002). These various
        stages of isolation have different degrees of importance among plants, free-spawning marine
        invertebrates, and other internally and externally fertilizing animals (Bernasconi et al.
        2004). Analysis of these stages of isolation, their interactions, and the evolutionary
        pressures they are under therefore has broad implications for comparative reproductive
        biology across these varied groups.
        Finally, regardless of whether reinforcement has been a common pathway in speciation,
        its relevance may be increasing. Reinforcement is a possible outcome anytime species that
        are capable of hybridization come into contact. Human activity is increasing the incidence
        of secondary contact by altering habitat and introducing invasive species. This contact
        often results in hybridization (reviews in Rhymer and Simberloff 1996; Mooney and Cleland
        2001). It is important to identify and understand the properties of species pairs that make
        extensive introgression, extinction, stable hybrid zones, or reinforcement likely outcomes
        of such contact. If reinforcement has played a small role in the generation of current
        diversity, it may be because secondary contact itself has historically been a rare
        occurrence. It is the frequency of reinforcement among incidences of secondary contact that
        will determine its importance in the near future.
      
    
  

  
    
      
        
        “Dr. Octopus,” the villain that terrorizes the city in the most recent film of the
        popular 
        Spider-Man comic, is the ultimate characterization of a brain–machine
        interface (BMI) on the big screen. In 
        Spider-Man 2, the brain is that of nuclear physicist Dr. Otto Octavius,
        who dreams of harnessing nuclear fusion. The machine is a harness of four mechanical arms
        designed with tentacle-like flexibility, gripping and vision capabilities, and an
        artificial intelligence module that gives them some autonomy. The interface between the
        machine and the brain is at the spinal cord level, with an “inhibitor chip” to prevent the
        artificial intelligence module in the mechanical arms from taking over Octavius's brain.
        Controlling this mechanical device with his own thoughts, Octavius is able to manipulate
        hazardous materials during his fusion experiments. However, things go terribly wrong during
        the exhibition of one of these experiments: the mechanical arms fuse to Octavius's body
        while the inhibitor chip is disabled, resulting in the machine gaining partial control of
        his brain. Unable to subvert the machine to his will and conscience, Octavius, together
        with the BMI, becomes the villainous Dr. Octopus. At the end of the movie, in a flicker of
        sanity and heroism, Octavius dramatically sacrifices his life as the only way to terminate
        the evil machine and save the world.
        Although Dr. Octopus is a fictional character, a figment of a vivid imagination,
        audiences are fascinated by the fact that he is a human BMI. BMIs straddle the worlds of
        fact and fiction. While the entertainment industry has focused primarily on applications
        for augmenting cognitive and sensorimotor function, as seen in 
        Star Trek, Firefox, and many other science-fiction scenarios, the
        scientific community has targeted clinical applications, such as neuroprostheses for
        restoring motor function after traumatic lesion of the central nervous system. The current
        BMI approach is based on the idea that a human user could enact voluntary motor intentions
        through a direct interface between his brain and an artificial actuator in virtually the
        same way that we see, walk, or grab an object with our own natural limbs. Proficient brain
        control of an external device or actuator should be achievable through training using any
        combination of visual, tactile, or auditory feedback. As a result of long-term use of the
        BMI, the brain should be able to “incorporate” (or adapt to) the artificial actuator as an
        extension of its own body. With these goals in mind, the last five years have witnessed a
        dramatic increase in BMI-related studies in academic institutions around the world.
        Subjects have learned to utilize their brain activity for different purposes, ranging from
        electroencephalogram- and electrocorticographic-based systems (Wolpaw et al. 2002;
        Leuthardt et al. 2004), in which human subjects control computer cursors, to
        multielectrode-based systems, in which nonhuman primates control the movements of cursors
        and robots to perform different kinds of reaching and grasping tasks (Serruya et al. 2002;
        Taylor et al. 2002; Carmena et al. 2003; Musallam et al. 2004).
        These examples of what could be called the first generation of BMIs have something in
        common: they have been exclusively controlled by neural signals. Even with BMIs that use
        neural activity recorded with invasive electrodes to yield higher bandwidth and thus allow
        for the execution of more complex tasks, it remains unclear whether the quality of the
        signal will ever suffice for a patient to freely, safely, and effectively control a
        prosthetic arm to perform daily tasks. For instance, the level of motor skill required for
        dexterous finger manipulation is outstanding. Planning paths and avoiding obstacles while
        reaching and grasping in unconstrained environments requires similarly fine motor control.
        Thus, realistic motion through a complex environment with a BMI is extremely challenging
        and, perhaps, not feasible with the relatively low bandwidth (∼10 Hz) of current BMIs. Even
        if significant improvements are made in the algorithms used to decode neural activity by,
        for example, incorporating knowledge from neurophysiological experiments of how motor
        signals that underlie movements are encoded in the brain, current BMI bandwidth still may
        not be sufficient to reach the performance level an injured patient would desire.
        What does this mean for second-generation BMIs? We may find some inspiration in Dr.
        Octopus. The fictional BMI in 
        Spider-Man 2 is innovative in the sense that it is a hybrid system that
        incorporates both neuronal and artificial control signals. It makes perfect sense to take
        advantage of the fields of engineering (control theory) and artificial intelligence to
        build better BMIs—part brain and part robot. In principle, these hybrid BMIs would allow a
        patient to accomplish a task more efficiently than those relying on neuronal signals alone.
        For example, in a common task such as reaching for and grasping a glass of water, a hybrid
        BMI would be fed with both brain and machine control signals; the intention of movement
        would be decoded directly from neuronal signals, leaving obstacle avoidance and grasping
        stabilization to the artificial control module of the system. Such a module would get
        inputs from sensors embedded in the robot, and would produce a control signal that would
        fuse with the neuronal control signal to augment the final output command.
        What ratio of neuronal versus artificial signal would be needed for optimal control of a
        BMI? In the movie, Octavius's crisis is a severe unbalance in favor of machine control.
        Science fiction aside, we see the more realistic potential problems of having a physical
        device gaining autonomous control. Technically, this could be analyzed as too much gain in
        the artificial control signal, which, in a realistic scenario, would likely result in
        oscillating behavior, jerky grasping, etc. Hence, safeguarding measures (characterized in
        the movie as the inhibitor chip in Octavius's brain stem) would be needed to avoid
        dangerous situations when a chronic neuroprosthesis freely interacts with the real world.
        For both science and science fiction, the question is the same. Brain and machine: which
        one gets the power?
      
    
  

  
    
      
        
        How did life emerge from a soup of chemicals? How do patterns such as schools of fish
        form from individuals? How do voting patterns emerge? Such a diverse array of problems
        seems completely unrelated. However, they all involve “emergence of complexity.” When
        individuals come together, they form patterns, structures, and organizations that cannot be
        discerned from the individuals alone. The study of the emergence of complexity is one of
        the most active and important areas of research. It is important not only for understanding
        nature, but also for technological applications, including the fabrication of large-scale
        integrated nanocircuits using a bottom-up approach, and the preparation of multifunctional
        “smart” nanomaterials.
        DNA evolved to be the primary carrier of genetic information because of its
        extraordinary chemical properties. These same properties also make DNA an excellent system
        for the study of self-assembly and self-organization. Two complementary molecules of
        single-stranded DNA have paired bases that bond with each other and form the well-known
        double helix structure. Two molecules of double-stranded DNA (duplexes) can further
        associate together if they have complementary single-stranded overhangs (sticky ends).
        Intermolecular interactions can be precisely predicted by Watson–Crick basepairing (adenine
        to thymidine and guanine to cytosine). And, these interactions are structurally understood
        at the atomic level. Given the diversity of the DNA sequences, we can easily engineer a
        large number of pairs of DNA duplexes that associate with each other with sequence
        specificity and in a well-defined fashion. This property is not common among other
        molecular systems. Small organic and inorganic molecular pairs can interact with each other
        with specificity and in well-defined structures, but the number of such pairs is limited
        and their chemistry varies greatly. Protein molecules, such as antibody–antigen pairs, have
        great diversity and high specificity. However, it is extremely difficult, if not
        impossible, to predict how proteins interact with each other. In contrast, DNA as a
        molecular system fulfills all the aforementioned criteria.
        In nature, DNA occurs predominantly as a linear molecule, and if its conformations were
        limited to linearity, it would not be very useful for studying self-assembly. Fortunately,
        branched DNA structures can be engineered. Holliday junctions, for example, are
        intermediates that occur during genetic recombination. To model Holliday junctions, a
        stable four-arm junction has been constructed in which, by design, no two strands are fully
        complementary to each other (Kallenback et al. 1983; Seeman 2003) (Figure 1A). For example,
        the 5′ half of strand 2 is complementary to the 3′ half of strand 1, but the 3′ half of
        strand 2 is complementary to the 5′ half of strand 3 instead of that of strand 1. Combining
        branched structures and the excellent molecular recognition of DNA, we are ready to
        engineer complicated DNA nanostructures and use them for studying self-assembly.
        Extensive studies have shown that the four-arm junction adopts an X-shape structure
        (Figure 1B) under physiological conditions, and the angle between its two helical domains
        can vary widely (Lilley 2000). It is impossible to construct well-defined large structures
        from flexible components. To overcome this problem, several well-behaved DNA motifs have
        been engineered. Double crossover (DX) (Fu and Seeman 1993) and triple crossover (TX)
        (LaBean et al. 2000) molecules are two early examples (Figure 1C and 1D). In such
        molecules, two or three DNA duplexes lie side by side. Two neighboring duplexes are joined
        by two crossovers, which prevent any duplex from twisting against its neighbor duplex.
        Thus, the interhelical angles become fixed at 0°. Other motifs quickly followed, including
        the paranemic crossover motif (Shen et al. 2004), rhombus/parallelogram motif (Mao et al.
        1999), cross motif (Yan et al. 2003), and several triangle motifs (Chelyapov et al. 2004;
        Ding et al. 2004; Liu et al. 2004). They all are stable, rigid, and readily designed for
        self-assembly.
        One simple example of self-assembly is the formation of two-dimensional (2D) periodic
        arrays or 2D crystals. This is also one of the greatest successes in the field of DNA
        self-assembly (Figure 2). The first 2D DNA crystals were assembled from DX motifs (Winfree
        et al. 1998). In a 2D crystal, each DX molecule contains four sticky ends (A–A′ and B–B′)
        distributed on its two component duplexes. The complementarity of the sticky ends is
        designed in such a way that a DX molecule will interact with another four DX molecules
        through its four sticky ends. Any two DX molecules can interact with each other though only
        one pair of sticky ends. Any pair of sticky end interactions will position the two DX
        molecules in a conformation such that no other sticky ends from these two molecules are in
        sufficient proximity to interact. As a result of this design, regularly ordered 2D arrays
        have formed (Figure 2). Following similar strategies, others have designed DNA motifs to
        assemble into 2D arrays, whose symmetries include tetragonal (Yan et al. 2003),
        pseudohexagonal (Mao et al. 1999; Liu et al. 2004), and hexagonal (Chelyapov el al. 2004;
        Ding et al. 2004).
        Inspired by early theoretical suggestions (Winfree 1998), experimental exploration of
        aperiodic self-assembly immediately followed. One study applied algorithmic self-assembly
        to TX molecules (Figure 3) (Mao et al. 2000). The assembling rule “exclusive OR” (XOR) is
        encoded in the TX molecules. Consider the value of all inputs and outputs as either 1 or 0.
        For XOR operations, if two inputs are the same, the output will be 0; otherwise, the output
        will be 1. If molecules X and Y are the input and output, respectively, Y
        i molecule takes the input from the X
        i and Y
        i−1 molecules. In other words, the values of the X
        i and Y
        i−1 molecules determine what Y molecule will be incorporated. There are
        four different types of Y molecules, whose inputs are (1, 1), (1, 0), (0, 1), and (0, 0).
        These four, and only four, Y molecules are enough to satisfy any input combination. Two C
        molecules connect the input and output molecules, which is necessary for the
        characterization but not essential for the self-assembly process. Sticky ends between the X
        molecules and the C molecules are longer than those between Y molecules and between Y and X
        or C molecules. Thus, the C and X molecules assemble first to form the inputs because the
        association between longer sticky ends is more stable than those between the shorter ones.
        Then the output Y molecules assemble to the assembled C and X molecules. In that study (Mao
        et al. 2000), two different input combinations were used, and one of them is shown in
        Figure 3. The resulting DNA structures are periodic with respect to the backbones, but they
        are aperiodic in their sequences. Though the resulting four-byte one-dimensional (1D)
        structures are quite simple, this study demonstrated that aperiodic structures are
        achievable through self-assembly.
        Winfree and co-workers in this issue of 
        PLoS Biology have extended the algorithmic self-assembly strategy from 1D
        to 2D (Rothemund et al. 2004). This achievement is certainly a milestone in the field of
        self-assembly. It overcomes a great challenge, as the structural complexity dramatically
        increases from 1D to 2D structures. These researchers have applied the same XOR algorithms
        to DX molecules in their study and achieved fractal structures, Sierpinski triangles
        (Figure 4). External inputs are in the bottom row. Each row takes inputs from the row
        immediately below, and sends the operation outputs to the row immediately above. Each
        position takes two inputs (identical or non-identical) from lower left and lower right
        positions, and sends identical output to both upper left and upper right positions. The
        arrows indicate the direction of information flow, or assembly sequences. In their
        experiment, the rules are encoded in DX molecules. This study is conceptually
        straightforward, but the experimental challenges are tremendous. One key challenge is
        assembly fidelity. The right molecules have to compete with partially matched molecules.
        The concentrations of the competing molecules further complicate the fidelity issue, as
        some molecules could be rapidly depleted from the solution. In that sense, the current work
        is quite stunning even though the assembly is far from perfect.
        In principle, a wide range of 2D patterns could be generated with the same set of
        molecules and the same strategy, changing only the first row of the assembly, which
        specifies the external inputs. Realization of this goal will critically rely on the
        elimination of assembly errors, or the introduction of error corrections (Winfree and
        Bekbolatov 2004).
        The current work represents a neat approach to understanding the emergence of
        complexity. It integrates both simulation and wet chemistry. It also provides a plausible
        approach to nanofabrications. Over the last decade, a variety of methods have been
        developed, which use biomacromolecules as templates to fabricate nanostructures (Braun el
        al. 1998; Douglas and Young 1998; Mucic et al. 1998; Fu et al. 2004). Limited by the
        complexity of the available biomacromolecular templates, simple nanostructures are the
        usual result: mostly nanowires, nanoparticles, and simple aggregates of nanoparticles. The
        current work illustrates the possibility of generating more complicated structures and
        promises unprecedented structural complexity for nanomaterials.
      
    
  

  
    
      
        
        Although mathematics has long been intertwined with the biological sciences, an
        explosive synergy between biology and mathematics seems poised to enrich and extend both
        fields greatly in the coming decades (Levin 1992; Murray 1993; Jungck 1997; Hastings et al.
        2003; Palmer et al. 2003; Hastings and Palmer 2003). Biology will increasingly stimulate
        the creation of qualitatively new realms of mathematics. Why? In biology, ensemble
        properties emerge at each level of organization from the interactions of heterogeneous
        biological units at that level and at lower and higher levels of organization (larger and
        smaller physical scales, faster and slower temporal scales). New mathematics will be
        required to cope with these ensemble properties and with the heterogeneity of the
        biological units that compose ensembles at each level.
        The discovery of the microscope in the late 17th century caused a revolution in biology
        by revealing otherwise invisible and previously unsuspected worlds. Western cosmology from
        classical times through the end of the Renaissance envisioned a system with three types of
        spheres: the sphere of man, exemplified by his imperfectly round head; the sphere of the
        world, exemplified by the imperfectly spherical earth; and the eight perfect spheres of the
        universe, in which the seven (then known) planets moved and the outer stars were fixed
        (Nicolson 1960). The discovery of a microbial world too small to be seen by the naked eye
        challenged the completeness of this cosmology and unequivocally demonstrated the existence
        of living creatures unknown to the Scriptures of Old World religions.
        Mathematics broadly interpreted is a more general microscope. It can reveal otherwise
        invisible worlds in all kinds of data, not only optical. For example, computed tomography
        can reveal a cross-section of a human head from the density of X-ray beams without ever
        opening the head, by using the Radon transform to infer the densities of materials at each
        location within the head (Hsieh 2003). Charles Darwin was right when he wrote that people
        with an understanding “of the great leading principles of mathematics… seem to have an
        extra sense” (F. Darwin 1905). Today's biologists increasingly recognize that appropriate
        mathematics can help interpret any kind of data. In this sense, mathematics is biology's
        next microscope, only better.
        Conversely, mathematics will benefit increasingly from its involvement with biology,
        just as mathematics has already benefited and will continue to benefit from its historic
        involvement with physical problems. In classical times, physics, as first an applied then a
        basic science, stimulated enormous advances in mathematics. For example, geometry reveals
        by its very etymology (geometry) its origin in the needs to survey the lands and waters of
        Earth. Geometry was used to lay out fields in Egypt after the flooding of the Nile, to aid
        navigation, to aid city planning. The inventions of the calculus by Isaac Newton and
        Gottfried Leibniz in the later 17th century were stimulated by physical problems such as
        planetary orbits and optical calculations.
        In the coming century, biology will stimulate the creation of entirely new realms of
        mathematics. In this sense, biology is mathematics' next physics, only better. Biology will
        stimulate fundamentally new mathematics because living nature is qualitatively more
        heterogeneous than non-living nature. For example, it is estimated that there are
        2,000–5,000 species of rocks and minerals in the earth's crust, generated from the hundred
        or so naturally occurring elements (Shipman et al. 2003; chapter 21 estimates 2,000
        minerals in Earth's crust). By contrast, there are probably between 3 million and 100
        million biological species on Earth, generated from a small fraction of the naturally
        occurring elements. If species of rocks and minerals may validly be compared with species
        of living organisms, the living world has at least a thousand times the diversity of the
        non-living. This comparison omits the enormous evolutionary importance of individual
        variability within species. Coping with the hyper-diversity of life at every scale of
        spatial and temporal organization will require fundamental conceptual advances in
        mathematics.
      
      
        The Past
        The interactions between mathematics and biology at present follow from their
        interactions over the last half millennium. The discovery of the New World by Europeans
        approximately 500 years ago—and of its many biological species not described in religious
        Scriptures—gave impetus to major conceptual progress in biology.
        The outstanding milestone in the early history of biological quantitation was the work
        of William Harvey, 
        Exercitatio Anatomica De Motu Cordis et Sanguinis In Animalibus (An
        Anatomical Disquisition on the Motion of the Heart and Blood in Animals) (Harvey 1847),
        first published in 1628. Harvey's demonstration that the blood circulates was the pivotal
        founding event of the modern interaction between mathematics and biology. His elegant
        reasoning is worth understanding.
        From the time of the ancient Greek physician Galen (131–201 C.E.) until William Harvey
        studied medicine in Padua (1600–1602, while Galileo was active there), it was believed that
        there were two kinds of blood, arterial blood and venous blood. Both kinds of blood were
        believed to ebb and flow under the motive power of the liver, just as the tides of the
        earth ebbed and flowed under the motive power of the moon. Harvey became physician to the
        king of England. He used his position of privilege to dissect deer from the king's deer
        park as well as executed criminals. Harvey observed that the veins in the human arm have
        one-way valves that permit blood to flow from the periphery toward the heart but not in the
        reverse direction. Hence the theory that the blood ebbs and flows in both veins and
        arteries could not be correct.
        Harvey also observed that the heart was a contractile muscle with one-way valves between
        the chambers on each side. He measured the volume of the left ventricle of dead human
        hearts and found that it held about two ounces (about 60 ml), varying from 1.5 to three
        ounces in different individuals. He estimated that at least one-eighth and perhaps as much
        as one-quarter of the blood in the left ventricle was expelled with each stroke of the
        heart. He measured that the heart beat 60–100 times per minute. Therefore, the volume of
        blood expelled from the left ventricle per hour was about 60 ml × 1/8 × 60 beats/minute ×
        60 minutes/hour, or 27 liters/hour. However, the average human has only 5.5 liters of blood
        (a quantity that could be estimated by draining a cadaver). Therefore, the blood must be
        like a stage army that marches off one side of the stage, returns behind the scenes, and
        reenters from the other side of the stage, again and again. The large volume of blood
        pumped per hour could not possibly be accounted for by the then-prevalent theory that the
        blood originated from the consumption of food. Harvey inferred that there must be some
        small vessels that conveyed the blood from the outgoing arteries to the returning veins,
        but he was not able to see those small vessels. His theoretical prediction, based on his
        meticulous anatomical observations and his mathematical calculations, was spectacularly
        confirmed more than half a century later when Marcello Malpighi (1628–1694) saw the
        capillaries under a microscope. Harvey's discovery illustrates the enormous power of
        simple, off-the-shelf mathematics combined with careful observation and clear reasoning. It
        set a high standard for all later uses of mathematics in biology.
        Mathematics was crucial in the discovery of genes by Mendel (Orel 1984) and in the
        theory of evolution. Mathematics was and continues to be the principal means of integrating
        evolution and genetics since the classic work of R. A. Fisher, J. B. S. Haldane, and S.
        Wright in the first half of the 20th century (Provine 2001).
        Over the last 500 years, mathematics has made amazing progress in each of its three
        major fields: geometry and topology, algebra, and analysis. This progress has enriched all
        the biological sciences.
        In 1637, René Descartes linked the featureless plane of Greek geometry to the symbols
        and formulas of Arabic algebra by imposing a coordinate system (conventionally, a
        horizontal x-axis and a vertical y-axis) on the geometric plane and using numbers to
        measure distances between points. If every biologist who plotted data on x–y coordinates
        acknowledged the contribution of Descartes to biological understanding, the key role of
        mathematics in biology would be uncontested.
        Another highlight of the last five centuries of geometry was the invention of
        non-Euclidean geometries (1823–1830). Shocking at first, these geometries unshackled the
        possibilities of mathematical reasoning from the intuitive perception of space. These
        non-Euclidean geometries have made significant contributions to biology in facilitating,
        for example, mapping the brain onto a flat surface (Hurdal et al. 1999; Bowers and Hurdal
        2003).
        In algebra, efforts to find the roots of equations led to the discovery of the
        symmetries of roots of equations and thence to the invention of group theory, which finds
        routine application in the study of crystallographic groups by structural biologists today.
        Generalizations of single linear equations to families of simultaneous multi-variable
        linear equations stimulated the development of linear algebra and the European re-invention
        and naming of matrices in the mid-19th century. The use of a matrix of numbers to solve
        simultaneous systems of linear equations can be traced back in Chinese mathematics to the
        period from 300 B.C.E. to 200 C.E. (in a work by Chiu Chang Suan Shu called 
        Nine Chapters of the Mathematical Art ; Smoller 2001). In the 19th
        century, matrices were considered the epitome of useless mathematical abstraction. Then, in
        the 20th century, it was discovered, for example, that the numerical processes required for
        the cohort-component method of population projection can be conveniently summarized and
        executed using matrices (Keyfitz 1968). Today the use of matrices is routine in agencies
        responsible for making official population projections as well as in population-biological
        research on human and nonhuman populations (Caswell 2001).
        Finally, analysis, including the calculus of Newton and Leibniz and probability theory,
        is the line between ancient thought and modern thought. Without an understanding of the
        concepts of analysis, especially the concept of a limit, it is not possible to grasp much
        of modern science, technology, or economic theory. Those who understand the calculus,
        ordinary and partial differential equations, and probability theory have a way of seeing
        and understanding the world, including the biological world, that is unavailable to those
        who do not.
        Conceptual and scientific challenges from biology have enriched mathematics by leading
        to innovative thought about new kinds of mathematics. Table 1 lists examples of new and
        useful mathematics arising from problems in the life sciences broadly construed, including
        biology and some social sciences. Many of these developments blend smoothly into their
        antecedents and later elaborations. For example, game theory has a history before the work
        of John von Neumann (von Neumann 1959; von Neumann and Morgenstern 1953), and Karl
        Pearson's development of the correlation coefficient (Pearson and Lee 1903) rested on
        earlier work by Francis Galton (1889).
      
      
        The Present
        To see how the interactions of biology and mathematics may proceed in the future, it is
        helpful to map the present landscapes of biology and applied mathematics.
        The biological landscape may be mapped as a rectangular table with different rows for
        different questions and different columns for different biological domains. Biology asks
        six kinds of questions. How is it built? How does it work? What goes wrong? How is it
        fixed? How did it begin? What is it for? These are questions, respectively, about
        structures, mechanisms, pathologies, repairs, origins, and functions or purposes. The
        former teleological interpretation of purpose has been replaced by an evolutionary
        perspective. Biological domains, or levels of organization, include molecules, cells,
        tissues, organs, individuals, populations, communities, ecosystems or landscapes, and the
        biosphere. Many biological research problems can be classified as the combination of one or
        more questions directed to one or more domains.
        In addition, biological research questions have important dimensions of time and space.
        Timescales of importance to biology range from the extremely fast processes of
        photosynthesis to the billions of years of living evolution on Earth. Relevant spatial
        scales range from the molecular to the cosmic (cosmic rays may have played a role in
        evolution on Earth). The questions and the domains of biology behave differently on
        different temporal and spatial scales. The opportunities and the challenges that biology
        offers mathematics arise because the units at any given level of biological organization
        are heterogeneous, and the outcomes of their interactions (sometimes called “emergent
        phenomena” or “ensemble properties”) on any selected temporal and spatial scale may be
        substantially affected by the heterogeneity and interactions of biological components at
        lower and higher levels of biological organization and at smaller and larger temporal and
        spatial scales (Anderson 1972, 1995).
        The landscape of applied mathematics is better visualized as a tetrahedron (a pyramid
        with a triangular base) than as a matrix with temporal and spatial dimensions.
        (Mathematical imagery, such as a tetrahedron for applied mathematics and a matrix for
        biology, is useful even in trying to visualize the landscapes of biology and mathematics.)
        The four main points of the applied mathematical landscape are data structures, algorithms,
        theories and models (including all pure mathematics), and computers and software. Data
        structures are ways to organize data, such as the matrix used above to describe the
        biological landscape. Algorithms are procedures for manipulating symbols. Some algorithms
        are used to analyze data, others to analyze models. Theories and models, including the
        theories of pure mathematics, are used to analyze both data and ideas. Mathematics and
        mathematical theories provide a testing ground for ideas in which the strength of competing
        theories can be measured. Computers and software are an important, and frequently the most
        visible, vertex of the applied mathematical landscape. However, cheap, easy computing
        increases the importance of theoretical understanding of the results of computation.
        Theoretical understanding is required as a check on the great risk of error in software,
        and to bridge the enormous gap between computational results and insight or
        understanding.
        The landscape of research in mathematics and biology contains all combinations of one or
        more biological questions, domains, time scales, and spatial scales with one or more data
        structures, algorithms, theories or models, and means of computation (typically software
        and hardware). The following example from cancer biology illustrates such a combination:
        the question, “how does it work?” is approached in the domain of cells (specifically, human
        cancer cells) with algorithms for correlation and hierarchical clustering.
        
          Gene expression and drug activity in human cancer.
          Suppose a person has a cancer. Could information about the activities of the genes in
          the cells of the person's cancer guide the use of cancer-treatment drugs so that more
          effective drugs are used and less effective drugs are avoided? To suggest answers to this
          question, Scherf et al. (2000) ingeniously applied off-the-shelf mathematics,
          specifically, correlation—invented nearly a century earlier by Karl Pearson (Pearson and
          Lee 1903) in a study of human inheritance—and clustering algorithms, which apparently had
          multiple sources of invention, including psychometrics (Johnson 1967). They applied these
          simple tools to extract useful information from, and to combine for the first time,
          enormous databases on molecular pharmacology and gene expression
          (http://discover.nci.nih.gov/arraytools/). They used two kinds of information from the
          drug discovery program of the National Cancer Institute. The first kind of information
          described gene expression in 1,375 genes of each of 60 human cancer cell lines. A target
          matrix T had, as the numerical entry in row g and column c, the relative abundance of the
          mRNA transcript of gene g in cell line c. The drug activity matrix A summarized the
          pharmacology of 1,400 drugs acting on each of the same 60 human cancer cell lines,
          including 118 drugs with “known mechanism of action.” The number in row d and column c of
          the drug activity matrix A was the activity of drug d in suppressing the growth of cell
          line c, or, equivalently, the sensitivity of cell line c to drug d. The target matrix T
          for gene expression contained 82,500 numbers, while the drug activity matrix A had 84,000
          numbers.
          These two matrices have the same set of column headings but have different row labels.
          Given the two matrices, precisely five sets of possible correlations could be calculated,
          and Scherf et al. calculated all five. (1) The correlation between two different columns
          of the activity matrix A led to a clustering of cell lines according to their similarity
          of response to different drugs. (2) The correlation between two different columns of the
          target matrix T led to a clustering of the cell lines according to their similarity of
          gene expression. This clustering differed very substantially from the clustering of cell
          lines by drug sensitivity. (3) The correlation between different rows of the activity
          matrix A led to a clustering of drugs according to their activity patterns across all
          cell lines. (4) The correlation between different rows of the target matrix T led to a
          clustering of genes according to the pattern of mRNA expressed across the 60 cell lines.
          (5) Finally, the correlation between a row of the activity matrix A and a row of the
          target matrix T described the positive or negative covariation of drug activity with gene
          expression. A positive correlation meant that the higher the level of gene expression
          across the 60 cancer cell lines, the higher the effectiveness of the drug in suppressing
          the growth of those cell lines. The result of analyzing several hundred thousand
          experiments is summarized in a single picture called a clustered image map (Figure 1).
          This clustered image map plots gene expression–drug activity correlations as a function
          of clustered genes (horizontal axis) and clustered drugs (showing only the 118 drugs with
          “known function”) on the vertical axis (Weinstein et al. 1997).
          What use is this? If a person's cancer cells have high expression for a particular
          gene, and the correlation of that gene with drug activity is highly positive, then that
          gene may serve as a marker for tumor cells likely to be inhibited effectively by that
          drug. If the correlation with drug activity is negative, then the marker gene may
          indicate when use of that drug is contraindicated.
          While important scientific questions about this approach remain open, its usefulness
          in generating hypotheses to be tested by further experiments is obvious. It is a very
          insightful way of organizing and extracting meaning from many individual observations.
          Without the microscope of mathematical methods and computational power, the insight given
          by the clustered image map could not be achieved.
        
      
      
        The Future
        To realize the possibilities of effective synergy between biology and mathematics will
        require both avoiding potential problems and seizing potential opportunities.
        
          Potential problems.
          The productive interaction of biology and mathematics will face problems that concern
          education, intellectual property, and national security.
          Educating the next generation of scientists will require early emphasis on
          quantitative skills in primary and secondary schools and more opportunities for training
          in both biology and mathematics at undergraduate, graduate, and postdoctoral levels (CUBE
          2003).
          Intellectual property rights may both stimulate and obstruct the potential synergy of
          biology and mathematics. Science is a potlatch culture. The bigger one's gift to the
          common pool of knowledge and techniques, the higher one's status, just as in the potlatch
          culture of the Native Americans of the northwest coast of North America. In the case of
          research in mathematics and biology, intellectual property rights to algorithms and
          databases need to balance the concerns of inventors, developers, and future researchers
          (Rai and Eisenberg 2003).
          A third area of potential problems as well as opportunities is national security.
          Scientists and national defenders can collaborate by supporting and doing open research
          on the optimal design of monitoring networks and mitigation strategies for all kinds of
          biological attacks (Wein et al. 2003). But openness of scientific methods or biological
          reagents in microbiology may pose security risks in the hands of terrorists. Problems of
          conserving privacy may arise when disparate databases are connected, such as physician
          payment databases with disease diagnosis databases, or health databases with law
          enforcement databases.
        
        
          Opportunities.
          Mathematical models can circumvent ethical dilemmas. For example, in a study of the
          household transmission of Chagas disease in northwest Argentina, Cohen and Gürtler (2001)
          wanted to know—since dogs are a reservoir of infection—what would happen if dogs were
          removed from bedroom areas, without spraying households with insecticides against the
          insect that transmits infection. Because neither the householders nor the state public
          health apparatus can afford to spray the households in some areas, the realistic
          experiment would be to ask householders to remove the dogs without spraying. But a
          researcher who goes to a household and observes an insect infestation is morally obliged
          to spray and eliminate the infestation. In a detailed mathematical model, it was easy to
          set a variable representing the number of dogs in the bedroom areas to zero. All
          components of the model were based on measurements made in real villages. The calculation
          showed that banishing dogs from bedroom areas would substantially reduce the intensity of
          infection in the absence of spraying, though spraying would contribute to additional
          reductions in the intensity of infection. The model was used to do an experiment
          conceptually that could not be done ethically in a real village. The conceptual
          experiment suggested the value of educating villagers about the important health benefits
          of removing dogs from the bedroom areas.
          The future of a scientific field is probably less predictable than the future in
          general. Doubtless, though, there will be exciting opportunities for the collaboration of
          mathematics and biology. Mathematics can help biologists grasp problems that are
          otherwise too big (the biosphere) or too small (molecular structure); too slow
          (macroevolution) or too fast (photosynthesis); too remote in time (early extinctions) or
          too remote in space (life at extremes on the earth and in space); too complex (the human
          brain) or too dangerous or unethical (epidemiology of infectious agents). Box 1
          summarizes five biological and five mathematical challenges where interactions between
          biology and mathematics may prove particularly fruitful.
        
      
    
  

  
    
      
        
        Life is complicated. It comes in all sorts of shapes, sizes, places, and combinations,
        and has evolved a dizzying variety of solutions to the problem of carrying on living. Yet
        look inside a cell and life takes on, if not simplicity, then at least a certain
        uniformity—a genetic system based around nucleic acids, for example, and a common set of
        chemical reactions for turning food into fuel. And looked at in broad swathes, life shows
        striking generalities and patterns. Every mammal's heart will beat about one billion times
        in its lifetime. Both within and between species, the density of a population declines in a
        regular way as the size of individuals increases. And the number of species in all
        environments declines as you move from the equator towards the poles.
        Wouldn't it be good if there were a simple theory that used life's shared fundamentals
        to explain its large-scale regularities, via its diversity of individuals? In the past few
        years, a team of ecologists and physicists have come up with just such a theory. At its
        heart is metabolism: the way life uses energy is, they claim, a unifying principle for
        ecology in the same way that genetics underpins evolutionary biology. They believe that
        energy use, in the form of metabolic rate, can be understood from the first principles of
        physics, and that metabolic rate can explain growth, development, population dynamics,
        molecular evolution, the flux of chemicals through the environment, and patterns of species
        diversity—to name a few.
        The work, its originators insist, is not a theory of everything for biology, or even
        ecology. But it can often seem that way. “We're making advances on a broad range of
        questions almost on a weekly basis,” says James Gillooly, of the University of New Mexico,
        Albuquerque. “We've been having an awful lot of fun.”
      
      
        Beneath the Surface
        Metabolic ecology, as it has become known, is still controversial. Some think its
        mathematical foundations are unsound, and that it explains nonexistent trends. It also
        divides researchers on philosophical lines—those that see life's patterns as fundamental
        versus those who think that variation is the key, those who think that simple, general
        ideas can help us understand nature versus those who think that complicated problems
        require complicated answers. A lot is riding on the debate: “If the theory is right, it's
        one of the most significant in biology for a long time,” says ecologist David Robinson of
        the University of Aberdeen. “It would provide a common functional basis for all
        biodiversity.”
        Scientists have known for nearly two centuries that larger animals have relatively
        slower metabolisms than small ones. A mouse must eat about half its body weight every day
        not to starve; a human gets by on only 2%. The first theories to explain this trend,
        developed in the late nineteenth century by the German nutritionist Max Rubner and the
        French physiologist Charles Richet, were based on the ratio between an animal's surface
        area, which changes with the square of its length, and its volume, which is proportional to
        its length cubed. So large animals have proportionately less surface area, lose heat more
        slowly, and, pound for pound, need less food. The square-versus-cube relationship makes the
        area of a solid proportional to the two-third power of its mass, so metabolic rate should
        also be proportional to mass
        2/3 . For many years, most biologists thought that it was.
        But in 1932, Max Kleiber, an animal physiologist working at the University of
        California's agricultural station in Davis, re-examined the question, and found that, for
        mammals and birds, metabolic rate was mass
        0.73 —closer to three quarters than two thirds. Kleiber looked at
        animals ranging in size from a rat to a steer. By the mid-1930s, other workers had put
        together a “mouse to elephant” curve that supported the three-quarter-power law, and by the
        1960s, the plot had been extended for everything from microbes to whales, still seeming to
        show the same relationship. Quarter-power scaling also began to stretch beyond metabolic
        rate. Biological times, such as lifespan and heart rate, were found to be proportional to
        mass
        1/4 , and fractions related to one-quarter show up in other scaling
        relationships: the diameter of the aorta and tree trunks is proportional to mass
        3/8 , for example.
        It was, however, much harder to find a theoretical reason for why metabolic rate should
        be proportional to mass
        3/4 —and more generally, why quarter-power scaling laws should be so
        prevalent in biology. The impasse meant that by the mid-1980s interest in scaling had
        waned. But it sparked back into life in 1997, when two ecologists—James Brown of the
        University of New Mexico, Albuquerque, and his graduate student Brian Enquist, now at the
        University of Arizona, Tucson—and a physicist, Geoffrey West of the Santa Fe Institute,
        developed a new explanation of why metabolic rate should equal the three-quarter power of
        body mass.
        West, Brown, and Enquist's theory is based on the structure of biological distribution
        networks, such as blood vessels in vertebrates and xylem in plants. The trio assumed that
        metabolic rate equals the rate at which these networks deliver resources, and that
        evolution has minimized the time and energy needed to get materials from where they are
        taken up—the lungs or roots, for example—to the cells. They also assumed that, although
        organisms vary greatly in size, the terminal units in their distribution networks, such as
        blood capillaries or leaf stalks, do not.
        Bigger plants and animals take longer to transport materials, and so use them more
        slowly. In West, Brown, and Enquist's model, the maximally efficient network that serves
        every part of a body has a fractal structure, showing the same geometry at different
        scales. And the number of uniform terminal units in such a network—and so the rate at which
        resources are delivered to the cells—is proportional to the three-quarter power of body
        mass.
      
      
        Pattern versus Variation
        Whether metabolic rate really varies with the three-quarter power of body mass is still
        debated—some researchers still favor two-thirds, others think that no one exponent fits all
        the data—but a majority of biologists favor three-quarters. And whether the fractal theory
        really explains the relationship of metabolic rate to body size is also still contentious.
        In the most wide-ranging critique so far, published this April, two Polish researchers, Jan
        Kozlowski, of Jagiellonian University, Krakow, and Marek Konarzewski of the University of
        Bialystok, claimed that the theory's maths could not simultaneously contain both uniform
        terminal units and three-quarter-power scaling, that large animals built along such lines
        would have more blood than their bodies could contain, that biological scaling laws were
        not built around quarter powers, and that biological networks were not generally branching
        fractals.
        “I don't believe there's anything to explain—there's no universal scaling exponent,”
        says Kozlowski. He is also struck by what is left unexplained when size is accounted for:
        animals of the same size can still show more than an order of magnitude variation in
        metabolic rate. “What's striking in nature is the variability,” he says. “There are
        regularities that call for explanation, but that doesn't mean ignoring the variability is
        correct.” Kozlowski is the co-author of a theory that relates metabolic rate to cell size
        and the amount of DNA an organism has, one of several alternative explanations of the
        scaling of metabolic rate published since West, Brown, and Enquist's model.
        The criticisms are serious, says Robinson. “The jury is out—questions about the
        fundamental maths are worrying a few people.” On the other hand, he says, West, Brown, and
        Enquist's model seems a plausible template for designing an organism, and its predictions
        fit real-world data remarkably well. Whether this fit truly captures the physical and
        chemical mechanisms underlying the patterns remains to be seen; Robinson hopes that
        criticism can strengthen West, Brown, and Enquist's model, perhaps leading to a new,
        improved theory.
        The metabolic theory's authors are not budging. “We've yet to see a criticism we feel we
        can't answer pretty readily,” says Brown. Kozlowski and Konarzewski's arguments are based
        on a misreading of the work, he says, and criticisms that focus on one aspect, such as the
        structure of mammalian vascular systems, miss the key point, which is generality: “If we're
        wrong on quarter powers, why do they keep showing up in everything from life-history
        processes to evolutionary rates?”
      
      
        From Sharks to Tomatoes
        After accounting for size, Brown's group turned its attention to the second most
        important influence on metabolism: temperature. The effect is exponential, and a 5 °C rise
        in body temperature equals a roughly 150% rise in metabolic rate. The team built an
        equation for metabolic rate that combined the mass
        3/4 term with the Boltzmann factor. The latter is an expression of the
        probability that two molecules bumping into each other will spark a chemical reaction. The
        higher the temperature, the greater the probability, and the faster the reaction.
        Adding temperature explained much of the variation in metabolic rate that remained after
        adjusting for size. It also explained some of the metabolic differences between groups. For
        example, a reptile has a slower metabolic rate than a mammal of the same size. But
        adjusting for its lower body temperature removes much of the difference, suggesting that
        the two groups share fundamental metabolic processes. The same even goes for plants and
        animals. “When you correct for size and temperature, the metabolic rates of a shark, a
        tomato plant and a tree are remarkably similar,” (Figure 1) says Gillooly, who joined
        Brown's group as a grad student to work on the temperature question. It's not yet clear
        what the activation energy represents, says Gillooly. It could be a kind of average for all
        the hundreds of chemical reactions in metabolism, or maybe the energy needed to get over
        one crucial hump in the path.
        The metabolic theory's third component, resources, is also something of a black box at
        this stage. Nutrient supply, the team reasons, is the next most important determinant of
        metabolic rate, and will account for some of the remaining unexplained variation. As with
        temperature, the overall effect could be a balance of many processes, or it could be due to
        one limiting element—the growth of lake phytoplankton is often limited by phosphorus, for
        example, while for marine phytoplankton iron is usually the crucial nutrient. “It's a work
        in progress,” says Brown. “But our vision for a metabolic theory of life is ultimately
        going to include material resource limitation.”
        These three things still do not account for all the variation in metabolic rate, but
        more detailed knowledge of species can yield more precise predictions. Using body size,
        altitude, and diet, Brian McNab, of the University of Florida, Gainesville, has explained
        99.0% of the variation in metabolic rate for birds of paradise (Figure 2), and 99.4% of the
        rate variation in leaf-nosed bats. Nevertheless, when McNab sees attempts to explain
        variation in metabolism using a few parameters applied across a wide range of sizes and
        taxonomic groups, what isn't explained strikes him as forcefully as what is.
        “I have serious reservations as to whether there is a single relationship for body size
        and metabolic rate,” he says. “I think we will be able to find generalizations in ecology,
        but they're not going to be simple—there will be a bunch of clauses and restrictions, and
        animals have a lot of ways to bend the rules.”
        No theory matches data exactly, Brown points out; having a baseline prediction for
        metabolism lets you identify exceptional cases worthy of further investigation. Viewed from
        this angle, the metabolic theory is a kind of null hypothesis of how organisms work. “Until
        you have a theory that makes a prediction, you don't know how to interpret any of the
        variation,” says Brown. And, he adds, despite this variation, the underlying trends are
        also meaningful. “There are themes of life that are deep-seated and fundamental.”
        All the business of life needs energy. So if you know the rate at which an organism
        burns fuel—or if you know how big and hot it is, and apply the metabolic theory—you can
        make a suite of predictions about its biology, such as how fast it will grow and reproduce,
        and how long it will live.
        By correcting for mass and temperature, Brown, Gillooly, and their colleagues believe
        they have revealed underlying similarities in all the rates of life. The hatching times for
        egg-laying animals, including birds, fish, amphibians, insects, and plankton, turn out to
        follow the same relationship—if a fish egg were the same size and temperature as a bird
        egg, it would take equally long to hatch (Figure 3). The same goes for growth: a tree and a
        mammal of equal size and temperature would gain mass at the same speed. And size and
        temperature even explain much of the variation in mortality rates between species—which one
        might have thought to be strongly dependent on external factors such as predators—perhaps
        through metabolism's influence on aging processes, such as free-radical damage to the
        genome.
      
      
        One Rule for All?
        If all organisms work in the same way, understanding individual biology offers an
        obvious route to explaining nature's patterns—ecological processes become a kind of
        meta-metabolism. Indeed, the team has used their theory to predict the flux of carbon
        dioxide through forests—a measure more usually used to determine individual metabolic rate.
        They have also found that body size and temperature predict the densities and growth rates
        of populations. So hotter environments should support lower population densities, as each
        individual consumes resources more quickly, leaving less to go round. One thing that does
        not scale with a quarter power of body size is the area of animals' home ranges; this
        increases more or less linearly with body size. But in October, Brown, along with
        researchers from Princeton University and the Institute of Zoology in London, published a
        model that brought this, too, into the metabolic theory. They borrowed another trick from
        physics, using an equation that describes colliding gas molecules to model the interactions
        between neighboring animals.
        Temperature could also explain why biodiversity peaks at the equator, the team believes.
        Organisms with faster metabolisms have faster mutation rates. So the genomes of smaller,
        hotter animals change more quickly, and they will also get through their generations more
        rapidly. One would therefore expect to see more new species created in small organisms and
        warm environments. The large-scale trend in all these rates—hatching time, individual and
        population growth, ecosystem metabolism, DNA substitution—is closely proportional to a
        quarter power of body mass.
        In the future, Brown's group plans to examine the dynamics of colonial organisms and
        societies through the lens of metabolic ecology; instead of capillaries, the terminal units
        of the networks would become ants, or people. There are also many applied problems within
        the theory's scope, including some of the most significant human impacts on the biosphere.
        Carbon emissions and the consequent global warming are increasing both the temperature and
        nutrient supply. And exploited populations, such as fisheries often show a decrease in
        individual size, as larger animals are preferentially killed. Both these would tend to
        speed up biological processes. Another team of United States and Italian researchers has
        found that the same model that describes the growth of individuals can also predict the
        growth of tumors, hinting that metabolic ecology may have medical applications.
        Brown hopes that metabolic ecology will one day become an uncontroversial part of
        researchers' toolkits, like the theories population geneticists use to predict changes in
        the frequencies of genes. Before that happens, both the theory's proponents and its
        opponents have years of work ahead of them. Adopting the theory may also require a shift in
        ecologists' worldview. Most ecologists work by carrying out experimental manipulations on
        small groups of similar organisms: the warblers in a woodland, for example, or the grasses
        of a meadow. When they build models, they do so from empirical data, not from physical
        first principles. The philosophy behind metabolic ecology disconcerts many researchers,
        says Robinson. “A lot of traditional biologists are uncomfortable with thinking about data
        in these terms.”
        Kozlowski doubts that simple theories can make precise predictions about the behavior of
        biological systems on large scales. He believes that metabolic ecology risks leading the
        discipline up a blind alley: “If I'm right, and the basic model contains an error,
        correcting the results will be a very long process. If they're not right, they'll have done
        a disservice to ecology.”
        But many ecologists are more optimistic that some unifying principles of nature can be
        found, and that metabolic ecology, and the debate around it, is a step in the right
        direction. Some think the theory may be part of an even grander idea. Stephen Hubbell, of
        the University of Georgia, is one of the architects of another idea causing a stir among
        ecologists. Called neutral ecology, it proposes a general explanation of how competition
        between individuals produces the dynamics of birth, death, and migration seen in
        ecosystems, and its predictions match closely the abundance and diversity of species in the
        wild. He believes that metabolic and neutral ecology can become elements of some larger
        theoretical framework.
        “I've never been more excited in my life,” says Hubbell. “Ecology now is like quantum
        mechanics in the 1930s—we're on the cusp of some major rearrangements and syntheses. I'm
        having a lot of fun.”
      
    
  

  
    
      
        
        Evolutionary developmental biology is motivated by the premise that the differences we
        see between species are caused by changes that have occurred in the genes that regulate
        their developmental programs. Beginning in the 1980s, general principles began to emerge
        about the evolution of development in animals. The identification of the 
        Hox genes in 
        Drosophila melanogaster and the subsequent discovery of their
        conservation and similar expression in different Metazoans led to the revolutionary
        realization that many of the mechanisms critical to basic animal development have been
        conserved across more than 500 million years of evolution. Many other developmental
        pathways, such as those specifying the heart and the central nervous system, have since
        been elucidated and promptly subjected to successful comparative analysis.
        These celebrated discoveries illustrate ways that very different organisms are, at a
        fundamental level, similar to one another. But not all developmental processes are so
        conservative; an outstanding example is sex determination. The majority of animal species
        produce two sexes, and current phylogenies (e.g., [1]) suggest that sexual dimorphism was
        likely a feature of the last common ancestor of the coelomate bilaterians, a vast clade of
        animals that excludes only sponges, ctenophores, cnidarians, and acoel flatworms. However,
        though critical for development and reproduction, the mechanisms that specify sex
        determination are among the least-conserved known. Marked variation exists in both the
        primary sex determination signal and in the downstream genetic pathways that interpret the
        signal. We are thus presented with our first conundrum: sexual differentiation appears to
        be an ancient, and potentially homologous, feature of animal biology, yet its genetic
        specification suggests multiple origins.
      
      
        Bewildering Variety
        The variety of primary sex determination cues was appreciated long before the advent of
        molecular genetics [2]. The two broadest categories are genetic sex determination (GSD), in
        which the sex of offspring is set by a sex chromosome or an autosomal gene, and
        environmental sex determination (ESD), in which sex is determined by temperature (as with
        turtles), local sex ratio (as with some tropical fish), or population density (as with
        mermithid nematodes). Though little is known about the molecular mechanisms of ESD, within
        the GSD systems many different mechanisms have been uncovered. Dual sex chromosome systems,
        in which either the female (ZW/ZZ) or the male (XX/XY) is heterogametic, are common, as are
        systems set by the ratio of the number of X chromosomes to sets of autosomes (X:A). There
        are also systems in which heterozygosity at a single locus is required for female
        development (known as complementary sex determination; [3]), as well as systems involving
        sex determination via multiple genes with additive effects.
        Molecular genetic investigations of GSD in model systems such as 
        Drosophila , 
        Caenorhabditis , and mice have revealed a clear lack of
        conservation, underscoring the diversity. For example, although the primary sex
        determination signal in both 
        D. melanogaster and 
        C. elegans is the X:A ratio, the fruit fly pathway consists of a
        cell-autonomous cascade of regulated mRNA splicing, while that of the nematode follows a 
        Hedgehog -like intercellular signaling pathway [4]. GSD in
        mammals depends (with some interesting exceptions—see [5]) upon a Y-specific dominant gene
        (
        Sry ) encoding a transcription factor. In the face of such
        impressive differences, perhaps we should question our assumption of homology: could it be
        that sex determination in different taxa has arisen independently over and over again in
        evolution? Until 1998, this seemed like a good bet.
        The discovery of the homology of the key sex-determining genes 
        doublesex in 
        Drosophila and 
        mab-3 in 
        C. elegans provided the first evidence for a common evolutionary
        basis of sex determination in animals [6]. Soon, related 
        doublesex-mab-3 (DM)-family genes with roles in male sexual development
        were discovered in vertebrates and even cnidarians [7,8]. Here at last was a smoking gun
        that could link the diverse metazoan sex determination systems (Figure 1). But as
        satisfying as the result was, it immediately gave birth to another mystery: if the enormous
        diversity of sex determination systems are all derived from a common ancestor, how could
        they possibly have been modified so radically? After all, sexual differentiation and
        reproduction are hardly unimportant developmental processes!
      
      
        Focusing on Close Relatives
        To understand how such diversity came to be, we need to look at the differences between
        closely related species. This approach allows the discovery and interpretation of
        small-scale sex determination changes before they are obscured by subsequent changes. The
        processes discovered in this way might then be reasonably extrapolated to explain the
        seemingly unrelated systems of more deeply diverged taxa. Work in dipterans [9] and
        nematodes [10] has revealed three evolutionary phenomena that characterize shorter-term sex
        determination evolution.
        The first of these is the often astounding rate of molecular evolution at the level of
        nucleotide and aminoacid sequences. Although some sex-determining genes are well conserved,
        many show unprecedented substitution rates [11]. An extreme example is the central
        integrator of the X:A ratio in 
        Caenorhabditis , 
        xol-1 . The 
        xol-1 orthologues of the closely related nematodes 
        C. elegans and 
        C. briggsae are a mere 22% identical [12], even though genes
        surrounding 
        xol-1 are much better conserved (Figure 2A). Remarkably, the 3′ neighbor
        of 
        xol-1 , the immunoglobulin 
        dim-1 , is only 5 kb away and is essentially identical between
        species.
        A second phenomenon, best exemplified by dipteran insects, is the modification of
        genetic control pathways through the gain or loss of key pathway components (Figure 2B). In
        
        Drosophila , the first gene to respond to the X:A ratio is 
        Sxl , whose transcription is regulated by both autosomal and X-linked
        factors very early in development [4,13]. When X: A = 1 (i.e., in female embryos), 
        Sxl transcription occurs and produces Sxl protein. Later in development,
        transcription from a second promoter occurs in both sexes, but these transcripts cannot be
        productively spliced without the earlier burst of 
        Sxl expression. As a result, only females sustain 
        Sxl expression, and in turn only females can productively splice the mRNA
        of 
        tra , its downstream target. Productive splicing of 
        tra is required to produce the female-specific form of 
        dsx , a founding member of the DM family mentioned above.
        In a series of groundbreaking papers, Saccone and colleagues investigated the pathway in
        the more distantly related heterogametic Mediterranean fruit fly 
        Ceratitis capitata . The first surprise was that although a
        highly conserved 
        Sxl homologue exists in 
        Ceratitis , it does not undergo sex-specific regulation similar
        to that of 
        Drosophila , which suggests that it does not play a key switch
        role (Saccone et al. 1998). Similar results have also been found for the housefly, 
        Musca domestica [14], indicating that the role of 
        Sxl in sex determination may be restricted to 
        Drosophila and its closest relatives. In contrast, 
        tra and 
        dsx are key sex regulators in all dipterans examined thus far.
        A further surprise came when the 
        Ceratitis tra homologue was characterized [15]. In the case of
        this gene, clear evidence for sex-specific regulation was found, and as with 
        Drosophila , only females productively splice 
        tra mRNA. However, this splicing difference can be explained nicely by a
        positive feedback, similar to that seen in 
        Drosophila Sxl , in which Tra protein regulates its own
        splicing. In 2002, Pane et al. proposed that the dominant, male-specifying M factor on the
        Y chromosome inhibits this autoregulation [15]. As a result, males cannot make functional
        Tra protein, and the male form of Dsx is produced. These experiments show not only how a
        pathway can evolve, but also, importantly, how X:A and heterogametic GSD systems can be
        interconverted by modifying the cue that regulates a conserved molecular switch gene (the
        splicing of 
        tra mRNA). A detailed scenario for how this might occur has recently been
        proposed [16].
        Finally, recent studies of 
        Caenorhabditis nematodes have shed light on the genetic basis of
        the convergent evolution of sex determination related to mating system adaptations. An
        important factor in this area are new phylogenies of the genus [17,18], which consistently
        suggest the surprising possibility that the closely related hermaphroditic species 
        C. elegans and 
        C. briggsae acquired self-fertilization independently, from
        distinct gonochoristic (male/female) ancestors (Figure 2C). Although this scenario is
        somewhat uncertain purely on parsimony grounds, recent work on the genetic control of the
        germline bisexuality that defines hermaphroditism has tipped the balance toward parallel
        evolution.
        Working with 
        C. elegans , Clifford et al. [19] cloned 
        fog-2 , a gene required for spermatogenesis in hermaphrodites but not in
        males. Upon doing so, it became clear that 
        fog-2 is part of a large family of F-box genes and was produced by
        several recent rounds of gene duplication. The 
        C. briggsae genome sequence suggested that while 
        C. briggsae possesses a similarly large family of F-box
        proteins, the duplication event giving rise to 
        fog-2 was specific to the 
        C. elegans lineage. In this issue of 
        PLoS Biology , Nayak et al. [20] extend this work by rigorously
        demonstrating that 
        fog-2 is indeed absent in 
        C. briggsae . The authors also identify a short, C-terminal
        domain that makes FOG-2 uniquely able to perform its germline sex-determining function.
        This domain is probably derived from a frame-shifting mutation in an ancestral gene.
        Working with 
        C. briggsae , Stothard et al. [21], Haag et al. [22], and Hill
        et al. (unpublished data) have also found evidence of important species-specific regulation
        of germline sex determination. RNA interference and gene knockout approaches have shown
        that while 
        C. elegans requires the male-promoting genes 
        fem-2 and 
        fem-3 to produce sperm in hermaphrodites, 
        C. briggsae requires neither. Given that both genes have
        conserved roles in male somatic sex determination, this suggests that 
        C. briggsae evolved hermaphroditism in a way that bypasses these
        genes.
        The long-standing mystery of sex determination and its diversity began by comparisons
        between distantly related species. Recent work on closer relatives has uncovered processes
        that through a reasonable extrapolation enable the connection of these disparate dots into
        a fascinating picture of developmental evolution. Though the divergence is extreme, it is
        likely that a better understanding of the evolution of sex determination genes and pathways
        holds lessons about the evolution of development in general. The next major challenge will
        be to integrate the comparative developmental data with the ecological and population
        processes that are driving the evolution of sex determination. Only then will we be able to
        say that the picture is complete.
      
    
  

  
    
      
        
        Armed with billions of cells, elaborate circuitry, and a seemingly animate anatomy,
        capable of growing as it learns, the brain is a marvelously enigmatic organ. Much to the
        chagrin of those that study it, the brain remains perhaps too mysterious.
        Although genetic information exploded out of the Human Genome Project, it has been of
        little consequence to neuroscience—a discipline still grappling with the boundaries and
        names for distinct brain regions. According to United States National Institute of Mental
        Health Director Thomas Insel, over 99% of the neuroscience literature focuses on only 1% of
        the estimated 15,000–16,000 genes expressed in the brain. David Van Essen, a neurobiologist
        at Washington University in St. Louis, Missouri, likens the current genetic map of the
        brain to a 17th century map of Earth. A voyage around the Earth had already proven it was
        round, but landmass resolution was still vague at best.
        Magellan's benefactors, though, never bankrolled a technical advance quite like the
        Allen Brain Atlas. Neuroscience's unlikely sugar daddy, Microsoft cofounder and the world's
        fifth wealthiest man, Paul Allen, created the $100 million dollar Allen Brain Institute in
        Seattle, Washington, two years ago. The first explicit goal of the institute was to create
        an open-access, visual, searchable online map of genes expressed in the brain, as well as
        of brain circuitry and cell location. Roughly one petabyte of data—equal to the memory
        necessary to hold the information held in about 50 Libraries of Congress—will be produced
        as a result.
        In mid-December, the first 2,000 genes were uploaded. By 2006, the Allen team plans to
        have as many as 24,000 genes online. While the ultimate goal is to map the human brain, the
        atlas ushers in a new era of neurogenetics—an attempt to make connections between
        anatomical, genetic, and behavioral observations.
      
      
        Of Mice and Men
        The initial effort will focus on the standard, inbred lab mouse strain known
        affectionately as C57BL6. Like it or not, mice are remarkably similar to humans—sharing 99%
        of our genes. Humans, at this time, provide too many hurdles—not the least of which is a
        lack of willing brain donors that are the same age. Since the C57BL6 strain is inbred, the
        mice are also much more uniform than humans—a key to constructing the most accurate
        representative map possible of one species' adult brain.
        With a map of mouse genes in hand, scientists will be able to develop informed
        hypotheses about genes that may affect human brain function and dysfunction. “People will
        be able to look first in the mouse atlas, then more selectively focus on human cases,” says
        Gregor Eichele, director of the Max Planck Institute for Experimental Endocrinology in
        Hannover, Germany. Indeed, researchers may do well to focus their efforts on those specific
        cells in which homologous genes are expressed in the mouse. Insel suggests an initial
        mental health application: find 
        dyspyndin , a gene linked to schizophrenia. Insel is banking on the atlas
        to locate genes linked to conditions including bipolar disorder, schizophrenia, and autism.
        Once identified, such critical genes can be examined in detail and used in studies aimed at
        disease cures or drug screens. Presumably, the atlas will be a boon for drug discovery and
        development by providing information on drug targets present within brain cells.
        Eichele points out that the Allen Brain Atlas will eliminate time wasted testing
        fruitless hypotheses. “If you think gene X may be involved in [hippocampal-dependent]
        memory, but it's not expressed in the hippocampus, you shouldn't bother following that line
        of research,” he says. Insel agrees that where genes are expressed in the brain will be
        most telling. “In the brain, more than any other organ, function follows form,” he
        says.
        Cellular resolution of expression patterns will prove necessary to uncover as yet
        unknown relationships between circuitry, cell type, and gene expression in the brain, says
        Arthur Toga, a neuroscientist at the University of California, Los Angeles, and Allen Brain
        Atlas advisor. Ed Lein, a neuroscientist at the Allen Brain Institute, thinks that mapping
        at the cellular scale will also redefine anatomy. Traditionally, neuroanatomists have
        delineated brain regions pretty much by eye, identifying clusters of cells and patterns of
        connections that look the same. “We're starting to redefine boundaries of regions by cell
        type,” says Lein, defining cell type by gene expression pattern.
        The difficulty, according to Allen Brain Institute scientific advisor David Anderson, a
        neuroscientist at the California Institute of Technology in Pasadena, California, is in
        understanding how a gene mutation affects behavior. “It is impossible without a knowledge
        of the circuits in which a gene is expressed,” he says. Anderson himself studies innate
        behaviors such as fear responses. Specifically, he's most interested in understanding the
        function of different regions of the almond-shaped amygdala. Once he finds genes expressed
        in neurons within regions of the amgydala implicated in fear, he plans to determine neuron
        function by creating transgenic animals in which specific neuron activity has been
        silenced.
        Thus far, Anderson's laboratory has endured the slog of using microarray techniques to
        identify genes with expression patterns linked to fear behavior, followed by in situ
        hybridization, which traditionally involves twenty-odd complex, error-prone steps, to find
        just some of the genes expressed in different parts of the amygdala. “The Allen Brain Atlas
        will identify a whole set of genes that we would have had to spend several years to find,”
        he says. Indeed, locating where even one gene of interest is expressed in the brain eats up
        valuable research time, especially when there are so many potentially interesting genes. In
        most cells, 10,000 genes can be expressed.
        Eichele improved the slow error-prone process into an automated, fast, “high throughput”
        method that caught the attention of the Allen team because it was capable of meeting the
        needs of such an ambitious project. In situ hybridization uses labeled probes for specific
        messenger RNA sequences, allowing scientists to test individual brain tissue samples for
        gene expression.
        Automated in situ hybridization data will be generated for the entire mouse
        transcriptome—the full complement of activated genes in a particular tissue at a particular
        time—on a genome-wide scale. The mouse is a young adult at 56 days old, free from the
        confounding factors of development. After it is sacrificed, the mouse brain is immediately
        frozen, then sliced very thinly—to get forty sections from each millimeter of thickness—so
        that the probes for hybridization can expose gene expression in individual cells.
      
      
        Mining the Mind for Riches
        The in situ data will be matched in a three-dimensional framework to the reference atlas
        developed by Allen's team. The resulting images will be turned into a virtual microscope,
        allowing users to focus down on genes expressed in regions of interest.
        While the Allen Brain Atlas is somewhat like other genomics projects in scale, it is
        unique. “No one's gone into a 3-D structure like a tissue and examined it in a systematic
        way,” says Allan Jones, senior director of Allen Brain Atlas Operations. In that way, he
        adds, it's a much richer dataset than the Human Genome Project. “As we're ramping up—fully
        by spring of next year—we'll be generating about 1,000 microscope slides a day with four
        mouse brain sections on each slide,” says Jones. Each day, those sections are scanned and
        stitched together electronically into 300-megabyte batches. “Scaling up a lab process is
        currently the biggest challenge,” says Jones. “In effect, we're turning an art form into
        something that gives high-quality data day in and day out,” says Jones. “If you are off
        slightly when cutting a 2-D brain slice, it becomes very difficult to map back into a 3-D
        context.”
        While it is a logical starting point, a spatial understanding of gene expression is just
        one way to mine the brain. Other avenues currently being pursued explore individual
        variability, development, and comparisons between species (See Box 1). The sheer
        comprehensive nature of the Allen Brain Atlas will be its crowing achievement, and its
        complement to the other ongoing atlas efforts. However, the magnitude of the project also
        poses its greatest hurdle—about which onlookers have expressed some concern.
        “It's clear that they want to do a first-rate job of working with all this information,
        but I'm not sure they entirely appreciate how hard it's going to be to manage the
        staggering amount of information they're getting,” says Van Essen. As manager of his own
        comparative anatomy database, he understands the magnitude of the task. He points out that
        properly digitizing the data in electronic form and registering one particular slice to a
        standardized reference with meaningful coordinates is not trivial.
        “In an atlas, there will be considerable variability from one individual to the
        next—even in inbred mouse strains,” says Van Essen. Even if it's only 20% variability, it
        is still going to pose challenges for managing experimental data. Van Essen acknowledges
        that this isn't just an impediment for Allen's team, but for neuroscience as a whole.
        “These tools, in general, don't emerge from a vacuum. They emerge best when rich,
        challenging datasets are staring people in the face,” he says.
        An even larger problem is communicating the data effectively. Figuring out how to
        navigate the tremendous morass of data will be a bioinformatic stumbling block. Lein
        acknowledges that figuring out how to best annotate the data is one of the bigger
        challenges for the future—particularly in cases where gene expression doesn't match the
        agreed-upon boundaries of anatomical regions.
        For now, users will be able to mine the data by gene name only. The initial release
        consists of an image viewer to view the in situ hybridization data for one or several genes
        at a time, along with a reference atlas to determine the structures in which genes are
        expressed. Future releases will allow the user to conduct more sophisticated searches, such
        as by anatomical structures.
        “Not only is there this 3-D structure, but there are lots of studies where people are
        trying to understand what drives the turning on and off of genes,” says Jones. “At the end,
        if the atlas has a big impact, it will be in providing the precise coordinates for those
        people to tease apart what specific DNA elements drive expression within regions or
        structures.”
        And while the Allen Brain Atlas will provide a fine level of detail, there are
        limitations. Much like the Human Genome Project, the information will be the starting point
        and not the end point of understanding brain function. “It won't change strategy for doing
        experiments,” says Nobel laureate and Columbia University neuroscientist Eric Kandel. “The
        atlas will be a catalyst rather than a direction setter.”
      
      
        The Final Frontier
        Overall, neuroscience is entering a new era. Insel notes that recent work has proven the
        brain to be extraordinarily dynamic, birthing neurons throughout a lifespan. Brain
        functions seem more modular than global. And there is no real separation between the mind
        and brain. “Mental disorders are brain disorders,” he says.
        Over the next 5–10 years, neurogenomics will fuel a golden age of discovery in
        neuroscience. In fact, scientists may even reach an overarching goal—understanding how the
        wild card of environment impacts brain function. “We want predictive genetics able to
        accommodate environmental differences,” says Rob Williams, neurobiologist at the University
        of Tennessee in Memphis.
        For Kandel, one thing is certain. “Most of the mysteries of the brain lie ahead of
        us.”
      
    
  

  
    
      
        
        “I didn't want to be just another MBA,” says Pascal Herzer, one of the first recipients
        of a new graduate credential known as the professional science master's, or PSM. “Not many
        people have the ability to understand science and business, and [the PSM] program was
        designed for that very purpose.”
        PSMs are two-year American master's degrees financed in large part by the Alfred P.
        Sloan Foundation to cultivate science managers. Sloan's ultimate goal is to make science
        careers more attractive to talented young people like Herzer, a 2003 PSM graduate in
        Applied Biosciences from the University of Arizona, who believes his PSM makes him more
        marketable to science-based businesses. “I am at the true junction of science and
        business,” he says.
      
      
        The Missing Degree
        Fortunately for Herzer, the business of science is booming. Jobs for scientists and
        engineers grew four times faster than the United States national average since 1980, and
        should outpace the market until at least 2010. Surprisingly to many academics, most of
        these jobs are in industry. In 1999, the last year with complete data, two out of three
        employed science and engineering (S&E) graduates worked in industry, including the
        great majority of bachelor's and master's degree holders, and 40% of doctorates. In other
        words, industry, not academe, now drives American S&E employment, and will for the near
        future.
        Like academia, industry needs scientifically literate personnel; unlike academia,
        industry wants employees with business savvy as well. However, in the past, graduate
        students received either science or business instruction, not both. “Industry simply hired
        regular master's-degreed people, or MBAs, or more likely PhDs, and just expected them to
        learn their weaknesses on the job,” says Eleanor L. Babco, Executive Director of the
        Commission on Professionals in Science and Technology, a nonprofit corporation with funding
        from the Sloan Foundation to assess PSM graduates.
        For science-based businesses, then, the American S&E doctorate—viewed by many as the
        worldwide gold standard for science education—is too specialized for their needs (see Box
        1). But a master's degree may be just right.
      
      
        Bridging the Gap
        During the 20th century, the master's degree evolved as a professional credential in
        many fields, including business, education, and social work, and more recently, pharmacy,
        physical therapy, and accounting. In the 1990s, non-incidental master's in the sciences—in
        other words, intentionally terminal degrees, not consolation prizes for failing out of
        graduate school—crept into engineering and applied mathematics, too, as companies grew more
        reliant on computational analysis and hired accordingly. From 1981 to 2000, for example,
        the number of earned master's degrees in mathematics and computer science more than
        doubled.
        With hopes of spurring a “significant movement,” in 1997 the Sloan Foundation bet big on
        professional master's degrees, eventually spending $11 million on almost 100 programs
        across the US. Sloan Foundation–backed PSM programs now operate at 45 universities in 20
        states, in such fields as microbial biotechnology and applied genomics; similar programs
        have also developed independently of the Sloan Foundation, such as the Master of Science in
        Bioinformatics at Johns Hopkins. And while most PSM-style programs are currently in the US,
        this may soon change: the 1999 “Bologna Agreement” requires all European Union universities
        to adopt uniform undergraduate and graduate degrees “relevant to the European labour
        market”; so master's-level industry-centric degrees are sure to follow. At Leiden
        University in the Netherlands, for example, students can now add a “science-based business”
        focus to any research master of science (MSc) program.
        Like all graduate programs, PSMs offer advanced coursework in a (science or math)
        specialization, usually in an emerging or hybrid field such as bioinformatics. Most PSMs
        also provide business courses—including finance, project management, regulatory affairs,
        and intellectual property law—and information technology classes as well. PSMs are
        “industry relevant” by design, with external advisory committees populated by local
        business leaders, weekly colloquia led by corporate representatives, special arrangements
        for employed students, and industry internships or final projects exploring realistic
        business scenarios (see Box 2).
        A key principle underlying the PSM model is interdisciplinarity. PSM students are
        encouraged to reach out to other departments and broaden their expertise in multiple areas,
        to better understand the collaborative culture of industry-style scientific enterprise. To
        promote such connections, PSM programs explicitly teach teamwork and effective scientific
        communication, with authentic case studies analyzed alongside MBA students, classroom
        presentations and public seminars, and open defenses of final projects. Consequently, PSM
        graduates, unlike many doctoral graduates, are trained to possess a wide array of
        interactive skills, including sizing up an audience for their ability to comprehend the
        presented material and adapting appropriately.
        In a science-based business, ideas must flow freely between scientists and
        non-scientists in and out of the company—between researchers and marketers, say, or
        inventors and patent lawyers—to capitalize on discoveries and comply with regulations. When
        non-scientists misunderstand the science underpinning a business model, profits suffer. But
        the presence of a central employee who streams data between differently educated members of
        the network may boost the bottom line. PSM students are specifically trained to act as such
        “science translators.” “[My PSM] allows me to serve as an efficient mediator between
        corporate entities, university personnel, and scientists,” says Herzer.
        For this reason, small companies and start-ups, which cannot afford specialists for
        every position, may particularly benefit from PSM-credentialed employees, able to connect
        different people and function in multiple roles; indeed, many PSM graduates have job
        descriptions expressly created for them. “We need generalists rather than specialists,”
        says James L. Ratcliff, Chairman and CEO of Rowpar Pharmaceuticals, a dental products
        company in Scottsdale, Arizona. For small companies like his, Ratcliff says, “PSM graduates
        have an appropriate combination of project management expertise, an understanding of
        business environments and priorities, and advanced knowledge in the physical and life
        sciences.”
        Although it is too early for comprehensive assessment, employment outcomes for PSM
        graduates have been examined, and this result is clear: they are getting industry jobs.
        According to The Conference Board, an independent business management organization funded
        by the Sloan Foundation to survey PSM alumni, by 2002, 91% of the first PSM graduates had
        obtained full-time positions within their field despite a white-collar recession,
        two-thirds with salaries of $50,000 or more. A separate analysis by the Commission on
        Professionals in Science and Technology found that 61.5% of employed respondents were hired
        by businesses. Employment opportunities range from marketing to bioinformatics (see Box 3).
        “Companies need people that can work in companies,” says Lindy A. Brigham, coordinator of
        the Applied Biosciences PSM program at the University of Arizona.
      
      
        Not a Perfect Cure
        Although most scientific careers demand a graduate degree, a professional master's in
        many hard sciences still encounters entrenched academic opposition. According to Lee-Jen
        Wei, then acting chair of the Department of Biostatistics in the Harvard School of Public
        Health, quoted in the 
        Wall Street Journal , “Harvard tries to create leadership in industry,
        academics and government, and our philosophy is we don't think that with a master's degree
        people can fill that role very easily.”
        The government appears to agree with this view. While most doctoral candidates receive
        federal funds for tuition and other expenses, there is little money for master's students,
        who disproportionately end up in industry regardless of specialization. PSM students are
        especially affected by this problem because “interdisciplinary” equals “expensive.”
        Similarly, “interdisciplinary” can also mean “hard to find”—companies with targeted
        recruitment often miss PSM students, who are not “in” any particular department—and
        “confusing”—differences in these new, still somewhat vaguely defined programs can make
        hiring comparisons difficult. But perhaps the most conspicuous drawback to PSMs is their
        newness, and resulting obscurity: almost half of graduates say they are “not sure”
        employers will value their PSM, or the unique skill set it affords.
      
      
        But Will They Succeed?
        Still, many observers of higher education support the PSM concept. Judith Glazer-Raymo,
        author of the forthcoming book 
        Professionalizing Graduate Education: The Master's Degree in the
        Marketplace , argues that converging market forces will lead to the success of the
        professional master's degree in science. These forces include: rapid technological change;
        the rise of alternative learning channels such as online and distance education, corporate
        universities, and hi-tech certification programs; the proliferation of degrees in general,
        and in multidisciplinary fields specifically; and a fundamental societal shift away from
        public service and toward entrepreneurship, profitability, and competition.
        Kenneth R. Smith, former dean of the Eller College of Business and Public Administration
        at the University of Arizona, and others make the case that PSMs may protect students'
        careers from outsourcing to foreign countries. The American S&E labor pool is
        shrinking, and industry has already responded by transferring much of its research and
        development overseas; however, companies are mostly moving lab scientists, not strategic
        analysts. Cross-training in both science and business could thus provide an edge for
        domestic workers in the near-term employment environment; in fact, PSM programs have a
        higher proportion of US citizens and residents than S&E doctoral programs.
        Further, in its 2003 report, the National Science Board urged the government to better
        align S&E graduate education with “expected national skill needs,” including
        “interdisciplinary skills.” The report also recommended federal funding for a wider range
        of educational options and more attention on the real economic concerns of students—code
        words for support of professional master's degree initiatives. In the same vein, top
        universities now advocate “interconnections” between their professional schools and
        traditional departments, as a way of strengthening the overall academic mission, and many
        countries are sponsoring initiatives to stimulate university–industry links, to maximize
        marketing of technological innovations.
        For advocates, then, the PSM both advances the cause of science education reform and
        addresses changing employment conditions with one big idea: reinvention of the two-year
        graduate credential for an entrepreneurial age. Herzer, for one, now a technology
        development representative at the Scripps Research Institute, has staked his future on the
        potential of professional master's degrees. “Scientists rarely understand business
        dealings, and business personnel rarely comprehend scientific discoveries,” he says. “The
        overlay of the two is crucial for any successful business transaction of scientific
        origins.”
      
    
  

  
    
      
        
        As a species, we pride ourselves on the uniqueness of our brain. Relative to our body
        size, the human brain is bigger than that of any other animal. It may also contain unique
        structures and patterns of organisation that presumably underlie our intelligence and
        ability to manipulate our environment. But how did our unique brain originate, and under
        what selective pressures did it evolve? Some of the answers may lie in the genetic
        differences that researchers are now uncovering between us and our closest relatives.
      
      
        What Is So Different about the Human Brain?
        When we compare our brain to those of other animals, the first thing that strikes us is
        its size. Human brains weigh on average 1,300 grams; a squirrel brain weighs six grams.
        Some of this difference is because, as larger animals, we need more brain to run our
        bodies. However, the brains of our nearest relatives, the great apes, weigh only 300–500
        grams, even though their body size is similar to ours (Figure 1). “Humans sit on the top of
        the pile when it comes to relative brain size”, notes geneticist Bruce Lahn (University of
        Chicago, Illinois, United States) (see Box 1).
        Throughout mammalian and primate evolution, there has been a gradual increase in brain
        size, superimposed with “spikes” of fast growth such as the tripling in human brain size
        that occurred about 1.5 million years ago, 4 million years after the human lineage diverged
        from that of the great apes. “Even in the ape lineage, the brain has been expanding but
        along the human lineage it has really taken off”, says Lahn.
        In addition, over time, different parts of our brain have increased in size at different
        rates. The cerebral cortex has expanded more than other areas, and within the cortex, some
        areas have expanded differentially while others have lagged behind.
        
          “Humans sit on the top of the pile when it comes to relative brain size.”
        
        Paleoanthropologist Ralph Holloway (Columbia University, New York, United States) uses
        endocasts to look for macroscopic differences in the brains of our human ancestors. “We
        fill human fossil skulls with vulcanised rubber and once it has set, we pull it out of the
        large hole at the base of the skull and the rubber snaps back into the shape of the skull”,
        Holloway explains. Endocasts are particularly useful for comparing brain sizes, but they
        also provide information on when the asymmetries that are present in our brain first
        appeared. These often reflect cerebral specialisation, and Holloway believes that some of
        the asymmetries he sees in human fossil skulls may indicate when our ancestors acquired
        language.
        More details about how the shape of our brain differs from that of our closest living
        relatives are emerging from the work of neuroscientist Karl Zilles (Institute of Medicine,
        Research Center Jülich, Germany). He prepares magnetic resonance images of monkey, ape, and
        human brains and then uses a nonlinear elastic algorithm to transform one brain into
        another (Figure 2). “We know what forces we have to apply to the images to do this”, he
        explains, “which tells us which areas of the brain have changed most during primate
        evolution”. Zilles and his colleagues also are currently using molecular imaging techniques
        to update the existing maps of the different areas within our brains. Until we have this
        information, it is hard to make meaningful comparisons between our brain and that of
        chimpanzees. Already, Zilles has discovered that there is much more interindividual
        variation in human brain organisation than anyone suspected. This means, says Zilles, “that
        a general statement like ‘the neocortex is bigger in human brains than in ape brains’
        actually tells us very little. It gives us the general direction that evolution has taken
        but not whether an ape brain is different because of its sensory, motor, or association
        areas.”
        Scientists are also using other techniques to investigate more subtle changes in the
        organisation of the human brain compared to the brains of other mammals and primates.
        Indeed, says Holloway, the reorganisation of the brain during evolution has been at least
        important as its increase in size. Neurobiologist John Allman (California Institute of
        Technology, Pasadena, California, United States) and his collaborators, for instance, have
        discovered that a special type of large spindle-shaped neuron, first described in the early
        20th century by Constantin von Economo, is unique to apes and humans and much more numerous
        in the latter. These neurons are found in brain areas that are implicated in decision
        making in uncertain situations so, Allman speculates, they may help humans to interact
        rapidly in complex social situations.
      
      
        Costs and Benefits
        A bigger, more complex brain may have advantages over a small brain in terms of
        computing power, but brain expansion has costs. For one thing, a big brain is a metabolic
        drain on our bodies. Indeed, some people argue that, because the brain is one of the most
        metabolically expensive tissues in our body, our brains could only have expanded in
        response to an improved diet. Another cost that goes along with a big brain is the need to
        reorganise its wiring. “As brain size increases, several problems are created”, explains
        systems neurobiologist Jon Kaas (Vanderbilt University, Nashville, Tennessee, United
        States). “The most serious is the increased time it takes to get information from one place
        to another.” One solution is to make the axons of the neurons bigger but this increases
        brain size again and the problem escalates. Another solution is to do things locally: only
        connect those parts of the brain that have to be connected, and avoid the need for
        communication between hemispheres by making different sides of the brain do different
        things. A big brain can also be made more efficient by organising it into more
        subdivisions, “rather like splitting a company into departments”, says Kaas. Overall, he
        concludes, because a bigger brain 
        per se would not work, brain reorganisation and size increase probably
        occurred in parallel during human brain evolution. The end result is that the human brain
        is not just a scaled-up version of a mammal brain or even of an ape brain.
        For natural selection to work, the costs of brain evolution must be outweighed by the
        advantages gained in terms of fitness. For many years, explains ecological psychologist
        Robin Dunbar (University of Liverpool, United Kingdom), “people thought that the ability to
        hunt or forage better was what drove the evolution of our brains. But a better diet had to
        come before we could grow a bigger brain.” Dunbar believes instead that brain evolution in
        primates and more generally in mammals “has been driven by the need to manage social
        relationships, and in primates, in particular, to coordinate coherence in social groups
        through time and space”. More complex social interactions, he says, mean that individuals
        are better able to pool resources to solve problems like finding food, and so they survive
        better.
        This theory, says Dunbar, is supported by a correlation between social group size and
        neocortex size across primates and modern humans. Furthermore, during primate brain
        evolution, the trend has been to add more material to the front than the back of the brain.
        The front of the brain is where information from the rest of the brain is interpreted, and
        the capacity to interpret information underlies social interactions, says Dunbar. The
        number of problem-solving cognitive tasks you can do may well depend on how much frontal
        lobe volume you have and how it is organised. Just think of how few moves you can run a
        chess game into the future with a 1980s personal computer compared to a 21st century
        mainframe machine, he suggests.
        
          The human brain is not just a scaled-up version of a mammal brain or even of an ape
          brain.
        
      
      
        The Genetics of Human Brain Evolution
        Selective pressures like those considered by Dunbar and, before him, by scientists like
        Holloway work on the raw material of random gene mutations, and molecular biologists now
        have some clues to the gene changes that may underlie brain evolution. Take brain size, for
        example (Figure 3). Research groups, including those led by Lahn, neurologist Christopher
        Walsh (Harvard Medical School, Boston, Massachusetts, United States), and clinical
        geneticist Geoffrey Woods (University of Leeds, United Kingdom), wondered whether the genes
        that cause microcephaly, an inherited human disorder in which brain size is greatly
        reduced, might include genes involved in human brain evolution. In 2002, mutations in the
        genes 
        ASPM (abnormal spindle-like microcephaly associated) and 
        microcephalin were identified as two causes of microcephaly. Three groups
        have since reported that both these genes have been under selective pressure during primate
        evolution. 
        ASPM encodes a protein involved in spindle formation, so it is tempting
        to think that changes in its sequence might result in an increased rate of cell division
        and hence brain size. But, cautions Walsh, “we really have no idea yet how or even if 
        ASPM is involved in brain evolution”.
        Both Lahn and Walsh believe that 
        ASPM and 
        microcephalin may be only the tip of the iceberg when it comes to genes
        that have helped to shape our brains. For example, Walsh has recently reported that
        deletion of a gene called 
        Nde1 produces mice with very small brains. “Our experiments indicate that
        the loss of 
        Nde1 causes neurons to mature prematurely. This stops them dividing so
        the mice end up with small brains”, explains Walsh, who is now investigating whether human 
        NDE1 variants have been positively selected during human evolution.
        Lahn is also searching for additional candidate genes that might help to explain how our
        brains evolved. In a recent 
        Cell paper, Lahn and his colleagues identify several hundred genes that
        are involved in nervous system biology and show that, as a group, there are significantly
        higher rates of protein evolution in these genes in primates than in rodents. Protein
        evolution rates are particularly high in the lineage leading from ancestral primates to
        humans, notes Lahn, “so some of these genes may regulate brain size and behaviour”.
        However, he warns, as with 
        ASPM and 
        microcephalin , “definitive proof for this will only come from functional
        studies, which are difficult to do”.
      
      
        Enter Glutamate Dehydrogenase
        For one gene, evidence for an effect on brain function may be emerging. Geneticist
        Henrik Kaessmann (University of Lausanne, Switzerland) studies the origin of new genes in
        primates, in particular genes that arise when a DNA copy of an mRNA transcribed from an
        existing gene is integrated back into the genome. Usually this new “retrocopy” is not
        expressed, but if the DNA inserts near an active promoter, it can become a transcribable
        “retrogene”. This is the origin of 
        GLUD2 , a retrogene derived from 
        GLUD1 , which encodes glutamate dehydrogenase. 
        GLUD2 , which first appeared 18–23 million years ago in hominoids,
        probably immediately picked up a brain-specific promoter and then over the next few million
        years acquired two critical amino acid changes, explains Kaessmann. These allow 
        GLUD2 -encoded glutamate dehydrogenase to work better in the brain than
        the 
        GLUD1 -encoded enzyme. Because glutamate dehydrogenase recycles the
        neurotransmitter glutamate, the presence of 
        GLUD2 may permit a higher neurotransmitter turnover and greater neuronal
        activity in hominoid brains than is possible in monkey brains, which lack 
        GLUD2 , suggests Kaessmann.
      
      
        Gene Expression
        Kaessmann plans to search his extensive database of retrocopies in the human genome for
        other functional genes that could, like 
        GLUD2 , be implicated in brain evolution. By contrast, evolutionary
        neurobiologist Todd Preuss (Yerkes Primate Research Center, Emory University, Atlanta,
        Georgia, United States) hopes to identify genes involved in human brain evolution by
        comparing gene expression patterns in different primates. Preuss, who began training as a
        paleoanthropologist before turning to neuroscience, has been comparing post-mortem human
        and chimpanzee brains since the mid 1990s, believing that “if we want to understand human
        brain evolution, we really have to compare humans with chimpanzees, our nearest relatives”,
        even though chimp brains have been evolving separately from ours for 5–7 million years.
        But, warns Preuss, “we have to do these studies now. There are few chimps left and if we
        lose the opportunity to study them and their brains, we will lose forever a fundamental
        source of insight into our own species.”
        To begin with, Preuss used staining techniques that exploit antibodies to examine the
        neural components of chimpanzee and human brains. Then in 1998, he was asked to collaborate
        in a microarray project. “My antibody approach was very labour intensive so I jumped at the
        opportunity to screen 10,000 genes at once”, he says.
        Preuss and his collaborators now know that more than 100 genes are differentially
        expressed in chimpanzee and human brains. “Importantly, when we go back into tissue with
        probes for these gene products, in some cases there are remarkably different spatial
        patterns of expression in humans, chimps, and macaques”, notes Preuss. “We don't know yet
        what these differences mean in terms of functional organisation in these different brains
        but our results open up new and exciting vistas”, particularly since many of the
        differentially expressed genes have not previously been considered as being potentially
        involved in brain evolution. The microarray data produced by Preuss and other researchers
        also indicate that many of the gene expression changes that have occurred during brain
        evolution involve gene upregulation. For example, there is increased expression of genes
        involved in metabolism, synaptic organisation, and synaptic function. “All told, it seems
        that the human brain may be more dynamic than ape or monkey brains”, says Preuss. “The
        human brain seems to be running hot in all sorts of ways.”
      
      
        Scratching at the Surface
        As far as understanding how our brains evolved, more questions remain than have been
        answered. One problem is that we don't really know enough about how our brains differ from
        those of other mammals and primates, although work by Zilles and others is helping here. We
        also know very little about how the areas of our brain are physically linked up, and we
        need to understand that before we can see how we differ from our nearest relatives. And as
        far as identifying the gene changes that were selected during evolution, although we have
        several candidates, we don't know how or if these gene variants affect our cognitive
        abilities. It is one thing, concludes Dunbar, to identify genetic or anatomic differences
        between human and ape brains, but quite another to know what they mean in terms of actual
        cognitive processes.
      
    
  

  
    
      
        
        How do we relate human thought processes to measurable events in the brain? Mental
        chronometry, which has origins that date back more than a century, seeks to measure the
        time course of mental operations in the human nervous system [1]. From the late 1800s until
        1950, the field was built almost entirely around a single method: measuring and comparing
        people's reaction times during simple cognitive tasks. As far back as 1868, Franciscus
        Donders [2] subtracted the time taken to make a single response to an unvarying
        stimulus—what he called an instructed reflex—from the time it took to make the same
        response to one of two events, obtaining the time required to discriminate between the two
        stimuli. Further, he subtracted the time to discriminate two stimuli from a situation in
        which there were also two possible responses in order to obtain the time required for
        choice.
        In the 1950s, studies of reaction time were combined with the then-developing
        mathematical theory of information [3] to address issues such as the maximum transmission
        rate of the human nervous system [4] and how coding in the brain of stimuli and their
        responses could influence these limits [5]. These studies revealed that reaction time alone
        was not sufficient to elucidate the exact processes by which the brain achieved the human
        ability to process information. However, when combined with other methods, the latency of
        responding can help connect brain studies to the behavior of humans in real situations.
        Recording the average event-related electrical potentials from scalp electrodes became a
        research tool in the 1960s, with the advent of analog and then digital computers to
        accomplish the recording and averaging. It became clear that components of the
        event-related potential could be systematically related to sensory and motor stages of
        information processing. For instance, a visual stimulus was found to evoke a short-latency
        scalp response from the primary visual cortex at about 60 milliseconds, followed by
        positive and negative voltage changes in neighboring visual areas. Similarly, scalp
        recorded potentials from the frontal cortex could be recorded in relation to motor
        activity. It was now possible to observe some of the sensory and motor stages that were
        inferred from Donders's subtractive method (see [6] for a review).
        Saul Sternberg [7] developed a much-improved method for dividing reaction time into
        successive or serial stages, called the additive factors method. Subjects were asked to
        determine whether or not a probe digit had been present in a just previously presented
        series of digits. Sternberg argued that the time to complete the task could be divided into
        a sensory stage, dependent on stimulus parameters such as the intensity or clarity of the
        probe; a comparison stage, dependent only on the number of items in memory; and a response
        stage that reflected the difficulty of the specified response. Factors that influenced one
        stage (e.g., stimulus intensity) would be additive with those that influenced another stage
        (e.g., motor output). With this simple framework, it was now possible to determine at which
        stage(s) a new factor (e.g., nicotine, sleep deprivation, or Parkinson's disease) had its
        influence.
        In the 1950s, the advent of microelectrode recordings of single neurons from
        anesthetized monkeys allowed for an even finer resolution of neurophysiological processes
        and seemed to provide support for the view that the brain does indeed process information
        in serial stages. Hubel and Wiesel [8] argued that successive levels of the visual system
        could be seen as accomplishing successive analyses of input. The microelectrode strategy
        was quickly adopted to alert animals, making it apparent that higher level brain areas
        involved in operations upon input might feedback their influences on earlier processing
        stages [9,10]. These control systems, often called attention, posed something of a problem
        for completely serial views of information processing. However, they also provided evidence
        of localized brain areas within the parietal lobe of the monkey that could be
        systematically related to processing operations involved in attention—which were then being
        investigated by mental chronometry in patients with parietal and other lesions [11].
        In the late 1980s, neuroimaging experiments made possible the examination of activity in
        localized brain areas, first through the use of injected radionuclides detected by positron
        emission tomography (PET) [12] and later through the use of an externally imposed magnetic
        field in functional magnetic resonance imaging (fMRI) [13]. Over the last ten years, fMRI
        has improved in spatial and temporal resolution and can now provide evidence of quite
        specific brain areas, in the millimeter range, that are involved in cognitive tasks. Most
        studies have shown a small number of widely distributed brain areas that must be
        orchestrated to carry out a cognitive task. Although, as in all sciences, there are
        disagreements, the convergence of results in areas of attention and language seem to me
        particularly impressive.
        When the fMRI method for localization is brought together with methods that can
        accurately measure timing of the same activity (i.e., electrical or magnetic event-related
        potentials) they can provide considerable insight into the nature of thought. Consider the
        simple task of deciding whether a presented digit is above or below five [14]. Dehaene
        argued that the task could be divided into four stages. The first involves obtaining the
        identity of the probe input (encoding), the second making a comparison against the stored
        representation of the digit five, the third selecting a response, and lastly, checking the
        output for error. According to additive factor theory, a variable that effects overall
        reaction time by varying the time to complete one stage will be additive with the effects
        of variables that affect other stages. The input or encoding stage was varied by using
        either Arabic or spelled digits (e.g., “3” or “three”). The comparison stage was varied by
        comparing digits close to five (e.g., six) with those far from five (e.g., nine). The
        response stage was varied by specifying a response from either the dominant or non-dominant
        hand and error monitoring was examined by comparing error with correct trials. Each of
        these variables influenced only the appropriate stage and was additive in its effect with
        each of the other variables (see Figure 1).
        Moreover, scalp-recorded, event-related potentials showed a separate scalp distribution
        and latency for each variable [14]. Subsequent fMRI data has confirmed and increased the
        precision of the anatomy proposed for each of these stages.
        Of course, not all human activities involve a set of exhaustive and independent serial
        stages that can be shown to add up to the overall reaction time. While tasks like the
        number comparison discussed above can be usefully divided into stages, some components may
        deal with simultaneous operations and may be limited only by a total capacity of central
        mechanisms. We know that many situations involve parallel processing and feedback loops at
        many levels. Sternberg has attempted to apply a modified version of additive factor theory
        to brain systems using neuroimaging that allows for some of these possibilities
        [15,16].
        Laboratory studies often use the simultaneous execution of two different tasks (dual
        tasks) to simulate the more realistic situations where humans time-share activities. In
        this issue of 
        PLoS Biology , Sigman and Dehaene [17] provide a model that further
        extends the additive factor logic to the dual task situation. They propose that for tasks
        that can be broken down into three consecutive stages—perception, decision based on noisy
        integration of evidence, and response—the perceptual and motor stages can operate both
        simultaneously with and independently of stages of another task and are thus easily
        amenable to additive factor analysis. The decision stage, however, appears to represent a
        kind of “cognitive bottleneck” for which the reaction times of the two tasks become
        interdependent. The model adds considerably to the range of situations to which an additive
        factor approach can be applied, allowing investigators to seek more information about how
        new variables influence hidden processing stages.
        Many cognitive and emotional tasks studied with neuroimaging have implicated a small
        number of brain areas that are consistently active. Mental chronometry plays a role in
        suggesting the cognitive operations that each of these areas performs and how they are
        organized in real time. The toolkit of new techniques provides the basis for further tests
        to evaluate whether a chronometric model reveals a crucial set of connected computations
        (circuit) for carrying out the task. For example, using a magnetic pulse delivered outside
        the skull, it is now possible to induce a reversible lesion at the time of a particular
        computation to determine whether the specific computation assigned to a given area is truly
        needed to carry out the task. Studies using diffusion tensor imaging can examine whether
        there are large-scale connections between neural areas posited by a particular model. In
        describing the links between brain and behavior, mental chronometry is still a cornerstone
        that binds psychology to the techniques of neuroscience.
      
    
  

  
    
      
        
        The study of genetic material from ancient specimens was, in its early years, dominated
        by a race to sequence DNA from extinct species like the dodo and the woolly mammoth. Now
        that the supply of these crowd-pleasing curiosities has run dry, scientists are starting to
        ask new questions of ancient DNA (aDNA) that are revealing how the genetic make-up of
        prehistoric populations changed through time. These findings look set to trounce
        assumptions about how evolution really unfolded. However, there is still concern that many
        studies are not paying enough attention to the exacting protocols needed to overcome the
        technical challenges of the discipline and to defend it from the ridicule that has plagued
        it in the past.
        In 1994, while 
        Jurassic Park was still taking in millions of dollars at the box office,
        scientists claimed to have extracted and sequenced DNA from an 80-million-year-old dinosaur
        [1]. When sceptical researchers took a look at the sequence, it turned out to be of human
        rather than dinosaur origin. “To make that mistake, you'd have to try really, really hard,”
        says Alan Cooper, head of the Henry Wellcome Ancient Biomolecules Centre at Oxford
        University in the United Kingdom. If you think you've sequenced some dinosaur DNA, the
        first thing you'd do is run a phylogenetic analysis on it, he says. “Had they done that
        properly, with any mammal at all involved in the tree,…they would have found that their
        sequence was grouping with the mammals and not with the reptiles or the birds,” says
        Cooper. Perhaps they'd watched Michael Crichton's inventive fiction one too many times, he
        suggests.
      
      
        Setting the Standards
        It was this kind of bungling study that highlighted the need for an exacting protocol
        that would steer researchers around the significant pitfalls posed by DNA decay and
        contamination. A list of “authenticity criteria” emerged during the 1990s, aimed at
        preventing similarly bogus claims from entering the literature [2]. This list includes
        stringent laboratory controls; cloning of products amplified by polymerase chain reaction
        (PCR); replication of results from a second, independent extract; and, for really new or
        unexpected results, replication of results by an independent research group.
        Such requirements have allowed work on aDNA to move on and mature. Now, it's possible to
        focus on the really interesting questions that aDNA can answer. “What we're able to do with
        ancient DNA is really look at evolution,” says Cooper. The fossil record can only hint at
        how evolution unfolded. “It just shows you there's a bear and then there's not a bear,” he
        says. “It doesn't show you where it came from or what the relationship between the groups
        is.” By contrast, aDNA can do just that, giving researchers a window onto the population
        genetics of the past and revealing how evolution really played out. And the signs are that
        descriptions of the evolutionary process based on the fossil record and modernday gene
        pools are far too simple. “The modern data is clearly misleading us,” says Cooper.
        “Evolution is much, much more complex and dynamic than we would hope.”
      
      
        Thinking Big
        Because of the decay that occurs with time, there is a limit to how far back aDNA can
        gaze (Box 1). “Your ideal preservation conditions are something that falls under ice,
        freezes instantly, and stays frozen until you get it,” says Cooper. “As soon as we get up
        to 2 million [years ago] we can't get anything to work, and that's even under deep-frozen
        conditions.” But within the past 60,000 years, there are several major evolutionary events
        that are worth studying—including a glacial maximum around 18,000 years ago, the invasion
        of the New World by humans about 12,000 years ago, and a global mass extinction about
        11,000 years ago. These relatively recent events should be a good model for working out how
        similar events affected genetic diversity throughout evolutionary history.
        Cooper's latest work has analysed DNA from over 400 bison fossils from Beringia—the
        frozen wastes between eastern Siberia and the Canadian Northwest Territories [3]. “What
        we've done is carbon-date a shitload of bison and get DNA out of them.” It's the largest
        aDNA study to date, he says (Figure 1). The icy conditions mean that good quality
        mitochondrial DNA could be extracted from most of the specimens. The bison could also be
        dated accurately. This allowed Cooper and his colleagues to trace the changes in the bison
        genetic diversity from 150,000 years ago to the present. It was even possible to predict
        the effective population size throughout this period of bison evolution. “Our analyses
        depict a large diverse population living throughout Beringia until around 37,000 years
        before the present, when the population's genetic diversity began to decline dramatically,”
        they note.
        This finding challenges some common assumptions. It has been argued that modern bison
        are descended from Beringian bison, but Cooper's data suggest otherwise. “All modern bison
        belong to a clade distinct from Beringian bison,” he and his colleagues report.
        Furthermore, the dramatic decline in the numbers of bison occurs long before humans arrive
        on the scene, scuppering the idea that hunting pressure was primarily responsible for the
        demise of the bison. As the glacial maximum approached 18,000 years ago, the cooler, dryer
        conditions were probably responsible for the downturn in the bison population, argues
        Cooper. “Climate change is giving the animals an absolute whacking,” he concludes.
        A similar analysis of brown bear DNA excavated from permafrost and cave deposits in the
        Arctic is also challenging conventional evolutionary wisdom [4]. Being able to get both a
        radiocarbon date and some DNA from a specimen pins a particular genetic sequence to a
        particular moment in time. These data suggest that genetically and geographically distinct
        groups of bear have replaced each other relatively often during the last 60,000 years.
        Regional extinctions and replacements seem to be tied to climate change and competition
        with the much larger short-faced bears, the authors argue (Figure 2).
        Recent analysis of aDNA from Haast's eagle has also thrown up a surprising result. This
        New Zealand giant had a wingspan of up to three metres and a weight of around 14 kilograms,
        says Michael Bunce, an anthropologist at MacMaster University in Ontario, Canada. Analysis
        of aDNA from 2,000-year-old specimens indicates that this extinct creature is closely
        related to the little eagle from Australia and New Guinea, which typically weighs less than
        one kilogram. The common ancestor of these two eagles lived as recently as 1 million years
        ago, he and his colleagues estimate [5]. “It means an eagle arrived in New Zealand and
        increased in weight by 10–15 times over this period,” says Bunce. “Such rapid size change
        is unprecedented in terrestrial vertebrates.”
        In addition to illuminating these natural events, the study of aDNA can also show
        changes in the frequency of key genes that occurred during the domestication of crops and
        animals. For example, aDNA from samples of early maize reveals when certain desirable
        traits appeared [6]. “It's the first study of ancient DNA that looks at phenotype,” says
        Svante Pääbo, an evolutionary anthropologist at the Max Planck Institute in Leipzig,
        Germany. “One can actually look at specific genes that early humans selected during
        domestication of an important crop.” Pääbo's analysis suggests that the alleles typical of
        contemporary maize were already present in Mexican maize 4,400 years ago, so just a couple
        of thousand years after its initial domestication from the wild grass teosinte (Figure 3).
        “Quite early on, properties were selected that were not only the structure of the plant but
        also the biochemistry,” he says.
        aDNA is also being used to decipher human origins. Mitochondrial DNA from Neanderthals
        looks quite different from the mitochondrial DNA of early modern humans [7]. This lends
        support to the hypothesis that modern humans have a “single African origin” rather than the
        alternative hypothesis of “multiregional evolution”, where the ancestors of modern humans
        bred with Neanderthals. aDNA could also, in principle, be used to shed light on the
        evolutionary position of the 18,000-year-old “hobbit” recently unearthed on the Indonesian
        island of Flores [8]. Both Cooper and Pääbo have offered to have a go at isolating DNA from
        the “hominid” skeleton, but the early signs are that DNA has not survived. “The somewhat
        moist and tropical preservation conditions make the recovery of DNA improbable,” says Peter
        Brown, the paleoanthropologist at the University of New England in Armidale, Australia, who
        led the hobbit study. Efforts to extract DNA from other bones collected at the same site as
        this tiny hominid have not produced results. “We have made attempts with Stegodon molars,”
        he says, “but so far without success.”
      
      
        Ongoing Controversy
        However, in spite of the authenticity criteria and this transition towards testing the
        big questions in evolutionary biology, aDNA research continues to invite controversy. In
        2000, a team of United States researchers claimed to have cultured a bacterium sealed
        inside a 250-million-year-old salt crystal [9]. For Cooper, this is the sort of study that
        should require replication by an independent laboratory before publication. “When we
        repeated that work with the same primers, we were pulling up halobacteria from everywhere,”
        he says. “We took some dust from the top of the natural history museum in Oxford, extracted
        [DNA], used their supposedly halospecific primers and extracted a whole bunch of sequences,
        including some that fell within their diversity.” This strongly suggests, says Cooper, that
        the bacterium that was cultured was a modern bacterium, rather than an ancient specimen. “I
        can't see any logic for having 250 million years without any evolution.”
        But Russell Vreeland, a microbiologist at West Chester University in Pennsylvania and
        first author of the salt-crystal study, is adamant that his methods were exacting. “The
        probability of having a contaminant in our sample was one chance in a billion,” he
        calculates. “If you use a Band-Aid today on your skin or your children, you are 1,000 times
        more likely to have an infection from that Band-Aid than I am to have a contaminant.” It's
        completely unscientific to argue that the cultured bacterium was a result of contamination
        simply because it resembles modern bacteria, says Vreeland. “That's throwing out the baby
        with the bathwater. If you can show that nothing has penetrated your sample and the DNA is
        inside, then the age of the DNA has to be equal to the age of that rock,” he says. “I think
        you can make your criteria so stringent that you miss reality.”
        Others are alert to this danger. Sticking rigidly to the authenticity criteria can be a
        problem, argues Tom Gilbert of the Department of Ecology and Evolutionary Biology at the
        University of Arizona. “[The criteria] can both hinder the publication of good studies that
        do not adhere to all the criteria, and also enable the publication of erroneous results
        that adhere strictly to them,” he says. Part of the problem is that many referees of aDNA
        papers do not have a background working with aDNA, so are inclined to use the authenticity
        criteria as a checklist rather than critically evaluating each bit of research on a
        case-by-case basis. For example, he says, a recent high-profile study that followed all the
        criteria found that aDNA from two Cro-Magnon-type humans was very similar to DNA from
        modern humans [10]. But this could just mean that the specimens were contaminated by modern
        humans. “As no information was provided on the sample's handling history,” says Gilbert,
        “it becomes impossible for a reader to decide whether the sequences are authentic or
        contaminant” (Box 2). Such papers will continue to appear as long as the authenticity
        criteria are used by authors and referees as a checklist, he says. This does not mean the
        criteria should be relaxed, he adds, but they should be used in a more intelligent way.
        But allowing authors the freedom to use the criteria as they see fit could come at a
        cost, says Cooper. “The trouble with a case-by-case basis is that it basically equates to
        no standards, because then people will do what they feel like doing and we're back to the
        1990s again,” he says.
      
      
        Agreement
        This ongoing disagreement over how aDNA studies should be judged does, however, stem
        from a common concern. As more and more biologists come to appreciate the unique ability of
        aDNA to probe the evolutionary process, it is more important than ever to stress the
        immense challenges of working with just a few fragments of degraded DNA that might have
        come from several different sources. It is obvious why an archaeology lab might want to set
        up its own aDNA facility. But this is like creating molecular biologists without a license,
        says Pääbo. “You wouldn't buy an accelerator and say 'I will now start doing my own carbon
        dating,'” he says. “You have to really have experience working with low copy number.”
        However, in spite of the continual problem of eager but inexperienced biologists trying
        to extract DNA from specimens in the university museum, there is a sense that aDNA is
        starting to fill in the gaps in our understanding of key moments in evolutionary history.
        So at the start of 2005, as aDNA research enters its 21st year, the discipline is, perhaps,
        coming of age.
      
    
  

  
    
      
        
        Despite the biases that a single author inevitably brings to a subject, only one or a
        few closely interacting authors can bring coherence, synthesis, and vision to a broad and
        complex topic. A symposium volume just doesn't do the job. Few topics in biology are as
        simultaneously encompassing, complex, and controversial as the origin of species, i.e.,
        speciation. Speciation is, after all, the process responsible for biological diversity, at
        least of sexual organisms, so it is hardly a minor topic. But even though recent years have
        seen the publication of symposia on speciation and books on the ever-contentious issue of
        species concepts, it has been 23 years since Verne Grant's authoritative 
        Plant Speciation [1] and 41 years since Ernst Mayr's magisterial and
        highly influential 
        Animal Species and Evolution [2]—the last syntheses of research on
        speciation. Now, two outstanding new books not only treat speciation as a conceptually
        unified topic in both plants and animals for the first time, but also provide rich review
        and analysis of a vast subject that has progressed at least as much since Mayr and Grant
        wrote as in the century that preceded their work.
        These books are very different, but wonderfully complementary. Gavrilets reviews and
        adds to mathematical theories and simulation studies of speciation and related issues, such
        as fitness landscapes and selection in heterogeneous environments. A deep reading of his
        book will require considerably more mathematical competence than most evolutionary
        biologists (including this reviewer) have, but Gavrilets provides excellent verbal
        explanations of the models' assumptions and conclusions, as well as comparisons and
        critiques of related models. Gavrilets cites empirical studies (with which he has very
        broad familiarity) plentifully, but as a theoretician, he does not evaluate them or
        describe them in depth. That task is undertaken by Coyne and Orr, who introduce most topics
        with a verbal overview of theory, review empirical evidence and its bearing on hypotheses,
        and conclude with incisive assessments of what they think we know and what remains
        uncertain or unexplored. Like Gavrilets, they offer a number of novel ideas or suggestions
        about how to proceed. Coyne and Orr have both worked mostly on speciation genetics in 
        Drosophila , so it is hardly surprising that their treatment of
        speciation bears a strong genetic emphasis and draws heavily on 
        Drosophila work (perforce, since this is almost the only source
        of evidence on some topics, such as the genetics of hybrid sterility and inviability). Even
        the most drosophilophobic readers, however, will be pleased by the extent to which Coyne
        and Orr have conscientiously scoured the literature on nonmodel animals and plants.
        To appreciate the landmark status of these books, consider what has happened in
        speciation studies since Mayr and Grant published theirs. Mayr and Grant articulated
        positions on species and speciation that had developed during and soon after the Modern
        Synthesis of the 1930s and 1940s, when modern evolutionary theory developed from a
        reconciliation of genetics, systematics, and paleontology. Mayr and Grant drew on abundant
        systematic data on patterns of divergence and experimental data on genetic differences
        between related species. They rightly identified reproductive isolation (RI) as a critical,
        even defining, property of species, and allopatric divergence (i.e., in disjunct geographic
        areas) as the major geographic mode of speciation. They recognized that trait differences
        between species, including RI, usually have a polygenic basis, and that different coadapted
        (epistatically interacting) sets of genes underlie incompatibility (e.g., hybrid
        sterility). They emphasized the role of ecological selection as a driving force in
        speciation, largely by extrapolation from the primacy of selectionist thinking that
        developed during the Synthesis. They accepted that natural selection can reinforce
        prezygotic isolation (i.e., lack of mating or zygote formation) between species and thereby
        reduce production of unfit hybrids, even if Mayr did not share Dobzhansky's belief that
        this was the norm. Mayr combined selection with genetic drift in his theory of
        founder-effect speciation (divergence in populations founded by just a few individuals),
        which became widely accepted. (It became Eldredge and Gould's [3] theoretical foundation
        for punctuated equilibrium nine years after Mayr's book appeared.) Mayr and Grant wrote
        against a background that almost entirely lacked any mathematical theory of speciation
        (which I suspect neither of them would have drawn on even if it had been developed), any
        relevant molecular data (other than early allozyme studies by the time Grant published),
        and any rigorous phylogenetic methodology. Kimura's neutral theory of molecular evolution
        [4] had not yet been published when Mayr wrote, and had not been vindicated when Grant
        wrote, so genetic drift and a neutral (nonselectionist) interpretation of molecular data
        were still suspect. Detailed analysis of genetic architecture was decades away, and of
        course insights into selection and historical demography from DNA sequence data were a dim
        dream at best.
        Coyne and Orr and Gavrilets analyze a new world of speciation studies. Theoretical
        studies of speciation, for example, now include more than 100 papers on one topic alone,
        the evolution of prezygotic isolation. (Gavrilets laments that the theoretical work suffers
        from domination by simulation rather than analysis, so that it is often hard to draw
        general conclusions from models that use different assumptions, but he nevertheless draws
        some fairly strong conclusions, as I note below.) Molecular studies have provided important
        data on such issues as the absolute dates of speciation events, the duration of speciation,
        and the time course of the evolution of RI. The field of molecular phylogeography, which
        documents the history of spatial isolation and geographic expansion of populations, has
        developed. The relation between range overlap of related species and their molecularly
        dated time of divergence provides some evidence on the role of geographic versus sympatric
        speciation (i.e., speciation without geographic segregation). In all these areas and
        others, our knowledge has increased steadily. For instance, molecular markers enable more
        detailed dissection of the genetic architecture of species differences, and support the
        conclusion that they are usually rather highly polygenic, but that much of the variance can
        be explained by a few major gene substitutions. We now have good evidence, as Coyne and Orr
        emphasize, that at least in animals, hybrid infertility is caused by differences in gene
        action, not by structural chromosome differences or failure of meiosis.
        Regarding the mechanisms of speciation, evidence for the role of divergent ecological
        selection in allopatric speciation is sparse, because this crucial topic has been
        unaccountably neglected until recently. Very different kinds of data, ranging from DNA
        sequences to correspondence between RI and ecological divergence, support natural
        selection, but there is hardly enough evidence, in my opinion, to support Coyne and Orr's
        strong conclusion that “at least one important debate has been settled: selection plays a
        much larger role in speciation than does drift” (p. 410). Even more astonishing than the
        paucity of studies of the role of ecological selection in speciation is the fact that the
        likely role of sexual selection was not even recognized until almost 20 years after Mayr's
        book. I agree with Coyne and Orr that the theory and evidence for speciation by sexual
        selection is one of the most important advances in speciation studies, but it is important
        to recognize that the evidence consists mostly of correlations between diversification
        rates and indices of the likely strength of sexual selection; as Coyne and Orr note, there
        are no cases in which we understand just how sexual selection has caused speciation. This
        is a rich, largely unexplored area. Gavrilets feels that divergent evolution by sexual
        conflict (in which females evolve resistance to males' advances) is a potentially important
        process, whereas Coyne and Orr are skeptical that this will prove widespread. Coyne and Orr
        remark that populations may diverge in male signals because of intrasexual selection
        (competition among males), and that female mate preference may follow. Quite so, but even
        though Berglund et al. [5] summarized many examples in which male signals appear to serve
        both inter- and intrasexual functions, this topic has been almost ignored in the literature
        of both speciation and sexual selection. On a related theme, an important speciation
        process appears to be the extraordinarily rapid evolution of male reproductive proteins
        (e.g., sperm surface proteins), which may contribute to the “faster male evolution” that is
        a cause of “Haldane's rule” (that hybrid sterility and inviability first appear in the sex
        that has two unlike sex chromosomes).
        Despite their very different approaches, Coyne and Orr and Gavrilets arrive at rather
        similar conclusions on some of the most controversial issues in speciation. One such is the
        role of genetic drift in speciation. Gavrilets analyzes founder-effect speciation (which
        combines drift and selection), agrees with most other theoreticians (e.g., Barton and
        Charlesworth) [6] that it is very improbable, and argues instead for his model of evolution
        on “holey landscapes,” whereby allopatric populations can evolve by genetic drift along
        ridges of roughly equal fitness to different, incompatible gene constitutions. He admits
        that the time to speciation under this process will ordinarily be very long unless
        selection is involved. Coyne and Orr fully accept both Barton and Charlesworth's critique
        and Gavrilets's alternative model. But while admitting the plausibility of Gavrilets's
        models of speciation by genetic drift, they nevertheless maintain that “the models seem
        unnecessary when compared to adaptive ones” (p. 398).
        Coyne and Orr appear to adopt selection as the null hypothesis for speciation, whereas
        drift is generally taken as the null hypothesis in much of evolutionary genetics, for the
        simple reason that drift operates at all loci in all finite (i.e., real) populations,
        whereas selection need not. The burden of demonstrating that selection is 
        not responsible for an evolutionary event (i.e., demonstrating a
        negative) is, of course, far heavier than the burden of demonstrating selection; indeed,
        Coyne and Orr do not address the difficult question of what would constitute evidence for
        drift. Having, perhaps, stacked the deck, Coyne and Orr find almost no evidence that drift
        has contributed to speciation in nature, but conclude that there is “considerable evidence”
        that selection has done so. However, the amount of evidence is hardly on a par with, say,
        the evidence for allopatric speciation. It consists of only about eight studies of
        ecological selection, indications that diversification rates are associated with greater
        scope for sexual selection, selection signatures in a few genes that underlie genetic
        incompatibility, and a paucity of molecular evidence for bottlenecks (i.e., opportunities
        for founder events) in the history of recently formed species. But the evidence on the role
        of sexual selection is very indirect, and the high levels of genetic variation revealed in
        molecular studies argue against past bottlenecks only if this is ancestral variation,
        rather than variation generated anew since a possible bottleneck—a question that has been
        addressed in only a few cases. Assuming that experiments with laboratory populations can be
        validly extrapolated to natural speciation processes, founder-effect speciation may indeed
        be a moribund hypothesis, but I do not believe long-term genetic drift can yet be ruled
        out, and cannot agree that this “important debate has been settled” (p. 410).
        The geography of speciation continues to be one of the most difficult and contentious
        topics, and undoubtedly will remain so despite the careful analyses by these authors. They
        agree that parapatric speciation (evolution of RI between neighboring populations that
        exchange genes) is theoretically plausible, but Gavrilets notes that although it has become
        clear that its likelihood is sensitive to several model parameters, parapatric speciation
        is difficult to model and has been shamefully neglected. Coyne and Orr do not doubt that it
        is a fairly common mode of speciation, yet “it is almost impossible to demonstrate
        parapatric speciation in nature” (p. 118), and no cases have been well documented.
        Gavrilets provides an exhaustive analysis of the many models of sympatric speciation,
        and identifies some key issues that have been underemphasized. For example, the sympatric
        evolution of behavioral isolation by “matching traits” (e.g., genetically independent male
        signal and female preference) is generally much more difficult than “similaritybased”
        mating (in which females choose males that have the same phenotypic trait as themselves).
        Just how common the latter is in animals is an open question that Coyne and Orr
        unfortunately do not address. Gavrilets also identifies the cost of female choosiness as a
        critical issue: many models of sympatric speciation depend on the assumption that females
        always succeed in mating even if the male type they prefer is rare, so their choosiness has
        no cost. Gavrilets criticizes some popular models of sympatric speciation on these and
        other grounds, and while granting that sympatric speciation by divergent habitat or host
        preference is plausible, he concludes that it need not be faster than allopatric speciation
        and that “contrary to common claims in recent theoretical papers, conditions for sympatric
        speciation are not wide and sympatric speciation does not occur easily” (p. 404).
        For their part, Coyne and Orr feel that the prevalence of sympatric speciation is an
        empirical issue (but a very difficult one), and undertake a broad, detailed review. They
        identify three examples of completed speciation in which a sympatric scenario “seems
        plausible.” I see no reason to accept one of these cases, a pair of sister species of
        “parasites” (fig wasps) on the same host species, since allopatric speciation of a
        widespread parasite need not be accompanied by speciation of its host. Moreover, Coyne and
        Orr note weaknesses in all three cases, as well as in examples of “host races” that have
        been advanced as species 
        in statu nascendi . Coyne and Orr's conclusion echoes Gavrilets's: “It is
        hard to see how the data at hand can justify the current wave of enthusiasm for sympatric
        speciation” (p. 178). 
        Bravi!
        I have indicated some disagreements with Coyne and Orr, and could certainly cite others.
        But whatever weaknesses their book may have (more ecology and phylogeny, anyone?) are much
        less important than its strengths. The strengths of 
        Speciation are not only Coyne and Orr's comprehensive, scholarly coverage
        of an exceedingly broad subject, but also, and especially, their rigorous, incisive
        analysis, coupled with strongly stated conclusions and suggestions for how to resolve
        controversies. Many readers will have a visceral reaction against their position on
        sympatric speciation, reinforcement, founder-effect speciation, or other issues—but can
        these readers counter Coyne and Orr's arguments with equally cogent analysis? Or are these
        subjects that simply require more, and perhaps more imaginative, research?
        Together, these books provide a comprehensive, thoughtful synthesis of our current
        understanding of one of the most important processes in evolution. They are required
        reading for anyone who studies species and speciation. I recommend 
        Speciation and the nonmathematical final chapter (“General Conclusions”)
        of 
        Fitness Landscapes and the Origin of Species to all evolutionary
        biologists, students, and professionals alike. It may not take another two decades for the
        next foundational books on speciation to appear, but these books will fill that role for a
        long time to come.
      
    
  

  
    
      
        
        Biological databases offer access to formalized facts about many aspects of
        biology—genes and gene products, protein structure, metabolic pathways, diseases,
        organisms, and so on. These databases are becoming increasingly important to researchers.
        The information that populates databases is generated by research teams and is usually
        published in peer-reviewed journals. As part of the publication process, some authors
        deposit data into a database but, more often, it is extracted from the published literature
        and deposited into the databases by human curators, a painstaking process.
        Research literature and scientific databases fulfil different needs. Literature provides
        ideas and new hypotheses, but is not constrained to provide facts in formats suitable for
        use in databases. By contrast, databases efficiently provide large quantities of data and
        information in a standardised schema representing a predefined interpretation of the data.
        While the acceptance of a paper can enforce the submission of data to a central data
        repository, such as EMBL (www.ebi.ac.uk/embl/) or ArrayExpress
        (www.ebi.ac.uk/arrayexpress/), nobody receives credit for the submission of a fact to a
        database without an associated publication. As long as this practice continues, curation
        will be necessary to add the (re)formalised facts to biological databases.
        Given that publications are not about to be replaced with routine deposition of data
        into databases, is it possible to develop software tools to support the work of the
        curator? Could we automatically analyse new scientific publications routinely to extract
        facts, which could then be inserted into scientific databases? Could we tag gene and
        protein names, as well as other terms in the document, so that they are easier to
        recognise? How can we use controlled vocabularies and ontologies to identify biological
        concepts and phenomena? Fortunately, there are many groups that are now seeking to answer
        these questions, precisely with a view to extracting facts from text.
        Part of the motivation for this effort in text mining technology is the inexorable rise
        in the amount of published literature (Figure 1). This massive growth, coupled with the
        current inefficiencies in transferring facts into other data resources, leads to the
        unfortunate state that biological databases tend to be incomplete (for example, DNA
        sequences without known function in genetic databases), and there are inconsistencies
        between databases and literature.
        In theory, text mining is the perfect solution to transforming factual knowledge from
        publications into database entries. But computational linguists have not yet developed
        tools that can analyse more than 30% of English sentences correctly and transform them into
        a structured formal representation [1,2]. We can analyse part of a sentence, such as a
        subphrase describing a protein–protein interaction or part of a sentence containing a gene
        and a protein name, but we always run into Zipf's law whenever we write down the rules for
        how the extraction is done (Figure 2) [3]. A small number of patterns describe a reasonable
        portion of protein–protein interactions, gene names, or mutations, but many of those
        entities are described by a pattern of words that's only ever used once. Even if we could
        collect them all—which is impossible—we can't stop new phrases from being used.
      
      
        Curators—The Gold Standard
        Hand-curated data is precise, because the curator is trained to inspect literature and
        databases, select only high-quality data, and reformat the facts according to the schema of
        the database. In addition, curators select citations from the text as evidence for the
        identified fact, and those citations are also added to the database.
        Curators read and interpret the text at the same time, and if they don't understand the
        meaning of a sentence, they can go back and pick a new strategy to analyse it—they can even
        call the authors to iron out any ambiguities. Curators can also cope with the high
        variability of language described by Zipf's law. At present, no computer-based system comes
        close to matching these capabilities. In particular, it is difficult to convert all the
        curators' domain knowledge into a structured training set for the purposes of machine
        learning approaches.
        Curators fulfil a second important task: they know how to define standards for data
        consistency, in particular, the most relevant terminology, which has led to the design of
        standardised ontologies and controlled vocabularies (see Box 1 for an explanation of these
        and related terms). Examples of these include Gene Ontology (GO; www.geneontology.org/),
        Unified Medical Language System (www.nlm.nih.gov/research/umls/), and MedDRA
        (www.meddramsso.com/NewWeb2003/index.htm) [4]. These terminological resources help to
        relate entries in bioinformatics databases to concepts mentioned in scientific publications
        and to link related information in databases using different schemas. Text miners would
        love such standards to be used in text, but there is an understandable reluctance to impose
        and use standards that might limit the expressiveness of natural language.
      
      
        Curation and Text Mining—In Partnership
        The problem with curation of data is that it is time consuming and costly, and therefore
        has to focus on the most relevant facts. This compromises the completeness of the curated
        data, and curation teams are doomed to stay behind the latest publications. So, is it
        possible for curation and text mining to work together for rapid retrieval and analysis of
        facts with precise postprocessing and standardisation of the extracted information?
        There are several software tools that perform well in the identification of standardised
        terms from the literature. Examples include Textpresso and Whatizit [5,6,7,8]. Extensive
        term lists come from the Human Genome Organization (www.gene.ucl.ac.uk/hugo; 20,000 gene
        and protein names), GO (almost 20,000 terms), Uniprot/Swiss-Prot
        (www.ebi.uniprot.org/index.shtml; about 200,000 terms), and other databases. In addition,
        terms describing diseases, syndromes, and drugs are available from the Unified Medical
        Language System. Altogether, about 500,000 terms constitute the basis of domain knowledge
        in life sciences. To gain some perspective of this figure: an average individual handles
        2,000 to 20,000 terms in his or her daily language, and 
        Merriam-Webster's Collegiate Dictionary provides definitions for 225,000
        terms (www.merriam-webstercollegiate.com/).
        The identification of all terms by a text mining system still sets challenging demands.
        All variants of a term have to be taken into account, including syntactical variants and
        synonyms. In the case of ambiguities, relevant findings have to be distinguished from other
        findings—a process referred to as disambiguation. Depending on the curation task, it might
        therefore be advantageous to select only part of the terminological resources and thus
        restrict the domain of the terminology to the curators' needs (Figure 3).
        Available text mining solutions are concerned with named entity (NE) recognition
        (entities are, for example, proteins, species, and cell lines), with identification of
        relationships between NEs (such as protein interactions), and with the classification of
        text subphrases according to annotation schemata in general (thyroid receptor is a thyroid
        hormone receptor) [9,10,11,12,13,14,15]. Whilst the identification of a curation team's
        terminology in the scientific text under scrutiny is immensely valuable, there is still a
        long way to go before this becomes routine.
      
      
        Some Immediate Challenges
        Not all terms used in the literature (NEs) can actually be found in some kind of
        database (perhaps because of an author error, or an alternative name for an entity adopted
        by the community). Text mining methods therefore have to detect new terms and map the term
        to known terminology [16]. If several mappings are possible, the correct version has to be
        selected (disambiguation).
        Over the past several years text mining research teams have presented various approaches
        that train a software tool to locate representations of gene or protein names (for example,
        BioCreative, www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html, and JNLPBA,
        www.genisis.ch/~natlang/JNLPBA04/) [17,18]. These tools are scored with a statistic known
        as the F-measure, with the best methods scoring about 0.85. At the level of 0.85, curators
        still tend to be unhappy. However, analyses have shown that this score is in the range of
        curator–curator variation (unpublished data, measured as part of the project work for
        [19]), which suggests that such methods produce useful results.
        Additional information-extraction methods have been proposed, for example, for the
        documentation of mutations in specific genes and for the extraction of the subcellular
        location of proteins [11,13]. An even larger number of tools focus on the identification of
        appropriate terminology for the annotation of genes (GO terms) [7]. The evaluation of their
        usefulness depends on the demands of the user groups. Finally, another way to support
        curation teams would be to provide information-retrieval methods to guide the team members
        towards documents containing relevant information. For example, in 2002, the participants
        in the Knowledge Discovery and Data-Mining Challenge Cup
        (www.cs.cornell.edu/projects/kddcup/) had to select documents from a given corpus that
        contained relevant experimental results about 
        Drosophila [20].
      
      
        How Can Publishers Contribute?
        For all automated information-extraction methods, it is obvious that access to
        literature is crucial. Electronic access has, of course, already had a huge impact, but the
        structure and organisation of manuscripts could also be improved. For example, semantic
        tags could be integrated into the text. The markup would not appear on web pages or when
        the document is printed, but it would help software to deal with semantic aspects of the
        document. Inserting tags, for example, to mark protein names would allow retrieval software
        to find documents about proteins even if they look like common English words, such as “you”
        or “and”. Retrieval engines currently often ignore such terms. In addition, explicit tags
        would enable text mining methods, for example, when looking for protein–protein
        interactions, to use the correct semantic interpretation.
        Text mining systems already available today, such as Whatizit, can integrate semantic
        tags during submission, which have to be verified by the author. Text mining is ready to
        deliver tools whereby information is passed back to the authors about the proper use of
        terminology within their documents. If the use of a term raises conflicts or ambiguities or
        if the use of a term is wrong, the author is asked to provide feedback. The curation effort
        is resolved at the earliest possible time-point. Author, publisher, reviewer, and reader
        profit from consistent information representation, which leads to better dissemination of
        documents and journals and easily offsets the additional cost in the generation of an
        article. Publishers and authors have to agree on standards though.
      
      
        Is Text Mining Ready to Deliver?
        Text mining solutions have found their way into daily work, wherever fast and precise
        extraction of details from a large volume of text is needed. We have to keep in mind,
        however, that any text mining tool, just like other bioinformatics resources, will only be
        suitable for a limited number of tasks. For example, the same text may serve curators from
        different communities who extract different types of facts, depending on their domain
        knowledge. Furthermore, different communities have different expectations for accuracy. For
        example, curators dealing with a small set of proteins prefer tools with high recall,
        whereas curators dealing with a large number of proteins prefer tools with high
        precision.
        Although text mining cannot dissect English sentences completely, and cannot extract the
        meaning and put the facts into a database, text mining tools are becoming increasingly used
        and valued. Text mining is ready to deliver handling of complex terminology and
        nomenclature as a mature service. It is only a matter of time and effort before we are able
        to extract facts automatically. The consequences are likely to be profound. Not only will
        we have a more effective approach for the mining of knowledge from the literature, our
        approach to the publication process itself might change. If a fact is clear enough for
        automatic extraction, it could be reported in a fact database instead of a publication. As
        methods improve, authors will see more and more of their text being analysed and formalised
        in a database. If appropriate quality control is provided, and if authors receive due
        credit for their deposition of facts into databases, we might well see a shift towards
        original papers describing new creative ideas and visions rather than just listing
        facts.
      
    
  

  
    
      
        
        Viruses are intracellular pathogens that are subject to intense selective pressures
        during their ongoing battles within the host. To propagate successfully, they must exploit
        numerous machineries of the infected cell. Thus, studies of their replicative cycles have
        yielded fundamental insights into eukaryotic biology. A prime example is the human
        immunodeficiency virus (HIV), which is a lentivirus that causes the acquired
        immunodeficiency syndrome (AIDS). Unlike simpler oncoviruses that rely exclusively on host
        cell machinery, lentiviruses code for additional accessory and regulatory proteins that act
        as molecular switches at different stages of viral entry and exit from the infected cell.
        Studying the actions of these viral proteins has yielded understanding of diverse cellular
        functions such as the innate immunity against retroviruses, control of transcriptional
        elongation, export of macromolecules from the nucleus to the cytoplasm, and intracellular
        trafficking of proteins (reviewed in [1]).
        The transcriptional transactivator (Tat) is a key regulatory protein of HIV. It is
        expressed early after the virus integrates into the cell, and stimulates the elongation of
        RNA polymerase II (RNAPII). This type of transcriptional control had not been previously
        appreciated; thus, work on Tat established a new paradigm in the field of eukaryotic
        biology. Moreover, these findings impacted greatly studies of cotranscriptional processing
        of nascent mRNA. To understand these processes better, we need to start with the basics of
        transcriptional control.
        RNAPII is the enzyme that transcribes protein-coding genes in eukaryotic cells. Elegant
        studies in vitro first suggested that the simple recruitment of RNAPII to transcription
        units was not sufficient for the copying of genes and cotranscriptional processing of their
        transcripts. Rather, distinct steps could be defined, which began with the assembly of the
        preinitiation complex (PIC), promoter clearance, pausing, and arrest, and ended with
        efficient elongation of transcription (reviewed in [2]). The central component of PIC is
        the general transcription factor (GTF) TFIID, which contains the TATAbox- binding protein
        (TBP) and 12 to 15 TBP-associated factors (TAFs). TFIID acts as a “landing pad” for other
        GTFs and RNAPII to nucleate PIC assembly. Moreover, TAFs serve as coactivators to a diverse
        set of activators. Both an ordered stepwise assembly and the recruitment of the
        100-plus-subunit “holoenzyme” have been proposed to be critical for the positioning of
        RNAPII at start sites of transcription.
        Next, the GTF TFIIH unwinds the DNA, opens the transcription bubble, and phosphorylates
        serines at position 5 in the C-terminal domain (CTD) of the RPB1 subunit of RNAPII
        (reviewed in [2]). This phosphorylation is critical for the recruitment of complexes that
        put a 7-methylguanylate cap on the 5′ end of nascent transcripts. After the transcription
        complex clears the promoter, the negative transcription elongation factor (N-TEF) is
        recruited to the RNAPIIa (reviewed in [3]). It consists minimally of 5,6-
        dichloro-1-β-D-ribofuranosylbenzimidazole riboside (DRB)- sensitivity-inducing factor
        (DSIF) [4] and negative elongation factor (NELF) [5]. They bind and arrest RNAPII distal to
        the promoter cooperatively. Such arrested transcription complexes have now been found on
        many inducible genes in 
        Drosophila melanogaster (reviewed in [6]) and humans [7].
        The transition to robust elongation depends on the positive transcription elongation
        factor b (P-TEFb) (reviewed in [3]). P-TEFb contains the cyclin-dependent kinase 9 (CDK9)
        and one of four possible C-type cyclins. When recruited to stalled transcription complexes,
        P-TEFb phosphorylates serines at position 2 in the CTD [8], the Spt5 subunit of DSIF [9],
        and the RD subunit of NELF [10]. These modifications result in heavily phosphorylated
        RNAPII (RNAPIIo), the recruitment of the Elongator, which contains splicing and
        polyadenylation machineries, and the conversions of DSIF and NELF into elongation factors.
        RNAPIIo now copies the gene and directs the cotranscriptional processing, i.e., splicing
        and polyadenylation, of primary transcripts. Upon successful polyA addition, the CTD
        phosphatase FCP1 dephosphorylates RNAPIIo. RNAPIIa dissociates from DNA, and the
        transcription cycle starts all over again (reviewed in [2]).
        Tat is unique among transcriptional activators in eukaryotic cells in that it functions
        via RNA rather than DNA promoter elements (Figure 1). It binds the transactivation response
        element (TAR) that forms a stable RNA stem loop at the 5′ end of all viral transcripts.
        Thus, Tat requires minimally the transcription of TAR before it can stimulate HIV
        transcription from the long terminal repeat (LTR). Indeed, in the absence of Tat, RNAPIIa
        clears the HIV LTR successfully but soon arrests, yielding predominantly short viral
        transcripts [11]. Tat binds the 5′ bulge in TAR via its arginine-rich motif from positions
        49 to 57, where a central arginine (R52) is key for this interaction. However, this binding
        is not sufficient for Tat's function in vivo. Adjacent to the arginine-rich motif lie
        N-terminal core and cysteine-rich regions, which form the activation domain of the protein.
        This activation domain binds cyclin T1 (CycT1) from P-TEFb, whose partner is CDK9 [12]. As
        a consequence, P-TEFb and Tat bind TAR cooperatively. The final proof that P-TEFb is the
        cellular cofactor for Tat came from studies of HIV transcription in murine cells, where the
        introduction of the human CycT1 protein restores Tat function [12]. The same effect can be
        achieved by substituting just the tyrosine with the cysteine at position 261, such as are
        found in murine and human CycT1 proteins, respectively [13]. A paper in this issue of 
        PLoS Biology suggests that Tat and P-TEFb can also recruit
        TAF-independent transcription complexes to the HIV LTR [14] (Figure 1). Possibly, this
        assembly reflects interactions between CycT1 and the unphosphorylated CTD of RNAPIIa
        [15].
        The assembly and disassembly of the complex between PTEFb, Tat, and TAR is a regulated
        process in vivo. Whereas the phosphorylation of CDK9 strengthens this complex [16], the
        acetylation of the lysine at position 50 in Tat weakens it [17]. Upon this disruption,
        acetylated Tat is liberated from P-TEFb and recruits the p300/CREB-binding protein–
        associated factor (P-CAF) to the elongating RNAPIIo, most likely facilitating chromatin
        remodeling. In this issue of PLoS Biology, Pagans et al. now demonstrate that acetylated
        Tat is deacetylated by SIRT1 [18] (Figure 1). In this way, Tat can reassemble with P-TEFb
        on TAR.
        Clearly, P-TEFb plays a key role in the control of transcriptional elongation. Although
        Tat was the first activator known that could recruit P-TEFb to initiating RNAPII,
        additional members of this group were soon identified. They include the androgen receptor,
        c-Myc, the class II transactivator (CIITA), myoblast determination protein (MyoD), and
        nuclear factor κ-B (NF-κB). The last one is of great interest as it explains how the HIV
        genome can be transcribed before the synthesis of Tat [19]. Cellular activation triggers
        the nuclear translocation of NF-κB, where it binds the HIV enhancer, leading to the
        stimulation of viral transcription. It is not surprising that proviral latency, in which
        low levels of transcription or only short HIV transcripts containing TAR are observed,
        would in large part reflect the absence of these activators. Indeed, in many of these
        latently infected cells, the induction of NF-κB or the addition of Tat leads to the
        reactivation of viral replication and spreading of the infection [20,21].
        Recently, important aspects of the regulation of P-TEFb have been revealed (Figure 2).
        Of interest, P-TEFb exists in two complexes in cells [22,23]. The larger measures
        approximately 500 kDa and contains the hexamethylene bisacetamide (HMBA)–induced protein 1
        (HEXIM1) and 7SK small nuclear RNA (snRNA) in addition to P-TEFb [24,25]. In this large
        complex, Cdk9 is enzymatically inactive. HEXIM1 was identified as the inducible gene
        following the exposure of vascular smooth muscle cells to a potent differentiating agent,
        HMBA [26]. 7SK snRNA is one of the most abundant snRNA species, whose function remained a
        mystery for over a decade. Of interest, targeting of P-TEFb by HEXIM1 and 7SK snRNA
        contributes significantly to the control of cell growth and differentiation. For example,
        growth signals liberate P-TEFb from the large complex in the course of cardiac hypertrophy
        in mice, a disease characterized by the enlargement of myocytes due to a global increase in
        mRNA synthesis [27]. Also, following stress, ultraviolet light, or the administration of
        actinomycin D and DRB to cells, the large complex is converted to the small complex to
        stimulate transcription [22,23].
        How central is P-TEFb to eukaryotic transcription? In 
        Saccharomyces cerevisiae , there are two candidates for PTEFb,
        CTDK-1 and Bur1/2. CTDK1-negative but not Bur1/Bur2-negative yeasts still grow, albeit
        poorly and only on rich media (reviewed in [2]). In 
        Caenorhabditis elegans , genetic inactivation of CDK9 or CycT1
        and CycT2 resulted in the inhibition of all RNAPII transcription [8]. Moreover, in 
        D. melanogaster , following heat shock, PTEFb is recruited
        upstream of activated promoters [28]. Although no murine knockouts of subunits of P-TEFb
        have been reported, DRB and flavopiridol, two ATP analogs that inhibit the kinase activity
        of CDK9, can inhibit nearly all transcription by RNAPII in human cells [29]. Indeed, as
        P-TEFb is a coactivator of potent activators that mediate effects of enhancers and can
        itself activate transcription when placed on sites distal to promoter elements [15], it
        might mediate many more signaling events than those of heat shock, ultraviolet light,
        stress, and hypertrophy. Conversely, the inhibition of P-TEFb could explain the mode of
        action of some transcriptional repressors. Indeed, the global transcriptional repressor
        PIE-1, the regulator of embryogenesis in 
        C. elegans , binds the histidine-rich stretch in CycT1, thus
        decoying P-TEFb away from RNAPII and blocking the elongation of transcription [30].
        These are exciting findings and suggest a plethora of future experiments, including the
        genetic inactivation of subunits of P-TEFb and isoforms of HEXIM1 in the mouse. Of special
        interest are questions as to where to place this mechanism of transcriptional regulation in
        the hierarchy of competing or complementary processes. What roles do different P-TEFb
        complexes play in the transcription of specific genes? How central will the regulation of
        P-TEFb be to cellular growth, proliferation, and differentiation, and what roles will it
        play in normal development and disease states? As to HIV, how can we use our knowledge of
        P-TEFb to slow down viral replication and/or to eliminate the state of proviral latency in
        the host? Obviously, we are only at the beginning of this journey, which promises to change
        radically our view of eukaryotic transcription.
      
    
  

  
    
      
        
        All organisms have mechanisms to ensure that dividing cells produce new cells with the
        proper number of chromosomes. The dividing cell closely monitors that chromosomes are
        copied exactly once and then distributed correctly to daughter cells. After replication,
        the chromosomes (now comprising two chromatids) align at the center of the cell, and are
        attached to a structure known as the spindle apparatus. A key point of attachment is the
        centromere, a characteristic constriction carried by each chromosome. The spindle, which is
        composed of microtubules, pulls the chromatids apart so that two complete sets of
        chromosomes are gathered together at each pole of the cell, which can then divide. Cohesion
        between chromosome copies, which keeps the chromatids together until just the right time,
        therefore plays a critical part in this process.
        Chromosome cohesion is established during S phase (when the chromosomes are replicated)
        and is then dissolved completely in metaphase to allow sister chromatids to come apart. The
        dissolution of cohesion is highly regulated; human cell lines that have defects in the
        regulation of cohesion show the hallmarks of cancer cells [1]. Furthermore, it has been
        suggested that the abnormal karyotypes that result in diseases such as Down syndrome are
        the result of the improper dissolution of chromosome cohesion [2]. Finally, mutation of a
        factor required to load cohesin—the protein complex responsible for chromosome
        cohesion—onto chromosomes appears to cause Cornelia de Lange syndrome, a clinically
        heterogeneous developmental disorder that may include facial dysmorphia, upper-extremity
        malformations, hirsutism, cardiac defects, growth and cognitive retardation, and
        gastrointestinal disorders [3,4,5].
        Cohesion serves at least three roles in the cell with respect to accurate genome
        transmission. First, cohesion close to the centromere facilitates bi-orientation of
        chromosomes, such that each chromosome becomes attached to the two poles of the spindle
        [6]. Second, it prevents the splitting of chromosomes until all bipolar attachments are
        made [6]. The function of cohesion at the centromere is presumably to oppose the force of
        microtubules, which pull the chromosomes to opposite spindle poles; this force is not
        exerted along the chromosome arms, which means that cohesion at centromeres and along arms
        is functionally distinct. Third, cohesion along chromosome arms may be essential for proper
        chromosome condensation [7,8], although the function of cohesion at chromosome arms is
        something of a mystery.
      
      
        Differences between Arms and Centromeres
        Cohesion in eukaryotic cells is mediated by a multi-subunit protein complex called
        cohesin. Cohesin consists of four proteins: Smc1, Smc3, Scc1/Mcd1 (also known as kleisin),
        and Scc3 (SA2). The Smc (structural maintenance of chromosomes) proteins form
        intramolecular coiled coils that have been observed in the electron microscope to form a V
        shape with sides that are 50 nm long [9]. The cohesin complex has been proposed to form a
        ring structure that encircles sister chromatids [10]. Alternately, two rings may snap
        sisters together via interactions between the coiled coils of the Smc proteins [11]. All
        members of the cohesin complex are essential in budding yeast, 
        Saccharomyces cerevisiae , since mutation results in the
        precocious dissociation of sister chromatids. Functional orthologs of these proteins exist
        in all eukaryotes.
        There are at least two types of cohesin sites: (1) cohesin associated with the
        centromere and the nearby pericentric domain, and (2) cohesin associated with chromosome
        arms [12,13,14,15]. In 
        S. cerevisiae , cohesin at centromeric and pericentric domains
        is spread over a broad region (up to 50 kb), large quantities of the protein complex are
        bound, and binding is not affected by the natural transcriptional and coding status of the
        regions that are occupied. By contrast, binding sites in arms tend to be much smaller
        (about 1 kb)—at least in 
        S. cerevisiae , where they have been most extensively
        characterized—and of lower intensity, and are spaced at approximately every 11 kb (see
        Figure 1). Cohesin in arms localizes to regions lacking transcription in yeast [12,16,17].
        This reinforces the view that there may be functional differences in arm and pericentric
        cohesion and perhaps different mechanisms to load cohesin, as has been proposed for cohesin
        on meiotic chromosomes for 
        S. pombe [18]. A unifying feature of all cohesin-binding sites
        in 
        S. cerevisiae is high AT (adenine and thymine) content
        [12,15].
        Another important difference between cohesin binding along arms and at centromeres is
        that the arm sites do not appear to be dependent on a DNA consensus sequence, whereas
        binding to pericentric regions requires specific centromere sequence [13,14]. The 
        S. cerevisiae centromere sequence is composed of three DNA
        elements (CDEI, CDEII, and CDEIII). Studies of cohesion at the centromere reveal that as
        little as 100 bp (a portion of CDEII and the entire CDEIII) are required to direct cohesion
        [13,14,19]. Mutations in the protein Ndc10 have also been shown to affect cohesin
        deposition at centromeres. Ndc10 forms part of a structure known as the kinetochore, which
        forms around the centromere and is responsible for the attachment to the spindle;
        establishment and maintenance of cohesion at pericentric regions may therefore rely on both
        the centromere sequence and kinetochore function [13,20]. Presumably both arm and
        pericentric cohesion are important for chromosome dynamics, but the functional differences
        between the two are not well understood.
      
      
        Cohesion—It's Just a Phase
        Cohesion can be divided into four phases that occur during the cell cycle (Figure 2):
        (1) deposition in G1 (the gap in the cell cycle before S phase), (2) establishment in S
        phase, (3) maintenance in G2 (the gap between S and mitosis), and (4) dissolution in
        mitosis. During G1, Scc2 and Scc4 are responsible for loading cohesin onto unreplicated
        double-stranded DNA [21]. Then, during S phase, several proteins are involved in
        establishment of cohesion between replicated chromatids. Eco1 and Chl1 are required for
        establishing cohesion but not for maintenance [22,23,24]. The associations between cohesion
        and DNA replication have led to a model whereby cohesion is established coincident with the
        passage of the replication fork [25]. This requires an alternative replication factor C
        (RF-C) complex [26,27,28] and may require the origin recognition complex (ORC) [29].
        Cohesion is maintained during G2 by the cohesin complex, and is eventually dissolved in
        mitosis to allow sister chromatids to separate.
        The dissolution of cohesion is regulated by at least two mechanisms. First, subunits of
        the complex may be phosphorylated, which facilitates their removal. In 
        S. cerevisiae and human cells, phosphorylation of Scc1/Mcd1 by
        Polo kinase makes it a better substrate for proteolysis [30,31,32]. In this issue of 
        PLoS Biology , one of two related papers exploring the regulation of
        cohesin in vertebrates shows that phosphorylation of Scc3 (SA2) by Polo-like kinase is
        apparently sufficient to allow dissociation from chromosome arms, which occurs during
        prophase [32]. In 
        Xenopus extracts, phosphorylation of cohesin also depends on
        Polo-like kinase, and this phosphorylation reduces the ability of cohesin to bind to
        chromatin [8].
        The second mechanism that can facilitate the dissolution of cohesin is proteolysis; this
        may be particularly important at centromeres. The Scc1/Mcd1 component of the cohesin
        complex is cleaved by a separase (Esp1) whose activity is held in check by a securin (Pds1)
        until separation at the metaphase-to-anaphase transition [33,34]. Separase is a cysteine
        protease that cleaves Scc1/Mcd1, presumably resulting in the cohesin complex falling apart
        and being unable to hold sister chromatids together.
        Scc1/Mcd1 at pericentric regions is protected from phosphorylation during prophase—and
        therefore dissociation from chromosomes is prevented—by proteins known as shugoshins
        [35,36,37]. In the second paper on cohesin in this issue of 
        PLoS Biology , McGuinness et al. show that shugoshin specifically
        protects Scc3 (SA2) at the centromere, so that centromeric cohesion is preserved until the
        chromosomes are ready to separate [35]. Vertebrate shugoshin has been shown to have a
        strong microtubule-binding domain [36] and is found at the kinetochore [37]. Recent
        evidence suggests that shugoshin in 
        S. cerevisiae may sense tension between sister chromatids,
        acting as part of a spindle checkpoint that monitors whether chromosomes are properly
        aligned on the mitotic spindle [38]. It is currently unclear why the cell has two
        mechanisms to dissociate cohesin from chromosomes, although it is interesting to speculate
        that this could be related to different functions of cohesin at chromosome arms versus
        pericentric domains. For instance, cohesin in chromosome arms may help to organize or
        condense chromosomes, whereas cohesin at centromeres may be more directly involved in
        chromosome bi-orientation at the spindle and segregation. These functions may be important
        during different phases of the cell cycle.
      
      
        A Link between Chromatin and Cohesin
        Several results suggest that transcription and cohesin binding are incompatible. In 
        Drosophila , one of the components that loads cohesin (Nipped-B
        or Scc2) has also been shown to be required for long-range promoter–enhancer interactions
        [39,40]. One model proposed to explain this result is that cohesin can prevent long-range
        promoter–enhancer interactions and that removal of cohesin can restore these interactions
        and allow transcription to occur [41]. In this model, Nipped-B or Scc2 can act as both a
        loading factor and an unloading factor for cohesin. This model further speculates that
        rather than Cornelia de Lange syndrome stemming from a cohesin loading defect, the failure
        to unload cohesin from regions that need to be transcribed leads to transcriptional defects
        that cause the syndrome. In 
        S. cerevisiae it has been shown that driving transcription
        through a centromere via an inducible promoter prevents cohesin from associating and
        results in chromosome missegregation and cell death [13]. Cohesin is found at the
        boundaries of the HMR locus, the right telomere of Chromosome III, and the RDN1 array, all
        regions of silent chromatin [16]. Cohesin localizes to intergenic regions where
        transcription is converging [12,17].
        Since transcription and chromatin configuration are intimately related, it is possible
        that chromatin may play an important role in the localization of cohesin. Indeed, the
        chromatin remodeling complex RSC (remodels the structure of chromatin) has been shown to be
        important for establishment of cohesin binding [42], and another study suggests RSC is
        particularly important for cohesin association with chromosome arms [43]. The chromatin
        remodeling complex ISWI (SNF2h) has been shown to be essential for cohesin to localize to
        Alu repeats (certain DNA sequences) in human cells [44]. The possibility also exists that
        cohesin itself may influence transcriptional status and act as a transcriptional boundary
        [39,40,45]. The preferential location of cohesin in heterochromatin in pericentric regions
        in 
        S. pombe also supports the idea that chromatin
        modification/structure is a key determinant of cohesin localization [46,47]. It is
        interesting to speculate that chromatin differences and transcriptional differences between
        chromosome arms and centric regions will turn out to be related to different mechanisms for
        loading and removal of cohesin from these regions.
        While one of the primary roles for chromosome cohesion in bi-orientation and mitotic
        chromosome segregation is well-established, the complexities of the regulation of cohesion
        are still being discovered. Cohesin may be involved in multiple ways in chromosome
        dynamics. Future studies focusing on the differences between cohesion at chromosome arms
        versus pericentric domains and the link between cohesion and transcription will likely
        yield very interesting insights into the function of the cohesin complex in the maintenance
        of genome integrity.
      
    
  

  
    
      
        
        Imagine this scenario. You're the director of one of the world's largest medical
        research charities, and you receive notification from one of your funded investigators in
        Africa reporting some exciting progress toward the development of a vaccine for malaria.
        The work has just been published, so you log onto the Web to do a quick keyword search, and
        a link to the article is brought up on your screen.
        Then imagine the frustration when you click on the link to read the message, “Access
        Denied—access to this journal is restricted to registered institutional and individual
        subscribers.”
        And there's the rub: this actually happened to the Director of the Wellcome Trust. Prior
        to this, the committee that advises the Wellcome Trust Library were already asking whether
        the Trust should adopt a formal position on the continually increasing prices of journal
        subscriptions and the problems this trend was causing research libraries.
        These events encouraged the Trust to investigate the publication of scientific research,
        to see if there was anything research-funding organisations could be doing to stimulate
        change in what appears to be a failing market. As it turns out, there is quite a lot. I now
        believe it is the funders of research—charities, governments, and other publicly funded
        bodies such as national research agencies—who hold the purse strings that can untie
        scientific discoveries from a publishing market that is no longer serving the community as
        well as it could. That is why today the Trust is a leading advocate for enabling free
        access to research literature through support for new publishing models, such as that of
        the Public Library of Science, and the establishment of publicly accessible repositories,
        working in partnership with the United States National Institutes of Health–funded PubMed
        Central [1].
        It is worth noting that the Trust is not a novice in seeking better ways to disseminate
        research findings. The fact that the sequence of the human genome is an openly accessible
        work is due in large measure to the Trust's determination that this information be in the
        public domain and not hidden behind commercial subscriptions. As a consequence of that
        insistence, we believe, these data are a more widely used and valuable resource.
        
          “Trust-funded researchers will have to deposit an electronic version of their
          manuscripts in PMC to be made available for free via the Internet within 6 months of
          publication.”
        
        The Trust began its investigation of the scientific publishing sector by commissioning
        two pieces of research: one to inform itself of the economics of the publishing sector, and
        a second to explore whether there were alternative business models out there that could
        enable research to have the quality assurance it needs (peer review) whilst being available
        for free, using the Web as the medium of publication.
      
      
        The Economics of Publishing
        The first Trust-commissioned study described how scientific research publishing has
        traditionally worked and why it can be described, in economic terms, as a failing market
        [2]. Essentially, the producers (researchers as authors) and the consumers (researchers as
        readers) are isolated from any of the costs within the system. Researchers give away the
        copyright to their work, for free, to the publishers, who organise the peer review and
        copyedit the article. The publishers then sell it to libraries at prices that range from
        enough to cover their costs through to some pretty high profits—some over 30%. These
        profits escape from an otherwise self-contained financial cycle to satisfy shareholders or
        run learned societies; unlike typical publishing relationships, none are returned directly
        to the author (the researcher who wrote the piece) or even to the consulting experts (the
        researchers who provided the peer review).
        At the same time, researchers as readers access the material, if they are able to do so,
        through their employing institution, either using the library or—more typically now—via the
        Internet through the institution's subscription. To the researcher this access appears
        free, effectively creating a market system that has no pressures from the producers or
        consumers to change. One consequence of this is that publishers have been able to increase
        subscription prices well above inflation; the United Kingdom has seen subscription rates
        rise by more than 200% in the last ten years (Blackwell's periodical price indexes; [3]).
        The money used to fund UK libraries is all public money, and over 90% of the funds paying
        for research in the UK university system is either government or charitable [4]—so in a
        sense the people who are paying for the research cannot access its findings without paying
        an additional fee.
        This then begs the question of what alternatives there are to this traditional system,
        now that the Internet has become the researcher's tool of choice for searching and
        accessing the literature. The second piece of research commissioned by the Trust looked at
        different business models for research publishing, in order to address this question [5].
        It compared open-access journals, which often levy a charge to publish but provide the
        journal for free, and the majority of the traditional models, which take the research for
        free but charge readers to read it.
        This study convinced the Trust that the best way forward to improve access to research
        findings would be through open access to scientific research articles. This essentially
        means two things: first, that the copyright holder or holders must grant to the public a
        free, irrevocable, perpetual license to use, copy, distribute, and make derivative works of
        their research article, in any medium for any purpose (excepting those that constitute
        plagiarism or other dishonest acts, of course); and second, that a digital copy must be
        deposited in an open public archival repository (for example, the US National Library of
        Medicine's PubMed Central). Whilst a debate continues as to the most appropriate route to
        achieve open access to all research literature, it is important to bear in mind that the
        publication and the archiving of research articles are intrinsically linked. Both aspects
        of open access need to be explored and experimented with, and the Trust is actively
        pursuing solutions for the problems of both.
      
      
        Alternative Business Models
        The findings of the second report seem to have caused quite a controversy—particularly
        in the suggestion that moving wholesale to an open-access publishing model might produce
        savings of up to 30% [6]. One common misinterpretation of this conclusion is that any such
        savings would be due solely to discontinuing the printed versions of publications that are
        freely available online. This is incorrect. In fact, if savings are to be made in an
        open-access model, they will largely be found in the variable costs of journal
        production—since an open-access journal will not have to cover the costs of subscription
        management, licence negotiations, or sales, and little is required for marketing and
        distribution.
        In a comparison included in the report, an article in a good- to high-quality journal
        produced in the subscription model is estimated to cost US$2,750. The equivalent cost under
        an author-side payment model is estimated as US$1,950—a comparable saving of 30% on the
        costs, and a saving of 90% when the variable costs are compared. It must be remembered that
        cost does not equate to price, so to these figures, regardless of the mode of publication,
        must be added overhead expenses and, of course, profit. However, if a truly competitive
        market is created—where payments are directed to publishers not by third parties but by
        those directly involved in the scientific enterprise, who could easily compare the varying
        article processing charges of different open-access publishers—then the actual savings
        might well be substantially higher.
        At its essence though, the open-access debate is not about economics, it is about
        access. That is why the Trust has been in discussion with the US National Library of
        Medicine about the possibility of creating a UK PubMed Central (UKPMC) as a publicly
        accessible repository for Trust-funded research.
      
      
        UK PubMed Central
        The proposal is that a UKPMC will be run as a proper electronic library: it will
        collect, collate, and archive whole journals and be developed to receive single articles as
        well. Submission will be as straightforward as attaching a document to an email. UKPMC will
        be able to accept manuscripts in any format, including Microsoft Word, and it will be the
        responsibility of UKPMC to convert the files it receives into extensible markup language
        (XML) to enable the appropriate document type definition (DTD) to be assigned. UKPMC will
        also correct the structural, content, and consistency errors that occur when converting
        text for digital preservation, and provide the conversion process to print a “clear” PDF
        version of included articles to those users who download them. This is a process well used
        by the National Library of Medicine, and the one most suited for the long-term, digital
        preservation of articles.
        And once articles are in a digital format they can be searched and used in different
        ways. For example, genome sequence data, chemical compounds, or protein structures embedded
        within an article can be searched for in other articles and linked directly to genome or
        structural databases uncovering new genetic markers, drug uses, or protein functions. The
        articles themselves become live research material greatly improving the efficacy of the
        research itself.
        For a funder, having all its research in one format, “under one roof”, and searchable
        will improve the efficiency of strategy setting—for example, setting funding
        priorities—assessing the outputs of the funded research, and even gaining an insight into
        the impact of the work. As grants management becomes more electronic, there can be a direct
        link between original research proposals and the research outputs.
        For a medical charity like the Trust, I believe it is our duty to actively encourage the
        most efficient processes available to maximise the likelihood that the research we fund
        will have the greatest possible health benefit.
        That is why the Trust will be making it a requirement of its grant conditions that
        Trust-funded researchers deposit an electronic version of their manuscripts in UKPMC to be
        made available for free via the Internet within 6 months of publication. The delay means
        that this is not open access in the truest sense. However, the Trust considers that the
        development of a PubMed Central portal in the UK offers the best next step in the
        transition towards a situation where all high-quality peer-reviewed research is available
        for free via the Internet, whilst leaving all publishers room for manoeuvre in this
        changing market.
      
    
  

  
    
      
        
        Since their discovery in the 1970s and 1980s, giant tubeworms at hydrothermal vents and
        cold seeps have fascinated biologists and laymen alike—not only for their alien morphology
        (Figure 1), but also for epitomizing the perfect animal–microbe symbiosis. They are among
        the biggest worms on this planet—some over 3 m long—yet they do not eat other organisms.
        Tubeworms thrive independently of photosynthetic production [1]. They have even lost their
        entire digestive tract. One of the most exciting findings in early tubeworm research was
        the discovery that the worm's food is delivered by bacterial symbionts [2]. The
        chemoautotrophic symbionts live intracellularly in a specialized worm tissue called the
        trophosome. They are sulfide oxidizers, using the free energy yield from the oxidation of
        sulfide with oxygen to fix carbon dioxide with their bacterial Rubisco enzyme. In exchange
        for providing nutrition for the worm, the symbionts are sheltered from grazing, but most
        importantly, they receive a steady source of sulfide and oxygen via the highly adapted
        blood circulation system of the worm. (I will never forget how horrified I was as a young
        student by the amounts of almost human-like blood flowing into my lab dish while dissecting
        tubeworms to analyze trophosome enzyme activity.) Tubeworm blood physiology, in particular
        the hemoglobin molecules, are tailored specifically to the needs of the symbionts. However,
        the host metabolism in itself is not different from that of many other animals, the main
        source of energy being aerobic respiration of carbohydrates. In other words, tubeworms and
        their symbionts need oxygen as an electron acceptor—so, after all, they are dependent on
        photosynthesis, the main oxygen-producing process on earth.
      
      
        Classification of Host and Symbiont
        With their strange morphology, vent tubeworms were first classified as a novel phylum,
        Vestimentifera [3]. Recently they have been regrouped together with the pogonophoran
        tubeworms (Figure 2) into a family of annelid polychaetes called the Siboglinidae [4,5].
        Vestimentiferan tubeworms of hydrothermal vents grow on chimneys and other hard substrates
        in the vicinity of active vents, which emit reduced compounds like hydrogen and sulfide
        [6]. Vestimentiferan tubeworms living at cold hydrocarbon seeps, i.e., the lamellibrachids
        and escarpids, are adapted to a sedimentary environment, with a substantial part of the
        body and tube of many species extending into the mud. All vestimentiferan tubeworms found
        today at vents, seeps, and a few other reduced submarine habitats harbor sulfide-oxidizing
        endosymbionts in their trophosome. These symbionts belong to bacteria of the
        gamma-proteobacteria clade and are phylogenetically related to each other [7]. (For the
        only known exception see [8].)
      
      
        Tubeworm Mysteries
        The study of tubeworms is now in its fourth decade, and there are still many fascinating
        problems to be solved. One of the most interesting—but also most difficult—questions in
        tubeworm symbiosis is how this obligate and highly integrated interaction between microbes
        and animals evolved. How can a worm evolve into a perfect home for chemosynthetic bacteria?
        What are the main evolutionary steps towards this symbiosis, and in which order did they
        occur? Another intriguing problem is how the worms acquire their endosymbionts, which
        appear to be taken up from the environment—but so far have not been detected as free-living
        forms. How does the host recognize its specific symbiont from the vast diversity of
        gamma-proteobacteria and sulfide oxidizers in the environment? Furthermore, how do
        tubeworms populate new vents, seeps, and other reducing environments emerging from the
        ever-changing ocean floor—how do their larvae migrate and settle, and what determines the
        distribution and lifetime of tubeworm populations in the different mid-ocean ridge and
        continental margin habitats? Although these questions are still to be answered, new
        research and techniques are beginning to provide intriguing clues.
      
      
        Seep Vestimentifera and Their Energy Source
        At some seeps the vestimentiferan tubeworms are so abundant that they form a special
        habitat that is attractive for a host of other marine species [9]. Seep vestimentiferans
        are usually thinner, have slower growth rates, and have greater longevity than their vent
        relatives [10]. For example, a 2-m-long 
        Lamellibrachia luymesi individual is estimated to be more than
        200 y old and hence represents the longest-lived animal on earth [11,12]. At seeps,
        geological processes causing fluid and gas seepage can last hundreds to millions of years,
        whereas hydrothermal vents often have a lifespan on the order of decades. Vent tubeworm
        colonies will die when their chimneys stop venting, i.e., delivering sulfide, so they are
        adapted to a rapidly changing environment, as typified by their fast growth and high
        reproduction.
        Like vent vestimentifera, seep vestmentifera also depend on the availability of sulfide
        in their direct vicinity, but they are sessile, and anchor on hard substrates such as
        carbonates. Individual aggregations at seeps can consist of hundreds to thousands of worms,
        requiring sulfide fluxes of half a mole per day—and this for more than 200 y [12]. So an
        ecological problem that has always intrigued biologists and geochemists alike is how these
        tubeworms obtain their energy over the long term. Because vent and seep vestimentifera
        depend on sulfide-oxidizing symbionts, their distribution is limited to habitats with high
        sulfide fluxes lasting for at least a few reproductive cycles. However, at cold seeps,
        unlike hydrothermal vents, most of the chemical energy occurs in the form of hydrocarbons.
        Cold seeps are characterized by high fluxes of methane, higher hydrocarbons (such as
        ethane, propane, butane), and/or petroleum from deep subsurface reservoirs. Often the
        source fluids and gases do not contain much sulfide, because there are no high-temperature
        seawater–rock interactions involved in their formation, as there are at vents. Some
        pogonophoran tubeworms at seeps have teamed with methane-oxidizing symbionts to profit from
        the high availability of hydrocarbons, but seep vestimentiferans do not appear to be able
        to directly tap this resource. However, seep vestimentiferans are still capable of
        producing enormous biomass over many years with the help of their sulfide-oxidizing
        symbionts. So where does the supply of sulfide come from at seeps that enables such large
        aggregations to be maintained for so long?
        Only recently was it realized that anaerobic microbial processes, namely, the oxidation
        of hydrocarbons with sulfate, could produce astonishingly high fluxes of sulfide in cold
        seep settings [13,14]. At methane seeps, methanotrophic microbial communities inhabiting
        the surface sediments oxidize methane with sulfate, which results in very high sulfide
        fluxes [13]. If the seepage consists of other hydrocarbons such as petroleum, their
        degradation with sulfate supports an even higher production of sulfide [14]. In some seep
        sediments, sulfide concentrations can reach 25 mM in subsurface sediments (5–10 cm below
        the sediment surface). Such concentrations are not known from tubeworm habitats at
        hydrothermal vents.
        However, the zones of high hydrocarbon turnover and sulfide flux at seeps are often
        limited to only a few centimeters below the seafloor, depending on hydrocarbon flows and
        the rate of sulfate transport from the bottom water into the sediments. Sulfate is crucial
        because the free-living hydrocarbon-degrading microbes in seep sediments depend on this
        electron acceptor for an energy yield. Without sulfate to fuel the oxidation of
        hydrocarbons, sulfide production stops, even if there is still an enormous reservoir of
        hydrocarbon available. How might tubeworms, sulfide-oxidizing symbionts, and benthic
        hydrocarbon degraders overcome these limitations?
      
      
        Ménage à Trois—A Model Solution
        Cordes et al. [15] have now provided an answer to how the stability of sulfide
        production is maintained over such long periods and how the worms optimize sulfide uptake.
        Seep vestimentifera have specific adaptations to their habitat. A main adaptation is the
        subsurface part of the lamellibrachids called a “root.” The tubeworm root appears to have a
        special function in the energy cycle of the organism—as in plant roots. Several authors
        have proposed that the worm roots are not only important in sulfide uptake, but generally
        in geochemical engineering of the sediments in the direct environment [16,17,18]. Obviously
        such hypotheses are very difficult to test—today it is still hardly possible to measure
        gas, petroleum, and sulfide fluxes in the seafloor in situ at depth, especially below
        tubeworm aggregations. But it is also not possible to recover whole aggregations of worms
        and to keep them alive in the lab for biochemical and biogeochemical measurements—this
        would require simulation of seepage under pressure. Instead, Cordes et al. [12,15] have
        used geochemical and biological modeling to solve the intriguing question of seep
        vestimentiferan longevity and how they might also interact with free-living anaerobic
        microbes to increase sulfide availability.
        To explain the persistence of the large tubeworm colonies in the Gulf of Mexico, Cordes
        et al. suggest a broader mutualistic interaction between the tubeworm, its endosymbiont,
        and benthic hydrocarbon-degrading and sulfide-producing microbes. Seep tubeworms take up
        sulfide from the sulfide-rich subsurface sediment zones through the roots, but, crucially,
        they may also release sulfate through the roots as a byproduct of sulfide oxidation by the
        tubeworm's endosymbiont. Sulfate may also be ventilated through the tube into the
        sediments. Since anaerobic microbial communities in subsurface hydrocarbon-rich sediments
        are limited by sulfate influx, any additional supply of sulfate enhances their production
        of sulfide. Furthermore, the removal of sulfide by the worm will thermodynamically favor
        anaerobic hydrocarbon oxidation coupled to sulfate reduction. Hence, the tubeworm roots may
        provide an excellent habitat for anaerobic hydrocarbon oxidizers. For example, Cordes et
        al. predict in their model that nearly all of the sulfate released through the root will be
        utilized by benthic microbes for anaerobic hydrocarbon degradation in the direct vicinity
        of the worm. This process could provide 60% of the sulfide needed by a tubeworm aggregation
        to persist for 80 y. Hence, it may even be concluded that tubeworms farm anaerobic
        hydrocarbon degraders to provide a steady supply of sulfide to their endosymbionts.
        Especially at petroleum seeps, this would guarantee a lifelong energy source and help
        explain the extraordinary longevity of the worms. The mutual benefit arising from the
        association of sulfide oxidizers, sulfate reducers, and a host worm is known to be
        exploited by the oligochaete 
        Olavius algarvensis [19]. In this very effective “ménage à
        trois” the sulfate reducer has even become an endosymbiont of the worm. Interestingly, some
        of our recent studies at the methane seeps of Hydrate Ridge (Cascadia margin) also show
        that certain populations of anaerobic methane oxidizers are specifically associated with
        seep organisms—such as the symbiotic clam 
        Calyptogena and the giant filamentous sulfide oxidizer 
        Beggiatoa [20]. But many more examples may be out there, of
        bacterial and archaeal populations specifically growing in the “rhizosphere” of benthic
        organisms, potentially profiting from bioturbation, bioirrigation, fecal deposits, and
        exudates.
        The association and interaction between benthic fauna and sedimentary microorganisms is
        a very interesting field of study, although inevitably still very speculative. So far it
        has been limited by a lack of appropriate technologies, not only for in situ biogeochemical
        and biological measurements, but also for quantitative investigation of specific functional
        microbial populations. Some insight can be provided by clever environmental modeling
        approaches—such as the one developed by Cordes et al., but ultimately the models need
        empirical verification. Only very recently has it become possible to combine visually
        targeted sampling (Figure 2) and high-resolution measurements of geochemical gradients with
        molecular tools for the identification of microbes, such as 16S rDNA and
        organic-biomarker-based techniques. For the study of continental margin and deep-sea
        ecosystems, this requires the availability of underwater vehicles (Figure 3) as well as
        multidisciplinary research platforms and extensive, highly detailed lab work—so this is
        very expensive research. Yet this is the future, if we want to determine whether such an
        intriguing ménage à trois as proposed by Cordes et al. accounts for the presence and
        longevity of these extraordinary tubeworms, and possibly also other chemosynthetic
        symbioses, forming some of the most fascinating marine ecosystems at continental
        margins.
      
    
  

  
    
      
        
        The Fourth International Conference on Neuroesthetics was not a large event, but it was
        an unusual one. Held on a single day in the basement auditorium of the Berkeley Art Museum
        at the University of California at Berkeley, it brought together a typically motley
        collection of intellectuals who would willingly give over a sunny Saturday to an
        opportunity to learn from a panel of distinguished speakers. This was not the unusual part.
        Nor was it unusual that the meeting was touted as an interdisciplinary event, bringing
        together the best and brightest of different fields. These days, and perhaps it has always
        been the case, interdisciplinarity is the rule rather than the exception of innovative
        science.
        What set this meeting apart was the fluid progression from art to science, in content as
        well as in style. The artists were more or less scientific, the scientists more or less
        artistic. The topic was empathy (“Empathy in the Brain and in Art”)—more particularly,
        man's (and not just man's) ability to recognize and respond to the expressions of others.
        What do we respond to in an expression and what are the mechanisms in the brain that
        underlie these responses? And as the primatologist Frans de Waal (Emory University)
        highlighted, how much of our empathic natures do we share with our ape cousins?
        In a slide presentation of her work and sources of inspiration, portrait photographer
        Judy Dater clearly captured with great sensitivity an infinite variety of poignant
        expressions. However, when asked, she could not clearly articulate the choices she had made
        in posing and photographing her subjects, could not give dimensions to the criteria she was
        using. In contrast, the performance artist Leonard Pitt had clearly made a science out of
        expression. His physical demonstrations with Balinese masks, carved into iconic images of
        happiness, sadness, or anger, gave the audience insight into the variety of subtle
        expression that could be attributed to the mask with simple postural adjustments. Happiness
        melted into melancholy, sadness into ennui. “It's not about moving,” he observed, “it's
        about not moving.”
        The psychologist Paul Ekman (University of California at San Francisco) brought the
        official stamp of academia to his science of expression, documenting in the language of
        training-dependent effects on recognition the subtle range of expressions and
        microexpressions we can identify. For a practical example, he showed a clip from testimony
        in the O. J. Simpson trial of a moment in which the infamous “houseguest” Kato Kaelin was
        caught out in a lie. A fleeting hostile look crossed his otherwise carefully schooled
        features: invisible until pointed out, unmistakable after.
        Where the artist and psychologist show us the richness of the human behavioral
        repertoire, the neuroscientist tries to break behaviors down into manageable, testable
        predictions of the associated brain activity. In contrast to the feasts of expression
        presented by other speakers, the faces representative of basic emotions used by the
        cognitive neuroscientist Ray Dolan (University College London) to study the neural activity
        engendered by expressions seemed almost too caricatured to be meaningful. But Dolan,
        introducing his subject through the portraiture of American colonial artist Gilbert
        Stewart, deconstructed the information we derive from the expressions of others into five
        categories—familiarity, identity, emotion, intentionality, and character—and was able to
        describe neural activity associated with carefully constructed experiments to probe each of
        these facets.
        Physiologist Vittorio Gallese (University of Parma) prompted many nods of satisfaction
        from the audience with his findings of activity in areas of the brain controlling movement
        when people simply watched the actions of others (see also the Research Article by Iacoboni
        et al. in this issue of 
        PLoS Biology [DOI: 10.1371/journal. pbio.0030079 ]). Susan Langer, in her
        book 
        Mind: An Essay on Human Feeling , has defined empathy as the direct
        physical reaction inherent in the perception of others, an 
        involuntary breach of individual separateness, and to see the neural
        resonance, to see that the same activity patterns were being recreated in actor and
        observer, was to give substance to the intuition of empathy.
        Themed meetings, particularly when the theme does not conform to one discipline, are
        hard to pull off. It can be nearly impossible to convince successful professionals on the
        lecture circuit to modify the presentation of their own work to support such a theme. In
        that respect, this meeting was no different from many—some speakers were hard-pressed to
        conform to the theme, and it is not clear that many attendees learned information of
        practical value to their work from speakers across disciplines.
        However, it is not often that scientists have the luxury of stepping back and
        appreciating the context of their work in quite this way. It is not, for instance, usually
        appropriate to begin a paper on an apoptotic signaling pathway with a philosophical
        digression into the nature of Death. The abstract dimensions that the visual neuroscientist
        Alice O'Toole (University of Texas at Dallas) gave to facial characteristics are supposed
        to shed light on how we instantaneously recognize the friend we have not seen in 30 years.
        The electrophysiological signals in the brain that neurophysiologist Aina Puce (West
        Virginia University) described when we view simple movements is ultimately meant to explain
        how we identify with the subtle shrugging of shoulders that can transmute insouciance into
        insecurity.
        By reducing the problem to its simplest, most controlled form, scientists hope to shed
        light on the complexities of life. Auditory physiologists are supposed to tell us how we
        hear. And yet it will be a long time before they can explain “music heard so deeply that it
        is not heard at all, but you are the music while the music lasts” (T. S. Eliot, as quoted
        by the conference organizer, Semir Zeki [University College London]). But the richness of
        the goal makes the journey all the more rewarding.
      
    
  

  
    
      
        
        Harrowing tales of starvation and endurance epitomize Antarctica's “heroic age,” when
        men equipped with little more than fortitude struggled against a landscape seemingly
        designed to thwart their intentions. A menacing sea ice figures prominently in these
        improbable survival stories. Daunting to early-20th-century explorers—trapping (and
        ultimately crushing) Ernest Shackleton's 
        Endurance and derailing Robert Scott's 1901 
        Discovery expedition—the seasonal pack ice is the lifeblood of
        Antarctica's marine ecosystem.
        But as winter temperatures continue to climb in the Antarctic, the once-forbidding
        winter sea ice is starting to deteriorate. The ice pack is forming later and retreating
        earlier—and it's having a serious impact on the abundance of krill, the backbone of the
        Antarctic food chain.
        “Sea ice is the engine that drives Antarctic ecosystems,” says William Fraser, a
        principal investigator for the Palmer Long Term Ecological Research program (LTER) and
        president of Polar Oceans Research Group in Montana. “Many of the key species that govern
        ecosystem dynamics in the Antarctic have life histories that depend on the availability of
        winter sea ice. The most important species in most sectors of Antarctica is krill.”
        A major food source for Antarctic fish, penguins, pelagic seabirds, seals, and whales,
        krill (
        Euphausia superba ) look like shrimp, but weigh just a gram as
        adults and measure about six centimeters long (Figure 1). Norwegian for “whale food,” krill
        aggregate in super-swarms that can reach a density of 30,000 individuals per square meter,
        attracting whales, which can eat three tons of krill in a single feeding, and fisheries,
        which net on average 100,000 metric tons per year.
        The waters off the Antarctic Peninsula favor high krill concentrations. “Once you get
        into extreme environments such as the Southern Ocean, diversity will decrease but the
        number of individuals will increase because the production can be very high,” says Scott
        Gallager, a marine biologist at Woods Hole Oceanographic Institution (Woods Hole,
        Massachusetts, United States).
        Production is particularly high along the sea ice edge, he says, because the ice is
        thinner, which allows more sunlight to penetrate, and because ocean mixing processes along
        the continental shelf cause an upwelling of nutrient-rich deep water. Increased nutrients
        support increased primary production along the ice edge. “If ice forms too late,” says
        Gallager, “you don't get this higher production, which impacts zooplankton populations like
        the larval krill that graze the under-ice surface, feeding on ice algae. The ice edge is an
        absolutely critical habitat, a nursery, for larval krill.”
        But this krill hotspot is showing some of the most dramatic changes in sea ice extent.
        Fraser says the western Antarctic Peninsula has registered the “largest increase in
        temperatures on the planet”—on the order of 6 degrees Celsius—over the past 50 years. This
        warming trend, he says, has been particularly pronounced during the winter—“crunch time”
        for many key Antarctic species.
        “There was a time when almost every winter experienced very heavy sea ice,” Fraser says,
        “reaching out into the Drake Passage.” Where once heavy sea ice would form on average four
        out of every five years, he says, now it forms just one or two years out of five (Box 1).
        In a typical year, ice starts to form in the coldest regions along the southern coast in
        late March/early April (austral fall), then works its way up the coast. But late ice years
        are becoming more common, with ice forming two to three weeks later. “These patterns are
        completely different from patterns that existed as recently as 30 to 40 or 50 years ago,”
        says Fraser. “The whole system is becoming unhinged as a result of this enormous
        warming.”
      
      
        Retreating Sea Ice and Krill Declines
        In November 2004, the most comprehensive study to date of krill distribution and
        abundance in the Southern Ocean reported a catastrophic drop in krill numbers. Angus
        Atkinson, a marine biologist with the British Antarctic Survey, led the study. “We pulled
        together all the net samples we could lay our hands on that had been obtained in the
        Southern Ocean over the last 80 years,” Atkinson says, analyzing nearly 12,000 krill summer
        net hauls taken from 1926–1939 and from 1976–2003.
        “The Southern Ocean is an enormous area, and at least half of krill stocks were in this
        comparatively narrow sector between South Africa and the Antarctic Peninsula,” he says. His
        team found a positive correlation between winter sea ice cover and the abundance of krill
        the following summer. There's evidence of a general decline in winter sea ice extent and
        duration, Atkinson says, and of a general decline in krill populations—down 80% over the
        past 30 years—over the entire southwest Atlantic sector.
        Though krill populations showed big fluctuations in the early years, their average
        numbers were higher over a longer period, explains Volker Siegel, a krill biologist with
        the Sea Fisheries Research Institute in Hamburg, Germany, who worked with Atkinson. “Where
        in the early days you might have 100 krill per square meter on average—with fluctuations
        between, say 20 and 300—nowadays you might see 20 per square meter, which goes from 50 to
        five individuals per square meter.”
        Neither Atkinson nor Siegel can say for sure what's causing the decline, but both say
        the winter sea ice is clearly playing some role. Krill live about five to six years. During
        the breeding season, from December to March, embryos are released in the upper water
        column, and the larvae hatch at depths ranging from 400 meters to 1,500 meters, unlike many
        fish and other invertebrate larvae, which hatch at the surface. Larvae then have to swim up
        through the water column to reach the sea surface. Unlike adults, krill larvae don't have
        enough body fat to carry them through food shortages. “They'll starve if they have to rely
        on water-column food distributions, and that's where the sea ice comes in,” says Atkinson.
        “The ice may also shelter them from predators, but one way or another, ice in the winter is
        good for young krill.”
        “We've got to find out what's causing these changes and then we can start to predict
        what's going to happen with future scenarios of climate change,” Atkinson says. “The other
        thing we've got to do is look at alternative things which might be affecting krill. We
        might find there are other things declining as well as sea ice, such as their food, or
        there might be a change in the fertilization of the waters.”
        Accompanying the drop in krill abundance, Atkinson and Siegel found an increase in
        salps, transparent jelly-like creatures that typically inhabit warmer waters than krill.
        Expanding into the warmer waters, salp populations are increasing in the southern part of
        their range and replacing the krill. Most krill-dependent predators do not eat salp.
      
      
        Krill Declines Ripple up the Food Chain
        At a hearing on climate change impacts before the United States Senate Committee on
        Commerce, Science, and Transportation in May 2004, LTER's Fraser testified that the western
        Antarctic Peninsula's cold, dry polar marine ecosystem is gradually being replaced with a
        warm, moist maritime climate. While all the major components of the food web are responding
        to these changes, Fraser said, the clearest evidence comes from studies of two especially
        sensitive indicators of climate change: krill and penguins. “Trends in penguin populations
        provided some of the first evidence that sea ice conditions in some areas were
        deteriorating in response to climate warming,” Fraser told the senators.
        This evidence came from Fraser's own studies, over the past 30 years, of three Antarctic
        penguin species that share similar life histories (including a penchant for krill) but show
        striking contrasts in their relationship to the sea ice (Figure 2). For Adélie (
        Pygoscelis adeliae ) penguins, the presence of sea ice is
        absolutely essential for survival. Chinstrap (
        P. antarctica ) and gentoo (
        P. papua ) penguins, on the other hand, require the absence of
        sea ice. “Adélie penguins have experienced a nearly 70% decrease in their populations at
        our study sites on Anvers Island [Palmer Station] in the western Antarctic Peninsula over
        the last 30 years,” says Fraser, “and there's evidence that other krill-dependent predators
        are beginning to decrease.”
        That's a loss of 10,000 breeding pairs since 1975. The major factors underlying this
        precipitous decline, Fraser says, are retreating sea ice and increasing snowfall. (The loss
        of sea ice increases the flow of water vapor from the open ocean to the atmosphere,
        increasing precipitation.) “When sea ice forms,” he explains, “it covers these regions of
        high production and the birds are just able to plop into the water into very good feeding
        areas.” With a life history accustomed to the formation of sea ice at critical points in
        their life cycle, Adélies are finding themselves faced with an unpredictable sea ice cycle
        that outpaces their ability to adapt. “The birds just don't have the sea ice when they need
        it,” Fraser says. As if losing critical winter habitat weren't bad enough, Adélies must
        also contend with the effects of increased snowfall. When the snow melts in the spring,
        it's flooding their nesting areas and drowning their eggs and chicks.
        The population trends for “ice-avoiding” chinstraps and gentoos are quite different.
        Both species are increasing their populations and beginning to replace Adélie penguins
        across a broad range in the western Antarctic Peninsula. “We're seeing breeding gentoo
        penguins at Palmer Station,” Fraser says, with a trace of astonishment. “The
        paleoecological record does not show that species at our study sites in the last 800
        years.”
        Though chinstraps and gentoos also depend on krill, they've dealt with krill declines by
        eating more fish and squid (which, not incidentally, also eat krill). Fraser believes this
        dietary flexibility, along with the increased availability of open water and a late
        breeding schedule (which protects their eggs and chicks from spring snow meltwater),
        largely explain their range extensions.
        “Adélies don't seem capable of adjusting anything about their life history,” says
        Fraser. “They're hard-wired to their breeding area, returning to an area year after year
        after year, even though conditions are deteriorating.” And so, their numbers continue to
        plummet as more chicks perish. Another ice-dependent, krill-eating penguin species, the
        emperor (
        Aptenodytes forsteri ), is not faring any better. “We have just
        one emperor colony in our study region, and it's decreasing very fast,” Fraser says. “It's
        gone down from 300 breeding pairs to nine. They're on the verge of extinction in our study
        region.”
      
      
        A Question of Sustainability
        A major issue in devising strategies to protect krill populations concerns the impact of
        krill fisheries. Since drastic declines have occurred in the absence of heavy fishing, it's
        especially important to establish the population dynamics of Antarctic krill. Siegel works
        with the Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR) to
        develop sustainable fishing regulations. Based on a point estimate of 44 million metric
        tons of krill in the southwest Atlantic sector in 2000, CCAMLR calculated a potential yield
        of 4 million tons, far above the average 100,000-metric-ton catch today. Siegel believes
        the fisheries aren't posing a significant threat to krill stocks at this point, but says
        much remains to be learned about krill population structure.
        Gallager is not so sure about the impact of krill fishing. Some krill fisheries operate
        near the island of South Georgia, east of the Falkland Islands. “We know that these
        populations are not self-sustaining, but require recruiting of adults from other locations,
        and probably from regions along the western peninsula of Antarctica,” says Gallager. “If we
        go in and fish out the populations that are not self-seeding, then they could be entirely
        wiped out.” That's why it's crucial to know which populations, if any, are self-sustaining,
        he says. “And we don't know a lot about that at all.”
        Toward that end, Gallager is working on an under-ice “zooplankton observatory” that
        would sit on the bottom of the ocean floor and continuously release a flotation
        package—“once an hour, for hopefully the next ten years”—outfitted with sonar and optical
        sensors up to the surface, then winch itself back down again. The plan is to deploy the
        observatory off Palmer Station in May 2006. The hope is that data gathered on
        over-wintering larval krill will shed light on the factors influencing krill survival over
        the long term. “It may be that this coupling between larval krill and ice is actually
        underpinning the entire question of krill population dynamics,” says Gallager—in which
        case, understanding how the ice moves relative to the water currents and wind shear over
        the top will be critical.
        Many other aspects of krill biology remain obscure as well. Biologists still don't
        understand the precise mechanisms required to enhance larval growth, reproduction, and
        recruitment (replenishing populations with new individuals), or how temperature
        fluctuations affect metabolism and larval growth rate. Robust tests of long-held theories
        of how the under-ice habitat sustains krill larvae require much more quantitative data—over
        time and over a wide scale—on larval abundance, distribution, and foraging behavior. But
        netting krill is not easy. Larvae tend to wedge themselves into nooks and crannies, defying
        divers' attempts to nab them while protecting their expensive nets— loaded with even more
        expensive electronic gear—from the ice.
        Whatever is behind the correlation between sea ice decline and krill declines, the
        future of the Antarctic ecosystem hangs in the balance. A 2001 report by the
        Intergovernmental Panel on Climate Change predicted that the Antarctic Peninsula will
        experience some of the largest, most rapid climate changes on earth. If these trends of
        rising temperatures and decreasing sea ice continue, says LTER's Fraser, “what we are going
        to see in the next ten, 20, 30 years is a system that is completely different from the one
        that exists now. Adélies will become regionally extinct.”
        Concluding his testimony on climate change impacts, Fraser warned the US Senate
        committee that if future warming continues and the cycle of heavy ice years exceeds the
        life span of krill, the species will face a reproductive crisis. And that, he said, “will
        have catastrophic consequences to the integrity of this marine ecosystem.”
        In 1912, for the sake of a few emperor penguin eggs, Apsley Cherry-Garrard and two
        members of Scott's ill-fated polar expedition endured what Cherry-Garrard called “extremity
        of suffering” from which only “madness or death may give relief.” The group believed the
        eggs might prove that the penguins were the missing link between “birds and the reptiles
        from which birds have sprung.”
        Our understanding has advanced light years since then, rendering such notions nearly
        quaint. Cherry-Garrard and his companions thought the forbidding Antarctic landscape immune
        to human assaults. Today, with this notion, too, proven false, one wonders if the damage
        can be reversed. In Atkinson's diplomatic phrase, “there are political issues involved”
        where global warming is concerned. But the clock is ticking. If the Antarctic ecosystem
        collapses, it won't be because scientists were off on a misguided search for penguin
        eggs.
      
    
  

  
    
      
        
        The launch of 
        PLoS Biology —only 18 months ago—was just a first step for the Public
        Library of Science. Our initial goal was to create a flagship journal for the broader PLoS
        mission by providing an open-access alternative to the best subscription journals in the
        life sciences, and to put open access firmly on the map.
        Despite its youth, 
        PLoS Biology is already becoming established as a publication of high
        standing, and 
        PLoS Medicine is rapidly heading in the same direction. Submissions to 
        PLoS Biology have steadily grown, recently surpassing the 1,000 mark, and
        every month sees international coverage of 
        PLoS Biology articles in the media. With the increasing support for open
        access in general, the time is now ripe to build on the success of these two flagship
        journals by taking the next step and launching the PLoS community journals.
        The PLoS community journals will give authors an opportunity to publish a greater range
        of high-quality papers in open-access journals, so that anyone can read, use, and build on
        their work. But these publications will also serve another function—they will provide
        examples that other journals can follow, and will increase confidence in the sustainability
        of open-access publishing as a business model. More publishers are exploring ways to remove
        existing subscription barriers and edge towards open access. And more funding agencies are
        expressing their commitment to make the findings of the research they support freely
        available to the public. The PLoS community journals will lend further weight to this
        inexorable shift towards open access.
        Each PLoS community journal will cover a broad field of research—so the journals serve
        specific scientific communities. The journals are also run by the community—academic
        editors-in-chief and associate editors, supported by PLoS staff. And some of these journals
        will be collaborations with established community groups, such as the International Society
        for Computational Biology (ISCB), with whom we are partnering for the launch of 
        PLoS Computational Biology . The twin strands of open-source software and
        public databases of biological information converge on this burgeoning discipline, and the
        case for an open-access journal in computational biology is easily made. The ISCB has taken
        a bold step, in the best interests of its discipline and membership. We hope that this
        action by the ISCB will inspire other scholarly societies to follow suit and will be only
        the first of many such collaborations between PLoS and other organizations.
        The editorial teams running the first three PLoS community journals already comprise a
        group of over 80 researchers, each headed by an editor-in-chief: Philip E. Bourne
        (University of California, United States) for 
        PLoS Computational Biology (www.ploscompbiol.org), Wayne N. Frankel (The
        Jackson Laboratory, United States) for 
        PLoS Genetics (www.plosgenetics.org), and John A. T. Young (Salk
        Institute, United States) for 
        PLoS Pathogens (www.plospathogens.org). The willingness of so many
        leading researchers to devote precious time to these new journals is testament to the level
        of commitment to open access that now exists within the research community, and the trust
        that PLoS has gained as a publisher of science and medicine.
        And what of the relationship between the PLoS community journals and 
        PLoS Biology ? Does 
        PLoS Biology still want papers in the areas in which PLoS launches new
        journals? Will 
        PLoS Biology editors be rejecting more such papers in the knowledge that
        they will find a home in fellow PLoS publications? Let us assure you that we—the editors of
        
        PLoS Biology —remain committed to publishing the best research across all
        of biology.
        Moreover, our relationship with the PLoS community journals is one of strict editorial
        independence. There is some overlap of membership on the editorial boards of 
        PLoS Biology and the community journals, reflecting a level of dedication
        to open access, but as with all scientists who serve on multiple editorial boards and
        reviewers who review for multiple journals, these individuals are governed by
        confidentiality. That said, if an author would like a manuscript that has been turned down
        by one journal to be passed on to another, along with the reviewers' reports and their
        identities, we are happy to cooperate, subject to the permission of the reviewers. This can
        help to expedite the review process, saving time for authors, editors, and reviewers. The
        editors of 
        PLoS Biology and all the PLoS journals are committed to offering a
        peer-review service that is as constructive, transparent, and efficient as possible.
        In the world of scientific publishing, there is nothing quite like launching a journal,
        especially when the case for the journal is as strong as it is for the PLoS community
        journals. It's enthralling, nerve-racking, and relentless work. But when the manuscripts
        begin to arrive, the editorial process kicks into action, and the production team starts
        crafting the first accepted articles, it's hard to contain the excitement. PLoS is still a
        relatively small organization, and all our staff have played a part in preparing for the
        introduction of the new journals. But PLoS is much bigger than the people on the payroll.
        It's the research community that is making PLoS work, as demonstrated most emphatically by
        the editors-in-chief and editorial board members who have stepped up to launch the first
        three PLoS community journals. Please join us in making them a success, and enjoy your
        share of the excitement.
      
    
  

  
    
      
        
        
          
            
              “Alchemy: a process of transforming something common into something
              special”—Webster's Dictionary
            
          
        
        A growing number of studies are reporting the isolation of cardiac stem cells from a
        variety of tissue sources and examining their effects on promoting the repair of the
        injured heart. In the current issue of 
        PLoS Biology , the storyline takes an unexpected, interesting twist, as
        Neal Epstein and his colleagues report the isolation of a novel cell type from skeletal
        muscle that can adopt a highly differentiated cardiac muscle cell phenotype in vitro and in
        vivo [1].
        In this study, the authors use a differential isolation procedure to remove the skeletal
        muscle cells and myoblasts (immature muscle cells), and then collect the cells that are
        negative for a cell-surface marker called Sca-1. Under defined in vitro conditions, these
        cells adopt a cardiomyocyte phenotype that goes beyond the simple expression of
        cardiac-restricted biochemical and molecular markers, extending to the types of single-cell
        physiological functions that are hallmarks of authentic cardiomyocytes, including action
        potentials, calcium transients, and contractile activity. They call the cells Spoc cells,
        an acronym for “skeletal-based precursor of cardiomyocytes.”
        The study goes on to show that these cells can adopt this phenotype without the addition
        of cytokines or agents such as azacytidine that are known to activate the muscle gene
        program in nonmuscle cells. The cells also adopt the cardiac phenotype following their in
        vivo implantation into the ischemic heart following myocardial infarction, suggesting a
        potential therapeutic utility for these cells. Since Spoc cells were isolated from murine
        skeletal muscle, they may eventually allow the use of sophisticated conditional genetic
        tracking techniques to monitor the migration, maturation, and differentiation of the cells
        in the in vivo context.
        Of course, a study with results this unexpected also raises a number of intriguing
        questions. Identifying the native location of Spoc cells, as well as the in vivo niche that
        insulates them from entering the differentiated cardiac program, will be valuable. A
        rigorous exploration of their developmental origin and their relationship to the other
        well-known cell types in skeletal muscle should be forthcoming. In this regard, a set of
        skeletal muscle stem cells, distinct from myoblasts, has also been found [2], and the
        question arises as to whether Spoc cells are related to these other skeletal muscle
        progenitors.
      
      
        Cardiomyocyte Precursors Abound
        Other studies have reported the isolation of cells that can differentiate into cardiac
        muscle from diverse noncardiac tissues. In vitro and in vivo studies have suggested that
        these cells can adopt defined features of the cardiomyocyte phenotype when they are
        implanted into injured heart following myocardial infarction. It is possible that these
        studies are pointing to the existence of a rare circulating pool of precursor cells, which
        are able to home to the heart.
        To understand the provenance of these cells, it will become critical to identify a set
        of gene products, or markers, that are restricted to Spoc cells and to other potential
        progenitor cells. Since the protocols for the isolation of the Spoc cells have been
        developed in the mouse, it should also become possible to use state-of-the-art spatial and
        temporal control strategies for triggering irreversible lineage tracers in these cells in
        the in vivo context without isolating the cells per se. This will make it possible to
        follow the fate of these cells as they differentiate. Similar approaches have recently been
        used to identify a subset of rare, native cardiac progenitors (which are positive for the
        marker Islet-1) in the newborn hearts of mice, rats, and humans (Figure 1; [3]). Exploring
        the relationship of the Spoc cells to these native cardioblasts could reveal shared
        pathways that drive their formation, renewal, and differentiation.
      
      
        Defining a True Progenitor
        One of the inherent caveats of the present study is the potential for phenotypic drift
        that occurs with prolonged growth of any cell type. It is possible, for example, that the
        genetic program of these cells gradually changes with time such that they can more easily
        adopt the cardiac phenotype. Heart and skeletal muscle share many key gene regulatory
        factors and downstream genetic programs, and the loss of a few key negative regulatory
        checkpoints might allow the cells to differentiate towards a cardiac phenotype. Again,
        documenting the existence of these cells in the in vivo context, particularly during stages
        of embryonic myogenesis, will be key.
        There is already a clear precedent for this type of phenotypic drift between cardiac and
        skeletal muscle. H9c2 cells were originally derived from rat heart, but actually represent
        skeletal myoblasts, because they fuse and form myotubes following serum withdrawal. These
        cells express myogenic factors in the MyoD family, and do not express cardiac-restricted
        factors. Although the cells were initially isolated on the basis of their expression of
        cardiac markers, this is an example phenotypic drift—that is, the cells have drifted from a
        cardiac phenotype during repeated passage rather than simply representing skeletal muscle
        cells resident within the heart. Thus, the isolation of an immature cell from a heart does
        not necessarily denote that it is serving as a cardiac progenitor or stem cell. Potentially
        the same issue might hold for putative cardiac progenitor cells isolated from skeletal
        muscle, where the phenotype observed may not necessarily reflect its role in vivo within
        skeletal muscle.
        One of the difficulties with alchemy and this new age of cardiac myocyte biology is
        related to defining a rigorous set of criteria that allow one to make the claim that the
        cell type of interest is truly a cardiac progenitor or stem cell and is acquiring a fully
        differentiated phenotype [4]. In this regard, the paper by Epstein and colleagues has done
        an admirable job of scoring for functional phenotypes (for example, having action
        potentials and calcium transients) that are far beyond the simple expression of cardiac
        muscle markers, which has been the phenotypic endpoint for many previous papers in the
        field. In addition, the authors go on to show the acquisition of the differentiated cardiac
        phenotype in the in vivo state in the absence of fusion with neighboring cardiac muscle
        cells, which has been a confounding variable in most other studies of this type. If a
        precursor cell simply fuses with a fully differentiated cell in a tissue, one could easily
        be fooled into thinking that the precursor has itself differentiated, as has been found for
        many types of putative cardiac stem cells [5]. Fortunately, techniques have been devised to
        detect such events.
        In surveying the growing number of studies and claims of new cells that can acquire some
        type of heart cell phenotype in vitro or in vivo (for a review see [6]), it will become
        increasingly important to create a set of rigorous criteria that would distinguish between
        the following: authentic progenitor cells that are already committed to the cardiac
        lineage; pluripotent stem cells that can infrequently adopt the cardiac phenotype;
        phenotypic drift of other muscle progenitors with an increased propensity to enter cardiac
        lineages; and a variety of other cell types that can aberrantly express cardiac markers
        ectopically or by fusion with neighboring cardiac muscle cells.
        Finding cell-type-specific markers for these cells will be critical, as opposed to
        generalized markers that do not allow the in vivo discrimination of their precise
        localization, mobilization, and differentiation in the intact muscle. The gold standard
        would then become the isolation of the cells from the intact organ after differentiation
        has occurred by creating genetically based or antibody-based approaches to identify and/or
        purify the already differentiated progeny from the intact muscle. Alternatively, new
        two-photon confocal microscopy approaches to identify the cells and then to monitor cardiac
        function in the intact heart should prove valuable [7].
        For two decades, the bulk of our knowledge of molecular pathways that guide cardiac
        growth, development, and disease has been gleaned from a combination of in vivo studies in
        genetically engineered mice and primary cultures of neonatal and adult rat cardiomyocytes.
        Perhaps the most scientifically exciting aspect of this new age of cardiomyocyte biology is
        not simply related to cardiac repair. To date, there have been no continuous differentiated
        cardiac cell lines, a fact that has hampered the field for decades. The development of
        well-characterized cardiac progenitor cells offers the promise of using real genetic-based
        approaches to rapidly define the complex pathways that guide cardiac contractility,
        excitability, and lineage diversification into atrial, ventricular, and conduction system
        myocyte cell lineages.
      
    
  

  
    
      
        
        On the basis of his retrospective at the San Francisco Museum of Modern Art, it is
        evident that Roy Lichtenstein forged a narrow trajectory through the thickets of
        contemporary art. Unlike some of his protean fellow artists, such as David Hockney or Frank
        Stella, he found his style early in his career and followed its course with almost
        scientific precision for his entire life. Through an obsessive focus on the techniques of
        graphic production, he turned popular imagery into a high form of art and an exploration of
        the limits of visual perception.
        The core concept of Lichtenstein's art is the iconic—drastic simplification of line and
        shadowing to reduce to the Platonic ideal of the object depicted—golf ball, truck tire,
        composition book, girl in love, or even the basic brushstroke of a daub of paint [1]. The
        basic inspiration of these icons is advertising imagery and comic book frames. Throughout
        his artistic life, he drew his imagery from the detritus of printed material—gum wrapper
        comics and small ads from the backs of newspapers—deliberately elevating visual sources at
        the opposite pole from the high art of the Renaissance. His first comic works were
        motivated by his sons, who challenged him by saying if he was such a great artist, could he
        draw Mickey Mouse for them? Lichtenstein was so stimulated by the result that he worked in
        the comic book style from then on. Interesting, Chuck Close tells a similar story about his
        own work, although in reverse. In his case a group of school kids were not “getting” his
        art and challenged him in the same way to prove his expertise. Close passed the test, but
        continued in his compulsive style, exploring the minutiae of the pixel structure in his
        grandiose portraits, while Lichtenstein explored the iconographic largesse of the new
        domain opened up by this challenge.
        Lichtenstein's cartoon style of representation can be seen as an ironic commentary on
        the elitism of art, implying that art is merely a selection from the endless variety of
        images that bombard us. His self-mockery is exemplified by the second picture in the
        exhibition, a six-foot canvas bearing the letters “ART” [2]. In this piece Lichtenstein is
        explicitly interrogating the artistic community on the boundaries of the concept of art. Is
        the raw symbol placed in a gallery sufficient to constitute an artistic statement? This was
        a game that artists had been playing in earnest since the turn of the 20th
        century—Lichtenstein came up with one more variation on the theme. His later work consists
        of an extensive series of graphic “reflections” on the themes of classical art, from the
        second period of Pompeii, to the light explorations of the Impressionists, to Chinese
        landscapes, all in his egregious comic book pastiche.
        The iconic is by now commonplace—the idea that cartoons capture the essence of an image
        was popularized by Fred Attneave in 1954 [3], when he drew attention to the way the outline
        of an object (and, indeed, the points of maximum curvature of the outline) captures most of
        the information of the full object image. The rest of the image can be thrown away without
        significant loss of its import. Nobel-Prize-winning experimental work conducted by David
        Hubel and Torsten Wiesel in the late 1950s consolidated this notion with the discovery that
        individual neurons in the visual cortex can be characterized as simple line detectors,
        i.e., that they are most active when a line of a particular orientation is found in a
        particular part of the visual world [4]. The work of Hubel and Wiesel illustrated that
        neural processing encodes the most relevant features in complex images. Indeed, they
        discovered a population of neurons called end-stopped cells that respond to the ends of
        lines and points of high line curvature in the manner required by Attneave's analysis.
        Interestingly, this confluence immediately predated Lichtenstein's entrée into the world of
        the comic book image, but the relationship seems to be coincidental in view of the
        iconographic symbolism of his choices. The likelihood that he had heard of these scientific
        developments seems remote, particularly in view of the fact that Andy Warhol was following
        the same track of using cartoon material, reputedly in mutual ignorance of Lichtenstein's
        breakthrough.
        Beyond the iconographic, Lichtenstein plays with the visual impression derived from
        enlarging the halftone dots of the gum-wrapper comics. Through time, the dots become
        progressively larger and more insistent, emerging from their role as background fillers to
        dominate the entire canvas in a dizzying field of scintillations. In this sense,
        Lichtenstein seems to go beyond the role of cultural expositor and gadfly to explore the
        sensory implications of the optical redundancy of the printer's screen. The regular dot
        arrays shimmer and scintillate, ingraining themselves in our neural memory and projecting
        onto the gallery walls and neighboring paintings in a reminder of our visual fallibility.
        Such effects represent a resonance with so-called optical art (“op art”), a style promoted
        in the mid 1960s—by Bridget Riley and Victor Vasarely, in particular—that relies on visual
        illusion generated at the early levels of the nervous system: the retinal, the receptoral,
        the oculomotor, and the neural. Despite its name, it is not concerned with strictly optical
        effects such as diffraction, diffusion, interference, scintillation, polarization, and
        related optical phenomena. It is concerned with the visual and perceptual effects of
        dancing grids, jazzy dots, clashing colors, sliding waves, and so on.
        Many of Lichtenstein's effects are a by-product of the printer's screen structure that
        is enlarged along with the other details of the printed image structure. His work seems to
        have been a major precursor of the op art movement, although he is not generally identified
        as a member of it. Indeed, he plays with the dot-screen as a theme in his later works,
        notably in the vast 
        Mirror in Six Panels (1971), which shows nothing but the mirror surface
        reflecting empty space, apparently rendered in the transparent sheets of Benday dots in
        common use by graphic artists. Refreshingly, this is one of the few works that does not
        contain references to other art genres, but jousts with the concept of the image itself,
        again a reflection of nothing at all.
        For the visual scientist, the most compelling painting in the exhibition may be 
        Rouen Cathedral Set V (1969) [5], a meditation on Claude Monet's
        mediation on Rouen Cathedral, itself a series of impressionistic paintings of the cathedral
        in different lighting conditions. Here Lichtenstein abandons the cartoon-style bravura of
        line and text bubbles in a triplet of silk-screen close-ups of Monet's painterly
        impressions, differing only in the choice of colors for the three panels. The dot-screen
        now plays the role of a muslin or gauze curtain through which the cathedral is glimpsed,
        forming a vibrant haze that formalizes the image space into a kind of crystallized
        transparency that never quite settles into known categories of visual experience. While the
        left and right panels of the triptych are in bold shades of color, the central panel is
        rendered in accurate red-green isoluminance. As discovered by Richard Gregory, form
        processing is much weakened when the luminance differences are removed and forms are
        represented in colors that are accurately equated for their luminance values [6]. Although
        the colors are well seen, the form seems to shimmer and fluctuate, indicating that the
        shape-processing mechanisms are not well activated by the pure color differences. In
        Lichtenstein's Monet, the shimmer of the isoluminance interplays with the shimmer of the
        dot-screen to evoke a visual enigma, as we explore the image space to see whether the
        structure is indeed the same as in the flanking panels. One of the pleasures of art is its
        ability to slow down our sensory processing so that we become aware of the processes
        themselves, not just their symbolic role in our goal-oriented lives. Very few artists have
        played with the power of isoluminance to achieve this role in form processing: Lichtenstein
        seems to have been on to this property a decade earlier than Gregory, although he soon
        retreats back to the boldness of his cartoon pop-art style to explore a potpourri of the
        icons of classic sources.
        There is much more that could be analyzed to place Lichtenstein in an art-historical
        framework, but perhaps one should just enjoy the power of the concrete image, simplified to
        its high-tone essentials and projected at large visual angle onto our excitable retinas.
        One comes away from the exhibit with a sense of the power of raw imagery that one may not
        have felt since the grade-school days of reading illicit comics when one was supposed to be
        learning the dates of battle sequences through history. But what does Lichtenstein's dot
        obsession reveal about neural processing? Why does repeated fine-grain structure wreak such
        havoc with our visual stability? This question was raised, in particular, by Donald MacKay
        with his high-density radial ray figure (Figure 1), which generates powerful complementary
        effects in both current viewing and as an aftereffect [7]. Just why high-density dots and
        lines elicit such powerful responses from our visual apparatus remains unexplained. Indeed,
        the issue does not seem even to be a topic of current research interest, despite the
        proliferation of research activities in visual processing in general. Contemplation of an
        exhibit such as Lichtenstein's sparks a realization of the wealth of neural processes still
        to be studied and explored.
      
    
  

  
    
      
        
        Jakob von Uexküll coined the term 
        Umwelt to describe the subjective world of animals. The world that
        animals perceive is not an objective, veridical representation of the physical world, he
        argued, but is instead a product of the particular sense organs that each species has
        acquired in its evolutionary history [1]. Many animals have sensory abilities that humans
        don't, such as a magnetic compass sense in birds [2] or sensitivity to electric fields in
        fish [3]. But even within sensory modalities shared by many animals, such as vision,
        hearing, and olfaction, there are strong differences between species. For example, bees,
        but not humans, can see UV light [4,5] and smell carbon dioxide [6], and bats can hear
        ultrasound [7]. But what exactly is the structure of the perceptual worlds proposed by von
        Uexküll? Can we draw them on paper in the form of maps, allowing us to visualize a
        particular animal's subjective view of the world? Will such maps allow us to predict the
        similarity of two stimuli (e.g., two colors or two scents) by inspecting the distance
        between the loci they produce in a perceptual space? Does understanding the metrics of such
        maps help us predict how stimulus mixtures will be perceived?
        Even though perceptual sensations may strike us as ethereal, they must be based on
        patterns of activity in neuronal hardware—thus, we need to look into the brains of animals
        to see how the neuronal circuitry processes the information from the sense organs. The
        study by Guerrieri et al. [8] on odor space in honeybees in this issue is an excellent
        example of how behavioral studies can be paired with neurobiological data to access the
        perceptual space of an animal. Here, we contrast the complexity of scent perception with
        two examples of simpler perceptual worlds, vertebrate frequency perception and bee color
        vision.
      
      
        The Perception of Pitch in the Auditory System
        Sound is a mechanical vibration whose most defining characteristic is neatly arranged
        along one dimension—frequency (of waves with alternating high and low pressure). The
        mechanical structure of our inner ear tidily maps the frequency of sound onto different
        positions of the basilar membrane in the snail-shaped cochlea [9]. The width and
        flexibility of this membrane increases with distance from the oval window (the point of
        entry of sound). The result is that, when the frequency of sound is high, it will produce a
        peak of vibration near the oval window (Figure 1A); if the frequency is low, the peak of
        vibration will be nearer the far (apical) end of the cochlea [10]. In this sense, the
        cochlea processes sound similarly to how a prism acts on white light [11]: it decomposes
        mixtures of sound frequencies into their components, and maps them onto different spatial
        positions within the cochlea. The thousands of mechanoreceptors distributed along the
        basilar membrane need not be tuned to different frequencies, as in color vision, where each
        receptor type responds most strongly in a particular wavelength range. It is the
        mechanoreceptors' position in the cochlea that determines which sound frequency will
        maximally stimulate them [10,12]. This structure is maintained on a higher processing
        level: fibers in the auditory cranial nerve send the information from the receptor cells to
        the brain in parallel, i.e., in a first approximation, each receptor cell has its own
        neuronal “cable” to the cochlear nuclei, and beyond, to the auditory cortex, where we find
        a complete topographical map of the audible frequency spectrum (Figure 1B), mirroring the
        mapping of frequencies in the cochlea [11,13,14]. The perception of pitch, then, is
        arranged along one dimension, as on a piano keyboard. Because of the parallel processing of
        receptor information from the cochlea, we can hear mixtures of different frequencies, and
        analyze their components accurately, unless mixtures are very complex. We can identify the
        tones that a chord is made up of [15]. There are, of course, cases where mixtures have
        unique properties, e.g., in the case of the intensities of harmonic overtones (integer
        multiples of the fundamental frequency) that distinguish a C tone produced by a guitar from
        that of a flute—but we can still identify the fundamental, whose perceived pitch is not
        altered by the overtones. We would never perceive a mixture of 400 Hz and 800 Hz as an
        intermediate frequency (e.g., 600 Hz).
      
      
        Color Perception in Bees
        While these observations are seemingly trivial in the world of sound, the perception of
        mixtures in color vision is fundamentally different. If we mix yellow with red light, we
        will see orange—not only will we not be able to tell that the orange light has been
        produced by mixing two lights, but we will also be unable to distinguish the mixture from
        monochromatic orange light [16]. Similar mixture phenomena are well established in bees;
        for example, orange light (with a wavelength of 590 nm) can be mixed with blue light (440
        nm) to produce a mixture that is indistinguishable from monochromatic bluegreen light (490
        nm) [17,18]. In bees, as in humans, the perception of hues is arranged in a circular
        fashion around achromatic white or gray [19]. In this circular arrangement, complementary
        colors (i.e., those opposite on a circle) can be additively mixed to generate a neutral
        stimulus, e.g., UV light (350 nm) mixed in the appropriate ratio with bluegreen light (490
        nm) will be perceived as white by bees [17]. In the auditory system, mixing only two
        frequencies to produce the perception of white noise is inconceivable! Finally, the
        circular arrangement of hues also implies that it is possible to mix two ends of the
        perceptible spectrum to produce a sensation that is not contained in the spectrum: for
        humans, mixing violet with red light will produce purple, a sensation that has no
        equivalent in the visual spectrum; a similar unique percept can be produced by facing bees
        with a mixture of UV and green light [5,17].
        We (and other animals) can only ever see one color in a point; additive color mixtures
        are perceived as intermediates of their generative sources, and it is not possible to
        identify the physical components of a mixture (e.g., the perception of white can be
        generated mixing any two complementary colors). In contrast, we can hear several
        frequencies at once, and identify the components of at least simple stimulus mixtures such
        as triads [15]. In color vision, but not in hearing, multiple combinations of physical
        stimuli will generate identical sensations. The reasons for these fundamental differences
        between the visual and the auditory sensory modalities can be attributed in a
        straightforward way to the structure of the sense organs as well as post-receptor neural
        circuits.
        In color vision, there aren't thousands of receptor cells each responsible for receiving
        a narrow range of wavelengths (as there are in the auditory system, with its technique of
        frequency analysis). Instead, both humans and bees have only three color receptor types,
        each sensitive to a broad range of wavelengths (Figure 2A). Human color receptors are
        typically termed blue, green, and red receptors, while those in bees are most sensitive to
        UV, blue, and green light [5,20]. A single receptor cannot analyze the wavelength of the
        light it receives: it simply acts as a quantum counter, but the information about the
        wavelength identity of the quantum is lost on absorption. Any single photoreceptor might
        respond equally to a medium intensity light in the wavelength range of its peak
        sensitivity, and to a strong intensity light at the periphery of its sensitive range—hence
        it cannot disentangle wavelength from intensity [5].
        This means that the visual system has to compare the signals from receptors differing in
        spectral sensitivity. In insects as well as in vertebrates, it does this by means of color
        opponent neurons (Figure 2A) [18,21,22]. The minimum equipment for an animal with two color
        receptor types is one type of opponent neuron, receiving antagonistic inputs from the two
        types of cells. Using such an opponent mechanism, the visual system can “tell” whether
        there is a stronger signal from the short wavelength receptor or the long wavelength
        receptor—hence it can extract information about stimulus spectral quality. Theoretically,
        an animal with n color receptor types needs 
        n &minus; 1 chromatic opponent mechanisms [23]. Indeed, behavioral
        experiments with bees have shown that only two color opponent mechanisms are necessary to
        explain color discrimination data [18,24]. It is not clear which ones these
        are—physiologists have found at least seven different types of color opponent neurons in
        the bee optic lobes [5,18,21], and modeling has shown that almost any combination of two
        color opponent neuron types is adequate for color coding [24].
        Nevertheless, the two-dimensional color opponent space whose axes correspond to
        excitation patterns of color opponent mechanisms (Figure 2B) has proven extremely useful:
        distances in such a color space correlate well with behavioral color discrimination data.
        The color opponent space allows us to predict hue (by assessing angular position) and
        saturation (by measuring distance from the center), and it can be used to predict the
        perception of color mixtures, which fall between the loci of the colors used to generate
        the mixture [5]. Because information about the actual receptor signals is discarded in the
        very periphery of the sensory system, perception is only based on derived (color opponency)
        dimensions, which measure differences in receptor signals rather than absolute signals.
      
      
        Olfactory Perceptual Space in Bees
        Making sense of scents is a considerably messier affair. Odors are hardly presentable on
        a physical continuum (like the wavelength of light); they are multidimensional entities
        that can vary from small gaseous molecules to long-chained hydrocarbons [25,26]. Organic
        compounds vary in carbon chain length and functional group, i.e., the group of atoms that
        give substances their characteristic properties (e.g., alcohols, aldehydes, ketones, or
        alkanes). At any moment, the air around an animal may contain hundreds of different
        airborne substances, which fluctuate with wind, humidity, and multiple other factors
        [25].
        On the receptor level, the olfactory system shows a complexity that is unparalleled in
        any color vision system: the receptor family discovered by 2004's Nobel laureates L. Buck
        and R. Axel comprises about 1,000 receptor proteins in mammals, each of which only binds a
        narrow range of airborne molecules [27,28]. Only one of these proteins is expressed per
        receptor cell, so that there are indeed about 1,000 different odor receptor cell types in
        the mammalian olfactory epithelium [29]. In insects, the diversity of such receptors is
        lower, but still impressive: fruit flies appear to have 65 different odorant receptor genes
        [30]. In honeybees, the number of such genes will be accessible pending the publication of
        the honeybee genome; so far, screens by H. Robertson (pers. comm.) indicate a number
        >130. Some 60,000 olfactory receptor cells (of a few dozen different types) are
        distributed along the honeybee antennae ([31]; Figure 3A). How can the brain extract
        biologically useful information from such multidimensional sensory input?
        The first neuronal center of olfactory information processing, the antennal lobe (or its
        mammalian analogue, the olfactory bulb) achieves order in two ways. First, axons from like
        receptor cells (i.e., those that express the same receptor protein and therefore bind the
        same odorants) project to one or a few glomeruli, i.e., globular, anatomically distinct
        subunits within the antennal lobe [26]. The number of glomeruli ranges from a few dozen to
        several hundred, and corresponds roughly to the number of putative olfactory receptor types
        [25]. The honeybee's antennal lobe contains 160 glomeruli [31]. Individual chemicals
        reliably activate sets of identified glomeruli [32]. These micro-relays sum up the input
        from same chemoreceptors, tremendously increasing the signal-to-noise ratio, and thus
        facilitating reliable odorant detection [26].
        A second, and perhaps more remarkable, feature of the antennal lobe is that glomeruli
        coding for similar substances are located close together, while those that code for
        distinct scents are spatially segregated [29,32,33,34]. Carbon chain length, for example,
        is neatly represented in this glomerular map [32]. How the brain achieves such “chemical
        mapping” is something of a miracle: how would the developing brain “know” which axons
        belong to receptors that respond to chemically similar substances, so that these can be
        wired to neighboring glomeruli? In mammals, it was recently found that the receptor
        proteins that bind odor molecules are also expressed in the axon terminals of the receptor
        cells [29]. If we assume that similar receptor molecules bind similar odorants, then the
        developing nervous system could use receptor molecule similarity in the receptor cells'
        axon terminals to wire up the neural map in the antennal lobe.
        But does this neuronal activity map indeed correspond to the olfactory perceptual space?
        Theoretically, a perceptual space might have as many dimensions as there are distinct
        receptor types—or it might have as many axes as there are glomeruli with distinct response
        profiles. Is it possible that olfactory space in bees, then, has several dozen dimensions?
        To evaluate the structure of olfactory perceptual space, Guerrieri et al. [8] trained
        honeybees to memorize a wide variety of odors, and then tested how well bees could
        distinguish these odors from others. They then asked: how many axes must the olfactory
        perceptual space have, so that distances between odors can be used to predict how similar
        they will appear to bees? They found that the multidimensional receptor space might be
        collapsed onto very few perceptual axes: much of the similarity judgments between odors can
        be explained by a three dimensional space (Figure 3B). The most important axis spreads out
        scents according to carbon chain length, whereas the other two axes separate the odors
        according to functional group, i.e., they separate primary and secondary alcohols,
        aldehydes, and ketones. Distances between odor loci in this three-dimensional space
        correlate well with the discriminability of the odors to bees. They also correspond to the
        similarity of activation patterns of the glomerular map [32], although the actual
        mechanisms that evaluate this similarity remain to be identified. There are a number of
        complications, however, that indicate that the olfactory perceptual space is unlikely to
        submit to the relatively simple rules of perception of color and pitch.
        Take mixtures of different stimuli, for example. If odor perception followed rules
        similar to those of color perception, and if it relied exclusively on derived parameters
        (such as carbon chain length), then mixtures would be perceived as intermediates of their
        components. A mixture of two molecules differing only in carbon chain length would be
        perceived as indistinguishable from a single odorant with an intermediate carbon chain
        length. This remains to be tested, but we conjecture that this is unlikely to be the case.
        On the other hand, are odor mixtures simply perceived as a compound entity with distinct
        components (like a triad in vertebrate pitch perception), or are mixtures unique entities
        that are perceived as fundamentally different from their elements (like “white” in color
        perception)? Honeybees appear to employ a combination of the two: they can perceive the
        components of a mixture (and generalize to these components when faced with them
        individually), but also attach unique properties to the mixture [31,35]. A further
        complication is that Guerrieri et al. [8] found intriguing asymmetries in the bees'
        assessments of odor similarities: bees respond as if they find odor A more similar to
        trained odor B than they find B to trained odor A. It will be difficult to represent these
        complex phenomena in a simple map.
        These phenomena indicate that odor perception cannot be as easily visualized in a
        low-dimensionality space as other sensory modalities. However, for psychophysicists, just
        as for motorists navigating novel territory, even a rough guidance map is better than no
        map, and so this olfactory space is undoubtedly a useful tool to predict the bee-subjective
        similarity of scents. While the antennal lobe clearly structures the sensory input so as to
        extract derived chemical properties, the information about the input from individual
        receptor types might not be discounted in the periphery of the nervous system, as in visual
        perception, but relayed on to higher processing centers, such as the mushroom bodies and
        the protocerebrum [36,37]. If both modulated and unmodulated sensory information is
        available to the neuronal centers that ultimately “decide” on odor similarity, and
        especially if different individuals attend differently to different kinds of input—e.g.,
        because experience modifies interactions between glomeruli [31]—then the derivation of a
        static perception space that can be applied to all individuals of a species may be quite
        challenging.
      
      
        Conclusion
        Philosophers have correctly pointed out that we cannot actually imagine what it is like
        to perceive the world through other animals' sense organs [38]. But studies such as those
        by Guerrieri et al. [8] show elegantly how to attach scales and numbers to the inner worlds
        proposed by von Uexküll a century ago. Mapping these worlds quantitatively will allow us to
        compare them between related species operating in different environments, to see how the
        architecture of perceptual spaces is adapted to mirror biologically useful information from
        the real world in each species [39]. One of the most exciting future directions is to
        explore the extent to which such perceptual worlds are not just species-specific, but in
        fact individual-specific. In the honeybee, learning alters the response patterns of the
        glomerular map [40]. If the antennal lobe has a dual function in creating the primary
        dimensions of olfactory perception and storing olfactory memories, then one prediction is
        that individual experience will alter perception. Similar phenomena have been predicted in
        other sensory modalities [19,41], but remain to be shown directly by exposing different
        individuals to different environments during development.
      
    
  

  
    
      
        Introduction
        Chronic inhalation of tobacco smoke causes progressive lung destruction in susceptible
        individuals, resulting in chronic obstructive pulmonary disease (COPD) and emphysema, two
        well-described clinical syndromes with poorly understood pathogenesis [1,2,3]. A role for T
        helper cells in the pathogenesis of obstructive lung disease has been established with
        asthma, where T helper 2 (Th2) cells are strongly linked to both human and experimental
        disease [4,5,6,7]. A potential role for T cells in COPD has also been suggested in several
        recent studies that show CD8
        + T cells are increased in the lungs of people who smoke [8,9,10,11]. T
        cells cause tissue injury through their secreted products such as cytokines; in mice,
        overexpression of interleukin (IL)-13, a T cell cytokine that is strongly implicated in the
        pathogenesis of experimental asthma, resulted in increased production of proteases and
        enlargement of airspaces reminiscent of emphysema [12]. Further, airway limitation, another
        characteristic of human asthma, is clinically linked to an accelerated rate of loss of lung
        function in smoker individuals [13]. It has been suggested, therefore, that asthma and COPD
        may involve the same type of recruited inflammatory cells, differing only in their location
        within the lung [14].
        Chemokines, their receptors, and cell adhesion molecules regulate migration of immune
        cells into inflamed tissue [15,16,17,18]. T helper 1 (Th1) cells have been shown to secrete
        interleukin 2 and interferon gamma (IFN-γ), and express a distinct repertoire of chemokine
        receptors such as CCR5 and CXCR3 [19,20,21]. In contrast, Th2 cells that are biased to
        produce IL-4 and IL-5 express mainly CCR4 and CCR3 [22,23,24,25]. Immunofluorescent
        analysis of airway mucosal biopsies in patients with asthma showed that most T cells
        co-express IL-4 and CCR4, but, in contrast, T cells in airways of patients with COPD and
        pulmonary sarcoidosis produce IFN-γ and express high levels of CXCR3, while lacking CCR4
        expression [26]. In addition to T cells, a wide variety of other inflammatory cells have
        been shown to express distinct chemokine receptors that are critical for their homing,
        suggesting a universal mechanism for regulating immune responses. Interferon-inducible
        protein 10 (IP-10), monokine induced by interferon gamma (MIG), and interferon-inducible T
        cell alpha chemoattractant (I-TAC) are three known ligands for CXCR3 produced by normal and
        injured epithelial cells and T cells that are required for homing of Th1 cells [27,28,29].
        In addition to regulation of chemotaxis and homing, other functions have been ascribed to
        chemokines, including modulation of T cell fate by direct effects on differentiating T
        cells, and regulation of proteolysis in blood monocytes [19,30].
        In this study we determined the dominant T helper phenotype in lung samples from
        ex-smoker individuals with moderate to severe COPD and emphysema and control individuals
        with no evidence of smoking-related lung disease. Analysis of chemokine receptor expression
        on isolated peripheral lung lymphocytes from ex-smokers with COPD/emphysema indicated that
        both CD4 and CD8 T helper cells are strongly polarized to the Th1 phenotype compared to T
        cells isolated from lung tissue of normal individuals or individuals with
        non-smoking-related obstructive lung disease. The same cells spontaneously secreted more
        IFN-γ and CXCR3 receptor ligands MIG and IP-10 in the COPD and emphysema group than in the
        group without emphysema. Further, IP-10 and MIG, but not IFN-γ, upregulated macrophage
        metalloelastase (matrix metalloproteinase [MMP]-12) from isolated lung macrophages.
        Together, our findings reveal the strong association between COPD/emphysema- and Th1-driven
        adaptive immunity, suggesting a link to lung destruction mediated by IFN-γ, MIG, and
        IP-10.
      
      
        Methods
        
          Participants
          Twenty-eight non-atopic ex-smoker individuals (see Table 1) undergoing medically
          necessary lung resection were serially entered into the study: ten individuals with no
          COPD and no evidence of emphysema (control group) and eighteen individuals (diseased
          group) with moderate to severe COPD and evidence of emphysema as determined by pulmonary
          function tests, high-resolution computed tomography (CT), or conventional CT scan. All
          participants were ex-smokers who had quit smoking for a mean (SD) of 7 (2) y and 4 (2) y
          in COPD/emphysema and control groups, respectively. COPD was diagnosed according to the
          criteria recommended by the National Institutes of Health/World Health Organization
          workshop summary [31]. Participants in the control and COPD/emphysema groups had similar
          (mean [SD] of 54 [6] and 45 [5], respectively) “pack-year” smoking histories, where
          smoking one pack of cigarettes per day each year is defined as one pack-year.
          All participants were recruited from the surgical clinic at the Michael E. DeBakey
          Veterans Affairs Medical Center and the Methodist Hospital, and were undergoing lung
          resection for diagnostic or therapeutic purposes (Table 1). Study protocols were approved
          by the institutional review board for human studies, and informed consent was obtained
          from all participants. Participants had no history of allergy or asthma and had not
          received oral/systemic corticosteroids during the last 6 mo. At the time of study, all
          participants had been free of acute symptoms suggestive of upper or lower respiratory
          tract infection for the 6 wk preceding the study.
        
        
          CT-Based Evaluation for Emphysema
          High-resolution CT (two in emphysema group and two in control group) or conventional
          CT analysis was used to detect emphysema, characterized by the presence of areas of low
          attenuation contrasted with surrounding normal lung parenchyma [32,33]. CT scans were
          used by a radiologist to separate participants on the basis of the presence or the
          absence of any objective evidence for centrilobular, panacinar, or paraseptal emphysema
          with a detection limit of greater than 3-mm low attenuation density [34].
        
        
          Isolation of Lung Lymphocytes
          Lung lymphocytes were isolated by modifying established protocols, using a combination
          of mechanical fragmentation, enzyme digestion, and centrifugation procedures described
          previously [35,36,37]. Viable lymphocytes were separated from whole lung inflammatory
          cells (macrophages, eosinophils, and neutrophils) using an immunomagnetic positive
          separation technique (autoMACS, Miltenyi Biotec, Auburn, California, United States).
          Briefly, lung leukocytes were labeled with paramagnetic bead-conjugated anti-CD3, -CD19,
          and -CD56 to positively select T, B, and NK cells, according to the manufacturer's
          instructions. Each of the harvested cell populations was used directly for in vitro
          assays or was cryopreserved in aliquots of 1 × 10
          7 cells for future analysis.
        
        
          Antibodies
          The following monoclonal antibodies were purchased from BD Biosciences Pharmingen (San
          Diego, California, United States): FITC-, Cy5-, and PE-conjugated anti-CD4, -CD8, -CD3,
          -CD14, -CD69, -CXCR3, -CCR3, -CCR4, and -CCR5. For enzyme-linked immunosorbent assay
          studies, anti-human antibodies to IFN-γ, IL-4, IP-10, MIG, I-TAC, and the appropriate
          secondary reagents were purchased from R&D Systems (Minneapolis, Minnesota, United
          States).
        
        
          Quantification of Polarized Peripheral Blood and Lung Lymphocyte Subsets
          Phenotypic characterization of T cells was done by two-color flow cytometry (Epic XL
          FL, Beckman Coulter, Allendale, New Jersey, United States) using combinations of the
          following monclonal antibodies: FITC-conjugated anti-CD4, -CD8, and -CD14; PE- and
          Cy5-conjugated anti-CCR4, -CCR3, -CCR5, and -CXCR3. Freshly isolated lung lymphocytes
          were resuspended to 1 × 10
          7 cells/ml, and 50 μl of cells was incubated with antibodies to CD3
          and CD4 or CD8.
        
        
          Intracytoplasmic Cytokine Staining
          Lung lymphocytes were cultured in the presence or absence of phorbol myristate acetate
          (PMA)/ionomycin and brefeldin A for 12 h. Cells were harvested, fixed with formaldehyde,
          permeabilized with saponin, and intracellularly labeled for IFN-γ and IL-4, in addition
          to staining for surface CD69, CD4, and CD8 according to the manufacturer's
          recommendations (Fastimmune, BD Biosciences Pharmingen).
        
        
          In Vitro T Cell Culture and Cytokine Assay
          Lung lymphocytes were isolated from surgical tissue and cultured in vitro in
          triplicate for 4 d. Supernatants were collected and stored at –80 °C for future analysis.
          Standard antibody-based enzyme-linked immunosorbent assay was used to measure supernatant
          concentrations of IP-10, MIG, IL-4, and IFN-γ according to the manufacturer's
          instructions (R&D Systems and BD Biosciences Pharmingen).
        
        
          Detection of MMP12 by Western Blotting, and Real-Time PCR
          Peripheral blood mononuclear cells and lung macrophages were isolated by positive
          selection using immunomagnetic beads conjugated with anti-CD14, and cultured in
          serum-free medium (RPMI, L-glutamine, and Pen/Strep) prior to overnight stimulation with
          0, 50, 250, or 500 ng/ml of IFN-γ, IL-4, MIG, I-TAC, and IP-10. Supernatants were
          collected, and MMP12 was detected using anti-human MMP12 (R&D Systems) by Western
          blotting according to the manufacturer's instructions.
          Total cellular RNA was extracted from CD14
          + lung macrophages stimulated overnight with rIP-10 (500 ng/ml) in the
          presence or absence of blocking anti-CXCR3 antibodies (5 μg/ml, R&D Systems).
          Two-step real-time reverse transcription PCR was used to determine the relative
          expression of mRNA using the ABI Perkin Elmer Prism 5700 Sequence Detection System
          (Applied Biosystems, Foster City, California, United States) as described previously
          [38].
        
        
          Immunostaining and Histopathology
          Paraffin-embedded, and fresh-frozen lung sections (5 μm) were immunostained using
          monoclonal antibodies against human MMP12 (R&D Systems) or non-immune antisera by an
          immunoperoxidase protocol (Vectastain Elite, Vector Labs, Burlingame, California, United
          States) and counterstained with hematoxylin as recommended by the manufacturer.
        
        
          Statistical Analysis
          The Mann-Whitney test (non-parametric, two-tailed) and Student's T test (two-tailed)
          were used to compare differences between the two groups of subjects. 
          p < 0.05 was considered statistically significant.
        
      
      
        Results
        
          Th1 Immune Bias of Peripheral Lung Lymphocytes in Emphysema
          Inflammatory chemokines, cytokines, and their receptors are upregulated at sites of
          inflammation and play a key role in the recruitment of leukocytes to peripheral tissues
          in response to injury [17,39]. To detect Th1 polarization, we assessed lung lymphocytes
          for expression of CCR5 (a receptor for several Th1 chemokines) and CXCR3 (the receptor
          for IP-10, I-TAC, and MIG). We screened for the presence of Th2 cells by assessing T cell
          expression of CCR4—a receptor for eotaxin/CCL11, macrophage chemoattractant protein 3
          (CCL7), and thymus- and activation-regulated chemokine (CCL17) [40,41]—and CCR3, a
          receptor for eotaxin and related chemokines. Flow cytometry revealed very low expression
          of CCR3 and CCR4 (1%–3%) in control (
          n = 10) and emphysema (
          n = 18) groups, and did not discriminate between these populations
          (Figure 1A and 1B; data not shown). These findings were in sharp contrast to the enhanced
          expression of both CCR5 and CXCR3, as shown in representative histograms (Figure 1A).
          These Th1-specific chemokine receptors were expressed prominently on lung lymphocytes
          from all participants, but their expression was significantly enhanced in the setting of
          emphysema (Figure 1A–1C). Further, both CD4 and CD8 T cells expressed CCR5 at the same
          level (Figure 1C). In contrast, we found highly variable expression (0.5%–30%) of CCR4,
          CXCR3, and CCR5 on peripheral blood lymphocytes isolated from the same participants, and
          this variation did not correlate with the presence of disease in either group (data not
          shown). Furthermore, we compared the lung lymphocyte CCR5 and CXCR3 profiles among the
          eight participants with emphysema alone (lung volume reduction surgery for emphysema;
          non-cancer) and ten participants with emphysema and accompanying cancer (lung resection
          for treatment of small peripheral cancer), and found that these two groups cannot be
          distinguished based on these indices (Figure 1D; data not shown).
          Although human lung macrophages are not known to express CXCR3, we suspected based on
          the immunohistochemical localization of this chemokine receptor that CD14
          + cells in the lungs of ex-smoker individuals with emphysema accounted
          for much of the total lung CXCR3
          + immunoreactivity (Figure 2A; data not shown). To confirm this, we
          determined the percent of total lung cells expressing CD14 and CD11b—which are both
          markers of monocytes/macrophages—and CXCR3. We found that over 40% of CD14
          + cells from participants with emphysema but not control participants
          were also positive for CXCR3 (Figure 2). In addition, there was a significant negative
          association between CXCR3 expression on lung T cells and the percent of predicted forced
          expiratory volume in 1 s (FEV1), based on an 
          R
          2 goodness-of-fit statistic of 0.27 (Figure 2C; 
          p = 0.0089, 
          r = −0.52). Together, these data indicate that a strong type 1 bias is
          characteristic of the T cells isolated from the peripheral lung of participants with COPD
          and emphysema and that this immune phenotype correlates with the lung destruction that is
          characteristic of this disease. Further, we have shown for the first time, to our
          knowledge, that CXCR3 expression, a marker of Th1 inflammation, extends to lung monocytes
          and macrophages.
        
        
          IFN-γ, IP-10, and MIG But Not IL-4 Are Expressed by Lung Lymphocytes
          We sought additional functional data to confirm the apparent Th1 bias of peripheral
          lung inflammatory cells isolated from ex-smoker individuals. Freshly isolated lung
          lymphocytes that were not otherwise manipulated secreted high levels of IFN-γ, MIG, and
          IP-10, with significantly greater secretion of both cytokines from lymphocytes of
          participants with emphysema (Figure 3A–3C). Interestingly, we could not detect
          appreciable amounts of I-TAC, another known ligand for CXCR3, in lung lymphocytes of
          control participants or those with emphysema (data not shown). Similar results were
          obtained using intracytoplasmic cytokine staining of the same cells (Figure 3D), in which
          PMA/ionomycin stimulation strongly induced IFN-γ production from CD69
          + /CD8
          + lung lymphocytes. Surface staining for CD4 was not feasible with
          this protocol; however, the percentage of CD8
          − /IFN-γ
          + cells was approximately equal to that of CD8
          + /IFN-γ
          + cells (median [SD], 19[6] versus 16[4], respectively). Because total
          numbers of CD4
          + and CD8
          + T cells were approximately equivalent, this suggests that non-CD8
          + /IFN-γ
          + cells are largely CD4
          + , and therefore Th1 cells. Finally, the typical Th2 cytokine, IL-4,
          was not detected in either group, as determined by enzyme-linked immunosorbent assay or
          intracytoplasmic cytokine staining (Figure 3E; data not shown), confirming the marked Th1
          bias of the immune response that underlies smoking-related lung inflammation and
          emphysema.
        
        
          IP-10 and MIG But Not IFN-γ Directly Upregulate MMP12 through CXCR3
          Emphysema and irreversible airway limitation that is characteristic of chronic tobacco
          smoking are related to the destruction of elastin and the resulting loss of lung elastic
          recoil. Therefore, to be relevant to the pathogenesis of airway obstruction, type 1
          inflammation must be shown to promote lung elastolysis. Because loss of elastin is
          regulated by proteinases [42], we next determined if expression of MMPs, in particular
          the elastases MMP9 and MMP12, was regulated by IP-10, MIG, and IFN-γ, the principal
          cytokines detected in emphysematous lung. Indeed, isolated peripheral lung macrophages,
          but not isolated blood monocytes, secreted MMP12 in response to IP-10 and MIG, but not
          IFN-γ (Figure 4A; data not shown). These findings reflect a specific receptor–ligand
          interaction because in the presence of a CXCR3 function-blocking antibody, IP-10 failed
          to induce MMP12 (Figure 4B). Furthermore, immunohistochemical studies revealed that lung
          macrophages of participants with emphysema, but not control participants, specifically
          express MMP12 (Figure 4C and 4D). Together, these findings indicate that Th1, but not
          Th2, cytokines and related chemokines are required for establishing the pro-elastolytic
          lung environment that underlies human emphysema.
        
      
      
        Discussion
        In this investigation, we characterized T cells and lung macrophages isolated from
        emphysematous and non-emphysematous human lungs. Three principal findings emerge from our
        study. First, rather than being functionally diverse, as suggested by the heterogeneous
        nature of humans, lung T cells of ex-smoker individuals with emphysema are relatively
        homogeneous and characterized by a marked Th1 bias. Second, the principal Th1 chemokines,
        MIG and IP-10, are linked to a pro-elastolytic lung environment because these cytokines
        upregulate the elastase MMP12, which is associated with emphysema. Finally, we found no
        significant expression of Th2 chemokine receptors, such as CCR3 and CCR4, or IL-4
        production in lung lymphocytes. Together, our findings demonstrate the role of the adaptive
        immune response in COPD and suggest a primary role for Th1 cells in controlling the main
        smoking-related physiologic and structural changes of the lung.
        Upregulation of CCR5 and CXCR3 on T cells and accumulation of these cells in the lung
        periphery suggest that aberrant, unremitting pulmonary recruitment of these activated T
        cells is unique to people with smoking-related lung disease, despite cessation of exposure
        to the inciting agent, tobacco smoke. We showed that ex-smoker individuals without
        obstructive lung disease or emphysema have comparatively little Th1-biased inflammation in
        their lungs; thus, our findings reflect the inflammatory changes that are unique to the
        COPD microenvironment. Additionally, lung lymphocytes isolated from four lifelong
        non-smoker individuals with severe obstructive lung disease due to cystic fibrosis or
        bronchiolitis obliterans did not show a Th1 inflammatory bias of the lung (S. Grumelli, F.
        Kheradmand, D. B. Corry, unpublished data). This information confirmed our finding that the
        predominant Th1 bias in COPD/emphysema reflects the microenvironment unique to the lungs of
        ex-smoker individuals. The prevalence of asthma among people who smoke is currently not
        known, but in order to study COPD/emphysema in a population without other confounding
        variables, people who might have had asthma were excluded, and thus our findings are
        restricted to non-asthmatic individuals with emphysema.
        Our use of T cell chemokine receptor expression analysis to determine recruitment of
        lung T cells is not without precedent. Analysis by immunohistochemistry of airway mucosa of
        people with atopic asthma after antigen challenge revealed that large numbers of CCR4
        + and CCR8
        + T cells express IL-4, and CCR4 expression was prominent in people with
        severe atopic dermatitis, which decreased upon abatement of disease activity [26,43].
        Immunostaining of T cells in synovial fluid from individuals with rheumatoid arthritis
        showed that virtually all of the T cells associated with inflamed joints expressed CXCR3
        and CCR5, representing significant enrichment compared to blood T cells from the same
        participants. Furthermore, previous studies of smoker individuals with COPD and normal lung
        function showed the presence of CD8
        + /CXCR3
        + T cells in the airway epithelium and submucosa [44]. We extend these
        findings by showing CXCR3 expression on lung macrophages and CD4
        + T cells in emphysema patients and the functional interplay between
        Th1-related chemokines and elastolytic MMPs.
        In addition to detailing surface chemokine receptor expression, we have functionally
        confirmed the marked Th1 bias of peripheral lung T cells, demonstrating that either at rest
        or following stimulation, these cells secrete IFN-γ and not IL-4. Our findings therefore
        confirm the utility of chemokine receptor expression patterns in the initial assessment of
        T cell effector phenotype.
        Destruction of lung parenchyma in emphysema is thought to occur through excessive
        proteolysis mediated by the elastin-degrading enzymes MMP2, MMP9, and MMP12 from the MMP
        family, and by neutrophil elastase from the serine proteinase family [45,46]. Cytokines and
        chemokines are substrates for MMPs, but they also regulate expression of MMPs under
        pathological conditions [47,48]. We have shown here that IP-10 and MIG, two chemokines that
        are secreted from lung lymphocytes of participants with emphysema, upregulate specifically
        MMP12 and thus favor a proteolytic microenvironment that facilitates lung destruction.
        Strengthening the association between lung macrophages and IP-10/MIG-dependent MMP12
        secretion is the fact that we have demonstrated that in humans macrophages, like T cells,
        express CXCR3 and that this receptor is required for MMP12 secretion in response to
        IP-10/MIG stimulation. In addition to defining the predominant immune phenotype of
        emphysematous lung, these additional findings implicate the principal cell (macrophage),
        MMP (MMP12), and effector cytokines (IFN-γ, IP-10, and MIG) as likely underlying
        smoking-induced lung destruction. We have further shown that these enzymes may be regulated
        by proximal immune events driven by Th1 cells or Th1-associated cytokines. A question of
        major importance for future study is, therefore, the nature of the antigens and adjuvant
        factors that ultimately drive this inflammatory response.
        Although this was an entirely human study, our findings show remarkable parallels with
        studies performed in mice. MMP12 deficiency has been shown to protect mice against
        emphysema after chronic exposure to cigarette smoke, implying that MMP12 may be the key
        proteinase in the development of emphysema in this species [49,50]. Studies from both
        humans and mice therefore firmly suggest the importance of MMP12 in the pathogenesis of
        emphysema. Interestingly, in addition to solubilizing elastin, MMP12 is the MMP most
        efficient at degrading α1-antitrypsin, the primary physiological inhibitor of human
        leukocyte elastase [51,52]. Thus, chemokine-induced upregulation of MMP12 may orchestrate
        lung matrix degradation both directly and indirectly through inactivation of
        α1-antitrypsin. The therapy of COPD and emphysema is currently limited to pharmacologic
        bronchodilation to relieve dyspnea, antibiotics for intercurrent respiratory tract
        infection, and vaccination against prominent respiratory pathogens. Aside from efforts to
        prevent smoking or encourage cessation, there exist no measures that prevent development of
        emphysema or treat the specific causes of airway obstruction. By providing insight into the
        immunopathogenesis of COPD, our findings provide genuine hope that future therapies capable
        of preventing or halting smoking-related lung disease may be possible.
      
    
  

  
    
      
        
        As an intern, I took care of the first patients with HIV/AIDS at San Francisco General
        Hospital, and so I grew up with AIDS in the early days of my medical career. We struggled
        through the confusion about what was making people so sick, and each new day brought a new
        discovery about the disease and its consequences. I went through that evolutionary process
        along with everybody else, and it shaped me in many profound ways. Before long, I
        recognized that this wasn't a disease of “those people over there.” This was a disease that
        could strike anyone, anytime. And as physicians, we had to adjust our thinking about our
        own vulnerability to occupational risk, and to emphasize prevention, because there wasn't
        going to be a cure for a long, long while. And not only physicians had to rethink
        things—AIDS has reshaped society's very notions of the most basic human behaviors.
        I was in Uganda in 1985, early in the AIDS epidemic there. We knew then where that
        epidemic was going to go, absent an effective vaccine or cure, but few of us could have
        imagined that it would evolve so quickly without an end in sight. While the people of
        Africa have achieved a huge amount in tackling HIV/AIDS, particularly in Uganda, the
        epidemic is far from being under control on that continent and is spreading through other
        parts of the world with alarming speed.
      
      
        The Crisis of Human Resources
        The theme of this year's International AIDS Conference in Bangkok was “Access for All.”
        Over the past few years, it has become increasingly apparent that a critical component of
        assuring access to care and treatment is human capital. Like fiscal capital, human
        resources are essential to ending the AIDS pandemic. I visited Africa with US Health and
        Human Services Secretary Tommy Thompson and many AIDS experts last December, and we saw
        evidence of this critical need in every country we visited. The miracles of modern science
        are meaningless without systems and people to deliver them to those in need.
        The World Health Organization estimates that of the 40 million people worldwide infected
        with HIV, 6 million need immediate, life-sustaining antiretroviral therapy. Fewer than
        400,000 people in developing countries have access to such treatment (Figure 1) [1,2].
        There are too few skilled health care workers to provide reliable delivery and
        administration of these life-saving therapies. According to a recent Institute of Medicine
        report, and a study sponsored by the US Agency for International Development, the number of
        health care workers in many African countries is actually shrinking as they are lured to
        developed countries by better pay and professional opportunities (Box 1) [2,3]. Reversing
        this brain drain is essential over the long-term, as HIV treatment and care will be
        required for decades. In the short-term, the Institute of Medicine called for expanded
        efforts “to bring qualified volunteer initiative medical professionals into both urban and
        rural areas to support prevention, care, and training programs” [2]. I could not agree more
        that addressing the human resource needs will be essential as we move forward—and not just
        for HIV/AIDS programs, but for all aspects of public health and health care.
        It has now been shown, beyond any doubt, that even in resource-poor countries with the
        most basic health infrastructure, people get the same benefit from treatment and prevention
        interventions as those in the rich world [4]. In fact, surveys in Cape Town, Kampala,
        Khayelitsha, and Senegal found rates of adherence to antiretroviral therapy of 90%–94%,
        compared with estimates of 70% in developed countries [5,6,7].
      
      
        When You Have Seen the Faces
        We hear the numbers—the millions upon millions infected—and we grow numb. That is why we
        must go to the front lines—the households and communities—and start focusing on each
        individual living with HIV. I was at the dedication of an AIDS clinic in Kenya. It was
        raining, and we were waiting outside with our umbrellas. A 12-year-old girl in front of me
        turned around and leaned her head against my belly and said, “Could you take me to America?
        I need drugs.” If you take that girl's face and multiply it a thousand times—that is the
        memory I bring home from Africa: the faces of the children and their asking, “Why are so
        many of our parents dying? Why are we dying?”
        We visited a US Centers for Disease Control and Prevention (CDC) program in the very
        remote areas of Uganda where there are no roads and it is impossible for people to come
        into population centers to receive HIV testing and other services. Young staff from the CDC
        are working with Ugandans and community organizations in that area to deliver
        antiretroviral therapy. Some may think that the difficulties of delivering antiretroviral
        therapy into such a remote area are overwhelming—and some may question whether this is a
        sustainable intervention. But once you see firsthand what miracles are possible, your world
        view changes almost overnight.
        What we saw was the success of a wonderful home-based treatment and care program for
        people who don't have access through other means. And when I say “home-based care,” picture
        a hut without running water or electricity, where only motorcycles are available to deliver
        medications. The first step of the program is to provide clean water. Coliforms and other
        pathogens in the water supply for the household are removed through an inexpensive water
        vessel fitted with a filter and through a chlorination process. In addition, a
        cotrimoxazole tablet is given every day, which, in one patient's words, changed his life
        because he began to feel well almost immediately. Not only do the cotrimoxazole prophylaxis
        and the water treatment improve diarrheal illness, but malarial parasitemia also drops. So
        that is a very positive, unexpected consequence of just two very simple and inexpensive
        interventions. Many patients with HIV/AIDS in Africa also have tuberculosis and are put on
        tuberculosis therapy in addition to cotrimoxazole. As a result, they begin to feel better
        even before they begin antiretroviral therapy.
        We spent time with one of the patients in the home-based care program. As she began to
        participate in these programs, tests became available to measure her CD4 count. She
        explained to me what her CD4 count was, what it meant, and how it improved when she started
        the cotrimoxazole and tuberculosis therapy. She had begun taking three antiretroviral drugs
        and held up her pill box to explain her regimen in detail. Every week a Ugandan health aide
        delivered her supply of pills on a CDC motorcycle and monitored her adherence to the
        treatment. Not only was she extremely reliable in taking her medications, but she also knew
        more about them and their side effects than most of the patients I treated at San Francisco
        General. She was also an expert in HIV prevention. I asked her, “What do you do to protect
        your three young sons from this infection?” She replied, “Every day I take them by the
        hand, and I go out of the house and I say, ‘Do you see that mound of dirt? That is your
        father’s grave. Your father acquired this fatal infection through sex. Be careful.'” And
        then she talks to them about the “ABCs” (“A” for abstinence, “B” for being faithful, “C”
        for condoms).
        So when you see a story like that unfold in the middle of Africa, it's impossible not to
        be hopeful. And yet, it's also very sobering because we are reminded of our responsibility.
        The question is not what the international health community is accomplishing in these
        countries now, but what we could accomplish if we joined together to really fight this war
        on AIDS. Such a story also inspires hope because you can see the multiplier effect that
        comes from taking on one problem and can see the way that effort can expand to encompass
        and address a much greater set of problems.
      
      
        Beyond ABCs—Diagnosis and Responsibility
        When we think about successful prevention models in Uganda, “ABC” certainly stands out
        [8]. However, at this point in the epidemic curve, other letters must also be considered.
        Most HIV transmission is accounted for by infected people having risky sex with uninfected
        people. Both in the US and in Africa, studies show that most infected people engaging in
        risky sex are unaware of their infection status, and that when their infection is
        diagnosed, they usually take steps to protect the others with whom they are having contact
        [9,10,11,12]. So let's add the letter “D” for diagnosis. In fact, improving efforts to help
        people choose risk avoidance and to diagnose those who are already infected is the
        cornerstone of the CDC's new domestic HIV prevention strategy. Diagnosis is extremely
        important in many African communities, especially where the number of discordant
        couples—where one individual is infected and the other is not—is high. Sadly, many couples
        “being faithful” now do not realize that one partner is already infected and are not being
        reached with diagnostic testing programs. So “ABCD” is a concept that I would like to put
        out on the table as food for thought. Of course, there is another letter that we need to
        stress: the letter “R,” for responsibility: personal sexual responsibility is a critical
        component of HIV prevention. Many women and girls become infected after being raped by men
        or because their social circumstances rob them of the power to refuse sex. Men must be held
        accountable for greater sexual responsibility and for ending sexual violence and
        degradation of women and girls. HIV prevention programs need to emphasize responsibility,
        but not lose sight of the fact that responsibility can be practiced only with personal
        autonomy, which many women and girls simply do not have.
      
      
        Expanding the Team to Meet the Needs
        The innovative programs and ideas emerging in Africa can change the picture of the AIDS
        epidemic. The purchase of antiretroviral drugs for Africans is not the big challenge.
        Access to drugs will improve in Africa. The real challenges are delivering drugs in a safe
        and effective way, monitoring therapy, and sustaining the pipeline of drugs so that ongoing
        treatment can be guaranteed. In the example of the home-based program in Uganda, we have
        seen that these challenges can be overcome. Expanding access to prevention, care, and
        treatment services isn't going to be easy, but it is certainly possible. It will take
        unprecedented commitment by people in the public sector, the private sector, faith
        communities, and community organizations, and perhaps most importantly, individual
        volunteers who make up their minds to contribute in any way they can.
        Last fall, the US Peace Corps announced that it was activating programs in some
        countries that allow volunteers to help communities fight the AIDS epidemic, but this is
        just one of many steps that are being taken. The US president's Emergency Plan for AIDS
        Relief will provide $15 billion, including almost $10 billion in new funds, over five years
        for international AIDS assistance [13], and I am part of the team that is charged with
        making this plan happen. I look forward to learning from others in the global health
        community how we can best expand our impact and collectively find a way to support the
        delivery of prevention messages and life-saving medications to everyone in Africa—and
        especially to that little girl at the Kenyan clinic who touched my heart.
      
    
  

  
    
      
        
        In 1996, Richard Horton, editor of the 
        Lancet, chastised much of current surgical research and, in particular,
        questioned the usefulness of the case series as a predominant form of communication among
        surgeons [1]. He asked a poignant question: “Does surgical research have a future?” Nearly
        a decade later, it is important for surgeons and non-surgeons alike to revisit Horton's
        challenge.
      
      
        Why Surgeons Favor Case Series
        Randomized controlled trials (RCTs) have become the pillar of clinical research. Such
        trials attempt to obtain an unbiased randomization of patients with respect to known and
        unknown baseline conditions and to assess the effects of an intervention. However, only a
        minority of surgical studies involve a valid randomization scheme. The case series remains
        a favored method of clinical investigation in surgery.
        Case series are easy to perform, require less resources in terms of personnel and funds,
        can be performed at a single center, and, for many surgeons, represent a means to
        illustrate their surgical method and skills. In many instances, case series also serve as
        valuable intellectual background for future clinical or scientific work. For example,
        consider Dennis Burkitt's report on jaw tumors in African children, Alfred Blalock's
        initial efforts in cardiac surgery, or, more recently, Starzl and colleagues' observations,
        in a small collection of patients, of donor leukocyte chimerism, whereby recipients acquire
        tolerance to foreign donor cells. In all three cases, the authors' work led to powerful
        shifts in our understanding of the biology and treatment of disease [2,3,4]. All were case
        reports or case series—but under the current paradigm adopted by most journals and
        evidence-based databases, they would not be valued [5,6,7].
      
      
        Barriers to Surgical RCTs
        There are many reasons why RCTs in surgical patients may be more difficult to perform
        than those in non-surgical patients. One of the most important—though least understood—is
        that the complexities of human disease in surgical patients makes them a more difficult
        group to study. Surgical patients are often heterogeneous in many more ways than
        non-surgical patients. So it would be inherently easier, for example, to study a new
        medication for generally healthy young adults with essential hypertension than a surgical
        technique for older patients with hepatic failure needing transplantation.
        In addition, while there may be value in studying patients from multiple centers, there
        may be important differences in the skill levels of different surgeons, either between
        centers or across the country. For example, the skill levels of surgeons in trials of
        carotid endarterectomy may be greater than those across the surgical community as a whole.
        This makes the applicability of some surgical RCTs to the wider community less certain than
        trials of medical therapies.
        So when it comes to surgical research, for both researchers and funding agencies, it is
        easier to grapple with a difficult, but ultimately soluble, basic science question than to
        face the uncertainty of clinical research. Investigators understand these implicit issues
        and trim their sails accordingly.
      
      
        Improving the Rigor of Research
        Nonetheless, too much surgical work is conducted in the less rigorous format of the case
        series. What can and should be done to improve the rigor of surgical investigation? It
        would seem that improvements are required from within and beyond the surgical world.
        First, as Horner observed, and several eminent surgeons have since agreed, reforms must
        begin within the field itself [1,5,6,7]. Both during surgical training and in the early
        years of faculty development, surgeons must obtain a thorough grounding in the principles
        of basic research and proper clinical investigation.
        Second, surgeons must establish firm and friendly relations with biostatisticians so
        that the latter may play a strong role in helping to develop adequately powered studies
        that can answer critical questions raised by new therapies and techniques. This is an
        especially acute need in an accelerating age of targeted therapies and disease
        biomarkers.
        Third, surgeons must re-engage in the clinical research enterprise and resume leadership
        roles in local and national clinical trials that involve surgical patients. In the United
        States, for example, an important step in this regard has been the establishment of the
        American College of Surgeons Oncology Group, which invites surgeons from all sectors,
        including private practice, to become active participants in well-designed,
        multi-institutional trials [5]. Similar efforts are needed on a global level.
        Finally, similar to the pressures faced by their colleagues elsewhere in academia,
        surgeon clinician-investigators must be nurtured, protected, and valued by their colleagues
        and medical administrators. The financial health of academic medical centers relies heavily
        on the generation of clinical revenue, which in many centers falls disproportionately on
        the shoulders of surgeons. New paradigms for revenue generation and funding of clinical
        research are needed.
      
      
        Funding for Surgical Research
        Beyond the walls of the academic medical center, there also needs to be greater
        recognition of the value of scientifically sound surgical research and clinical
        investigation. However, the National Institutes of Health (NIH), the major source of
        biomedical funding in the United States, continues to convey a less welcoming attitude
        toward surgical research than toward other types of clinical or basic science[8,9].
        At the NIH, the principal instrument for performing peer review and making grant funding
        decisions is the study section, composed of about 10–20 members with expertise in a given
        field. There are few study sections devoted to surgically oriented clinical research and
        only two study sections (from among more than 100) in which surgeons make up even a
        reasonable minority of the committee members [8]. In comparison to those in other clinical
        departments, surgical grant proposals are less likely to be funded, and awards, when
        funded, are smaller [8].
        
          
            Funding agencies need to recognize the importance of the surgical endeavor to modern
            medicine.
          
        
        Surgical research is also impeded by processes affecting other types of research as
        well. The number of researchers under 35 years of age receiving a first RO1 grant, the main
        NIH mechanism for external funding, in any field, is below 4%. The average age of initial
        funding for US physicians is about 44 years, and shows a trend toward advancing age that
        has progressed significantly in the past two decades. Thus, the NIH appears to reward
        experience and proven results very heavily, which may stifle innovation and likely serves
        as an innate barrier for younger physician-investigators contemplating research careers
        [9].
        To help correct for this worrisome trend, the NIH created the “K” award system—career
        development grants designed to help starting researchers gain the experience needed to
        compete for RO1 grants. However, nearly 40% of the clinicians who receive KO8 awards never
        apply for RO1 funding [10], which suggests that the overall support—both explicit and
        implicit—for clinical research at the institutional and funding levels is inadequate.
        Finally, outside the US, surgeons face similar, if not greater problems. This bodes
        poorly for countries where the cost of evaluating new therapies and technologies may be an
        unaffordable luxury. These challenges to the surgical research enterprise are therefore
        global issues and should merit the attention of surgeons, medical institutions, and funding
        agencies in all countries.
      
      
        The Future
        What can be done? On the national and international level, funding agencies need to
        recognize the importance of the surgical endeavor to modern medicine. Recently, in the US
        the NIH unveiled a “roadmap” (http://nihroadmap.nih.gov) designed to provide “new pathways
        to discovery.” Clear, careful, scientific surgical investigation must be part of this
        roadmap, although it is not specifically mentioned. Outreach efforts to include surgeons in
        a variety of study sections should be made to ensure that important insights into the
        pathophysiology and treatment of disease, with which surgeons are concerned on a daily
        basis, are not overlooked. Additional efforts are needed to improve funding for clinical
        research, both for individuals at early stages of their careers and for multi-disciplinary
        clinical research and clinical trials. Locally, and individually, surgeons must join
        efforts to improve the clinical research enterprise by including training in clinical
        investigation at an early stage in medical school and during surgical residency training,
        fostering the careers of young surgeon-investigators through committed, protected time,
        participating in local and national clinical research groups, and recognizing that
        development as a clinical researcher takes time—many years in fact.
        These efforts may help ensure that surgical research is a vital part of the future of
        medicine and that it leads to the kind of high-quality work that shapes and remodels the
        face of medicine. To foster these efforts, surgeons must change and adapt to the currents
        of modern medical research. If this is successful, the case series will become the
        occasional rather than the common form of surgical communication. And surgeons, other
        clinicians, and, most importantly, basic scientists will be better able to take advantage
        of the new avenues of biomedical science opening before us.
        But the case series will always represent one important tool for early studies or
        uncommon conditions. It remains true that while the method one uses influences the answer
        one receives, it can be just as important to ask the right questions, which can be asked
        even in a series of one patient [11]. And surely that is the place one must begin.
      
    
  

  
    
      
        
        Gaucher disease is the most common lysosomal storage disorder (Box 1). A deficiency of
        the enzyme glucocerebrosidase (Figure 1) causes accumulation of the glycolipid
        glucocerebroside in macrophages throughout the body. In the viscera, glucocerebroside
        arises mainly from the biodegradation of red and white blood cells. In the brain,
        glucocerebroside arises from the turnover of complex lipids during brain development and
        the formation of the myelin sheath of nerves. The disease may be discovered as an
        incidental finding in the elderly because of mild thrombocytopenia or splenomegaly, or it
        may present early in life with hepatosplenomegaly, thrombocytopenia, anemia, and bone
        lesions.
        Until 1990, treatment consisted only of palliative measures such as splenectomy and hip
        replacement. The development of enzyme replacement therapy for Gaucher disease, that is,
        exogenous administration of the missing enzyme, is a triumph of translational medicine. At
        the same time, powerful commercial interests may have been influential in physicians
        adopting a high-dose rather than a low-dose treatment schedule. Moreover, the high cost of
        enzyme replacement therapy forces us to consider what society can afford in the way of
        palliative treatments for very rare diseases.
      
      
        The History of Enzyme Replacement Therapy
        The possibility that the therapeutic replacement of enzymes missing from lysosomes could
        be achieved was first raised by de Duve forty years ago when he wrote: “Any substance that
        is taken up intracellularly by an endocytic process is likely to end up within lysosomes.
        This obviously opens up many possibilities for interaction, including replacement therapy”
        [1].
        Type 1 Gaucher disease, the most common type, seems a particularly suitable target for
        enzyme replacement therapy because of the lack of central nervous system involvement
        (visceral damage in Gaucher disease is reversible whereas the brain damage usually is not).
        By the 1970s, the underlying enzyme deficiency had been identified, and methods had been
        developed to purify the enzyme from human placenta in a high state of purity. Three groups
        of investigators then attempted to treat the disease by infusing exogenous enzyme.
        In the United States, at the National Institutes of Health in Bethesda, Maryland, the
        unaltered enzyme was infused directly into the venous circulation [2]; at City of Hope in
        Duarte, California, it was entrapped in red cell membranes coated with antibody in an
        effort to direct it to macrophages [3]. In Harrow, United Kingdom, the enzyme was delivered
        entrapped in liposomes [4]. Although some mildly encouraging results were achieved, it was
        clear that none of these approaches was likely to be translated into a useful
        treatment.
        The needed conceptual breakthrough was provided by the identification of a mannose
        receptor on macrophages and the suggestion that this might prove useful in replacement
        therapy for Gaucher disease [5]. This led to the development of a modified enzyme,
        processed to expose mannose, and to its production on an industrial scale from placentas.
        After the gene encoding the enzyme was cloned [6], a recombinant product became
        available.
      
      
        The Pivotal Study
        The first study of commercially produced mannose-enriched glucocerebrosidase was carried
        out in Bethesda, Maryland, on only 12 patients, presumably because of a limited supply of
        the enzyme [7]. Given this small cohort of patients, only a single dose (60 units/kg) was
        administered. This dose was given every two weeks to ten of the patients, while two
        patients received it weekly. This is manifestly an unusual dose schedule for a preparation
        with a circulating half-life of only about 12 min that is being targeted to a relatively
        small number of receptors. Many of the patients studied did not live near Bethesda, and it
        is likely that the dose schedule that was chosen was based on convenience rather than on
        sound pharmacokinetic principles. Since it was unlikely that a second study would be
        launched if the first failed, the investigators wisely used a very generous dose of enzyme
        to maximize the probability that the trial would be successful. Intravenous administration
        of the enzyme produced objective clinical improvement (such as reduced liver and spleen
        size and increased hemoglobin levels and platelet counts).
        The enzyme was promptly approved and marketed. Since only a single dose had been tested,
        this was the dose that most physicians administered in clinical practice. But the
        preparation was extremely costly—about US$4.00 per unit. At the dose used in the pivotal
        trial, a 70-kg patient would receive enzyme costing US$16,800 every two weeks.
      
      
        Dosage Considerations
        
          Visceral organ responses.
          But was the large dose given actually the dose required? There were no data, and many
          physicians were unwilling to give less than the dose that had been used in the pivotal
          trial. Moreover, since most physicians took care of only one or at most two patients with
          the disease, they were not in a position to perform a dose-ranging study. And industry
          had no interest in supporting studies to show that a lower dose yielded equivalent
          results.
          But clinical trials carried out in our National Institutes of Health–sponsored General
          Clinical Research Center quickly established that a quarter of the dose given at more
          frequent intervals was fully effective [8]. By 2000, a considerable body of data had
          accumulated, making it possible to perform meta-analyses of the relationship between the
          total monthly dose, the interval at which the dose is administered, and the decrease in
          the size of the liver. The results were clear (Figure 2) [9]. Even a dose of only 15
          units/kg/mo, one-ninth of the dose given in the pivotal trial, resulted in an excellent
          clinical response. Most patients were receiving a substantial overdose of an extremely
          costly preparation. The data indicate that when very large doses are administered, the
          two-week time interval is adequate to give an optimal response, but when more modest
          doses are administered, more frequent infusions greatly improve the response [9].
          Recent “consensus recommendations,” which were supported in part by the Genzyme
          Corporation, the manufacturers of recombinant human glucocerebrosidase (imiglucerase,
          brand name Cerezyme), suggest that children be given an initial dose of 30 to 60 units
          every two weeks [10]. But there is no high-quality evidence that such a costly treatment
          regimen provides results superior to those achieved with smaller doses. The only support
          for recommending this high dosage comes from uncontrolled studies showing that in some
          children bone lesions may progress at low dosages. However, we know from our own
          published observations that skeletal progression and even fractures also occur in some
          individuals receiving high-dose therapy [11]. Thus, I would caution against any
          recommendations to give high-dose therapy that have not been based on well-designed
          randomized, controlled trials. Having said this, I recognize that most, but not all, of
          the patients that were included in our meta-analyses were adults, whereas the
          company-sponsored consensus recommendations refer to children. However, in the absence of
          any evidence-based rationale for administering large, costly doses of enzyme, I believe
          that the use of smaller, more frequent doses is the most prudent treatment approach.
          It is often assumed that patients with severe disease require larger doses of enzyme
          than those with mild disease, but a meta-analysis based on liver size or spleen size made
          it clear that this is not the case (Figure 3) [12]. Large organs shrink more rapidly than
          smaller ones, and this is true regardless of the dose that is used [13].
        
        
          Skeletal response.
          The response of enlarged viscera to enzyme infusion is much more rapid than the
          response of bones. In one early study, the large dose used in the pivotal trial was given
          for up to four years to patients with bone disease, and although the response was slow,
          gradual improvement occurred [14]. Strangely, the authors concluded that large doses were
          required—“strangely,” because they did not give smaller doses to any patient.
          Subsequently, it was shown that less than a quarter of the dose (only 30 units/kg/mo)
          produced an equivalent response [15].
        
      
      
        Whom to Treat
        The severity of Gaucher disease is very variable. We have estimated that some 60% of
        patients homozygous for the common 
        c.1226 C → G (N370S) mutation never come to medical attention [16].
        Accordingly, many—possibly most—patients with Gaucher disease require no treatment. In
        adults, the disease is rarely progressive [11,17]. What you see is what you have, more or
        less. Bone fractures, of course, are not gradual events but sudden ones. But almost
        invariably they occur in patients who already have very substantial, demonstrable bone
        disease. In children, the situation is different, and progression is common. It is only
        with proper awareness of the natural history of the disease that one can make rational
        judgments regarding who needs treatment.
      
      
        Individualized Treatment
        Evaluating dose–response relationships in patients with Gaucher disease has been
        difficult for several reasons. The number of new patients requiring therapy is relatively
        small, and the Genzyme Corporation has done little to encourage the performance of
        dose–response studies, making it difficult to enroll patients. But beyond that, the
        response of patients to any dose is variable. Some authors have suggested that this may be
        due to individual differences in dose requirements—that some patients are relatively
        resistant and require a large dose, while others do well on a small dose [18]. This is an
        attractive concept, but is it correct? Another meta-analysis indicates that it is not.
        Rather, there are patients who respond poorly to any dose and others who respond well to
        any dose [19]. Moreover, quadrupling the dose does not increase the rate of response
        [11].
      
      
        What Does the Future Hold?
        The quality of life for patients with Gaucher disease has been greatly improved by the
        development of enzyme replacement therapy. Manufacturing and selling the enzyme has also
        been enormously profitable for industry. This profitability has served as a stimulus for
        the development of enzyme replacement treatments for diseases less common and generally
        less responsive to treatment than Gaucher disease. Given the small target population, these
        treatments are enormously costly on a per-patient basis. Treatments for Fabry disease and
        Hurler-Scheie disease (also called mucopolysaccharidosis I) are already licensed, and
        others are on the way [20,21,22]. This brings us face-to-face with a major ethical dilemma.
        We do not put a price on human life. Yet health-care resources are a zero-sum game. What is
        spent on one disease cannot be spent on another. Is it better to treat one child with
        Hurler-Scheie disease [22] or to provide good prenatal care to 100 women who might not
        otherwise obtain it, or for that matter, to feed 1,000 malnourished children? These are
        difficult decisions that will be forced on us as enzyme replacement and other
        high-technology therapies come of age.
      
    
  

  
    
      
        
        Today the possibilities for a medical journal are almost limitless. The first medical
        journals reflected the needs of a closed group of doctors. But medicine, its place in the
        world, and the dissemination of information have changed utterly. So in starting afresh,
        what should a new medical journal retain, and what should it ditch?
        Most obviously, we should throw out the old way of disseminating information. In today's
        electronic age, it is no more difficult, and it is only minimally more costly, to provide
        access to one million people than it is to one person. So the revolutionary idea of anyone
        being able to read any article is possible. This idea—open access—which completely
        challenges the old subscription-based publishing model, is the driving force behind the
        launch of 
        PLoS Medicine . You can download and distribute articles without
        restrictions (feel free to make a thousand copies, translate articles into other languages,
        put articles into books—just give the author proper credit).
        We have also changed the way we involve the academic community in our journal. Our large
        global editorial board reflects the diversity of medicine today and is intimately involved
        in what we do. In particular, members of the editorial board are a crucial part of our peer
        review process. As academic editors they, along with a senior editor at the journal, take
        research papers through the peer review process in a way that we believe provides the most
        constructive and fair review. We are delighted that members of our editorial board have
        also shown their support for our journal by submitting papers to us, even before we
        launched.
        What will we publish? The research article on malaria in this issue reflects our
        priority of publishing papers on diseases that take the greatest toll on health globally.
        But we will also publish papers reporting a substantial advance in any specialty, whether
        that advance is in public health, such as the paper on the global burden of disease; drug
        effects, such as the paper on the effect of HIV drugs on lipids; or the molecular
        understanding of disease, such as the paper dissecting out the immune responses in lung
        disease caused by smoking.
        A good general medical journal should also be a place where the global medical community
        can discuss together what matters to them. The magazine section of 
        PLoS Medicine will be devoted to comment, lively debate, and diverse
        opinions, in particular giving neglected voices and diseases a place in the limelight. In
        this issue's magazine section you will see articles from five continents that cover a huge
        range of topics, from basic sciences (such as the pathology of emphysema) to global public
        health (such as palliative care in developing countries). You will find diverse
        opinions—for example, on whether President Bush is helping or hindering Africa's progress
        towards tackling HIV, and on whether health professionals should routinely screen women for
        domestic violence (tell us what you think by taking our poll at www.plosmedicine.org). And
        you'll find case-based learning materials on meningitis linked to an online video and an
        online quiz.
        
          
            The revolutionary idea of anyone being able to read any article is possible.
          
        
        Interpretation of results is an essential part of a medical journal's job. Although we
        expect that many of our readers will be doctors, we hope readers will range from patients
        wanting to learn about the latest research on their illness, to teachers wanting to use an
        article in the classroom, to policymakers. Hence, we have several levels of comment on
        original research. Perspectives, written by an expert, are aimed at readers who are already
        familiar with the topic. Synopses, written by 
        PLoS Medicine 's professional editors, should provide any health
        professional with a quick introduction to an article. Patient summaries provide a starting
        point for patients to assess the relevance to them of a research paper.
        We have decided not to be part of the cycle of dependency that has formed between
        journals and the pharmaceutical industry, an industry that focuses overwhelmingly on the
        most profitable drugs, thus sidelining many of the world's health problems. Medical
        journals have allowed their interests to become aligned with those of the pharmaceutical
        industry by printing advertisements for drugs, publishing trials designed by drug
        companies' marketing departments, and making profits on reprints used as marketing tools. 
        PLoS Medicine will not accept advertisements for pharmaceutical products
        or medical devices. Our open-access license allows free distribution of articles, so PLoS
        cannot benefit from exclusive reprint sales. And we consider as the lowest priority for
        publication papers that are simply aimed at increasing a drug's market share without
        obvious benefit to patients.
        We will aim to have the highest levels of transparency in our published papers. We
        require authors to tell us of any possible competing interests; we in turn will tell
        readers about them.
        But, information flow should not be just one-way. Our editorial doors (or at least our
        E-mail boxes) are always open. We want your feedback on the journal: send us an E-mail or
        submit an eLetter about any article in the journal, take part in our polls, contribute
        ideas for the magazine section and submit original research. 
        PLoS Medicine is a journal for the global medical community; we invite
        you to join in.
      
    
  

  
    
      
        
        Most patients with celiac disease can eliminate their symptoms—at a price: life-long
        adherence to a gluten-free diet. This means no wheat, rye, barley, and, until recently, no
        oats. Then some recent studies suggested that oats did not cause the intestinal
        inflammation characteristic of the disease, and thus oats are now often included in the
        celiac disease diet. This is good news for patients coping with severe restrictions on what
        they can and must not eat, but a study by Ludvig Sollid and colleagues in this issue of 
        PLoS Medicine suggests that oats are not safe in all cases.
        Like other chronic inflammatory diseases, celiac disease is caused by a complex
        interplay between genetic and environmental factors, but it is better understood than most.
        Long believed to be a relatively rare disorder, it is now thought to affect about one in
        250 people worldwide. Clinical symptoms are present in less than half of patients and vary
        considerably. Genetically, almost all patients have one of two predisposing HLA molecules,
        which determine the context in which their immune system encounters foreign antigens,
        including gluten proteins found in wheat and other cereals. In individuals with celiac
        disease, the immune system mounts an abnormal response to gluten, which is characterized by
        gluten-reactive intestinal T cells and by inflammation and compromised function of the
        small intestine.
        Ludvig Sollid and colleagues applied the current understanding of celiac disease and a
        range of molecular pathology tools to studying the response to oats of nine patients with
        celiac disease. The nine patients were not a random sample: all of them had been eating
        oats, and four of them had shown clinical symptoms after oats ingestion. The goal of the
        study was to characterize the intestinal T cell response to oats in these patients, and to
        relate it to clinical symptoms and intestinal biopsy results. All patients were on a
        gluten-free diet and ate oats that were free of contamination by other cereals.
        Three of the four patients who had reported problems after eating oats showed intestinal
        inflammation typical of celiac disease, and Sollid and colleagues studied intestinal T
        cells from these three patients. Two of the five patients who seemed to tolerate oats also
        had oats-reactive intestinal T cells. Functional study of these T cells showed that they
        were restricted to celiac-disease-associated HLA molecules and that they recognized two
        peptides derived from oat avenin that are very similar to peptides of gluten.
        Taken together, the findings show that intolerance to oats exists at least in some
        patients with celiac disease, and that those patients have the same molecular reaction to
        oats that other patients have to wheat, barley, or rye. However, identical reactions were
        also seen in two of the patients who were clinically tolerant to oats. The authors suggest
        that these reactions could develop into symptomatic disease after some time delay, but
        there is no proof that the presence of oats-reactive T cells is an indicator of future
        symptoms or even of enhanced susceptibility to clinical oats intolerance.
        Oats are not safe for all patients with celiac disease, but future studies are needed to
        determine the frequency of oats intolerance.
      
    
  

  
    
      
        
        Acidosis is a major cause of death in patients with malaria, although what causes
        acidosis is still unclear. One possibility is that hypovolemia contributes to the problem,
        and that rehydration therapy could be of benefit. Now, Sanjeev Krishna and colleagues have
        shown that in children with severe malaria dehydration is not severe and is not correlated
        with other measures of disease severity. “The optimum resuscitation approach in severe
        childhood malaria remains to be defined,” says Nick White (Mahidol University, Thailand),
        the academic editor of the paper. “The relative advantages of blood, colloids, and
        crystalloids need to be characterized.”
        Every year around 200 million people worldwide contract malaria, of whom over a million
        die. The vast majority of those who die are children under five years, mostly in Africa,
        since young children have had little chance to acquire any immunity. Fluid resuscitation is
        generally considered to be a cornerstone of treatment—but how much fluid should be given?
        Some researchers believe that surrogate signs of fluid depletion—such as tachycardia,
        reduced capillary refill time, and reduced urine excretion—suggest that there is
        substantial volume depletion. The reason that the amount of fluid given matters so much is
        that giving too much, especially of hypotonic solutions, can lead to electrolyte imbalance,
        especially hyponatremia and hypokalemia.
        Research efforts have been hampered by not having an easy way to assess in patients the
        fluid depletion in different compartments of the body, i.e., total body water and
        extracellular and intracellular water volume. Krishna and colleagues used heavy-water
        distribution to calculate the total body water and bromide distribution to determine the
        extracellular volume in 19 children with moderately severe malaria and 16 with severe
        malaria in Gabon. By subtracting extracellular volume from total body water, they were able
        to calculate intracellular volume for each child. They also used a less invasive and more
        rapid method of determining water volumes based on using bioelectrical impedance to
        calculate the volume.
        None of the children were severely dehydrated (defined as more than 100 ml/kg
        depletion), and only three of the children with severe anemia had fluid depletion, which
        was moderate (60–90 ml/kg depletion). “This challenges the view that dehydration is a major
        contributor to the pathology of this frequently lethal disease,” says White.
        So based on these data, obtained from a carefully studied, albeit small group of
        children, what should people who treat children with malaria do? The authors' first
        recommendation is that clinicians should think again about how vigorously they rehydrate
        children, and if they have access to ways of assessing fluid volume more precisely, they
        should do so (not a trivial undertaking in many hospitals where these children are
        treated). And certainly the methods used by Krishna and colleagues should undergo wider
        testing in larger groups of children to confirm their usefulness. Until the worldwide
        efforts to prevent malaria come to fruition, refining the management of infected children
        will remain a cornerstone of the efforts against this devastating disease.
      
    
  

  
    
      
        
        T lymphocytes may have an important role in the pathogenesis of smoking-related
        emphysema, according to a new study by researchers from Houston, Texas, United States. “We
        now know that T cells are not only present in chronic obstructive pulmonary disease [COPD],
        but are harmful,” comments Steven Shapiro from Brigham and Women's Hospital
        , Harvard Medical School, who was not involved in the study. “We also now
        have a pathway that could be interrupted to prevent lung destruction in COPD.”
        Farrah Kheradmand and colleagues took lung samples from 28 ex-smokers who had been
        admitted to hospital for lung resection: 18 patients had moderate to severe COPD as well as
        evidence of emphysema, and ten patients had none. The researchers isolated lung lymphocytes
        from the samples and used two-color flow cytometry to phenotypically characterize the
        cells. They found that lymphocytes taken from patients with emphysema expressed more CCR5
        and CXCR3 receptors, which are associated with a particular type of T cell called T helper
        1 (Th1), than did those from control individuals. By contrast, expression of CCR4
        receptors, which are found on T helper 2 (Th2) cells, was very low in both control and
        emphysema groups.
        In a separate experiment, Kheradmand's team showed that lung lymphocytes taken from
        patients with emphysema secreted more of three other proteins—interferon gamma, monokine
        induced by interferon (MIG), and interferon-inducible protein 10 (IP-10)—than control
        patients. MIG and IP-10 are known to be produced by injured epithelial cells and are
        ligands for CXCR3 receptors, which are expressed by Th1 cells. Importantly, the researchers
        were also able to show that isolated peripheral lung macrophages secreted matrix
        metalloproteinase-12 (MMP12), an enzyme that degrades elastin—a protein important for lung
        elasticity—in the lungs, in response to IP-10 and MIG. Together these findings, say the
        authors, indicate that Th1 cells, but not Th2 cells, are required for producing the
        elastin-destroying lung environment of emphysema.
        The researchers now intend to investigate the antigens that drive the Th1-based
        inflammation that underlies emphysema. “Ultimately, we seek to understand the biochemistry
        of tobacco smoke that triggers inflammation in the first place, and whether such insight
        might explain other environmentally triggered lung diseases,” explains Kheradmand. “To
        understand such detailed immune mechanisms, we really need an improved experimental model
        of disease, and this we are currently working on.”
      
    
  

  
    
      
        Tumor Antigen Recognition by Cytolytic T Lymphocytes
        CD8
        + cytolytic T lymphocytes (CTLs) are the primary effector cells of the
        adaptive immune system and have a major role in protecting us from a vast array of diseases
        including cancer. CTLs specifically recognize and lyse targets through the interaction of T
        cell receptors (TCRs) on the surface of the T lymphocyte with protein fragments (peptides)
        presented on the surface of target cells, in association with major histocompatibility
        complex (MHC) class I molecules. When a particular CTL interacts with a target cell, it
        rapidly divides to form a clonal population of T cells with the identical TCR.
        Townsend and colleagues first elucidated the molecular basis of target cell recognition
        by CTLs in 1985 [1]. They showed that antigens are processed inside the target cell into
        nine- or ten-amino-acid-long peptides, which are then presented at the surface in
        association with MHC class I molecules. This discovery suggested the possibility of using
        short synthetic peptides mimicking naturally processed antigens as immunotherapeutic drugs
        and vaccines. Short synthetic peptides are ideal for drug development because of the
        relatively low cost of production, easy storage, and high safety. However, not all peptides
        and MHC alleles work well together to stimulate CTLs. So for clinical use, either patients
        would have to be selected for treatment based on their MHC I type or it would be necessary
        to make multiple peptides to cover the majority of MHC class I alleles in a given
        population.
        Furthermore, before Boon and colleagues cloned the first antigen recognized by
        tumor-reactive CTLs in 1991 [2], it was not clear which antigens were recognized by
        tumor-reactive CTLs in humans; so, it was not possible to rationally design cancer
        vaccines. Now, however, a long list of more or less tumor-specific antigens has been
        generated [3]. Most of the peptides identified so far are either normal self proteins
        aberrantly expressed in cancer but not in most other adult normal tissues or
        tissue-specific antigens also expressed in certain types of cancer. Some patients show a
        spontaneous CD8
        + T cell response (occasionally at high levels) that is specific for
        several of these antigens. The development of such responses, however, requires a large
        tumor load, occurs late in the disease, and probably does not cause the efficient
        destruction of the tumor cells [4]. Thus, a central objective in cancer immunotherapy is to
        efficiently produce tumor-reactive CTLs at an earlier phase of the disease.
      
      
        Heteroclitic Tumor Antigen Peptides
        Unfortunately, some synthetic peptides, including some corresponding to immunodominant
        epitopes (those which cause the biggest part of the immune response) from tumor antigens,
        only seem to bind MHC class I molecules with medium to low affinity and/or are recognized
        by specific T cells with relatively low avidity. These characteristics are the likely cause
        of the poor immune reaction generated by these peptides [5]. One strategy to improve the
        immune reaction is to make what are called heteroclitic antigen variants. By improving
        either peptide binding to MHC, recognition by TCRs, or both, these variants have increased
        peptide antigenicity and immunogenicity.
        Solinger and colleagues were the first to describe antigen variants producing T cell
        responses that were stronger than those elicited by the parental sequences [6]. Some
        heteroclitic tumor antigen peptides that showed highly improved antigenicity and
        immunogenicity in preclinical studies, and which also cross-reacted well with CTLs
        generated against the parental sequence, were tested in clinical trials. The peptides
        selected for trials mostly contained substitutions of anchoring amino acids that were
        designed to increase peptide binding to the MHC molecule while minimally changing the shape
        of the epitope [7,8].
        In a study by Lee and colleagues in this issue of 
        PLoS Medicine [9], despite the careful study design, vaccination with
        these peptides resulted in the recruitment of T cells that bound antigens less efficiently
        and had lower tumor reactivity than those from the endogenous response to the tumor. The
        authors propose that the cause for the decreased affinity of vaccine-elicited CTLs could be
        the high antigen density of these synthetic peptides on antigen-presenting cells. An
        alternative explanation, however, is that the synthetic peptides used for vaccination
        simply fail to faithfully mimic the naturally processed antigens (Figure 1). The use of
        peptides that differ from those resulting from natural intracellular processing has
        previously given rise to similar problems [10,11]. In any case, the enormous diversity in
        the normal TCR repertoire provides a molecular explanation of the observed phenomenon.
        These results emphasize how difficult it is to translate findings, such as the spectacular
        results obtained by the vaccination of TCR transgenic mice with heteroclitic peptides [12],
        into an application for normal animals and humans.
      
      
        Conclusion
        It is increasingly clear that even the smallest alteration in the structure of the MHC
        peptide complex can result in significant changes in which TCRs are selected after
        vaccination. Thus, manipulating the immune T cell repertoire in vivo through the use of
        heteroclitic tumor antigen peptide variants could be harder than anticipated. As the field
        moves rapidly towards the use of new vaccine adjuvants with high immunogenic potential
        [13], reassessment of the immunogenicity of natural sequences could be worthwhile in some
        cases. In addition, the careful analysis of antigen-specific T cell clones, such as that
        reported here by Lee and colleagues, will be crucial to ascertain the quality of the
        elicited immune response.
      
    
  

  
    
      
        Introduction
        The immunotherapy of cancer holds promise in harnessing the host immune response to
        specifically target tumor cells without harming normal tissues. Strategies involve adoptive
        cellular therapy or active immune induction (commonly referred to as “cancer vaccination”).
        Cancer vaccines may consist of whole tumor cells or tumor lysates, but identification of
        tumor-associated antigens (TAAs) over the past decade has made possible the use of specific
        proteins or peptides as cancer vaccines. The anti-tumor potential of TAA-specific CD8+ T
        cells has been illustrated by the demonstrated capacity of adoptive T cell therapy to
        reduce tumor size [1]. While endogenous anti-tumor CD8+ T cell responses may already exist
        in some cancer patients [2], vaccination with TAA-derived peptides, and in particular
        heteroclitic peptide analogs, increases the frequency of TAA-specific T cell responses to
        detectable levels in many patients [3,4,5,6,7,8,9]. Heteroclitic peptide analogs are
        created by substitutions at anchor residues resulting in increased association of peptide
        with the major histocompatibility complex (MHC) [10]. Consequently, heteroclitic peptide
        analogs are predicted to be more immunogenic than their native counterparts because of more
        stable binding at the surface of antigen-presenting cells (APCs). Indeed, T cells capable
        of tumor lysis have been isolated from patients vaccinated with heteroclitic peptide
        [8,11,12,13]. However, the presence of TAA-specific T cells elicited by vaccination often
        does not correlate with clinical responses [3,14,15,16,17].
        Various reasons for the paradoxical coexistence of cancer cells and TAA-specific T cells
        within patients have been proposed [18,19]. One possibility is that elicited TAA-specific T
        cells are not optimally functional in vivo [2,18]. Another possibility is that T cells
        inefficient in tumor recognition or lysis are induced by vaccination [20]. It is becoming
        recognized that antigen-specific T cells may have substantially different requirements for
        cognate peptide (the peptide that is recognizable to a specific T cell clone) for efficient
        target lysis [20,21,22,23]. “Recognition efficiency” (RE) (also known as “functional
        avidity”) is a measure of the sensitivity of a T cell to different peptide concentrations
        for stimulation [24,25,26]. We hypothesized that high antigen densities on APCs resulting
        from vaccination with heteroclitic peptide may paradoxically drive T cells of predominantly
        low RE, which are not efficiently activated by the endogenous expression levels of native
        peptides on tumor cells. Consequently, such T cells would be ineffective in tumor cell
        destruction. Support for this notion is emerging: T cells with low RE are predominantly
        expanded in vitro with high peptide concentration [22]. Moreover, in vitro stimulation of T
        cells from healthy donors with heteroclitic peptides results in expansion of cells with a
        wide range of RE [23]. A similar phenomenon may occur in vivo, leading to TAA-specific T
        cells of low RE depending on the nature of antigen stimulation [20].
        While isolated T cell clones with low RE have indeed been generated from melanoma
        patients following heteroclitic peptide vaccination, the proportion of vaccine-elicited T
        cell responses these cells represent in vivo is not clear. If predominantly high-RE,
        tumor-cytolytic T cells are generated, then a small fraction of low-RE T cells generated
        would be of little consequence. However, if predominantly low-RE T cells are generated,
        then this low proportion of high-RE T cells may be an important factor in the observed lack
        of clinical effectiveness of current cancer vaccination strategies. To address this
        important issue, we undertook a systematic examination of the complexity of T cell
        responses induced by heteroclitic peptide vaccination, and compared these responses to
        endogenous anti-tumor T cell responses which develop in some patients. Typically, responses
        to vaccination are examined following in vitro expansion from patient samples, which may
        alter the composition of cells and consequently not reveal the proportion of cells in vivo
        having sufficiently high RE to lyse tumor targets. Although staining with peptide–MHC
        tetramers provides a direct estimate for the number of TAA-specific T cells present in
        vivo, and intensity of tetramer staining has been employed as a parameter for isolation of
        high-RE, tumor-lytic T cells [27], staining intensity does not correlate well with RE or
        tumor-lytic potential [28,29], and cannot be considered a reliable indicator for the
        functional status of TAA-specific T cells.
        To analyze and compare T cell responses in melanoma patients on a single-cell level, we
        generated and examined a large number of cytotoxic T lymphocyte (CTL) clones derived from
        post-vaccination or endogenous anti-tumor T cell responses. Each clone was analyzed for T
        cell receptor (TCR) variable chain beta (VB) expression, RE, and ability to lyse melanoma
        targets. Importantly, these clones were generated directly ex vivo through tetramer-guided
        sorting, which minimizes the selection bias that could be introduced by prior in vitro
        expansion. Therefore, data from these clones could be taken to estimate the complexity of
        the responses in vivo.
      
      
        Methods
        
          Patients and Samples
          All patients had resected stage III or IV melanoma, as determined by the 1988 modified
          American Joint Commission on Cancer staging system. They were required to have a magnetic
          resonance imaging or computed tomographic scan of the head and computed tomographic scans
          of the chest, abdomen, and pelvis showing no indication of disease within 4 wk of therapy
          to verify that they were clinically free of melanoma. Eligibility criteria included age
          18 y or older, creatinine of less than 180 μmol/l, bilirubin of less than 110 μmol/l,
          platelet count of 100 × 10
          9 /l or more, hemoglobin of 90 g/l or more, and total white blood cell
          count of 3.0 × 10
          9 /l or greater. Tests for human immunodeficiency virus, hepatitis C
          antibody (Ab), and hepatitis B surface antigen were required to be negative, and all
          patients were HLA-A2 antigen positive by a microcytotoxicity assay. All patients were
          required to comprehend and sign an informed consent form approved by the National Cancer
          Institute (NCI; Bethesda, Maryland, United States) and the Los Angeles County/University
          of Southern California Institutional Review Board. Analysis of the patient samples was
          approved by Stanford University's Institutional Review Board. Peripheral blood
          mononuclear cell (PBMC) samples were isolated from patients after vaccination with the
          heteroclitic peptides MART 26–35 (27L) (ELAGIGILTV) and gp100 209–217 (210M) (IMDQVPSFV)
          at the University of Southern California Norris Cancer Center (Los Angeles, California,
          United States). Clinical-grade peptides used were provided by the Cancer Therapy
          Evaluation Program of the NCI under an Investigational New Drug application BB 6123 held
          by the NCI. Immunizations (1 mg of each peptide emulsified with incomplete Freund's
          adjuvant) were administered every 2 wk for 8 wk, then every 4 wk for 12 wk, and then once
          8 wk later. PBMC samples were collected 4 wk after the final immunization and stored at
          −130 °C. Samples were thawed the day before an experiment for overnight culture in CTL
          medium. The following morning, viable cells were isolated by ficoll density
          centrifugation, washed, and resuspended to the appropriate concentration in a solution of
          90% Iscove's Modified Dulbecco's Medium (IMDM) and 10% fetal bovine serum (FBS).
        
        
          Flow Cytometry Analysis
          For isolation and detection of peptide-specific T cells, patient PBMC samples were
          stained and analyzed by fluorescence-activated cell sorting (FACS) as previously
          described [2]. Briefly, cells were stained with anti-human CD8− fluorescein
          isothiocyanate (Caltag Laboratories, Burlingame, California, United States) and
          CD19-CyChrome (BD Biosciences, Palo Alto, California, United States) Abs, and
          HLA-A*0201/peptide tetramer–phycoerythrin (PE). The final staining dilution of each Ab
          was 1/200 and 1/80, respectively. Tetramer–PE was titrated for optimal staining, usually
          between 1 and 10 μg/ml. For TCR VB typing, cells were divided in seven aliquots and
          stained with CD8 PerCP-Cy5.5 (BD Biosciences), tetramer–PE, and a panel of two or three
          different anti-VB monoclonal Abs labeled with fluorescein isothiocyanate, allophycocyanin
          (APC), or both. Cells were incubated at room temperature for 30 min, washed, then
          analyzed using a two-laser, four-color FACSCalibur (Becton Dickinson, Franklin Lakes, New
          Jersey, United States) or sorted using a FACSVantage flow cytometer (Becton Dickinson).
          Lymphocytes were identified by forward and side scatter signals, then selected for CD8+
          and tetramer positive. Up to one million events were acquired and analyzed using FlowJo
          (TreeStar, San Carlos, California, United States).
        
        
          CD107 Mobilization Assays
          
            Target cells
            The HLA-A*0201-positive melanoma lines Malme-3M and A375 and the T2 cell line were
            purchased from ATCC (Manassas, Virginia, United States) and maintained according to
            instructions provided by the ATCC. The HLA-A*0201-positive melanoma line mel526 was
            obtained from the Surgery Branch of the NCI. While Malme-3M and mel526 express both
            MART and gp100, A375 does not express MART or gp100 and served as a negative control.
            Expression (or lack thereof) of these antigens by each cell line was further confirmed
            by immunohistochemical staining. Cells were trypsinized using Trypsin/EDTA solution
            (GIBCO, San Diego, California, United States) before use. T2 cells were HLA-A2.1+ and
            were pulsed prior to assays with peptides indicated in the text.
          
          
            Effector cells
            Effector cells, which include clones, cell line, and PBMC samples, were frozen and
            analyzed in batches. The cells were thawed the day before an experiment for overnight
            culture in CTL medium. The following morning, viable cells were isolated by ficoll
            density centrifugation, washed, and resuspended to the appropriate concentration
            (usually 10
            7 /ml) in CTL medium.
          
          
            Experimental procedure
            All assays were done at least twice, with duplicates for each condition. The
            effector to target (E:T) ratio used was generally 1:2, with 2 × 10
            5 for clones or 10
            6 for the cell line and patient PBMC samples. To each well, the
            following was added in order: 1 μl of 2 mM monensin (Sigma, St. Louis, Missouri, United
            States) in 100% EtOH, 100 μl of target cells, 100 μl of effector cells, and 1 μl of
            CD107-APC Abs. The cells were mixed well using a multichannel pippetor. The plate was
            centrifuged at 300
            g for 1 min to pellet cells, then placed into an incubator at 37 °C
            for 4 h. After the incubation, the plates were centrifuged to 500
            g to pellet cells, and the supernatant was removed. Cell–cell
            conjugates were disrupted by washing the cells with PBS supplemented with 0.02% azide
            and 0.5 mM EDTA, and mixed vigorously, then stained with additional Abs.
          
        
        
          Generation of CTL Clones
          CD8+ T cell clones were derived by FACSorting individual tetramer-positive cells from
          PBMC samples prepared for flow cytometry as described above. CD8+ tetramer-positive T
          cells were sorted under sterile conditions into 96-well plates, one cell per well, using
          a FACS Vantage (Becton Dickinson). Wells contained 100 μl of CTL IMDM, with 10% FBS, 2%
          human AB sera, and penicillin, streptomycin, and L-glutamine, supplemented with 100
          units/ml IL-2. Sorted cells were expanded in vitro using standard protocols. Briefly,
          irradiated feeder cells (JY cells and fresh PBMCs) were added to wells containing the
          sorted T cells, and the 96-well plates were incubated at 37 °C with 7% CO
          2 to allow for growth. Potential clones became visible around day 14
          and were then transferred to 24-well plates containing 1 ml of CTL medium with 100
          units/ml IL-2. Wells were selected based on cell confluency for expansion and further
          analysis. Clones confirmed to be tetramer-positive were expanded in T-25 flasks
          containing irradiated JY cells and fresh PBMCs in 25 ml of CTL medium containing PHA.
          IL-2 was added to a final concentration of 50 units/ml on day 1 and then every 2 d
          thereafter for 2 wk.
        
        
          Cytotoxic Assays
          
            Target cells
            Target cells were as described above under CD107 Mobilization Assays, and were
            labeled overnight with 
            51 Chromium, washed, and resuspended to 10
            5 cells/ml. One hundred microliters of target cells were incubated
            with 100 μl CTL clones at 10:1 E:T ratio for 4 h. Percent specific release of 
            51 Chromium from target cells was calculated from 40-μl cell-free
            supernatants.
          
          
            Determination of RE
            Chromium-labeled T2 targets were pulsed with a range of peptide concentrations,
            generally starting at 10
            −7 M and decreasing by log steps to 10
            −13 M. T cell clones were incubated with T2 targets at 10:1 E:T
            ratios for 4 h, then chromium release was measured and percentage cytotoxicity
            calculated by standard methods. Prior to each cytotoxicity assay, clones underwent
            ficoll-hypaque centrifugation to remove dead feeder cells and were determined to be
            greater than 80% CD8+ tetramer-positive T cells by FACS. The E:T ratio was based upon
            live T and target cells. For each T cell clone, percent cytotoxicity was plotted
            against peptide concentration. The peptide concentration at which the curve crossed 40%
            cytotoxicity was defined as the RE of that clone [30].
          
          
            Microcytotoxic assay
            Cells were isolated directly from PBMCs from patient 422 by FACS as described above.
            Cells were collected in microfuge tubes containing 1 ml of ice-cold 90% IMDM with 10%
            FBS. Collected cells were washed and resuspended to 83,300 cells/ml in 90% IMDM with
            10% FBS. Targets were prepared as described above and resuspended to 8,300 cells/ml in
            90% IMDM with 10% FBS. A total of 2,500 sorted cells (30 μl) and 250 target cells (30
            μl) were transferred to a microcentrifuge tube (VWR International, West Chester,
            Pennsylvania, United States), centrifuged 1 min at 200
            g, and incubated 4 h at 37 °C. Percent specific release of 
            51 Chromium was calculated from 40 μl of cell-free supernatant.
          
        
        
          TCR VB Spectratyping
          RNA was extracted from clones and tetramer-positive cells using TRIzol (Invitrogen,
          Carlsbad, California, United States) and reverse-transcribed into cDNA using SuperScript
          II Reverse Transcriptase (Invitrogen). PCR was performed using 34 different 5′ primers
          that specifically amplify all functional TCR VB genes. Most of the 5′ primers used have
          been previously described [31]. These primers were used in combination with a common 3′
          primer based in the beta chain constant region, BC63 (5′- GTGTGGCCTTTTGGGTGT-3′). As an
          internal control, PCR for a section of the beta chain constant region was performed in
          parallel with VB-specific PCRs using the following primers: UpBC (5′-
          CGCTGTGTTTGAGCCATC-3′) and LoBC (5′- TGCTCAGGCAGTATCTGGA-3′). All primer concentrations
          were 200 nM. PCR was performed using an iCycler iQ thermic cycler equipped with a
          real-time detection system (Bio-Rad, Hercules, California, United States) and a
          QuantiTect SYBR Green PCR kit (Qiagen, Valencia, California, United States). PCR
          reactions were performed as follows: 94 °C for 9 min, followed by 50 cycles of 94 °C for
          30 s, 58 °C for 1 min, and 72 °C for 1 min, followed by 72 °C for 10 min. Specific
          amplification was determined relative to constant region control PCR. For spectratyping,
          PCRs were performed as described above with the following VB14- and VB17-specific 5′
          primers: VB14m (5′- ACCCAAGATACCTCATCACAG-3′) and VB17 (5′- GACAGGACCCAGGGCAAG-3′),
          followed by a run-off PCR with downstream VB-specific primers: VB14 (5′-
          GGGCTTAAGGCAGATCTACT-3′) and VB17m (5′- TTTCAGAAAGGAGATATAGCT-3′), and FAM6-labeled BC63
          3′ primer. Run-off PCR was performed as described above except that only five cycles of
          PCR were run with the 55 °C annealing temperature and QuantiTect Probe PCR kit (Qiagen).
          Labeled PCR fragments were run on an ABI Prism 377 DNA Sequencer (Applied Biosystems,
          Foster City, California, United States) and analyzed using GeneScan software (Applied
          Biosystems).
        
        
          Statistical Analysis
          A standard software package (SigmaPlot 5.0, Systat Software, Richmond, California,
          United States) was used to provide descriptive statistical plots. Barcharts were provided
          with standard errors on them. Linear plots were provided with standard errors computed at
          each point. A linear regression (using least squares) of percent specific lysis on
          recognition efficiency is shown in Figure 5A and 5B.
        
      
      
        Results
        
          T Cell Responses to TAAs in Patients with Melanoma
          To address the complexity of T cell responses against melanoma in vivo, patients with
          vaccine-induced or endogenous TAA-specific responses were selected. In recent cancer
          vaccine trials [3,4,5], many melanoma patients who received heteroclitic peptide vaccines
          gp100 209–217 (210M) (IMDQVPSFV; G209–2M) and MART 26–35 (27L) (ELAGIGILTV; M26) had
          measurable CD8+ peptide-specific T cell responses in PBMCs detected by peptide–MHC
          tetramer staining. In addition, TAA-specific T cell responses could be detected in some
          patients without vaccination, suggesting the existence of an endogenous anti-tumor T cell
          response in these patients. For the current study, we selected samples from six melanoma
          patients from these trials—four with vaccine-elicited responses (patients 422, 476, 517,
          and 520) and two with endogenous T cell responses (patients 132 and 461)—for detailed
          analyses of TCR VB usage, RE for the target peptide, and tumor cytotoxicity. The samples
          from these six patient had peptide-specific T cell populations detectable with
          G209–2M-tetramers (patients 422, 476, and 132) or M26-tetramers (patients 517, 520, and
          461) ranging from 0.1% to 2.5% of total CD8+ T cells (Figure 1A).
        
        
          Vaccine-Elicited T Cells Are Functional Directly Ex Vivo but of Variable Tumor
          Reactivity
          Patient 422 had the largest detectable TAA-specific CD8+ T cell response (2.5%
          G209–2M-tetramer-positive) and thus sufficient numbers for examination of lytic function
          immediately following isolation. To test whether peptide-vaccine-induced T cell responses
          were functionally active directly ex vivo, T cells isolated by G209–2M-tetramer-guided
          cell sorting from patient 422 were tested for lysis of peptide-pulsed and melanoma target
          cells in microcytotoxic assays (Figure 1B). The directly isolated tetramer-positive T
          cells from this patient specifically lysed T2 cells pulsed with high concentrations (1
          μg/ml) of G209–2M and native (G209n) peptides, but not with T2 cells pulsed with a
          cytomegalovirus-derived, HLA-A*0201-restricted peptide (NLVPMVATV) or melanoma targets.
          This suggests that while a significant portion of the vaccine-elicited T cells from
          patient 422 may be functional in vivo, they did not have significant tumor lysis
          activity.
          To assess the functional status of the smaller TAA-specific CD8+ T cell responses in
          the other five patients—which were too small for direct cytotoxicity assays after
          sorting—we utilized a novel FACS assay for degranulation based on CD107 mobilization
          [24]. All six TAA-specific populations exhibited robust functional responses ex vivo, as
          measured by percentage of G2090–2M- and M26-tetramer-positive cells that mobilized CD107
          and/or downregulated the CD3 complex upon incubation with T2 cells pulsed with cognate
          peptides (Table 1; 86%-99.6%). In response to melanoma targets mel526 and Malme-3M, which
          both express gp100 and MART-1 and are HLA-A*0201 positive, the two endogenous
          TAA-specific responses (samples from patients 132 and 461) also exhibited robust
          functional responses directly ex vivo (Table 1; 36.8%–87%), and these responses were
          specific as they had little response to A375, a HLA-A*0201-positive melanoma cell that
          does not express gp100 or MART-1 and served as a negative control for antigen-specific
          killing (Table 1; 2.7% and 3%). In contrast, the vaccine-elicited responses exhibited
          much lower reactivity to mel526 and Malme-3M (Table 1; 23.8%–32.5%). These data
          demonstrate that all six TAA-specific CD8+ T cell responses were functional ex vivo, but
          there were significant differences in reactivity to melanoma targets between endogenous
          and vaccine-elicited responses.
          To substantiate the generality of these findings, we analyzed four additional patients
          with vaccine-elicited responses. One subject responded to G209–2M only (patient 722), one
          to M26 only (patient 713), and two to both G209–2M and M26 (patients 721 and 735).
          Similar to the first four vaccine-elicited patients, these four additional patients (six
          TAA-specific responses in total) exhibited variable reactivity to melanoma targets,
          ranging from 13% to 49.6% (Table 1).
        
        
          Vaccine-Elicited T Cells Have Varied Capacity to Lyse Melanoma Targets
          To confirm and further investigate the differences in tumor reactivity between
          endogenous and vaccine-elicited responses, we reasoned that analysis of a set of clonal
          CTL lines that represented the tetramer-positive population would provide an accurate
          estimate of the complexity of the TAA-specific T cell response in each patient. A large
          number of clonal CTL lines (more than 200) were generated by FACS of individual G209–2M-
          and M26-tetramer-positive cells directly from PBMC samples (Table 2). Up to 85% of sorted
          cells expanded in various sorts (data not shown). Randomly selected expanding clones and
          the tetramer-positive population from which they were derived were examined for TCR VB
          expression using TCR VB-specific monoclonal Abs and VB-specific primers in PCR. Diverse
          TAA-specific T cell responses were found in the four vaccinated patients, with multiple T
          cells expressing different TCR VB, while the two endogenous responses were less diverse.
          All but one clone derived from patient 132 expressed VB17, while two dominating T cell
          populations in patient 476 expressed VB14 and VB17 (Table 2). The clonality of the
          dominant populations in these patients was evaluated by PCR fragment length analysis
          (Table 3). Identical length fragments were demonstrated in the four selected clones from
          476 BV14+ and 476 BV17+ populations. Identical length fragments were also demonstrated in
          all BV17+ clones from patient 132. Furthermore, analysis of sorted tetramer-positive
          cells from patient 476 demonstrated single fragment sizes for BV14 and BV17, which were
          identical to the fragment sizes generated from the selected clones, arguing for clonality
          of these dominant populations (Table 3).
          Peptide specificity and CD8 expression of each clone was confirmed by staining with
          G209–2M- and M26-tetramers and anti-CD8 monoclonal Ab (data not shown). To obtain an
          accurate reflection of the total T cell population detected with tetramer in each
          patient, we decided to rigorously examine at least one representative clone for each
          subpopulation expressing a different TCR VB (Table 4). Multiple clones were analyzed to
          determine dominating populations. From patients 132, 517, and 461, for which fewer clones
          were generated, all clones were included in the analyses (Table 4).
          To determine the effectiveness of tumor lysis by the different TAA-specific T cell
          clones that were propagated, clones were analyzed for their ability to lyse melanoma cell
          lines mel526 and Malme-3M. A375 cells served as a control for antigen-specific killing.
          In addition, each CTL clone was examined for antigen-specific lysis of T2 cells pulsed
          with high levels (1μg/ml) of G209–2M or M26 peptides. “Efficient lysis” in these
          experiments was defined as 40% or greater specific release of radiolabel from the target
          cells; 10% or less specific release was categorized as “low or no lysis,” and 10% to 40%
          was termed “intermediate lysis.” All but two of the CTL clones elicited from endogenous
          anti-tumor responses (from patients 132 and 461) exhibited “efficient lysis” of both the
          mel526 and Malme-3M melanoma cell lines (Figure 2). In contrast, only a few clones from
          the vaccine-elicited responses (from patients 422, 476, 520, and 517) efficiently lysed
          melanoma cells. The majority of clones examined from these vaccine-elicited responses
          either failed to lyse melanoma targets altogether or lysed them with intermediate
          efficiency (Figure 2). This lack of efficiency in melanoma cell lysis was not due to
          cellular dysfunction, since each clone efficiently lysed T2 cells pulsed with high levels
          of relevant, but not irrelevant, peptide (Figure 2). Overall, the majority of clones
          derived from endogenous anti-tumor responses (patients 132 and 461) lysed both mel526 and
          Malme-3M melanoma target cells more efficiently than clones from vaccine-elicited
          responses (patients 422, 476, 520, and 517) (Figure 3). These findings suggest that
          TAA-specific T cells elicited by heteroclitic peptide vaccination have different
          tumor-cytolytic potentials from those which develop endogenously to cancer.
        
        
          RE for Native and Heteroclitic Peptides of T Cells from Endogenous or
          Vaccine-Elicited Responses
          We hypothesized that CTL clones that did not efficiently lyse melanoma targets may be
          incapable of recognizing the relatively low surface densities of native peptide present
          on tumor cells. CTL clones selected for analysis of tumor lysis were also assessed for RE
          for the native and heteroclitic peptides via a ten-log range of dilutions. This is
          illustrated with clones 132.1 and 476.105 (Figure 4A). There were considerable
          differences in killing of peptide-pulsed T2 cells by these two clones. The differences in
          RE for G209n native peptide displayed by the two clones highlighted in Figure 3A
          correlated with their ability to lyse melanoma cells: the high-RE clone 132.1 efficiently
          lysed melanoma targets, whereas the low-RE clone 476.105 did not (Figure 4B). In contrast
          to the differences in RE for G209n peptide, similar assays revealed little difference in
          RE of the two clones for G209–2M heteroclitic peptide (Figure 4C), suggesting that these
          clones recognize the native and heteroclitic peptides differently, and that RE for the
          native, but not heteroclitic, peptide correlates with tumor-lytic potential.
          Similar RE assays were performed for the remaining clones from each patient selected
          for analysis. In order to compare REs of various CTL lines, each clone was assigned an RE
          score expressed as the negative log
          10 value of the peptide concentration required for 40% specific lysis
          at an E:T ratio of 10:1. For clones 132.1 and 476.105, these scores were 11.1 and 8.3 for
          assays with G209n peptide (Figure 4A), and 11.2 and 11.2 for assays with G209–2M
          heteroclitic peptide (Figure 4C), respectively. We compiled the data on clones from all
          patients, which showed a correlation between tumor-lytic potential and RE for native
          peptide (Figure 5A and 5B). Overall, clones generated from endogenous anti-tumor
          responses had higher RE for the native peptide than clones generated from post-vaccine
          responses (Figure 5C and 5D). We estimated the composite RE of the overall TAA-specific
          response (composed of a heterogeneous population of T cells) in vivo by summing the RE of
          each clone multiplied by its representation in the original mixture (the representation
          was estimated based on the proportion of TAA-specific cells expressing the same VB as the
          clone). These composite RE values are represented in Figure 5 as horizontal bars for each
          response. Clearly, the endogenous responses (patients 461 and 132) had a higher overall,
          and more homogeneous, RE for the native peptide than the vaccine-elicited responses
          (patients 422, 476, 517, and 520) (Figure 5C and 5D). Importantly, the vaccine-elicited
          clones also exhibited wide variations in RE even for the heteroclitic peptide, compared
          to the endogenous responses (Figure 5E and 5F). This suggests that the variation in RE
          for native peptides, and hence ability to lyse tumor cells, for vaccine-elicited
          responses is not merely a reflection of differential recognition of native and
          heteroclitic peptides by many clones. Rather, variations in RE may be a function of the
          manner in which these cells were elicited in vivo via vaccination.
        
      
      
        Discussion
        To achieve maximal clinical responses, the majority of T cells elicited by vaccination
        in cancer patients should be capable of responding to tumor targets. We have undertaken the
        most detailed analysis to date, on a single-cell level, of T cell responses elicited by
        cancer vaccination and have compared these with endogenous anti-tumor responses. To
        evaluate the full spectrum of T cells elicited in each patient by vaccination, we utilized
        tetramers made with the vaccine peptides (heteroclitic M26 and G209–2M) to isolate such
        cells. CTL clones were selected directly from patient PBMC samples without enrichment in
        culture to closely reflect the composition of the antigen-specific T cell response in vivo
        at the time of isolation.
        Our data revealed that T cell populations induced by vaccination were significantly
        different from endogenous responses: while some CTLs elicited by vaccination could kill
        melanoma targets, most were inefficient in tumor cell lysis. In contrast, nearly all clones
        from endogenous responses were efficient at melanoma cell lysis. This difference was
        related to RE for the native peptide. Clones that did not lyse tumor cells required up to
        10
        3 -fold higher concentration of peptide for similar levels of lysis of
        targets compared to T cell clones that were tumor-lytic. Side-by-side comparison of
        endogenous responses and vaccine-induced responses suggests that low RE TAA-specific T cell
        responses may be preferentially driven by heteroclitic peptide vaccination. Thus, high
        doses of peptide and/or the higher levels of expression of heteroclitic peptide on APCs may
        induce and actively propagate predominantly T cells with RE too low for recognition of
        physiological levels of the native peptide present on tumor targets. These data suggest an
        inverse relationship between antigen density and the RE of T cells elicited. This would be
        an important consideration in design of future vaccine strategies.
        Differential recognition of native and heteroclitic peptides by many T cells may also
        account for the induction of non-tumor-lytic clones by heteroclitic peptide vaccines, which
        has been suggested previously [23,32]. However, our data suggest that epitope density may
        be the dominant driving factor for RE in vivo. In all of the vaccine-elicited T cell
        responses, many of the T cells generated were either of low or intermediate RE not only for
        the native peptide, but also for the heteroclitic peptide, and exhibited no or intermediate
        lysis of tumor targets. In contrast, nearly all of the clones generated from the endogenous
        responses were of high RE. This suggests that the high dosage of peptides administered in
        vaccinations and the increased binding capacity of heteroclitic peptides to MHC
        molecules—the very quality that provides them with increased immunogenicity—drive the
        induction of many T cells with low RE for both heteroclitic and native peptides.
        Another implication of this study is that the number of cells measured by current
        methods, including ELISPOT or staining with MHC tetramers, may not correlate directly with
        the RE or tumor reactivity of T cell responses to vaccination. For example, of the nine
        clones analyzed from patient 517, none were efficient in tumor cell lysis, yet these cells
        were detectable by MHC tetramer staining. T cells with low RE for native TAA do not
        efficiently lyse tumor, and therefore are unlikely to have an impact on clinical outcome.
        Furthermore, it may be possible that low-RE TAA-specific T cells may interfere with
        elicitation of high-RE T cells, either by direct competition for antigen on APC surface
        [33,34] or down-modulation of peptide–MHC complexes.
        Our data support the notion that not only quantity, but quality, of the T cell response
        elicited by vaccination may be important for clinical efficacy. There are a number of
        strategies to increase the magnitude of T cell responses to peptide vaccines. These include
        using various adjuvants, such as incomplete Freund's adjuvant and immunomodulatory agents,
        such as IL-12 [4], GM-CSF [5], anti-CTLA-4 Abs [35], or heat shock proteins [36]. Thus far,
        none of these approaches have produced improved clinical outcomes. Our data suggest that in
        addition to driving higher numbers of vaccine-elicited T cells, strategies to modulate the
        relative RE of T cell responses are also needed. While the selective activation of high-
        versus low-RE T cells is relatively easy to manipulate in vitro via stimulation with
        limiting amounts of peptides, this may be more difficult to control in vivo. It is
        important to bear in mind that signals needed to drive a de novo naïve T cell response may
        be different from those required to drive further expansion of an activated T cell
        population [37]. Thus, a complete vaccination strategy may involve an initial induction
        phase, followed by progressive shaping of the response to higher RE. Although heteroclitic
        peptide vaccination may drive T cells of mixed high and low RE, such a strong stimulus may
        be needed to induce an initial de novo T cell response. Studies in mice suggest that once
        activated, effector CD8+ T cells may have an increase in RE of up to 70-fold compared to
        naïve cells [38,39,40]. Thus, naïve TAA-specific T cells, with inadequate RE to become
        activated by low densities of native peptides present on tumor cells, may become efficient
        in tumor lysis upon vaccination with heteroclitic peptide. This notion has support from
        studies in tolerized mice: vaccination with a heteroclitic peptide analog recruited T
        cells, which were responsive to secondary stimulation with native peptide [41,42].
        Therefore, optimized use of heteroclitic peptide to induce an initial peptide-specific T
        cell response, followed by selective expansion of the highest RE tumor-lytic T cells may be
        needed for an effective strategy with clear clinical application.
        In summary, we have demonstrated that vaccination with heteroclitic peptide at high
        concentrations may drive T cell responses of variable tumor-cytolytic potential in cancer
        patients—and that the ability to lyse tumor cells correlates with the T cell's RE for
        native peptides. This represents an important—but not sole—factor in explaining the lack of
        correlation between immunological and clinical responses after vaccination for cancer.
        Importantly, the situation is different in endogenous responses, in which cells are
        predominantly of high RE. This suggests that the manner in which T cells are elicited in
        vivo are different in these two settings and may underlie their differences in biology.
      
    
  

  
    
      
        
        There is a much-quoted saying, attributed to the epidemiologist Geoffrey Rose: “A large
        number of people exposed to a small risk may generate many more cases than a small number
        exposed to a very high risk.” This is true for many individual risk factors such as salt
        intake (linked to high blood pressure and cardiovascular disease) and speeding on the
        highway (linked to injuries and accidents). Does it apply to many other global health
        risks? The study by Anthony Rodgers and colleagues suggests that it does.
        To develop effective health policies, one must understand the existing health risks and
        disease burdens. On a worldwide scale, this is a tough challenge. The Global Burden of
        Disease Database, maintained by the World Health Organization (WHO), collects data from
        countries around the world on risk factors such as tobacco, malnutrition, childhood abuse,
        unsafe sex, childbirth, and cholesterol levels, as well as on disease burdens, for example
        depression, blindness, and diarrhea. A large group of scientists from all over the world
        has developed a framework to analyze these data. To compare different risks or burdens,
        they calculate disability-adjusted life-years, or DALYs—the number of healthy life years
        lost because of a particular disease or risk factor.
        Rodgers and colleagues used data from the WHO database for 26 risk factors and from 14
        epidemiological subregions of the world to calculate the proportion of
        risk-factor-attributable disease burden in different population subgroups defined by age,
        sex, and exposure level. For being underweight in childhood, for example—the leading risk
        factor for global loss of healthy life—they found that only 35% of the disease burden
        occurred in severely underweight children, the rest occurred in those only moderately
        underweight. The relative risks for the moderately underweight are much lower, but the
        number of children in that category is so large that the total attributable burden amounted
        to almost two-thirds of the total global burden of disease for that risk factor.
        The analysis confirms—and extends to a global level—previous research showing that many
        major health risks are important across the range of exposure levels, not just among
        individuals exposed to high levels of risk. It also points to risk factors that are
        particularly prevalent among specific populations and age groups, and for which highly
        targeted interventions could be effective.
        Despite numerous caveats and limitations of studies like this one, such analyses are
        essential aids in guiding the distribution of limited funds to lower the burden of life
        years lost to premature death and disability.
      
    
  

  
    
      
        
        Nevirapine and efavirenz are the most commonly prescribed of the class of antiretroviral
        drugs called non-nucleoside reverse transcriptase inhibitors (NNRTIs). Efavirenz has the
        advantage of once-daily dosing. In a recent study called the 2NN study (Lancet 363:
        1253–1263), it appeared to be only marginally superior to nevirapine in terms of clinical
        success and virological suppression. Van Leth and colleagues have now shown that while
        nevirapine and efavirenz both raise high-density lipoprotein (HDL) cholesterol (the “good”
        type of cholesterol), the overall lipid profile is better with nevirapine than with
        efavirenz.
        “These data suggest that nevirapine may be preferable to efavirenz in HIV-infected
        adults with other cardiovascular risk factors,” says the study's academic editor, Andrew
        Carr of St. Vincent's Hospital in Darlinghurst, Australia. “However, perceived
        cardiovascular risk is only one factor that would affect the choice between these two
        drugs.”
        Van Leth and colleagues prospectively analyzed the lipids of patients enrolled in the
        2NN study, a randomized, open-label efficacy study that included adults with HIV who had
        never been on antiretroviral drugs. All patients were given stavudine and lamivudine and
        were then randomized into three treatment groups: nevirapine, efavirenz, or both.
        For the lipid analysis, which was preplanned, the researchers included only the
        nevirapine and efavirenz groups (417 and 289 patients, respectively). This was because the
        2NN study showed that simultaneous use of nevirapine and efavirenz should be avoided—the
        combination is associated with increased toxicity without increased efficacy. The increase
        in HDL cholesterol was significantly higher with nevirapine than with efavirenz. There was
        a decrease in the ratio of total cholesterol to HDL cholesterol with nevirapine and an
        increase with efavirenz.
        The study does not prove, however, that the rise in HDL cholesterol seen with NNRTIs
        (especially nevirapine) actually leads to a reduction in coronary heart disease. “There are
        no vascular functional data,” says Carr, “or clinical vascular endpoint data that confirm
        that the statistically significant lipid differences observed are clinically
        significant.”
        The study was funded by Boehringer Ingelheim, the manufacturer of nevirapine. The
        authors clearly state that the company had “a nonbinding input on issues of study design
        and analyses” but it had “no influence on reporting of the data or the decision to
        publish.”
        Despite its limitations, van Leth and colleagues' study “moves clinicians and patients
        away from ‘one-size-fits-all’ antiretroviral therapy,” says Carr. “It takes us further
        along the path of choice of antiretroviral therapy being individualized according to other
        patient comorbidities and risk factors, as well as therapy simplicity and side
        effects.”
      
    
  

  
    
      
        
        Galvanized by rising costs, increased calls for greater, accountability, and an
        Institute of Medicine (Washington, D. C., United States) report suggesting that medical
        errors may kill nearly 100,000 Americans every year [1], United States health care experts
        have tried to boost the quality of patient care by focusing on the speed and precision of
        service delivery. Several insurance companies have already started to place a surcharge on
        patients who elect to receive care from “inefficient” providers (a definition that includes
        most teaching hospitals), hoping to encourage patients to seek more cost-effective service,
        and to encourage physicians to provide it [2].
        The problem is, most of these reform efforts, while critically important, only capture
        half the picture. Efficiency isn't everything, and unless we learn to cultivate creativity
        as avidly as we pursue consistency, future generations of patients may find themselves
        stuck with the same basic treatments they're receiving today. It will be the same medicine,
        just served quickly.
      
      
        Benefits of Quality Reform
        From its earliest days, medical training was based on an apprenticeship model, in which
        junior acolytes learned the art from senior practitioners. Even with the evolution of
        modern medical schools, which offered future physicians a rigorous common training, once
        doctors entered the real world they essentially did as they pleased. Consequently, there
        were pronounced differences in approaches to common problems from one clinician to
        another.
        There was also little to guarantee that once doctors had hung out their shingle, they
        were actually competent (and remained competent) to practice their craft. While most
        physicians remained committed to the general professional standard—do the best that you can
        for each individual patient— many well-meaning doctors ultimately were not delivering their
        patients the best care available.
        More recently, and largely due to the contagious spread of the so-called “business
        model,” there has been an increased emphasis on the consistency and quality of care. The
        clear goal is ensuring that all patients truly receive the very best care available, as
        defined by rigorous scientific studies.
        This discrepancy between what patients should be receiving and what patients are
        actually receiving is the major focus of quality reform, and reflects the new recognition
        that there are truly preferred approaches—pathways—to guide disease management. These
        pathways are not meant to represent a rigid algorithm reflexively applied to each patient,
        but are intended as a summary of the best available data, a useful template to guide
        further medical decisions.
        The renewed emphasis on quality has also resulted in a newfound appreciation for the
        role of experience and repetition in patient care. Study after study has shown that the
        best physician to treat a particular problem is the one who has treated it the most
        [3].
      
      
        What Gets Lost: Innovation
        The great paradox here is that the same reforms that are improving our current care may
        also be endangering our future health. As medicine has become more standardized and
        increasingly regulated, it turns out there is much less room for innovation. The spirited
        pursuit of the unknown—so long a defining quality of medicine—now seems seriously
        endangered. The new world of rapid throughput and endless documentation provides little
        time to reflect upon important clinical problems and consider fresh approaches. If
        anything, thinking about a patient or a question too much is now implicitly discouraged
        because it slows doctors down; contemplation is bad for productivity.
        Academic medical centers like our own have played a particularly important role in the
        history of medical discovery; the hallmark of these institutions is our commitment to
        thinking and reflecting about the patients we see, patients who are often extremely sick
        and whose management is exceptionally complex. Unfortunately, many of the measurements now
        used by insurance companies to assess quality pay little attention—if any at all—to the
        complexity of a patient's illness, or to the importance of spending time trying to define
        the underlying malady. Insurance companies' major concern seems to be how fast a patient is
        “processed,” ideally with as few tests as possible. These measures provide no mechanism for
        distinguishing between the addled physician who inappropriately orders every test that
        springs to mind, and the reflective physician who is trying to get to the bottom of a
        patient's complaint, rather than simply throw a Band-Aid over the symptoms [4].
        Situated on the front lines, clinicians have a unique opportunity to provide new medical
        insights and to identify critical, unanswered questions. Classic examples include Archibald
        Garrod, a British physician whose desire to understand why a patient produced black urine
        led to the hypothesis that diseases can result from defective metabolic enzymes, and Fuller
        Albright, a clinical investigator at Harvard whose thoughtful approach to his patients
        yielded insights that revolutionized the field of endocrinology. More recently, the astute
        clinical observations of UCLA immunologist Michael Gottlieb resulted in the original
        description of the Acquired Immune Deficiency Syndrome (AIDS) in 1981 [5].
      
      
        Preserving Creativity in Medicine
        But where are these types of insights going to come from today? It seems difficult to
        imagine that a medical care environment characterized by staccato-quick patient visits
        covering an ever-increasing number of compulsory topics will support or encourage such
        reflection and innovation. Our failure to nourish and sustain inquisitive physicians seems
        particularly tragic because medicine has traditionally attracted some of our brightest and
        most imaginative individuals. Even at the height of the dot-com boom, for example, there
        were still more medical school applicants than there were spaces to train them. But if
        current trends continue, many of these creative minds will head elsewhere, while those who
        stay will risk becoming stultified by repetitious routine.
        Several medical schools and a handful of foundations have recognized this emerging
        problem, and have initiated programs aimed at sparking curiosity in young doctors (our own
        school's program is called the PASTEUR initiative—see www.pasteur.hms.harvard.edu) [6]. But
        as well-intentioned as these efforts are, simply changing the curriculum isn't likely to
        fix the underlying problem. Unless ever-savvy medical students perceive that inquisitive
        thinking is truly valued in clinical medicine, and unless exasperated physicians are
        inspired to believe that they have the ability to change some aspect of the way medicine is
        practiced, nothing is going to change. We may lose the best hope we have of defeating the
        terrible diseases that now plague us.
        Even as we strive to improve the consistency of care—and striving is clearly a very good
        idea—we must continue to cultivate novelty and originality, rather than penalize it.
        Imagination is perhaps the most essential trait that medicine, and medical insurers, must
        again learn to recognize and reward. Even with the best algorithms and the brightest
        computers, the future of health care ultimately depends upon the creativity of the hardy
        men and women still entrusted with its delivery.
      
    
  

  
    
      
        Introduction
        The use of highly active antiretroviral therapy (HAART) can dramatically prolong the
        life of individuals infected by human immunodeficiency virus 1 (HIV-1) [1], but early hopes
        for virus eradication have not been realized [2]. The successful use of HAART is limited by
        drug-related toxicities, high costs, and drug resistance [3], factors which have led to the
        development of alternative therapeutic strategies, including the use of supervised, or
        structured, treatment interruption (STI). This approach, involving recurrent limited
        exposure to autologous virus, has not been successful in chronic infection [4,5], but has
        been shown to lead to at least transient containment of viremia after intervention in the
        acute phase of infection in humans and animals exposed to AIDS-associated retroviruses
        [6,7,8,9].
        In the present study, we performed a detailed longitudinal assessment of the impact of
        early treatment followed by STIs in patients treated during acute or early HIV-1 infection.
        The main hypothesis of the study was that early treatment of acute HIV-1 infection followed
        by STI would lead to immune boosting and subsequent control of viremia without the need for
        drugs. The primary endpoint was the time to viral rebound above 50,000 copies/ml once or
        above 5,000 copies for three determinations separated by a week each. The early results of
        this trial were previously reported, showing that five of eight patients were able to
        achieve a plasma viral load of 500 copies/ml or less at a median of 6 mo off therapy [6].
        The current study investigates the frequency and durability of control achieved with this
        intervention, with follow-up to a median of 5.3 y after infection, and with an increase in
        size of the cohort to 14 patients. Our results indicate that, although the majority of
        patients treated in the acute phase of infection go on to control HIV-1 to less than 5,000
        RNA copies/ml plasma for at least 6 mo off therapy, the ability to contain viremia below
        this level over the long term is maintained in a minority of patients.
      
      
        Methods
        
          Objective
          The hypothesis of the study was that early treatment of acute HIV-1 infection would
          confer immunologic maturation and subsequent control of HIV-1 without the need for
          ongoing drug therapy. Alternatively, if a breakthrough of virus replication was observed,
          this would provide a boost in HIV-1-specific immunity after reinstitution of antiviral
          therapy. The primary endpoint was the time to viral rebound to more than 50,000 copies/ml
          or viral loads above 5,000 copies/ml for three determinations separated by a week each.
          The secondary objective was to correlate immunologic and virologic parameters with any
          observed effects including evolution of HIV-1-specific T helper and cytotoxic T
          lymphocyte responses. The original study protocol, including the patient consent form and
          the institutional review board approval, can be found in Protocols S1–S4.
        
        
          Study Population
          Fourteen patients presenting with acute or early HIV-1 infection were enrolled in this
          study between July 1997 and January 2000 (Table 1). Acute HIV-1 infection was defined by
          the presence of HIV-1 RNA in the plasma, a negative or weakly positive HIV-1 antibody by
          HIV-1/2 ELISA, and the detection of no more than three bands in an HIV-1 Western blot;
          early HIV-1 infection was defined by a positive ELISA and confirmation of early infection
          by either detuned negative ELISA or previously known negative ELISA. All participants in
          the study had symptoms compatible with the acute retroviral syndrome and were treated
          with HAART (one protease inhibitor and two nucleoside reverse transcriptase inhibitors)
          within a median of 19 d (range, 9–33) from the onset of symptoms. Study participants were
          recruited from the Massachusetts General Hospital, the Brigham and Women's Hospital, and
          the Fenway Community Heath Care Center in Boston. All individuals gave written informed
          consent to participate, and the study was approved by the respective institutional review
          boards and conducted in accordance with the human experimentation guidelines of the
          Massachusetts General Hospital.
          Six of the 14 individuals were investigated in an interim study [10]. These patients
          were AC-02 (AS2 in [10]), AC-05 (AS5), AC-14 (AS1), AC-15 (AS3), AC-25 (AS6), and AC-46
          (AS4).
        
        
          STIs
          Entry criteria included treatment with HAART before or shortly after HIV-1
          seroconversion, viral suppression on HAART to less than 400 RNA copies/ml for 2 mo, HIV-1
          viral load at the time of entry into the study of less than 50 RNA copies/ml, and lack of
          significant mutations conferring drug resistance [11,12]. Lymphocyte proliferative
          response to recombinant HIV-1 p24 protein had to exceed a stimulation index of ten before
          treatment discontinuation, and net counts per minute had to be 800 or greater. All
          antiretroviral drugs were discontinued simultaneously. After a treatment interruption,
          patients were seen at least once a week for the first 24 wk, and then monthly, with a
          total of at least 30 visits for the first year after cessation of therapy. In the second
          year, visits continued monthly. Treatment was restarted if viral load remained above
          5,000 RNA copies/ml plasma for greater than three consecutive weeks, or was in excess of
          50,000 copies/ml on any single occasion.
        
        
          Human Leukocyte Antigen Typing
          High- and intermediate-resolution human leukocyte antigen class I typing was performed
          at a commercial laboratory (Dynal Biotech, Oxford, United Kingdom) by sequence-specific
          PCR as described [13].
        
        
          Detection of GB Virus C RNA
          GB virus C (GBV-C) RNA was detected using a two-step nested PCR amplification reaction
          from whole plasma RNA. Briefly, GBV-C RNA was extracted from plasma using the Qiagen
          Viral RNA Mini Kit (Qiagen, Valencia, California, United States) according to the
          manufacturer's instructions. Extracted RNA was reverse transcribed using the Qiagen
          OneStep RT-PCR Kit and amplified by nested PCR; in both steps primers specific for the 5′
          UTR of GBV-C were used, as described previously [14].
        
        
          Chemokine Receptor Genotyping
          In order to analyze the chemokine receptor (CCR) 5Δ32 deletion polymorphism, the
          region spanning the 32-nt deletion was amplified by PCR, and the two alleles were
          separated by gel electrophoresis [15]. The CCR2–64I polymorphism was detected by
          PCR–restriction fragment length polymorphism analysis as described previously [16].
        
        
          Synthetic HIV-1 Peptides
          We synthesized 410 synthetic peptides 15–20 amino acids long at the Massachusetts
          General Hospital Peptide Core Facility on an automated peptide synthesizer using Fmoc
          technology, as described [17]. Peptides overlapped by 10 amino acids and spanned the
          entire HIV-1 clade B 2001 consensus sequence.
        
        
          ELISPOT Assays
          ELISPOT assays were carried out as described previously [18]. Peripheral blood
          mononuclear cells (PBMCs) were incubated overnight at 50,000 to 100,000 cells/well in
          96-well polyvinylidene plates that had been precoated with 0.5 μg/ml anti-human
          interferon-γ monoclonal antibody (Mabtech, Stockholm, Sweden). The final concentration of
          the peptide per well was 14 μg/ml. The numbers of spots per well were counted using an
          automated ELISPOT plate reader (AID EliSpot reader system, Autoimmune Diagnostika,
          Strassberg, Germany). A response was considered positive if there were more than 50
          spot-forming cells (SFCs)/10
          6 PBMCs and if the well had at least three times the mean number of
          SFCs in the three control wells. The dependence of responses on CD8+ T cells was
          determined by measuring the depletion of CD4+ T cells using the Minimacs cell depletion
          system (Miltenyi Biotec, Bergisch-Gladbach, Germany). When HIV-1-specific CD8+ T cell
          responses were detected against adjacent peptides, and therefore might represent
          targeting of the overlap region, responses to the weaker peptide were excluded for
          calculations of magnitude and breadth, as previously described [19].
        
        
          Proliferation Assays
          Freshly isolated PBMCs (10
          5 cells) were incubated with baculovirus-derived recombinant p24
          protein (Protein Sciences, Meriden, Connecticut, United States) at 5 μg/ml for 6 d and
          then pulsed with 
          3 H thymidine at 1.0 μCi for 6 h before harvesting as previously
          described [20]. A stimulation index of five or greater was considered significant.
        
        
          Statistical Analysis
          Time to failure during the different treatment interruptions was assessed by
          Kaplan-Meier analysis. Patients who were still controlling viremia at the time of last
          visit, who failed because they restarted therapy without meeting the criteria of
          virologic failure, or who were lost in follow-up were included in the analyses, but the
          data were censored at the last evaluable time point. Equality of survival distributions
          for the first and second treatment discontinuations was evaluated using the Wilcoxon
          matched-pairs signed-ranks test. CD4+ T cell losses were calculated on regression lines
          based on least squares fit. A Cox proportional hazards regression model was used for the
          analysis of continuous variables such as days following onset of symptoms, CD4+ T cell
          count, HIV viral load, and time to rebound of viremia, as well as for the estimation of
          hazard ratios for the categorical variables of ELISA, Western Blot, coreceptor
          polymorphism, and GBV-C status. Statistical analyses of CD8+ and CD4+ T cell responses
          were based on a Student's 
          t test, a multiparametric ANOVA test, a Wilcoxon matched-pairs
          signed-ranks test, or a Mann-Whitney 
          U test, as indicated. 
          p -values lower than 0.05 were considered to indicate statistical
          significance, and all reported 
          p -values are two-sided. Statistical analysis and graphical
          presentation were performed using SPSS, SAS, and Prism software packages.
        
      
      
        Results
        
          Longitudinal Assessment of Control of Viremia following Treated Acute or Early
          Infection
          Fourteen patients identified at the time of acute or early infection (Table 1; Figure
          1) were entered into this protocol, and they were followed for a median of 5.3 y from the
          time of infection (range, 494–2,475 d). Patients underwent successive treatment
          interruptions after an initial treatment period of at least 8 mo (median, 508 d; range,
          245–1,096 d) and were required to restart therapy when viral load exceeded 50,000 RNA
          copies/ml plasma on a single occasion, or 5,000 copies/ml for three consecutive weeks.
          For purposes of analysis, patients who dropped out of the study or who reinitiated
          therapy without meeting criteria were considered to have lost the ability to contain
          viremia.
          Using these criteria for reinitiation of therapy and to define failure, 11 of 14
          patients (79%) were able to achieve virologic control to less than 5,000 RNA copies/ml
          plasma for at least 90 d after one, two, or three treatment interruptions (Table 2). The
          period of longest containment was after one interruption for five patients, after two
          interruptions for eight patients, and after three interruptions for one patient (Table
          3).
          Once control was achieved, the majority of the patients experienced a subsequent rise
          in viremia. The median time between cessation of therapy and rebound of viremia (having a
          viral load greater than 50 copies/ml) was 17 d (range, 7–169 d).
          Six of 14 patients (43%) achieved a period of control after stopping therapy for 1 y,
          but only three of 14 (21%) were able to control viremia off therapy at less than 5,000
          RNA copies/ml plasma for more than 2 y. Duration of viremia control during successive
          treatment interruptions was highly variable, and there was no increase in the
          sustainability of viral containment during successive STI cycles. The three patients
          achieving control of viremia for more than 2 y did so during the first (AC-10), the
          second (AC-02), and the third (AC-14) treatment interruption, respectively (Figure 2A). A
          paired comparison (Wilcoxon matched-pairs signed-ranks test) showed no significant
          difference in the length of viremia control with subsequent treatment interruptions.
          Although patients experienced rebound viremia with discontinuation of therapy, none of
          the patients experienced recurrence of symptoms associated with acute HIV-1
          infection.
          These data show that at least transient control of viremia to less than 5,000 RNA
          copies/ml plasma was achieved in the majority of study participants during at least one
          of the treatment interruptions, but that durable viral control in participants following
          treated acute infection occurred infrequently. Moreover, the data do not show a
          consistent pattern of augmentation of viral control with sequential treatment
          interruptions.
        
        
          Effect of Treatment Interruptions on CD4+ T Cell Counts
          Although viral load is a strong predictor of disease progression, CD4+ T cell loss is
          an additional, independent predictor [21]. Early treatment of acute HIV-1 infection led
          to normalization of CD4+ T cell counts in most patients (median, 753 cells/mm
          3 ; range, 492–986), but the effect of treatment interruption was
          variable, even in those doing well, as defined by sustained low viral loads. Overall, 11
          of 14 patients interrupted therapy for at least 12 mo, and these individuals were
          evaluated regarding the effect of treatment interruption on CD4+ T cell loss (Figure 2B
          and 2C). The rate of change in CD4+ T cell counts during the first year of the longest
          period off treatment ranged from +157 to −438 cells/mm
          3 /y (median, −192). Of the three patients who did not meet viral load
          criteria for restarting therapy for more than 2 y, one (AC-02) had an increasing CD4+ T
          cell count of 157 cells/mm
          3 /y, one (AC-10) had a stable CD4+ T cell count (−9 cells/mm
          3 /y) , and one (AC-14) experienced a decline of 344 cells/mm
          3 /y. Comparison with data from the Multicenter AIDS Cohort Study
          (MACS) showed that the kinetics of CD4+ T cell loss was faster (Mann-Whitney 
          U test , 
          p = 0.02) than in untreated patients with early chronic HIV-1 infection
          (average loss of −67 cells/mm
          3 /y in patients with a CD4+ T cell count of more than 350 cells/mm
          3 at baseline). However, CD4+ T cell loss rate was in the same range
          as what has been described after treatment interruption in chronic HIV-1 infection
          [22,23]. Analysis of CD4+ T cell decline during the second year for the three individuals
          who controlled viremia for more than 2 y revealed similar trends in CD4+ T cell slopes,
          although they were less steep: AC-02, +88 cells/mm
          3 /y; AC-10, +44 cells/mm
          3 /y; and AC-14, −110 cells/mm
          3 /y. When the first 3 mo off therapy were excluded in order to
          minimize the potential effects of recent treatment on CD4+ T cell number, the rate of
          change in CD4+ T cell counts during the first year off therapy no longer differed
          statistically from the MACS data (median, −207 cell/mm
          3 /y; range, +119 to −699; Mann-Whitney 
          U test, 
          p = 0.07). A possible reason for steep CD4+ T cell slopes may be high
          CD4+ T cell counts at time of treatment interruption. Comparison with MACS data (Figure
          2C) showed that several of the study participants still behaved as outliers when this
          factor was considered. These results indicate that periods of relative control of viremia
          were associated with declining CD4+ T cell counts in most patients.
        
        
          Correlation of Clinical and Genetic Markers with Duration of Viremia Control
          Although the study was small, we evaluated clinical and laboratory parameters to see
          if any was predictive of duration of viral control. Analyses included clinical and
          laboratory parameters at time of presentation with acute HIV-1 infection, genetic markers
          associated with different rates of disease progression, and the presence or absence of
          GBV-C coinfection. All patients presented with symptomatic acute infection. Time between
          onset of symptoms and institution of therapy did not affect duration of control following
          STI (Cox proportional hazards regression model, 
          p >0.05). The individuals who controlled viremia for a longer time
          either during the first STI or during any of the treatment interruptions were not
          different from those who experienced earlier breakthrough as measured by ELISA and
          Western blot status at initiation of HAART, coreceptor polymorphisms (CCR5delta32, CCR2
          V64I), or the presence or absence of GBV-C coinfection (Cox proportional hazards model, 
          p >0.05 in all comparisons; data not shown). The only parameter
          that was predictive of prolonged viral control during the first treatment interruption
          was a low viremia at time of institution of therapy (
          p = 0.01): there was a 2.8-fold increase in hazard per order of
          magnitude increase in viral load. This factor was no longer predictive when the period of
          longest control of viremia was considered. The time to rebound of viremia (>50
          copies/ml or >400 copies/ml) did not correlate with the duration of viral control.
          Although 11 out of 14 individuals achieved at least transient control of viremia, and
          three experienced prolonged control, none of these patients possessed the HLA alleles B27
          or B57 associated with better disease outcome [24,25].
        
        
          Relationship of Magnitude and Breadth of HIV-1-Specific CD8+ T Cells to Duration of
          Viremia Control
          To assess the relationship between the clinical outcome and evolution of
          HIV-1-specific CD8+ T cells, we longitudinally analyzed the breadth and magnitude of CD8+
          T cell responses using an interferon-γ ELISPOT and a panel of 410 overlapping peptides
          spanning the entire HIV-1 clade B consensus sequence. At the beginning of the first STI,
          HIV-1-specific CD8+ T cells were weak (median of 590 SFCs/10
          6 PBMCs) (Figure 3A) and narrowly directed at a median of two epitopes
          (Figure 3B). CD8+ T cell responses increased significantly (
          p <0.05) during the first off-treatment period, reaching a median
          total magnitude of 2,725 SFCs/10
          6 PBMCs and targeting a median of eight epitopes, and then were
          sustained when therapy was reintroduced. A further increase in the magnitude and breadth
          of HIV-1-specific CD8+ T cells was observed in the subsequent off-treatment periods,
          although these augmentations failed to reach statistical significance. The CD8+ T
          cell–mediated immune responses emerging during these consecutive cycles of treatment
          interruption were broadly directed, targeting all structural and most accessory and
          regulatory HIV-1 gene products (data not shown). However, the magnitude of HIV-1-specific
          CD8+ T cell responses at the beginning of the first (
          r = 0.01, 
          p = 0.76), second (
          r = 0.16, 
          p = 0.54), or third (
          r = 0.1, 
          p = 0.55) treatment interruptions was not predictive of the time the
          study participants were subsequently able to stay off therapy according to study
          criteria.
          The periods off treatment allowed for assessment of the relationship between exposure
          to virus and evolution of immune responses. There was a highly significant positive
          association between time until virologic failure during the first treatment interruption
          and change in the magnitude of HIV-1-specific CD8+ T cell responses (
          r = 0.92, 
          p <0.001) (Figure 3C). Similarly, the longer a patient remained off
          therapy during the second and third interruptions, the greater the augmentation of the
          total magnitude of HIV-1-specific CD8+ T cell responses (
          r = 0.83, 
          p <0.016; 
          r = 0.74, 
          p = 0.05, respectively). The increase in CD8+ T cell epitopes targeted
          during the first treatment interruption was also linearly correlated to the duration
          until virological failure (
          r = 0.81, 
          p <0.001) (Figure 3D). However, no significant relationship was
          observed between the augmentation of epitopes targeted during the second and third
          treatment pauses and the time the study participants were able to remain off therapy in
          the respective treatment interruption. These data suggest that the duration of a
          treatment interruption, and therefore the duration of exposure to plasma virus,
          correlates positively with the magnitude and breadth of HIV-1-specific CD8+ T cell
          responses that emerge during off-therapy time periods. Yet, CD8+ T cell responses prior
          to treatment interruptions were not significantly predictive of the duration of time that
          patients are able to spontaneously control HIV-1 replication, as defined by the study
          criteria.
        
        
          Relationship of Magnitude of Lymphocyte Proliferative Responses to p24 Antigen to
          Duration of Viremia Control
          We next analyzed evolution of lymphoproliferative responses to recombinant HIV-1 p24
          Gag protein in order to assess HIV-1-specific CD4+ T cell function. Most individuals had
          no detectable response at baseline prior to treatment, consistent with prior reports of
          patients with acute HIV-1 infection [20]. After initiation of therapy, all individuals
          generated HIV-1-specific lymphoproliferative responses (Figure 3E), which was a criterion
          for inclusion in the study. During treatment interruptions, there was a variable decline
          in magnitude, and comparisons between responses on the first day of treatment
          interruption and last day off therapy did not reach statistical significance (first STI, 
          p = 0.72; second, 
          p = 0.12; and third, 
          p = 0.60, respectively). These HIV-1-specific CD4+ T cell responses
          also tended to rise with reinitiation of therapy, and some of them were very robust, with
          stimulation indices over 50 detected in several individuals (Figure 3E). Similar to CD8+
          T cell responses, the magnitude of HIV-1-specific CD4+ T helper cell responses at the
          beginning of the first (
          r = 0.05, 
          p = 0.43) (Figure 3F), second (
          r = 0.16, 
          p = 0.54), or third (
          r = 0.1, 
          p = 0.55) treatment interruption was not statistically predictive of
          the time the study participants were subsequently able to stay off therapy according to
          study criteria.
        
      
      
        Discussion
        Although early treatment of acute HIV-1 infection followed by treatment interruptions
        may enhance control of viremia [6,8], the durability of this control remains unclear. Here
        we analyzed the long-term impact of initiation of antiviral therapy during acute HIV-1
        infection followed by STIs in a cohort of 14 patients. Although initial control of viremia
        to less than 5,000 RNA copies/ml plasma was achieved in the majority of the individuals
        studied, a gradual increase in viremia and decline in CD4+ T cell counts was observed in
        most patients, even after a year or more of viral containment. Durable virologic control
        occurred infrequently, despite the presence of robust HIV-1-specific CD4+ and CD8+ T cell
        responses detected by standard assays. Moreover, even during periods of successful control
        of viremia, progressive loss of CD4+ T cells was frequently observed. These data indicate
        that although early treatment of acute and early infection is frequently associated with
        transient control of viremia after STI, ongoing low-level viral replication is associated
        with ultimate virologic breakthrough in most patients.
        The standard immunologic assays and virologic assessments in this cohort revealed
        considerable heterogeneity among the study participants, and did not show a consistent
        pattern in duration of viremia control during successive treatment interruptions. Eleven of
        14 patients (79%) were able to maintain a viral load of less than 5,000 copies/ml for at
        least 90 d, but progressive loss of control ensued in the majority of patients and only
        three patients (21%) were able to maintain control for more than 2 y. These three patients
        did so during the first (AC-10), the second (AC-02), and the third (AC-14) STI. Clinical,
        genetic, and immunological parameters did not distinguish these three individuals from the
        other 11 patients, nor did they predict the duration of control following treatment
        interruption. Indeed, the longer a patient was off therapy, the stronger and more broadly
        directed the CD8+ T cell responses became, but these were still not sufficient to maintain
        prolonged control in most patients. Although three patients did not complete the study as
        initially intended (patient AC-45 withdrew from the study after viral breakthrough on the
        first STI, AC-13 restarted therapy despite a viral load of less than 5,000 copies/ml during
        both the first and second STIs and then withdrew, and AC-05 restarted therapy prematurely
        during the second STI but then failed to control during the third STI), the results are not
        substantially different if these three are censored rather than considered to have failed
        to control.
        Loss of viral control in this cohort occurred not only in the presence of strong CD8+ T
        cell responses, but in most cases also in the presence of virus-specific CD4+ T cell
        responses, although the CD4+ T cell responses often declined during periods of viremia. In
        addition, total CD4+ T cell numbers were also monitored and declined in most patients over
        time, including one of the three patients who were able to maintain low viral loads for at
        least 2 y. Mechanisms leading to rapid CD4+ T cell loss need to be further studied in
        future STI trials. Other parameters including chemokine receptor polymorphisms [26] and
        GBV-C coinfection [27,28] similarly failed to explain the different courses following
        treatment interruption. The only parameter found to be associated with longer control of
        viremia during the first treatment interruption was a lower viral load at time of
        institution of antiviral therapy. Given the multiplicity of comparisons made, the true
        significance of this finding is uncertain.
        The reasons for progressive loss of control despite augmentation of virus-specific CD4+
        and CD8+ T cell responses remain to be defined. In one individual (AC-06), HIV-1
        superinfection in the setting of strong and broadly directed HIV-specific cellular immune
        responses was associated with the loss of viral control, as previously reported [29]. No
        other cases of superinfection have been identified in these patients (data not shown). The
        immunologic studies performed failed to show an association between increases in viral load
        and loss of immune responses, but this may be due to the use of the current standard IFN-γ
        assays to quantify immune function. Numerous studies now indicate that IFN-γ production
        alone is not associated with viral load [19,30,31] but rather that functional
        characteristics of CD4+ and CD8+ T cells may be better associated with viral control
        [32,33,34,35]. Such studies will be important to pursue. In particular, even a low level of
        viremia correlates with a low or undetectable frequency of interleukin-2-producing
        HIV-1-specific memory CD4+ T cells endowed with proliferative capacity in vitro
        [36,37,38,39], thus abrogating CD4+ T cell help crucial to maintain efficacy of CD8+ T cell
        functions. In an interim study of a subset of six of the 14 patients presented here
        (patients AC-02, AC-05, AC-14, AC-15, AC-25, and AC-46), a fully differentiated effector
        phenotype of HIV-1-specific CD8+ T cells for selected epitopes was found to be associated
        with better control of viremia [10]. Other factors that may contribute include functional
        defects in antigen-specific cell-mediated immunity [35,37,40,41,42], and progressive immune
        escape [43,44,45]. HIV-1-specific humoral immunity can also affect viral control after
        treatment interruption [46], and viral factors including viral fitness [47,48] and
        infection with multiple viral variants [49] can influence viral set point and the rate of
        disease progression. Virus sequencing studies currently in progress in this cohort indicate
        that viral breakthrough is associated with sequence changes within and outside known CTL
        epitopes (data not shown). Full evaluation of the relationship between immune escape and
        viral breakthrough will require extensive additional analyses, including detailed analysis
        of responses to autologous virus [50,51]. Assessing the changes in CD4+ and CD8+ T cell
        functions over time as well as viral evolution under immune selection pressure will be
        important to evaluate immune correlates in this cohort.
        These data are important in light of other recent data on treatment interruption in both
        acute and chronic infection. In chronic HIV-1 infection, STI studies showed only marginal,
        if any, improvements of HIV-1 viremia control following a number of treatment interruptions
        cycles, despite at least transient increases in HIV-1-specific CD8+ and CD4+ T cell
        responses [4,5,52,53,54,55]. In the setting of infection with a multidrug-resistant virus,
        this strategy may even be deleterious [56]. Other studies of STI after treated acute HIV-1
        infection have shown limited benefits [9], including recent trials such as the PrimSTOP
        trial [57] and the QUEST study [58]. However, little is known about the relationship
        between scheduling of HAART and treatment interruptions and the characteristics of viral
        rebound after therapy has been discontinued.
        Although durable control of viremia was not achieved, it is noteworthy that the majority
        of patients were able to achieve transient relative containment of viremia, providing
        rationale for future studies aimed at further enhancing immune control. Early treatment
        alone should still be considered an important therapeutic option. Therapeutic vaccinations
        administered after treated acute HIV-1 infection and before cessation of therapy have given
        disappointing results thus far [9], but the availability of new and more potent immunogens
        requires reassessment of this approach. Indeed, the ability to enhance CD4+ T helper cell
        responses in the chronic phase of infection has been demonstrated [59], but whether this
        will enhance CD8+ T cell function requires additional studies. Some promising results have
        been obtained using immunomodulatory drugs, including cyclosporine [60] and hydroxyurea
        [61], in combination with antiviral therapy, presumably because of the limitation of T cell
        activation. Administration of granulocyte-macrophage colony-stimulating factor blunted the
        viral rebound following interruption of HAART, and largely prevented a decrease of CD4+ T
        cell counts in an STI trial in chronic HIV-1 infection [62]. These additional therapeutic
        interventions deserve further investigation in future STI studies.
        Although the present study shows progressive viral breakthrough, it was not designed to
        address whether there might be a change in set point viremia achieved or overall clinical
        benefit through transient early treatment of acute HIV infection. The definition of failure
        chosen for this study was a viral load of greater than 5,000 RNA copies/ml plasma, which at
        the time the study was initiated corresponded to the level of viremia at which treatment
        was recommended. Larger randomized trials will be needed to determine the potential
        clinical and virologic benefit of approaches based on STIs. In studies of untreated
        infection, there is only a 5-fold difference in viremia separating the quartile with the
        slowest disease progression from the quartile with the most rapid progression [63],
        suggesting that small differences in steady-state viremia may influence clinical outcome.
        In the meantime, STI probably should be avoided outside the setting of controlled clinical
        trials. The data in this study may also be relevant to current efforts to develop a
        therapeutic AIDS vaccine designed to retard disease progression rather than prevent
        infection, since they suggest that durable maintenance of low-level viremia may be
        difficult to achieve.
      
      
        Supporting Information
      
    
  

  
    
      
        
        The total projected spending on health care in the United States for 2004 is $1.79
        trillion—15.5% of its gross domestic product [1]. That amounts to $6,167 per person, almost
        twice what most nations with comprehensive systems spend on care. Most policy analysts
        agree that this level of spending should be more than enough to provide all Americans with
        high quality, comprehensive health care. Yet the United States falls far short of these
        goals. What are the flaws in the United States health system that prevent Americans from
        receiving value from this huge health care investment? And what are the options for
        improvement?
      
      
        Physicians for a National Health Program
        First, I should reveal my personal bias. Physicians should be well represented in the
        forefront of reform. As we look back on the past half century of failed health policy
        decisions, we see that the dominant physicians' organization in the United States, the
        American Medical Association (AMA), has opposed most reform measures that would result in
        an equitable, affordable system for everyone. Instead, the AMA has supported an agenda that
        promotes physicians' freedom to maximize their personal financial reward, even though those
        policies may deprive tens of millions of Americans access to affordable care. The AMA
        agenda has contributed significantly to the current high costs of American health care and
        to our failure to adequately address the mediocrity that characterizes health care in the
        United States.
        Many American physicians—including myself—believe that the funding infrastructure should
        be redesigned to maximize heath care resource allocation for the primary benefit of
        patients. Because of the failure of organized medicine to advocate on behalf of our
        patients, we decided that a new organization was needed. We established Physicians for a
        National Health Program (www.pnhp.org) [2,3].
      
      
        The Uninsured and the Poorly Insured
        There are 45 million Americans with no health care coverage, and not surprisingly, lack
        of insurance is associated with worse health outcomes [4]. About 18,000 young adults die
        each year because they lack health insurance [4]. The uninsured are less likely than the
        insured to receive the professionally recommended standard of care for their chronic
        diseases, such as diabetes (Figure 1) [5]. And if you have a serious health crisis while
        you are uninsured, you risk major debt or bankruptcy.
        Even the insured are inadequately covered. Employers and individuals who purchase
        coverage are rebelling at the high price of insurance premiums. To maintain competitive
        premiums, insurers are designing products that reduce the benefits they pay out by
        increasing the out-of-pocket portion that patients are required to pay for services
        received. Insured patients may have to pay cash for care until a designated amount is
        reached (the deductible)—which could be thousands of dollars. In addition, patients are
        often required to pay a dollar amount (co-payment) or a percentage of the charges
        (coinsurance) each time services are received.
        Insurers may also exclude specified services from coverage, such as maternity benefits
        or mental health services. Most insurance plans now use lists of contracted physicians and
        hospitals, and impose severe financial penalties for using health care providers that are
        not contracted. All of these measures reduce the value of insurance by shifting costs from
        the insurers to the patients who actually need care.
        Inadequate insurance coverage is making average-income Americans poorer. A recent study
        found that for 29% of individuals who had average or greater-than-average incomes and were
        continually insured, medical bills had caused significant financial problems [6]. For those
        who were not continuously insured, the percentages were even higher. These financial
        barriers are impairing access to beneficial services. The United States insurance market is
        now dominated by insurance plans that provide neither adequate health security nor
        financial security.
      
      
        Does Higher Health Spending Mean Better Quality?
        There is a widespread belief that the high spending in the United States means that high
        quality care is being delivered to the majority, who can afford both comprehensive coverage
        and the attendant out-of-pocket expenses. But international comparisons of industrialized
        nations have shown that the United States is in the bottom quartile of population health
        indicators such as life expectancy and infant mortality [7]. And in regional comparisons
        within the United States, increased levels of spending have not produced a commensurate
        improvement in health care outcomes. In fact, a recent study found that in a state-by-state
        comparison, there is an inverse relationship between spending and quality outcomes—the more
        expenditure, the worse the quality [8].
        In 2000, the World Health Organization rated the United States first in its health
        expenditures per capita, but 37th in its overall health system performance, below most
        industrialized nations [9]. The United States is clearly not receiving adequate value for
        its health care investment.
        Some contend that the poor performance of the United States system is due to the funding
        of health care in the private sector, and that all would be well if the government would
        just take over funding. But it is not quite that simple. The greater part of health care in
        the United States—59%—is already funded by the tax system. On a per capita basis, the
        public, taxpayer-funded health care expenditures alone total more than the health care
        spending of every other nation's public and private funding combined (with the exception of
        Switzerland, in which total spending per capita equals our public spending alone) [10].
      
      
        Flaws in Funding and Allocation
        How can the United States spend as much as it does and end up with such mediocre health
        care? Of the many reasons that exist, two are particularly important. The United States has
        a highly flawed system of funding health care and a flawed system of allocating its health
        care resources.
        In the United States, a multitude of private health plans cover the lucrative sector of
        society—low cost, healthy workers and their healthy families. But public programs must
        cover the higher costs of the elderly, individuals with permanent disabilities, and some
        low-income individuals. Since the uninsured are frequently unable to pay for the care they
        receive, the costs for their care are shifted to government programs or private plans, or
        to the charity of providers, even if unintended. The costly administrative excesses of
        private health plans, especially when contrasted to government programs, have been well
        documented [11]. This fragmented system of funding care places an even greater
        administrative and financial burden on the providers of health care. Although the exact
        amount is disputed, most policy analysts agree that replacing this fragmented system of
        funding care with a single, universal, publicly administered insurance program could
        recover 200 billion dollars or more, which are currently being wasted on useless and
        sometimes detrimental administrative services [11].
        And what is wrong with the way that the United States allocates its resources? Many
        studies have confirmed that supporting a strong primary care base provides better outcomes
        at a lower cost [12]. But in the United States, specialized, high-technology care is
        heavily marketed, and providers of that high-tech care are rewarded more generously than
        primary care professionals. Yet studies show that these greater expenditures result in no
        additional benefit—and sometimes even in worse outcomes [8,13]. Excessive resources are
        allocated to inappropriate expansion of high-tech facilities and to training an excessive
        number of specialists to provide high-tech services [8,13].
      
      
        Health Care Reform
        What has been the response to these deficiencies in the United States health care
        system? In the 1990s, the Clinton administration attempted to introduce a comprehensive
        system of funding universal health care. The system would have used marketplace principles
        in a program of managed competition, but their complicated idea pleased no one, and it was
        never even brought to a vote. Because of this miserable political failure, policymakers
        decided that any comprehensive approach should be avoided, and that reform must take place
        in incremental steps. To date, with the notable exception of the State Children's Health
        Insurance Program (http://www.cms.hhs.gov/schip), the accomplishments of these incremental
        health reform measures have been unimpressive.
        Over the past decade, those interested in reform have been preoccupied with managed care
        measures and, more recently, with consumer-directed measures that increase costs to
        patients by requiring greater out-of-pocket spending. But these measures are designed more
        to control costs than to increase coverage and access. In the debate on universal coverage,
        three general concepts have been put forward: (1) the expansion of our current system of
        public and private programs, (2) the establishment of a national health service with
        government ownership of the system, or (3) the replacement of all current funding with a
        single, publicly administered, publicly funded program of social insurance that does not
        alter the existing ownership status of the delivery system.
        The greatest political support today is for incremental expansions of our current
        programs, which, theoretically, would eventually result in universal coverage. There are
        innumerable variations of this approach. Most would increase the affordability of insurance
        premiums for private group and individual plans by providing financial assistance through
        tax policies and by modifying the benefits and coverage of the plans. Some policy analysts
        recommend that employers be mandated to offer coverage to their employees. Others recommend
        that individuals be required to purchase their own coverage. Since some individuals would
        be left without coverage, a public program, such as the existing Medicaid program for
        low-income individuals, would be used to cover everyone else. Many simulation studies have
        shown that these approaches could be effective in covering almost everyone, but they are
        the most expensive models of reform since they leave in place the administrative excesses
        of the fragmented system of funding care [14]. Also, to keep premiums affordable, these
        approaches may fall short on comprehensiveness of coverage and on the affordability of the
        out-of-pocket component, especially for those individuals with greater health care
        needs.
        In contrast, simulation of both the national health service and public social insurance
        models of reform have shown that they would provide truly comprehensive benefits for
        everyone, and that they are the least expensive models [14]. By integrating funding with
        the health care delivery system, both models are well suited for the introduction of an
        integrated information technology system. Such a system would provide invaluable data to
        assist with decisions on resource allocation, enabling incentives to be established that
        would strengthen the primary care base. It would also improve capacity planning for
        high-tech and specialized services, thereby ensuring appropriate access without excessive
        queues [15].
        The political threshold for adopting a government-owned health service model in the
        United States is very high, since most citizens fear the specter of “socialized medicine.”
        In contrast, the Medicare program, an insurance program for the retired and for those with
        long-term disabilities, is very popular. There is an increasing public perception that we
        may need to accept a greater government role in health insurance if we are to adequately
        address the deteriorating status of our health care system. Correcting the flaws in
        Medicare and then using the program to cover everyone may be a concept that can gain
        political traction in the United States.
      
      
        Conclusion
        Our political process is currently dominated by those who are enticed by the siren song
        of the market theorists and turn a deaf ear to the health policy scientists who plead for
        health care justice. The debate needs to focus on defining the best role for government in
        ensuring that people receive the best health care value. That debate needs to be guided by
        a thorough understanding and diligent application of sound health policy science.
        The continuing deterioration of affordability, coverage, and quality in health care
        makes it imperative that United States policymakers broaden their reform efforts beyond the
        ineffectual tinkering of incrementalism. A universal, single-payer, publicly funded and
        publicly administered program of social insurance would ensure access to affordable,
        comprehensive, high-quality health care for all. It should be the standard by which any
        other proposals are judged. If a better proposal can be crafted, now is the time to do it.
        People are dying while we delay.
      
    
  

  
    
      
        
        The devastating effects of HIV infection worldwide are reason enough for AIDS
        researchers to grasp at thin rays of hope. But seldom has a single anecdotal case
        stimulated as much hope as the 1999 report of an acutely infected patient who appeared to
        control HIV replication after two short treatment interruptions [1]. This report generated
        the hypothesis that early antiretroviral treatment (during or very soon after symptomatic
        seroconversion) allows the incompletely damaged immune system to recover and respond
        appropriately to virus antigens during treatment interruptions. This, in turn, according to
        the hypothesis, leads to control of viral replication by a healed and appropriately
        stimulated immune response to the patient's HIV infection.
        Consistent with this hypothesis was the prior finding that early antiretroviral therapy
        led to induction of HIV-specific proliferative responses similar to those that had been
        observed in patients with long-term, non-progressing HIV [2]. This led Rosenberg and
        colleagues to ask whether HIV-specific proliferative responses were a necessary and
        sufficient cause of long-term non-progression or just an immunologic consequence of
        controlled virus replication. Their report of virologic control in patients who interrupted
        therapy after early treatment raised hope that if HIV infection was treated early enough,
        the immune system could be repaired sufficiently to allow for long-term immunologic control
        of HIV replication [3]. Unfortunately, that's where the good news ends.
      
      
        Enthusiasm Fades
        A series of discoveries from clinical trials began to chip away at the enthusiasm for
        both early treatment of HIV infection and supervised treatment interruptions (STIs) as a
        way to boost the immune response.
        Several small trials of STIs in chronically infected patients were carried out [4],
        buoyed by the reasonable desire of patients for respite from the unpleasant side effects of
        the drugs. These trials gave disappointing results, up to and including the emergence of
        antiretroviral drug resistance in patients randomized to receive STIs. HIV-specific immune
        responses did increase off therapy, but so did viral loads. The so-called immune boosting
        probably reflected an immune response to greater viral antigen load but did not represent
        constructive immune enhancement.
        Larger trials clearly showed that STIs were of little if any benefit in chronic
        infection and that when therapy was stopped, viral loads invariably returned to
        pre-treatment levels [5]. Other studies indicated that HIV-specific CD4+ T cells were being
        preferentially infected, often massively, during treatment interruptions [6], and that
        proliferative responses were more likely to be a consequence—rather than a cause—of
        decreased HIV replication [7]. Despite multiple attempts, early reports of an inverse
        correlation between simple HIV-specific T cell responses and virologic control were not
        confirmed [8]. Where complex T cell functions did show such a correlation, the data
        indicated that viral replication was adversely affecting the character of the T cell immune
        response to HIV, and not the other way around [9]. Thus, no evidence of “immune boosting”
        during STIs and subsequent viral control in the absence of antiretroviral drugs was ever
        established. Finally, one of the acutely treated patients within Rosenberg's cohort became
        superinfected with a second strain of HIV despite excellent control of viral replication
        and significant recognition of the superinfecting strain by the pre-existing T cell
        response [10].
      
      
        New Findings
        Now comes a study in this month's 
        PLoS Medicine that found that in 14 patients who were treated early and
        who had controlled viral loads for at least 90 days, the virologic control was only
        transient [11]. While one could look at this as a glass half full—these patients achieved a
        reasonable period of time off antiretroviral therapy—closer scrutiny of the data limits
        this view.
        There was a disconnect between the low viral loads and an unexpectedly high rate of CD4+
        T cell decline in several patients. While the small number of patients and the single-arm
        nature of the study preclude definitive comparisons, it is possible that the early
        treatment and STIs did not result in a delay in CD4+ T cell decline (and, therefore,
        initiation of antiretroviral therapy) beyond what would have occurred had the patients
        received no early treatment.
      
      
        Implications of the Study
        This study raises important questions in our understanding of HIV pathogenesis,
        treatment, and vaccine development.
        First, why is it that early antiretroviral treatment, even if it does lead to better
        control of viral replication, does not protect against CD4+ T cell depletion? It is
        possible that by the time patients present with acute retroviral syndrome their CD4+ T cell
        reserves (in gut and lymphoid tissues) have been severely depleted, despite the fairly
        normal CD4+ profile of their peripheral blood. Thus, even low-level viral replication is
        then sufficient to deplete the remaining central and peripheral reserves [12].
        Second, how do these findings affect treatment guidelines during acute infection? None
        of the current treatment guidelines in either resource-rich or resource-poor settings
        recommend early antiretroviral therapy. In the light of these new data [11], there does not
        appear to be a rationale for early antiretroviral therapy in the absence of a clinical
        trial to assess other interventions in concert with early therapy. The use of therapeutic
        vaccination is an obvious intervention that still needs to be tested, despite limited
        efficacy results in treated chronic infection. As such, practice guidelines should continue
        to caution against early treatment unless associated with a randomized clinical trial.
        Finally, is this good or bad news for HIV vaccine development? Since most current
        vaccine strategies are based upon the hypothesis that induction of T cell immunity will
        lead to control of viral replication, it is difficult to be optimistic when a strong and
        broad immune response is unable to prevent disease progression. However, one must recall
        that phenotypic and functional assessments of HIV-specific T cell responses, even in
        antiretroviral-treated patients, show that these responses clearly differ from responses
        against viruses that are normally cleared or controlled by the immune system [9].
        Therefore, the T cell responses in the patients treated for acute HIV infection in Kaufmann
        et al.'s study were induced upon a dramatically altered immune background. It remains to be
        determined how much this adversely affects the HIV-specific immune response, and whether an
        immune response generated by vaccination 
        before any HIV replication (a prophylactic vaccine) might be better able
        to control virus replication. Far be it for us to stop grasping at rays of hope.
      
    
  

  
    
      
        
        As the Hispanic world well knows, the word in Spanish for advertising is ‘propaganda’,
        its meaning derived literally from the propagation of the faith, the antithesis of
        science's Enlightenment ideals. The old word somehow seems perfect for describing the new
        world of drug promotion and its growing use of the famous face. Like the catholic cardinals
        of the 17th century, many of the feted celebrities of the 21st are now engaged in spreading
        the word. Now, as then, the religion promises miraculous breakthroughs, wonder cures, and
        sometimes even eternal life. The difference is that this time around, the stars are earning
        fat fees from the marketing departments of giant pharmaceutical companies. And if the
        latest revelations from industry insiders are anything to go by, their hefty investments in
        celebrity selling are well worth it.
      
      
        Celebrity Selling
        The epicentre of this phenomenon is of course the United States, where companies
        routinely hire celebrities to attract attention to the latest drugs and the diseases that
        go with them. Pfizer famously paid presidential hopeful Bob Dole to promote awareness of
        erectile dysfunction as sildenafil (Viagra) was hitting the market. Wyeth hired supermodel
        Lauren Hutton to hawk hormone replacement therapy and menopause. GSK contracted football
        star Ricky Williams to sell social anxiety disorder, helping make paroxetine
        (Paxil)—briefly—the world's top-selling antidepressant. Even the dead are raising
        awareness, with the estate of Errol Flynn now enlisted to help promote cardiovascular
        disease as a household name [1]. The celebrity, living or dead, becomes integral to a drug
        marketing strategy that includes paid advertising and aggressive public relations campaigns
        that can produce media appearances on the likes of 
        Oprah and 
        The Today Show . According to celebrity brokers, the star's remuneration
        package, though always confidential, can range from $20,000 to $2 million.
        ‘A partnership between a celebrity and a brand has an intangible sort of magic’, writes
        a senior marketing executive at Amgen, in an extremely candid piece published recently in
        an industry trade magazine [2]. Amgen is the Californian biotech firm that hired handsome
        ‘West Wing’ star Rob Lowe to help market an anti-infection drug. Lowe was reportedly paid
        more than $1 million by Amgen, though there is speculation that part of the fee might flow
        to charity [3]. In her report, Amgen's Osnat Benshoshan shares some thoughtful tips with
        her peers among the pharmaceutical marketing fraternity: ‘use an A-list celebrity’; find a
        ‘news-hook’ that links the celebrity and your product; develop some simple messages; and
        make sure the celebrity delivers them at every appearance.
        Benshoshan then reveals why on-air talk-show appearances on ‘top-tier media venues’ like
        
        The Rosie Show can be better forums for celebrities than straight
        advertisements, which are governed by regulations. ‘The great advantage over advertising is
        that the airtime is practically free, and there is no fair balance to worry about’ she
        writes [2]. The downside with a media interview, she laments, is that compared to a
        scripted ad, ‘the situation is less controllable. It can be tricky for the celebrity to
        ensure that all product messages are delivered….’ Her other big tip for drug-makers is to
        rate your prospective celebrity with a ‘Q score’, a measure of their likeability and
        recognisability with the public. Apparently Rob Lowe's Q score was high with women over
        fifty, a key target of the Amgen campaign [3].
        Another recent report from within the industry draws on public opinion survey data to
        guide drug company marketers on the selection and ‘effective use’ of celebrity
        spokespersons [4]. The survey was conducted by a Seattle firm called NexCura Inc., in
        partnership with the trade magazine that published the study. The major findings echo the
        insights of the Amgen executive about credibility, and underline the importance of your
        star being perceived as generally trustworthy, and specifically knowledgeable about the
        condition on which they are hired to speak. Perhaps not surprisingly, the survey found that
        people diagnosed as suffering chronic conditions were far more attentive to celebrity
        messages on health than the general public.
        The issue of credibility is important, the NexCura Inc. researchers point out, because
        ‘the credibility rating is used as a surrogate for “buying” behavior’—an intermediate
        measure of whether the star can persuade people to request the target drug from their
        doctor. The survey found that Bob Dole was still the most recognisable celebrity marketer
        with the United States public, but that the skater Dorothy Hamill—currently promoting
        Merck's arthritis medication rofecoxib (Vioxx)—took the lead in the credibility stakes.
        Significantly though, almost three-quarters of those surveyed were correctly able to
        identify Bob Dole with Pfizer's Viagra, despite the fact that the advertisements in which
        he appeared were ‘unbranded’ ads for erectile dysfunction. The researchers concluded by
        recommending that drug companies choose a celebrity with personal experience of the target
        condition; choose someone trustworthy—perhaps a newsreader or sports figure; and choose
        someone who will promote a single cause or brand rather than multiple ones.
        Ironically, the NexCura survey also found two-thirds of medical consumers agreed with
        the proposition that celebrities were ‘just doing it for the money and can't be
        trusted’.
      
      
        The Trouble with Celebrity Selling
        The first problem here is that the public is often not even informed whether a celebrity
        is receiving money from a drug company. In the case of TV star Rob Lowe, there was no
        mandated requirement for him to disclose his link with Amgen when appearing on media shows
        watched by millions. According to one industry insider familiar with the case, who did not
        want to be named, ‘it depended if he remembered to say it, and whether he was asked’. The
        media's failure to disclose relevant conflicts of interest when covering healthcare is well
        established [5]. When 
        Frasier star Kelsey Grammer and his wife were promoting irritable bowel
        syndrome on top-rating TV shows, viewers thought the pair were speaking on behalf of an
        independent foundation. In fact the couple's fee had flowed from GSK, which was at that
        time preparing the market for alosetron (Lotronex), a controversial new drug that carried
        modest benefits and severe side effects, including possible death [6].
        Equally as serious is the lack of any formal requirement for stars or media outlets to
        spell out drug side effects along with benefits when celebrities are pushing products or
        conditions. Lauren Hutton can be quoted, in magazine articles read by millions of readers,
        as saying, ‘My No. 1 secret is estrogen’ without any need for her, or the magazine, to list
        the dangers of the hormone replacement therapy made by her sponsor [7]. But perhaps most
        troubling is the way celebrities, with their star power, can help to fundamentally shift
        the public debate about major health problems.
        While Prince Charles's companion Camilla Parker Bowles takes no money from drug
        companies, she did choose to make an important public statement about the bone condition
        osteoporosis at an international conference funded by Lilly, a company promoting a
        medication for the condition [8]. Camilla's call for early intervention and greater use of
        expensive tests and technologies for the primary prevention of osteoporosis drew on
        materials sponsored by the pharmaceutical industry, and was synchronised with simplistic
        industry marketing messages. Camilla's high-profile intervention at a drug company
        sponsored forum, albeit unwittingly, helps keep the focus on biochemical causes of, and
        biochemical solutions to, the much wider public health problem of fractures. Moreover these
        simple marketing messages undermine the complexity of the cost-effectiveness arguments that
        are central to any rational debate about the equitable distribution of health care
        resources. Other high-profile figures attending the same conference eagerly accepted Lilly
        money, and one, former Texas Governor Ann Richards, blatantly promoted Lilly's drug during
        an interview on CNN's Larry King Show just days later [8].
      
      
        The Future of Celebrity Selling
        With pharmaceutical marketing, it is clear that nothing short of a Vatican II-style
        reform is required, though there are already encouraging signs of change. Scientific
        journals are slowly disentangling themselves from unhealthy industry influence over what
        they publish, and public access to clinical trial data is daily a closer reality [9].
        However, a less distorted scientific record about healthcare products is meaningless
        without regulations on how important science is communicated to the public. Celebrities
        paid by drug companies to promote drugs, or ‘raise awareness’ about disease, should be
        subject to the same rules as direct-to-consumer advertising, which would mean prohibition
        in many nations and much more fulsome disclosure in the United States than is currently the
        case. At the very least, public disclosure of a product's risks and benefits, and the
        magnitude of the celebrity's fee, should be mandatory and routine. Let's see what that does
        to their Q rating.
      
    
  

  
    
      
        Introduction
        Dietary and lifestyle changes during the last century have entailed an unprecedented
        epidemic of obesity and associated metabolic diseases, including type 2 diabetes and
        atherosclerosis [1]. Many individuals suffer simultaneously from more than one of these
        conditions, and epidemiological studies in humans, as well as studies in animal models,
        suggest that obesity-related insulin resistance is a common pathogenic feature [2]. Indeed,
        insulin resistance is the keystone of the “metabolic syndrome,” a major cardiovascular risk
        factor even in the absence of demonstrable glucose intolerance or diabetes [3]. Obesity and
        insulin resistance are strongly associated with systemic markers of inflammation, and,
        indeed, inflammation may contribute to insulin resistance [4]. Similarities and overlap
        between obesity and inflammatory states are emerging. Inflammatory cytokines such as tumor
        necrosis factor α (TNF α) and interleukin (IL)-6 are produced by adipocytes as well as by
        monocytes and macrophages, and they circulate at increased levels in individuals with
        obesity [5,6]. Moreover, bone-marrow-derived macrophages home in on adipose tissue in
        individuals with obesity [7,8], and adipocytes and macrophages may even be interconvertible
        [9]. Furthermore, inflammation is increasingly recognized as a major component and
        predictor of atherosclerotic vascular disease, a major clinical consequence of insulin
        resistance [10]. Hence, the interrelationships between obesity, insulin resistance, and
        atherosclerosis are of great scientific and clinical interest.
        We originally identified and characterized resistin as a circulating mouse adipocyte
        gene product that is regulated by antidiabetic drugs [11]. In rodents, resistin is derived
        exclusively from adipocytes [11,12], circulates at increased levels in obese animals [11],
        and causes dysregulated hepatic glucose production, leading to insulin resistance [13,14].
        A syntenic gene exists in humans, but is expressed at higher levels in monocytes and
        macrophages than in adipocytes [15,16], raising questions about the relationship between
        resistin and human metabolic disease. Recently, several studies have suggested that
        metabolic abnormalities are associated with polymorphisms in the human resistin gene
        [17,18]. Furthermore, several studies, though not all, have reported increased serum
        resistin levels in patients with obesity, insulin resistance, and/or type 2 diabetes
        [19,20,21,22,23,24,25,26]. However, the mechanism and importance of increased resistin
        levels in human metabolic disease are not known.
        Here we show that the endotoxin lipopolysaccharide (LPS), a potent inflammatory
        stimulant, dramatically increases resistin production by inducing secretion of inflammatory
        cytokines such as TNFα. This increase in resistin production is blocked by both aspirin and
        rosiglitazone, drugs that have dual anti-inflammatory and insulin-sensitizing actions and
        have been shown to antagonize NF-κB. Indeed, activation of NF-κB is sufficient to induce
        resistin expression, and loss of NF-κB function abolishes LPS induction of resistin.
        Resistin serum levels are increased dramatically by endotoxemia in humans, and correlate
        with a marker of inflammation in patients with type 2 diabetes. Thus, systemic inflammation
        leads to increased resistin production and circulating levels in humans. The increased
        level of resistin in humans with obesity is likely an indirect result of elevated levels of
        inflammatory cytokines characteristic of states of increased adiposity. Hence, obesity and
        acute inflammation are both hyperresistinemic states associated with insulin
        resistance.
      
      
        Methods
        
          Differentiation of Primary Human Macrophages
          Peripheral blood mononuclear cells were isolated from whole blood of healthy donors
          following apheresis and elutriation. Greater than 90% of these monocytes expressed CD14
          and HLA-DR. Cells were plated in 24-well plates at a density of 10
          6 cells per well, allowed to adhere for 4 h, then washed with
          Dulbecco's Modified Eagles Medium and further cultured in 10% FBS in Dulbecco's Modified
          Eagles Medium supplemented with 5 ng/ml GM-CSF (Sigma, St. Louis, Missouri, United
          States) to promote macrophage differentiation. All experiments were performed after
          overnight equilibration with macrophage serum-free medium (GIBCO, San Diego, California,
          United States; Invitrogen, Carlsbad, California, United States) supplemented with 5 ng/ml
          GM-CSF. Cells were treated with LPS (Sigma), aspirin (Sigma), SN50, and/or control
          peptide (Biomol, Plymouth Meeting, Pennsylvania, United States), MG132, PD98059, SB20358
          (Calbiochem, San Diego, California, United States), and TNFα (R&D Systems,
          Minneapolis, Minnesota, United States). Neutralizing antibodies to TNFα, IL-6, and
          anti-IL-1β, as well as control IgG, were obtained from R&D Systems. Adenovirus
          expressing activated IKK in pAD easy with GFP and control vector was a generous gift from
          Steven Shoelson.
        
        
          RNA Isolation and Quantification
          RNA was isolated using RNeasy Mini Kit (Qiagen, Valencia, California, United States),
          then subjected to DNase digestion followed by reverse transcription (Invitrogen). mRNA
          transcripts were quantified by the dual-labeled fluorogenic probe method for real-time
          PCR, using a Prism 7900 thermal cycler and sequence detector (Applied Biosystems, Foster
          City, California, United States). Real-time PCR was performed using Taqman Universal
          Polymerase Master Mix (Applied Biosystems). The primers and probes used in the real-time
          PCR were the following: Sense-Resistin, 5′- AGCCATCAATGATAGGATCCA-3′; Antisense-Resistin,
          5′- TCCAGGCCAATGCTGCTTAT-3′; Resistin Probe, 5′-Fam-
          AGGTCGCCGGCTCCCTAATATTTAGGG-TAMRA-3′; Sense human 36B4 sense, 5′-
          TCGTGGAAGTGACATCGTCTTT-3′; Antisense 36B4, 5′- CTGTCTTCCCTGGGCATCA-3′; and 36B4 Probe,
          5′-FAM- TGGCAATCCCTGACGCACCG-TAMRA-3′.
          Primer and probe for TNFα were obtained from Applied Biosystems. The cycle number at
          which the transcripts of the gene of interest were detectable (CT) was normalized to the
          cycle number of 36B4 detection, referred to as deltaCT. The fold change in expression of
          the gene of interest in the compound-treated group relative to that in the
          vehicle-treated group was expressed as 2
          −deltadeltaCT , in which deltadeltaCT equals the deltaCT of the
          compound-treated group minus the deltaCT of the chosen control group, which was
          normalized to 1.
        
        
          ELISA
          Resistin concentrations, in cell media and human plasma, were assessed with a
          commercially available ELISA (Linco Research, St. Charles, Missouri, United States) and
          normalized to cell protein. The average correlation coefficient for standards using a
          four-parameter fit was 0.99. Intra-assay and inter-assay coefficients of variance were
          4.7% and 9.1%, respectively. Direct comparison of standard curves generated by the Linco
          kit with those yielded by another commercially available resistin ELISA (Biovendor
          Laboratory Medicine, Brno, Czech Republic) yielded high correlation (rho = 0.99, 
          p <0.001), except that the Biovendor values were approximately 30%
          lower than those determined with the Linco assay. This appeared to be related to the
          standards used for calibration. Discrepant absolute values among different assays,
          including the Biovendor assay, were recently described by others [22]. Resistin levels in
          40 plasma samples were measured using both Linco and Biovendor ELISA kits, with moderate
          correlation (rho = 0.66). Levels of soluble TNFα receptor 2 (sTNFR2) were measured using
          a commercially available immunoassay (R&D Systems). Intra-assay and inter-assay
          coefficients of variance were 5.1% and 9.8%, respectively.
        
        
          Human Endotoxemia Study
          Healthy volunteers (
          n = 6, three male and three female), aged 18–45 y with BMI between 20
          and 30 and on no medications, were studied. The University of Pennsylvania Institutional
          Review Board approved the study protocol, and all participants gave written informed
          consent. Following screening and exclusion of individuals with any clinical or laboratory
          abnormalities, participants were admitted to the General Clinical Research Center at the
          University of Pennsylvania for a 60 h stay. Serial blood samples were collected during
          the 24 h prior to and 24 h following the intravenous administration of
          human-research-grade endotoxin (obtained from National Institutes of Health Clinical
          Center, reference endotoxin [CCRE] [lots 1 and 2; National Institutes of Health Clinical
          Center PDS #67801]) at a dose of 3 ng/kg given at 6 AM. Plasma and whole blood RNA (PAX
          tube isolators, Qiagen) samples were isolated from blood, and stored under appropriate
          conditions for subsequent assays.
        
        
          Type 2 Diabetes Study
          Participants with type 2 diabetes (
          n = 215, 167 male and 48 female), aged 35–75 y and free from clinical
          cardiovascular diseases, were recruited through the diabetes clinics at the University of
          Pennsylvania Medical Center and the Veterans Affairs Medical Center, Philadelphia,
          Pennsylvania, to an ongoing study of cardiovascular risk factors in type 2 diabetes. The
          sample was composed of 59% Caucasians and 35% African-Americans. All participants were
          evaluated at the University of Pennsylvania General Clinical Research Center in a fasting
          state at 8 AM. The University of Pennsylvania Institutional Review Board approved the
          study protocol, and all participants gave written informed consent. The patient
          population is described in more detail elsewhere [27].
        
        
          Statistical Methods
          Data are reported as mean and standard error of the mean (SEM) for continuous
          variables. Because of baseline variation in cell populations between batches of primary
          human monocytes isolated from multiple donors, cell culture experiments were performed in
          triplicate and data from representative experiments are presented. For cell culture
          experiments with multiple treatments, analysis of variance (ANOVA) was used to test for
          differences in means across treatment groups. When significant global differences were
          found, post hoc 
          t -tests were used to compare specific treatment groups to the control.
          Data from the human endotoxemia experiment were analyzed by repeated measures ANOVA. In
          the type 2 diabetes study, Spearman correlations of plasma levels of resistin with plasma
          sTNFR2 levels are presented.
        
      
      
        Results
        
          Induction of Resistin Gene and Protein Expression by Endotoxin Treatment of Human
          Macrophages
          The regulation of resistin expression was studied in primary cultures of human
          monocytic cells. Immediately upon plating of elutriated primary human monocytes, resistin
          gene expression was detectable but highly variable from experiment to experiment (data
          not shown). One day after plating, resistin gene expression remained detectable at low
          levels (Figure 1A). Subjection of the cells to a protocol leading to differentiation
          along the macrophage lineage led to a modest, time-dependent enhancement of resistin gene
          expression (Figure 1A). In agreement with a previous report [28], treatment of primary
          macrophages with the endotoxin LPS led to a dramatic, dose-responsive increase in
          resistin gene expression (Figure 1B). We also determined that this effect of LPS was
          paralleled by an increase in resistin protein secretion into the medium (Figure 1C). Of
          note, activated mouse peritoneal macrophages harvested after thioglycolate treatment did
          not express detectable levels of mouse resistin, even after treatment with LPS (data not
          shown).
        
        
          Endotoxin Induction of Resistin Is Delayed with Respect to TNFα
          Induction of resistin gene expression by LPS exposure of human macrophages began
          between 6 and 24 h after treatment, with peak expression at 24 h (Figure 2A). This time
          course of resistin induction was delayed relative to induction of TNFα gene expression,
          which was detectable at 2 h and peaked 6 h after LPS exposure (Figure 2B). The secretion
          of TNFα followed a similar time course (Figure 2C). By contrast, secretion of resistin
          did not increase until much later, more closely following the pattern of the appearance
          of sTNFR2, a marker of TNFα action (Figure 2C) [29].
        
        
          Endotoxin Induction of Resistin Is Blocked by Immunoneutralization of Multiple
          Cytokines
          Resistin gene expression was also induced by TNFα treatment of primary human
          macrophages (Figure 3A) [28], and resistin secretion increased in parallel (Figure 3B).
          Since LPS induction of TNFα preceded the increase in resistin (see Figure 2C), we
          hypothesized that TNFα, or a similar cytokine produced early after LPS exposure, was
          responsible for the later induction of resistin. Indeed, neutralizing antibodies to TNFα
          markedly attenuated the increase in resistin gene expression (Figure 3C). LPS treatment
          also induces other cytokines, including IL-6 and IL-1β [30], and IL-6 induces resistin
          modestly (data not shown) [28]. Antibodies to IL-6 and IL-1β individually had minor
          effects on LPS stimulation of resistin (Figure 3C). However, the combination of
          antibodies to TNFα, IL-6, and IL-1β markedly attenuated LPS induction of resistin (Figure
          3C). These data clearly show that resistin induction by endotoxin is mediated by a
          cascade in which the primary event is secretion of inflammatory cytokines that, in turn,
          induce resistin.
        
        
          Induction of Resistin Is Blocked by Anti-Inflammatory Insulin-Sensitizing Drugs
          That Target NF-κB
          Mouse resistin, produced exclusively by adipocytes, is down-regulated by antidiabetic
          thiazolidinediones, including rosiglitazone [11]. Consistent with an earlier report [16],
          rosiglitazone down-regulated resistin gene expression (Figure 4A) in LPS-stimulated human
          macrophages. Resistin protein secretion was also significantly reduced by rosiglitazone
          (Figure 4B). Hence, macrophage expression of resistin and its induction by LPS is
          species-specific, but down-regulation of resistin by thiazolidinedione occurs both in
          rodents and humans. Rosiglitazone has marked anti-inflammatory effects on macrophages
          [31]. This led us to examine the effect of aspirin, an anti-inflammatory compound that
          targets IκB kinase and has insulin-sensitizing effects [32]. Remarkably, aspirin
          dramatically decreased endotoxin-induced resistin expression in a dose-dependent manner
          (Figure 4C). Both aspirin (via IκB kinase) and rosiglitazone (via PPARγ) inhibit NF-κB
          [31,32], which is activated by LPS. Indeed, treatment of the macrophages with the
          proteasome inhibitor MG132, which prevents NF-κB activation [33], abrogated
          endotoxin-induced activation of resistin expression (data not shown). Moreover, treatment
          of the macrophages with SN50, a cell-permeable peptide that specifically prevents
          activation of NF-κB by inhibiting its nuclear translocation [34], nearly abolished
          endotoxin-induced activation of resistin expression (Figure 4D). Thus, activation of
          NF-κB is required for LPS induction of resistin in human macrophages. Furthermore,
          constitutive activation of NF-κB by adenoviral expression of activated IκB kinase was
          sufficient to induce resistin in primary human macrophages (Figure 4E). The magnitude of
          this activation was less than that caused by LPS, which is known to also activate
          MAP-kinase (MAPK). Indeed, inhibition of either p42 MAPK by PD98059, or p38 MAPK (using
          SB20358) partially blocked the induction of resistin by LPS (Figure 4F). Together these
          results show that NF-κB activation is necessary and sufficient for resistin induction by
          LPS, with MAPK activation increasing the magnitude of the response.
        
        
          LPS Robustly Increases Circulating Resistin Levels in Healthy Humans
          Next, we asked whether our findings from ex vivo studies of human macrophages would
          translate into in vivo observations in humans. Six healthy volunteers were injected with
          LPS, using a protocol similar to that shown to produce insulin resistance [35]. Baseline
          circulating resistin levels were approximately 4 ng/ml, and remained relatively constant
          for several hours prior to LPS infusion (Figure 5A). Remarkably, resistin levels rose
          dramatically because of endotoxemia, peaking 8–16 h after LPS administration (Figure 5A).
          The time course of hyperresistinemia paralleled the increase in circulating levels of
          sTNFR2, although the increase in resistin levels was more marked and sustained (Figure
          5A). The increase in resistin protein levels correlated with increased resistin gene
          expression in peripheral blood mononuclear cells following systemic endotoxemia (Figure
          5B).
        
        
          Circulating Resistin Levels Correlate with the Inflammatory Marker sTNFR2 in
          Patients with Type 2 Diabetes
          Patients with type 2 diabetes and insulin resistance, many of whom are obese, have
          elevated levels of several inflammatory markers, including IL-6, TNFα, and sTNFR2 [36].
          LPS administration has been shown to induce acute insulin resistance in humans [37].
          Given that LPS infusion increased resistin levels, we measured resistin in a cohort of
          215 patients with type 2 diabetes. Circulating resistin levels were significantly
          correlated with levels of sTNFR (Figure 6A). Thus, there is an association between
          resistin levels and systemic inflammation in patients with type 2 diabetes.
        
      
      
        Discussion
        We have demonstrated that, in human macrophages, an inflammatory cascade with secretion
        of cytokines, including TNFα and IL-6, is sufficient and necessary for the induction of
        resistin. Insulin sensitizers that have anti-inflammatory properties, including a synthetic
        PPARγ agonist as well as aspirin, suppress macrophage resistin expression, as does direct
        inhibition of NF-κB. Experimental endotoxemia in healthy volunteers, based on the
        well-established gram-negative bacterial inflammatory response in humans [38,39,40],
        induces a dramatic elevation of circulating resistin levels. Hence, resistin gene and
        protein expression are increased by inflammatory stimuli both ex vivo and in vivo.
        In rodents, resistin is produced exclusively by adipocytes, regulates normal glucose
        homeostasis, and causes insulin resistance at high circulating levels [11,13]. Translation
        of resistin's metabolic effects from rodents to humans has been problematic because
        peripheral blood mononuclear cells and macrophages appear to be a primary source of
        resistin in humans [15,16]. This species difference in primary locus of expression is yet
        another example of the close and functionally overlapping relationship between adipocytes
        and macrophages [41]. Numerous studies have reported that circulating resistin levels are
        increased in human obesity [20,25,26,41] and diabetes [19,20,23,42,43]. Our data suggest
        that, whereas hyperresistinemia in obese rodents derives directly from adipocytes, human
        resistin is indirectly regulated by the inflammatory internal milieu of obesity (Figure
        6B). Indeed, obesity is associated with elevated levels of cytokines whose systemic
        administration leads to impaired glucose homeostasis [36,44,45], such as TNFα and IL-6,
        which we show here to mediate the inflammatory induction of human resistin. Thus, in both
        species, adipose tissue is an endocrine organ containing adipocytes as well as macrophages
        that regulates energy metabolism and glucose homeostasis through secretion of multiple
        factors, including inflammatory cytokines [46].
        Clearly the relationship between obesity, inflammation, and resistin expression is
        complex, and needs to be systematically studied in larger and varied patient populations.
        Intriguingly, we found a strong correlation between plasma levels of resistin and sTNFR2,
        the soluble cleavage product of the activated TNFα receptor, in diabetic patients. A
        comparable correlation between resistin and sTNFR2 (
        R = 0.31, 
        p <0.001) was found in a cohort of 879 non-diabetic individuals, in
        whom resistin levels independently correlated with coronary atherosclerotic disease (M. P.
        Reilly, M. Lehrke, M. L. Wolfe, A. Rohatgi, M. A. Lazar, and D. J. Rader, unpublished
        data).
        LPS binds to pathogen-associated-molecular-pattern innate immune receptors, such as CD14
        and Toll-like receptor 4, activating signal cascades involving NF-κB and MAPK [47] and
        thereby inducing the transcription and secretion of early cytokines, including TNFα and
        IL-1 [48]. We have shown here that these early cytokines are responsible for secondary
        induction or enhancement of resistin expression in macrophages. Hyperresistinemia impairs
        glucose homeostasis in rodents [49,50], and inflammatory states are associated with insulin
        resistance [36], which may serve as a physiological attempt to increase the provision of
        glucose to the brain under stress conditions. Indeed, induction of acute inflammation by
        administration of LPS causes insulin resistance in humans [37], and here we have
        demonstrated the concomitant induction of resistin. Interestingly, the peak in TNFα and
        IL-6 levels after LPS administration to humans precedes a phase of prolonged insulin
        resistance that begins approximately 6 h after LPS administration [37], closely
        approximating the time course of resistin induction. Hence resistin is a potential mediator
        of insulin resistance in humans with acute inflammation. Moreover, obesity is associated
        with activation of innate immunity [6], including the inflammatory mediators that induce
        resistin. In this context it is intriguing that resistin levels are increased in obesity
        [25,26] and that insulin-sensitizing agents such as aspirin and rosiglitazone, with
        disparate primary molecular targets, antagonize resistin induction. Indeed,
        thiazolidinedione suppression of resistin levels has recently been correlated with hepatic
        insulin sensitization [43]. Future work will be needed to better understand the
        relationship between circulating resistin levels and the insulin resistance characteristic
        of inflammatory states, including obesity.
      
    
  

  
    
      
        
        In a compelling essay in this issue of 
        PLoS Medicine , Mike Clarke, the director of the United Kingdom Cochrane
        Centre, lays down a challenge to clinical researchers and journal editors [1]. He argues
        that researchers should do a study only if there is a systematic review that shows that the
        new study is needed. If no review exists, the researchers should do one themselves before
        embarking on their research. And journals, he argues, should publish a study only if an
        updated systematic review is incorporated into the study, or published alongside it or
        shortly thereafter. How should editors respond to his challenge?
        First, we would argue that by the time a paper is sent to a journal, it is surely too
        late in the process to be insisting on systematic reviews. If a clinical trial report meets
        our criteria for originality, importance, and quality, it makes little sense for us to
        reject it just because the authors failed to systematically review the literature when
        designing their study. The time to mandate that researchers do a review is much
        earlier—when they apply for funding, register their trial, or seek ethics committee
        approval.
        There is no doubt that the best research builds on previous knowledge. But
        unfortunately, the current medical publishing system hides much of this knowledge behind
        subscription or “pay per view” charges, which discriminates against researchers who do not
        work for well-funded institutions. A group of researchers in Indonesia, for example,
        recently told a depressingly familiar story of trying to search the medical literature in
        preparation for a research project [2]; access barriers got in their way. So our second
        response to Clarke's challenge is that it will remain difficult for researchers,
        particularly in resource-poor settings, to do systematic reviews unless the medical
        literature is made a freely available public resource.
        Many clinical trials, especially negative ones, remain unpublished, which prevents
        researchers from reviewing all the data on an important health issue. There are two main
        reasons why certain trials are not published: one is that the pharmaceutical industry has a
        long history of suppressing data that are commercially unfavorable and the second is that
        medical journals and the popular media favor publication of positive over negative trials
        (after all, negative trials do not make for a provocative newspaper headline). While we
        support the recent announcement on trial registration by the International Committee of
        Medical Journal Editors—as a condition of considering a trial for publication, member
        journals will require registration of the trial in a public trials registry [3]—we believe
        that this policy addresses only part of the problem.
        The scientific literature will remain biased unless the publishing industry changes its
        practices and provides a place where the results from all registered trials can be
        published. 
        PLoS Medicine is committed to publishing high-quality negative trials. In
        this issue, for example, we publish an important randomized controlled trial of a malaria
        vaccine in 372 Gambian men, which found that the vaccine was ineffective at reducing the
        natural infection rate.
        The internet makes it possible for every single clinical trial to be publicly and
        seamlessly tracked through three tiers. The first tier is registration in a publicly
        available database. The second is the publication of a peer-reviewed summary of every
        trial, regardless of its outcome, in a traditional journal format, with annotations and
        critiques that help readers understand the trial's implications. The third is the
        deposition of detailed trial data in a structured, computable format that allows
        sophisticated searching and analyses across trials. This format will allow the development
        of better tools to help clinicians apply trial results to their practice. For trial data to
        be as useful as possible, all three tiers must be publicly accessible. Assessment of each
        trial's validity is critical, but should not stop crucial information about all trials
        being placed in the public domain.
        Trial registries exist, such as ClinicalTrials.gov and the International Standard
        Randomised Controlled Trial Number registry. Moreover, many trials are registered in a
        semi-public database maintained by the United States Food and Drug Administration, and
        there are compelling arguments (which Turner articulates in an essay published online ahead
        of our December issue [4]) for making this a truly public resource. Publicly accessible
        trial databases (such as the Trial Bank Project at http://rctbank.ucsf.edu) are under
        development. And as a publisher committed to open access, PLoS will provide the second,
        essential tier—journals capable of peer reviewing and publishing an annotated report of
        every trial. Traditional medical journals, with their subscription-based model, are
        unlikely to be able to provide this service, because in order to attract subscribers they
        need to publish only the highest-profile trials. We believe that an open-access model—in
        which the research funder pays a publication fee to recover the costs of peer review and
        for hosting the report on a secure server—is the best mechanism for creating such venues.
        We are working to make that happen.
        Returning to Clarke's challenge, our final response is to say that we have a bold vision
        of a freely accessible online world of clinical trials—from registration to annotated
        summaries to trial databases. That world would be even richer if every systematic review
        were made freely available. We challenge the Cochrane Collaboration to put the full text of
        all of its reviews into the public domain. We hope the Cochrane Collaboration will join us
        in the open-access revolution.
      
    
  

  
    
      
        
        A malaria vaccine called ME-TRAP, which targets the pre-erythrocytic stage of the
        disease, was not effective at reducing natural infection rates in semi-immune African
        adults, according to the report of a randomized controlled trial published this month in 
        PLoS Medicine . “This first field efficacy trial was an important
        milestone in the progression of new recombinant vectored vaccines to deployable products,”
        says Adrian Hill (University of Oxford, United Kingdom), the lead investigator of the
        study. “The safety profile was excellent and the efficacy data provide a first indication
        of the levels of cellular immunogenicity that will be required for preventing infection,”
        he says.
        Hill and his co-workers used a heterologous prime–boost vaccination technique. They gave
        the volunteers two vaccines—a DNA priming vaccine followed by a modified vaccinia virus
        Ankara (MVA) that acted as a booster. The DNA and MVA vaccines both had the same insert
        coding for thrombospondin-related adhesion protein (TRAP; a pre-erythrocytic antigen) and a
        string of T cell epitopes (called ME for “multiple epitopes”).
        Hill's team had previously shown that ME-TRAP vaccines given in prime–boost sequence
        could induce large T cell responses in healthy volunteers from the UK and could delay
        parasitemia in a sporozoite challenge test (Nat Med 9: 729–735). The next step was to do a
        randomized controlled trial in Gambia to determine whether this vaccination strategy could
        provide protection against natural 
        Plasmodium falciparum infection.
        The researchers recruited volunteers from 13 Gambian villages that were close to the
        alluvial flood plain and so were at high risk of developing malaria. They randomly assigned
        the 372 volunteers to receive either two doses of the DNA ME-TRAP vaccine followed by a
        single dose of MVA ME-TRAP, or three doses of rabies vaccine. This three-dose schedule is
        similar to the one used by the World Health Organization/United Nations Children's Fund
        Expanded Program on Immunization. Two weeks before the third dose was given, all the
        volunteers received antimalarial drugs to clear blood-stage 
        P. falciparum infections.
        The time to first infection, the primary end point of the study, was similar in the two
        groups, with an estimated vaccine efficacy of only 10%. However, the effector T cell
        response to the TRAP antigen T9/96, measured one week after the third vaccination, was 80
        times higher in the DNA/MVA vaccine group than in the rabies vaccine group.
        “It is absolutely crucial that results like these are published, since the failures, as
        well as the successes, need to be documented if we are to move towards rational strategies
        for optimizing malaria vaccines,” says Tom Smith from the Swiss Tropical Institute, who was
        not involved in the study. “At the same time, it makes sense to move on quickly without
        shedding too many tears, in a field that is moving much faster than it was before the
        recent injections of money from the Gates Foundation, but where it is still impossible to
        second-guess the results of field trials. This is partly because we do not have any good
        proxy measures of effective immunity in 
        P. falciparum , and partly because this is a fertile area for
        trying out new techniques, such as DNA vaccines, where there is still a lot to learn.”
        Hill is planning to do further trials that address the important question of whether
        this type of vaccine can prevent the symptoms of malaria. “The next step,” says Hill, “is
        to assess newer vaccine regimes that employ two viral vectors rather than DNA and to study
        prevention of malaria rather than infection.”
      
    
  

  
    
      
        
        Highly active antiretroviral therapy (HAART) for the treatment of individuals infected
        by HIV-1 is limited by high costs, drug resistance, and drug-related toxicities. This has
        led researchers to investigate new treatment options, including ways to boost immune
        responses to better control HIV. One such approach has been termed supervised treatment
        interruption (STI)—in which HAART is intermittently stopped once viral load has been
        reduced to a low level, in order to boost natural immunity by brief exposure to virus. The
        goal is to allow for the eventual discontinuation of drug treatment.
        Preliminary evidence, published by Bruce Walker and his colleagues from Harvard Medical
        School in 
        Nature in 2000, suggested that this approach worked in persons treated in
        the earliest stages of acute HIV infection. HIV-1 viral loads in newly infected patients
        remained suppressed for a median of six months after therapy had been stopped. However, a
        follow up paper, published this month in 
        PLoS Medicine by the same research group, shows that the viral load
        rebounded in eight of the 14 patients by one year.
        “The findings are very straightforward and very important,” comments Danny Douek from
        the Vaccine Research Center, National Institutes of Health, United States, who was not
        involved in the study. “In almost every case, virus rebounded and no clinical benefit from
        the interruption could be determined.”
        Walker's team first considered the possibility of STI in 1997 after they demonstrated
        that HAART given to patients recently infected with HIV could protect T helper cells, which
        are normally destroyed in the earliest stages of infection. They hypothesized that early
        treatment of acute HIV-1 infection with HAART might boost the immune response, allowing it
        to control the HIV-1 infection without the need for continuous therapy. “We did not know at
        that time whether the T helper cells would be functional,” explains Walker. “The only way
        to tell this was to stop medications and see if the immune response could control the
        virus.”
        To test this hypothesis the researchers did an open-label trial of STIs; they published
        data from six months follow-up in the 
        Nature paper. “The key finding was that we were able to get at least
        transient control of virus in all eight persons studied, and in five of eight the viral
        load was less than 500 copies (very low!) at the time of publication,” explains Walker.
        However, at that point they did not know how long the protective effects would last.
        The first evidence that protection was not complete came two years later when Walker's
        team reported a case of superinfection; one of the patients in the original experiment was
        infected with a second strain of HIV, even though the first virus was still well
        controlled. “This paper was important because it indicated that the amount of immunity
        might be enough for the person's own virus, but might not protect against closely related
        viruses circulating in the population,” says Walker.
        The 
        PLoS Medicine study adds more concern since it shows that although most
        persons can indeed transiently control their own virus, they do so for only a limited
        amount of time. “We expanded the study to 14 persons, and now have about five years of
        follow-up on some of the patients,” says Walker. “Although we were able to use early
        treatment and structured treatment interruption to boost immunity and have 11 of 14
        patients control their virus, most of the persons ultimately ‘broke through,’ meaning that
        they had a recurrence of viremia.” At the present time the researchers do not know what
        causes the loss of viral control.
        Walker and colleagues conclude that treatment interruptions should probably be avoided
        outside the setting of controlled clinical trials, whereas Douek goes a step further: “The
        study shows that even early short-term treatment and structured treatment interruptions,
        using current strategies, impart only transient benefit and are unlikely to serve as a
        reasonable therapeutic option in the future.”
      
    
  

  
    
      
        
        Obesity, in particular visceral adiposity, is positively correlated with insulin
        resistance and type 2 diabetes. Although the link is well established in humans and in
        rodent models, the mechanisms involved in obesity-related insulin resistance are not clear.
        One possibility is that hormones secreted by adipocytes compromise peripheral insulin
        sensitivity, and a number of candidates for such adipocyte signals have been identified.
        One of them, resistin, was discovered a few years ago by Mitchell Lazar and colleagues, who
        showed that the protein is expressed by mouse adipocytes and regulated by a group of
        anti-diabetic drugs called thiazolidinediones. Several lines of evidence from functional
        studies in rodents suggested that resistin could be the missing mechanistic link between
        obesity and diabetes.
        The human homolog of resistin has subsequently been under intense investigation, but
        initial studies revealed more differences than similarities between the human and rodent
        proteins: human resistin is mostly expressed in macrophages, not in adipocytes, and its
        serum levels do not correlate as clearly with obesity, insulin resistance, or diabetes.
        Similarly, genetic association studies between allelic variants of the resistin gene and
        metabolic abnormalities have so far been inconclusive. These results prompted some of the
        scientists in the field who had jumped on the resistin bandwagon after the initial results
        in rodents to jump off again. Others, including the resistin discoverers, continue their
        quest to uncover resistin's role in humans, and have started to think outside the framework
        defined by the mouse data.
        Starting with the role of macrophages in inflammation and encouraged by the fact that
        obesity and insulin resistance are associated with markers of systemic inflammation, Lazar
        and colleagues examined the resistin response to inflammatory stimulators. As they report
        in this issue, resistin production in macrophages and serum levels in patients are
        significantly increased by these stimulators. This response can be blocked by the
        thiazolidinedione rosiglitazone and by aspirin, two drugs that have dual anti-inflammatory
        and insulin-sensitizing actions and antagonize the immune regulator NF-kappaB. The
        researchers go on to show that activation of NF-kappaB is sufficient to induce resistin
        expression. And NF-kappaB is necessary for the resistin response to inflammatory
        stimuli.
        Lazar and colleagues now view obesity as a state of chronic inflammation and speculate
        that in obese individuals inflammatory cytokines lead to elevated production of resistin by
        macrophages and elevated serum resistin levels, which in turn contribute to insulin
        resistance and diabetes. This is consistent with some studies that have found higher
        resistin levels in obese individuals and patients with insulin resistance and/or diabetes,
        but not all studies have found such differences.
        Jeffrey Flier, an obesity researcher who was not involved in the study, calls the
        article “an excellent and timely paper that demonstrates the fact that inflammatory
        pathways induce resistin expression and levels in human monocytes ex vivo, and in intact
        humans. The work appears to provide a novel link between inflammation and insulin
        resistance, through monocyte derived resistin.” He points out, however, that “several other
        factors also appear to contribute directly to insulin resistance in inflammation (e.g.,
        cytokines themselves, without invoking resistin) so the full biologic implications of the
        high resistin levels for insulin resistance in humans cannot be determined from this
        study.” Resistin, it seems, continues to resist easy interpretations.
      
    
  

  
    
      
        
        The immune system has a remarkable capacity for fending off infectious diseases, and it
        has become clear that these same defenses can recognize and destroy cancer cells. In fact,
        they do so on an ongoing basis, and cancer develops only when immune surveillance breaks
        down. Many patients with established tumors also mount an immune response against some
        antigens that are specific to, or enriched in, the tumor. This response, however, is rarely
        effective against the disease.
        The idea of enlisting the immune system to fight cancer has been around for a long time,
        and has led to the development of various cancer vaccines designed to alert the immune
        system to the presence of a tumor and to induce a response that, selectively and potently,
        will eliminate tumor cells. Vaccines include whole tumor extracts or specific proteins and
        peptides that are selectively expressed or enriched in tumors, by themselves or with a
        variety of adjuvants.
        There have been some spectacular successes, in particular with immune therapy to
        malignant melanoma, a tumor type that seems naturally to be more immunogenic than others.
        However, even in melanoma, success is usually restricted to a fraction of the patients,
        with no obvious explanation of why the strategy works for a particular patient and fails in
        most others. The emphasis has consequently shifted from clinical outcomes to monitoring a
        patient's immune response. What type of response is necessary and sufficient to eliminate
        tumor cells is still unclear, but the hope is that understanding the immune response in
        patients that show clinical benefit will answer that question.
        Peter Lee and colleagues used state-of-the art technology to dissect the endogenous
        immune response to vaccination with heteroclitic melanoma peptides, i.e.,
        melanoma-associated peptides that have been engineered to elicit a stronger immune
        response. They focused on cytotoxic T lymphocytes (CTLs), and compared CTL clones from four
        melanoma patients who had vaccine-induced T cell responses and two melanoma patients with
        spontaneous anti-tumor T cell responses. The researchers analyzed several hundred CTL
        clones (to get a sense for the complexity of the responses in individual patients) for T
        cell receptor variable chain beta expression, recognition efficiency, and ability to lyse
        target melanoma cells. Most T cells isolated from vaccinated patients were poor at tumor
        cell lysis compared with T cells from endogenous responses to cancer.
        The authors suggest that the high doses of peptides administered in vaccinations and the
        increased binding capacity of heteroclitic peptides to major histocompatibility complex
        molecules—the very quality that makes them more immunogenic—induce many T cells with low
        recognition efficiency for the native peptides they encounter on the tumor cells. Their
        findings also bring into question the ability to deduce the recognition efficiency and
        tumor reactivity of T cell responses from ELISPOT and tetramer staining assays—the two
        standard measures of T cell responses to vaccines—which has implications for rational
        vaccine design in general.
      
    
  

  
    
      
        PRESENTATION of CASE
        A 38-y-old woman with Down syndrome was admitted to hospital for investigation of a 6-mo
        history of anorexia and weight loss of 40 lbs. Six months prior to admission, her weight
        was 175 lbs, and her body mass index was 36. On admission, her complete blood count was
        normal, but over a 2-wk period she developed acute pancytopenia (Figure 1). During this
        acute episode, her lowest hematological parameters were as follows: hemoglobin (Hb), 80 g/l
        (normal range 120–160 g/l); mean cell volume (MCV), 121 fl (80–95 fl); white blood cell
        count (WBC), 2.9 × 10
        6 /l (4.8–10.8 × 10
        6 /l); absolute neutrophil count, 1.1 × 10
        6 /l (1.5–6.2 × 10
        6 /l); and platelet count, 76 × 10
        6 /l (150–350 × 10
        6 /l).
        She had been taking carbamazepine for 17 y and sodium valproate for 13 y for a mixed
        seizure disorder. At age 22 y, before starting any anticonvulsants, her baseline
        hematological parameters were as follows: Hb,123 g/l; MCV, 106 fl; platelets, 296 × 10
        6 /l; and WBC, 10.7 × 10
        6 /l. After starting carbamazepine, her WBC dropped to 4.5–5.5 × 10
        6 /l, and her absolute neutrophil count dropped from 9 × 10
        6 /l to about 2.5 × 10
        6 /l. When the sodium valproate was added, her MCV increased to 112 fl,
        her platelet count fell to 100–150 × 10
        6 /l, and her Hb dropped to 110–120 g/l.
        The patient's other medications on admission were carnitine and low-dose L-thyroxine for
        hypothyroidism; she had been taking both for several years. She had not been recently
        exposed to any new medications, environmental toxins, or over-the-counter dietary
        supplements. There was no family history of aplastic anemia. The patient lived at home with
        her mother, who cares for her and has legal guardianship.
        A bone marrow biopsy (Figure 2) showed a hypocellular bone marrow with a normal
        myeloid:erythroid ratio and no malignant cells or megaloblastic changes. Cytogenetic study
        showed a Robertsonian (i.e., of the whole arms) translocation of Chromosome 14 and 21
        involving the long arm of Chromosome 21. Except for mild adrenal insufficiency noted on an
        adrenocorticotropic hormone stimulation test, all her tests, including viral and
        immunological investigations to identify known causes of aplastic anemia, were negative.
        The patient's thyroid function tests were normal. She was started on low-dose
        hydrocortisone for her adrenal insufficiency and megestrol acetate to increase her
        appetite.
        The patient's carbamazepine was discontinued. Within 10 d, her WBC had risen to 6.7 × 10
        6 /l and her platelet count to 248 × 10
        6 /l. Serial complete blood counts showed worsening anemia over the next
        15 wk, requiring two packed red blood cell transfusions at week 6 and week 14, when her Hb
        was 31 and 53 g/l, respectively. Throughout this period, her reticulocyte count was low, at
        about 0.18% (normal range 0.5%–2%), while her WBC and platelet count were normal. We made a
        diagnosis of pure red cell aplasia (PRCA). A second bone marrow biopsy under minimally
        conscious sedation was unsuccessful, and the patient declined further attempts. After the
        first transfusion she was started on prednisone and erythropoietin for her PRCA. Meanwhile,
        because of her worsening seizures, we increased her sodium valproate dose, increasing her
        serum valproate level from 70 mcg/ml to 110 mcg/ml. After the second transfusion, we
        discontinued her sodium valproate and started her on clonazepam and oxcarbazepine as
        alternative anticonvulsants.
        The patient had a brisk reticulocyte response. Her reticulocyte count rose from 0.36% to
        0.78% in the second week and to 6.61% in the fourth week after stopping the valproate. Six
        weeks after stopping the drug, her Hb was 125 g/l and her MCV was 106 fl, suggesting
        replacement of transfused red blood cells with newly formed red blood cells (which have a
        higher MCV than older transfused cells). Over the next 30 mo of follow-up, she had no
        relapse of her aplastic anemia.
        We were unable to identify a specific cause for the patient's anorexia and weight loss.
        We found no evidence of malignancy on admission or subsequent follow-up. Within 1 wk of
        stopping her sodium valproate, her appetite improved and she put on 15 lbs over the next 6
        wk. By the fourth month after stopping the drug, she had gained 45 lbs.
        We last saw the patient in August 2004. Her seizure control had worsened, and she had
        developed signs of early dementia. Her current anticonvulsants are clonazepam,
        oxcarbazepine, and zonisamide, and she continues taking synthroid. Her last complete blood
        count was stable: WBC, 6.4 × 10
        6 /l; Hb, 147 g/l; MCV, 104.9 fl; and platelets, 262 × 10
        6 /l.
      
      
        Discussion
        Sodium valproate and carbamazepine are associated with rare but potentially lethal
        hematological complications. There are five case reports of PRCA shortly after initiation
        of sodium valproate, with the longest interval between the initiation of therapy and the
        onset of aplasia being 2 y [1,2,3,4,5]. Acute bone marrow suppression with leucopenia and
        thrombocytopenia associated with carbamazepine most often occur within 4 mo of starting
        treatment [6,7]. As far as we know, our case report is unique in that the patient developed
        acute bone marrow suppression and PRCA after 17 y of carbamazepine and 13 y of sodium
        valproate therapy.
        Our investigations ruled out most of the known causes of acute bone marrow suppression,
        making the anticonvulsants the most likely cause. Malnutrition was an unlikely cause: her
        WBC and platelet counts had recovered before any increase in appetite or weight gain; her
        body mass index was normal despite her weight loss; and her bone marrow iron stores were
        adequate and her serum folate and vitamin B12 levels were high, suggesting adequate
        nutrient supply.
        We considered and rejected the possibility of Down syndrome–associated aplastic anemia.
        There have been six case reports of this condition, and in all cases it occurred in young
        children, suggesting a genetic predisposition [8]. Half of these patients died, and half
        responded partially to androgenic steroids. Our patient was an adult, and her bone marrow
        had responded to the withdrawal of her anticonvulsants and not to the administration of
        androgenic steroid. She had failed to respond to a 6-wk course of high-dose non-androgenic
        steroid and erythropoietin. Furthermore, our patient did not have a relapse of her aplastic
        anemia in 30 mo of follow-up, suggesting a lack of genetic disposition.
        The brisk return of her WBC and platelet counts upon discontinuing carbamazepine, and
        her brisk reticulocytosis upon discontinuing sodium valproate, were both consistent with
        previous reports of hematological toxicity due to these drugs [1,2,3,4,5,6,7]. What was
        unusual in our case was the extremely late onset of bone marrow suppression after
        initiation of drug therapy. The persistent suppression of erythropoietic elements for 15 wk
        after stopping the carbamazepine was unlikely to be due to persistent residual marrow
        suppression by carbamazepine, since carbamazepine-induced PRCA responds quickly (within 1–2
        wk) to stopping the drug [6,9]. We believe that the continued use of sodium valproate,
        after stopping the carbamazepine, caused the persistent suppression of erythropoietic
        elements.
        We found no cause for the patient's anorexia and weight loss, but her appetite returned
        and she gained weight after stopping the sodium valproate. There is a known association
        between this drug and anorexia [10].
      
    
  

  
    
      
        Arthur Caplan's Viewpoint: Nobody Is Perfect—But Why Not Try to Be Better?
        Perfection has come in for a lot of bad press recently. A torrent of books and articles
        has recently appeared [1,2,3,4,5,6,7,8,9], all raising serious ethical questions about the
        wisdom and morality of trying to use biomedical knowledge to perfect ourselves or our
        offspring.
        Biomedical scientists and physicians might be inclined to ignore this literature as just
        so much abstract philosophical handwringing. After all, it is almost impossible to find
        mainstream scientists arrogant enough to proclaim their interest in perfecting anything,
        much less themselves or their fellow human beings.
        Beating up on the pursuit of perfection is silly. As Salvadore Dali famously pointed
        out, “Have no fear of perfection—you'll never reach it.” Critics of those who allegedly
        seek to perfect human beings know this. While often couching their critiques in language
        that assails the pursuit of perfection, what they really are attacking is the far more
        oft-expressed—albeit far less lofty—desire to improve or enhance a particular behavior or
        trait by the application of emerging biomedical knowledge in genetics, neuroscience,
        pharmacology, and physiology. Those who might accurately be termed “anti-meliorists” wonder
        how we will ever resist the obvious temptation to put this knowledge to use to alter
        ourselves. They are quick to note that we have already given in to such temptation—we
        augment our breasts, smooth our wrinkles, and pump ourselves full of antidepressants.
        Putting the brakes on biologically driven human betterment would have real consequences
        for science. Some lines of research would be slowed or restricted [3,5,8]. Their
        application would be declared off-limits or at least tightly regulated
        [1,2,3,4,5,7,8,9].
        Why is the drive to improve ourselves so disturbing to the anti-meliorists? Their
        arguments cluster around three key worries: that the pursuit of perfection by biomedical
        means is vain, selfish, and unrewarding [1,2,3,6,7], that improving ourselves is unfair
        [1,3,4], and that enhancement or improvement violates human nature [2,4,5,7,8,9] and may
        actually destroy it [2,5,7,9]. It is the last of these arguments that is at the core of
        anti-meliorist concerns.
        It cannot simply be the pursuit of improvement that is making anti-meliorists nervous.
        Many religious traditions and spiritual movements seek perfection [10,11,12,13], but these
        evoke no negative commentary from the anti-meliorists. Nor do efforts to improve animals
        and plants set this crowd aflutter. Rather, it is biomedical knowledge being applied to you
        and me that is the crux of their concern. They fear that in applying new biomedical
        knowledge to improve human beings, something essential about humanity will be lost. If
        biomedical tinkering is allowed, we will destroy the very thing that makes us human—our
        nature.
        Anti-meliorism rests, however, on a very shaky foundation. To support their position,
        the anti-meliorists must state what human nature is. They do not. They must also be very
        clear about why they see human nature as static. They are not. And they must advance an
        argument about why human nature, which has presumably evolved in response to an enormous
        array of random forces, tells us anything about what is good or desirable in terms of the
        traits humans should possess. They cannot.
        The fight over whether there is any such thing as human nature is a long-standing one
        [14]. But one can concede that we are shaped by a causally powerful set of genetic
        influences and still remain skeptical as to whether these produce a single “nature” that
        all members of humanity possess. Is there a single trait or fixed set of traits that
        defines the nature of who we are and have been throughout our entire existence on this
        planet? Unless they can articulate this Platonic essence, anti-meliorists do not have a
        foundation for their argument that change, improvement, and betterment are grave threats to
        humanity.
        Worse still for anti-meliorists, we are clearly creatures who have long tinkered with
        ourselves, using all manner of technologies from clothing to telescopes to computers to
        airplanes. Our view of our “nature” is closely linked to the technologies that we have
        invented and to which we have adapted [15]. We are already technological creatures.
        Nor is there any normative guidance offered by our evolutionary history that shows why
        we should not try to improve upon the biological design with which we are endowed.
        Augmenting breasts or prolonging erections may be vain and even a waste of scarce
        resources, but seeking to use our knowledge to enhance our vision, memory, learning skills,
        immunity, or metabolism is not obviously either.
        Ultimately, anti-meliorism posits a static vision of human nature to which the
        anti-meliorists mandate we reconcile ourselves. If anything is clear about human nature, it
        is that this is not an accurate view of who we have been or what we are now, or a view that
        should determine what we become.
      
      
        Carl Elliott's Viewpoint: Pharma's Gain May Be Our Loss
        Those of us who worry about medical enhancement are usually less worried about the
        technologies themselves than about the larger social effects of embracing them too
        enthusiastically. Just as you do not need to object to cars to worry about urban sprawl,
        you do not need to object to enhancement technologies to question where these technologies
        may be taking us. It is not just technophobes who wonder whether a society that consumes
        90% of the world's supply of methylphenidate (Ritalin), where the most profitable class of
        drugs is antidepressants, and where cosmetic surgeons perform liposuction on prime-time
        television is a society that has somehow lost its way.
        Let's look at three of the most commercially successful medical enhancements of recent
        years: selective serotonin reuptake inhibitors, hormone replacement therapy, and the diet
        drug fenfluramine-phentermine (Fen-Phen). What can we learn from these interventions?
        First, the manufacturers of enhancement technologies will usually exploit the blurry
        line between enhancement and treatment in order to sell drugs. Because enhancement
        technologies must be prescribed by physicians, drug manufacturers typically market the
        technologies not as enhancements, but as treatments for newly discovered or
        under-recognized disorders. Selective serotonin reuptake inhibitors were marketed not as
        personality enhancers, or even only as treatments for clinical depression, but as
        treatments for questionable illnesses like “premenstrual dysphoric disorder” [16]. Fen-Phen
        was sold not as a mere diet drug but as a treatment for obesity, which Wyeth, the
        manufacturer, portrayed as a dangerous public health problem [17]. Estrogen replacement
        therapy was initially marketed as a risk-free way for women to extend their youthfulness.
        But when a 1974 study found that estrogen replacement therapy was associated with an
        increased risk of endometrial cancer, the manufacturers added progesterone, renamed the
        combination “hormone” replacement therapy, and recast it as a treatment for medical
        problems associated with menopause such as osteoporosis [6].
        Second, an alarming number of supposedly risk-free enhancements have later been
        associated with unanticipated side effects, some of them deadly. Wyeth has set aside over
        $16 billion to compensate the thousands of patients who have developed valvular heart
        disease and pulmonary hypertension after taking Fen-Phen [18]. A 2002 National Institutes
        of Health study found that hormone replacement therapy was associated with such an elevated
        risk of heart disease, stroke, pulmonary emboli, and breast cancer that the study was
        stopped prematurely [19]. Selective serotonin reuptake inhibitors are currently embroiled
        in controversy over whether they are associated with an elevated risk of suicide [20].
        Third, the most successful enhancement technologies have been backed by tremendously
        influential public relations campaigns. These campaigns have included ghostwritten journal
        articles, industry-funded front groups, and lucrative payments to academics, professional
        societies, and university centers [21]. For example, GlaxoSmithKline marketed paroxetine
        (Paxil) by promoting the previously obscure diagnosis of “social anxiety disorder” through
        phony support groups, celebrity spokespeople, a direct-to-consumer illness awareness
        campaign, and generous payments to key opinion leaders [22]. The manufacturers of estrogen
        replacement therapy marketed the hormone in the 1960s by funding a “research foundation”
        for Robert Wilson, the gynecologist and author of the best-selling book 
        Feminine Forever [6]. Wyeth marketed Fen-Phen by funding obesity research
        centers, launching public fitness campaigns, contracting with a medical education company
        to produce a series of ghostwritten journal articles, and making generous payments to
        academic physicians who then published extensively and testified for the drug's safety to
        the Food and Drug Administration [17].
        The traditional worry about enhancement technologies is that users of the technologies
        are buying individual well-being at the expense of some larger social good. I may improve
        my own athletic ability by taking steroids, but I set off a steroid arms race that destroys
        my sport. I may get cosmetic surgery for my “Asian eyes” or use skin lighteners for my dark
        skin, but I reinforce the implicitly racist social norms that say that Asian eyes or dark
        skin are traits to be ashamed of. The worry is that some aspect of the way we live
        together, collectively, is going to be damaged by actions that we take individually
        [4].
        A market-driven health-care system brings this worry much closer to home. The
        pharmaceutical industry is now the most profitable and politically powerful industry in the
        United States [23]. It also has a huge financial interest in creating a demand for
        enhancement technologies. The pharmaceutical industry can buy politicians to pass
        industry-friendly legislation; it can buy academic scientists to publish favorable journals
        articles; it can buy professional societies and patient support groups to spread the word
        on the newly medicalized disorders that its interventions are developed to treat [24]. It
        can even buy bioethicists to dispense with any moral concerns [25]. In this kind of
        political and economic climate, how likely is it that dissenting voices will have any
        effect before it is too late?
      
      
        Caplan's Response to Elliott's Viewpoint
        Elliott professes to be unhappy about enhancement. What arguments does he present to
        support his unhappiness? Not many, and the arguments that he does offer miss the point
        completely.
        If people want to feel better, sleep less, have fewer hot flashes, better vision, or
        fewer wrinkles, then they may want to use enhancement technologies to achieve these things.
        Technology in itself isn't driving us in any particular direction—I believe that we decide
        where it should go. Elliott, however, gravely warns us that you and I do not really decide
        a direction when it comes to matters of enhancement. It is—listen carefully for the Darth
        Vader–esque hissing—drug companies!
        The rest of Elliott's viewpoint amounts to what is his increasingly familiar harangue
        against the pharmaceutical industry. The drug companies sucker us into buying enhancement
        by getting us hooked on pseudotherapies. The drug companies rob us of our will to fend off
        their siren-like messages of better living through their chemistry. And the drug companies
        get us feeling so bad about ourselves that we empty our wallets on their latest overpriced
        geegaws.
        Pharmaceutical companies may be evil incarnate. And we may be putty in their pecuniary
        little hands. But that has nothing at all to do with the question of whether there is
        anything wrong with pursuing enhancement. When Elliott eagerly dons his hair shirt to
        bemoan Big Pharma, he finds so much sin to revel in that he forgets to give a reason, any
        reason, why enhancement is, in itself, immoral.
        At most he presents an argument for keeping the pharmaceutical industry out of
        enhancement. Okay, so let's take Big Pharma out of the picture. If we left the
        encouragement of enhancement to the government, the military, schools, foundations,
        doctors, or parents, would this now be morally acceptable? I think sometimes it would be.
        And nothing that Elliott says provides any reason to think otherwise.
      
      
        Elliott's Response to Caplan's Viewpoint
        Caplan does not defend medical enhancement so much as attack its critics. Or rather, he
        attacks a small group of conservative critics who want to preserve “human nature.” He
        dispatches those critics with admirable precision, but I am not sure why he believes that
        group of critics includes me. My worry about enhancement technologies has little to do with
        human nature. My worry is that we will ignore important human needs at the expense of
        frivolous human desires; that dominant social norms will crowd out those of the minority;
        that the self-improvement agenda will be set not by individuals, but by powerful corporate
        interests; and that in the pursuit of betterment, we will actually make ourselves worse
        off.
        It's no secret that many Americans are deeply ashamed of their personal shortcomings and
        inadequacies. Nor is it any secret that these shortcomings and inadequacies can be
        exploited for commercial profit. But do we really want to submit our health-care system to
        the same forces that have made millionaires out of motivational speakers and diet book
        authors?
        Skepticism about enhancement technologies is not equivalent to a wish to set back
        medical research and declare some applications off-limits. This is a debate about enhancing
        human traits, not curing human illness. To say that our medical research agenda will be set
        back if we restrict enhancement technologies makes no more sense than saying that cancer
        surgery will be set back if the American Broadcasting Corporation cancels its cosmetic
        surgery reality TV show 
        Extreme Makeover .
        We live in a country where 46 million uninsured people cannot get basic medical care,
        while the rest of us spend a billion dollars a year on baldness remedies. It is not just
        the inequity here that is so impressive. It is the fact that we have gotten so accustomed
        to the inequity that we do not see it as obscene.
      
    
  

  
    
      
        
        Only about 1% of newly developed drugs are for tropical diseases, such as African
        sleeping sickness, dengue fever, and leishmaniasis [1]. While patent incentives and
        commercial pharmaceutical houses have made Western health care the envy of the world, the
        commercial model only works if companies can sell enough patented products to cover their
        research and development (R&D) costs. The model fails in the developing world, where
        few patients can afford to pay patented prices for drugs.
        It is easy and correct to say that Western governments could solve this problem by
        paying existing institutions to focus on cures for tropical diseases. But sadly, there does
        not appear to be enough political will for this to happen. In any case, grants and patent
        incentives were never designed with tropical diseases in mind.
        Two main kinds of proposals have been suggested for tackling the problem. The first is
        to ask sponsors—governments and charities—to subsidize developing-country purchases at a
        guaranteed price [2,3,4]. The second involves charities creating nonprofit venture-capital
        firms (“Virtual Pharmas”), which look for promising drug candidates and then push drug
        development through contracts with corporate partners. In this article, we discuss the
        limitations of these two approaches and suggest a third, “open source,” approach to drug
        development, called the Tropical Diseases Initiative (TDI). We envisage TDI as a
        decentralized, Web-based, community-wide effort where scientists from laboratories,
        universities, institutes, and corporations could work together for a common cause (see
        www.tropicaldisease.org).
      
      
        Why Open Source?
        The idea behind asking sponsors to subsidize developing country purchases at a
        guaranteed price is that this will prop up drug prices and restore incentives for
        developing new drugs [2,3,4]. In other words, it is a way of fixing the patent problem.
        However, subsidies have an important weakness: it is almost impossible to determine
        correctly how large the subsidy should be. In principle, the most cost-effective solution
        is to set a subsidy that just covers expected R&D costs. But how large is that? R&D
        costs are very poorly known, with the published estimates quoting uncertainties exceeding
        $100 to $500 million per drug. If the subsidy is set too low, companies cannot cover their
        R&D costs and nothing will happen. Set the subsidy too high, and the sponsor's costs
        skyrocket. To date, no sponsor has tried to implement these proposals.
        In the “Virtual Pharma” approach, governments and philanthropies fund organizations that
        identify and help support the most promising private and academic research. Examples
        include the Institute for One World Health (www.iowh.org), a not-for-profit pharmaceutical
        company funded mainly through private sources and the Gates Foundation, and the Drugs for
        Neglected Diseases Initiative (www.dndi.org), a public sector not-for-profit organization
        designed to mobilize resources for R&D on new drugs for neglected diseases.
        Virtual Pharmas have clearly started to bear fruit, and are responsible for most
        candidate treatments for tropical diseases currently under development. For example, the
        Drugs for Neglected Diseases Initiative has a portfolio of nine projects spread out across
        the drug development pipeline for the treatment of leishmaniasis, sleeping sickness, Chagas
        disease, and malaria [6]. But Virtual Pharmas face three important problems. The first is
        similar to the problem faced by subsidy proposals: guessing private-sector R&D costs.
        One needs to understand what a product costs in order to negotiate the best possible
        price—and guessing wrong is likely to be expensive. Second, Virtual Pharma's development
        pipelines will run dry without more upstream research. Research has been particularly weak
        in exploiting genomic insights [7]. Third, tropical disease research is badly underfunded.
        For this reason, Virtual Pharma cannot succeed without rigid cost containment.
        We believe that a new, community-wide consortium, the Tropical Disease Initiative, can
        help solve these problems. Its success would help keep Virtual Pharma's R&D pipeline
        full. Furthermore, it would use open-source licenses to keep its discoveries freely
        available to researchers and—eventually—manufacturers. As we explain below, well-designed
        open-source licenses are the key to containing Virtual Pharmas' R&D costs.
        While we expect the final choice of license to be made by TDI's members, the guiding
        principle should be to pick whatever license lets developing country patients derive the
        most benefit from TDI's work. Possible choices are shown in Box 1.
      
      
        How It Works
        To date, open-source methods have made little headway beyond software [8]. However,
        computing and computational biology are converging. In the same way that programmers find
        bugs and write patches, biologists look for proteins (“targets”) and select chemicals
        (“drug candidates”) that bind to them and affect their behavior in desirable ways. In both
        cases, research consists of finding and fixing tiny problems hidden in an ocean of
        code.
        What would open-source drug discovery look like? As with current software
        collaborations, we propose a Web site where volunteers use a variety of computer programs,
        databases, and computing hardware (Figure 1). Individual pages would host tasks like
        searching for new protein targets, finding chemicals to attack known targets, and posting
        data from related chemistry and biology experiments. Volunteers could use chat rooms and
        bulletin boards to announce discoveries and debate future research directions. Over time,
        the most dedicated and proficient volunteers would become leaders.
        Ten years ago, TDI would not have been feasible. The difference today is the vastly
        greater size and variety of chemical, biological, and medical databases; new software; and
        more powerful computers. Researchers can now identify promising protein targets and small
        sets of chemicals, including good lead compounds, using computation alone. For example, a
        SARS protein similar to mRNA cap-1 methyltransferases—a class of proteins with available
        inhibitors—was recently identified by scanning proteins encoded by the SARS genome against
        proteins of known structure [9]. This discovery provides an important new target for future
        experimental validation and iterative lead optimization. More generally, existing projects
        such as the University of California at San Francisco's Tropical Disease Research Unit (San
        Francisco, California, United States) show that even relatively modest computing,
        chemistry, and biology resources can deliver compounds suitable for clinical trials [10].
        Increases in computing power and improved computational tools will make these methods even
        more powerful in the future.
        Just as they do today, Virtual Pharmas would choose the best candidates. The difference
        is that open-source drugs could not be patented in developing countries. This would not
        stop Virtual Pharma from developing promising discoveries. (S. Nwaka, V. Hale, personal
        communications). Importantly, TDI would be a great boost to the efforts of Virtual Pharmas,
        because it would help to contain the costs of discovering, developing, and manufacturing
        drugs.
      
      
        Cost Containment
        TDI would contain costs in three important ways. First, TDI would ask volunteers to
        donate their time (and any patentable discoveries) to the collaboration. Instead of
        financial incentives, TDI would offer volunteers non-monetary rewards, such as ideological
        satisfaction, the acquisition of new skills, enhancement of professional reputation, and
        the ability to advertise one's skills to potential employers. Software collaborations have
        demonstrated that these incentives are a good way to attract and motivate programmers [11].
        Similar incentives should work equally well for biologists, chemists, and other
        scientists.
        Second, we have already pointed out that existing proposals have difficulty containing
        costs. The root cause is patents. Normally, society relies on competition to keep prices
        low. Patents—by design—short-circuit competition by giving the owners the legal right to
        prevent others from using (or even developing) their invention. TDI, on the other hand,
        would restore competition by making drug candidates available to anyone who wanted to
        develop them. We expect sponsors to exploit this advantage by signing development contracts
        with whichever company offers the lowest bid. Such competitive bidding is a powerful way to
        contain costs, and is also a good way to develop drugs. Virtual Pharma has extensive
        experience supervising contract research.
        Third, the absence of patents would continue to keep prices low once drugs reached the
        market. The generic drug industry shows what happens once drug makers are allowed to
        compete. US drugs frequently fall to about one-third their original price when patents
        expire [12].
      
      
        Intellectual Property Rights
        Would universities and corporations really let their people volunteer? Won't they insist
        on intellectual property rights? The practical answer is that sensible managers do not care
        about intellectual property rights unless they expect to earn a profit. This explains why
        sophisticated university licensing offices seldom bother to interfere with open-source
        software projects that are not commercially valuable [13]. The same logic would apply to
        open-source drug discovery. We would hope that life sciences companies would make a similar
        calculation. But permitting employees to participate is only the beginning. We think that
        universities and companies will also donate the data, research tools, and other resources
        needed to make TDI even stronger. The reason, once again, is that they have little to lose.
        The value of their intellectual property depends almost entirely on US and European
        diseases. For this reason, it costs very little to share their information with tropical
        disease researchers. In fact, drug companies already do this [14]. TDI's main challenge
        will be to show donors that an open-source project can keep members from diverting donated
        information back into the commercially lucrative diseases that affect patients in the
        West.
        Finally, there are precedents for private companies developing drugs off patent. During
        the 1950s, March of Dimes (see www.marchofdimes.com) developed polio vaccines without any
        patents at all [15]. It then signed guaranteed purchase contracts with any drug maker
        willing to develop commercial-scale production methods. The incentive may not have been
        conventional, but it worked. And why not? The contracts made good business sense: contract
        profits may have been small compared to the profits on patented drugs, but so was the risk.
        Fifty years later, contract research still makes sense. Generic drug companies, developing
        world drug manufacturers, contract research organizations, and biotech firms have all said
        that they would consider contracts to develop open-source drug candidates. (M. Spino, S.
        Sharma, F. Hijek, and D. Francis, personal communications).
      
      
        Next Steps
        So far, we have described a shoestring operation that exists mainly on the Web. Except
        for computer time, budgets would be more or less the same as existing software
        collaborations. Computing would be expensive but manageable. Today's biologists routinely
        scrounge resources from university machines or borrow time on home computers [16, 17]. This
        Web-centric approach would be a good start, but not a complete solution. Computational
        biology works best when it can interact with experimental chemistry and biology.
        Nevertheless, a low-budget computational approach is probably enough to generate new
        science, suggest ideas for follow-up experiments, and make new drug candidates available
        under licenses designed to yield maximum benefit to the developing world.
        In practice, an open-source drug discovery effort is likely to include modest
        experiments. Many academic scientists control discretionary resources and, in some cases,
        tropical disease grants. Furthermore, good science generates its own funding. We expect
        experimentalists to turn the collaboration's Web pages into grant proposals.
        That said, TDI's volunteers will be most productive if sponsors back them. Charities
        could support open-source drug discovery by making wet chemistry and biology experiments a
        top priority. Corporations could also help by donating funds, laboratory time, or
        previously unpublished results. One low cost/high value option would be for companies that
        have already tried a particular research direction to warn TDI if the collaboration was
        about to investigate a known dead end. (R. Altman, personal communication)
      
      
        Conclusion
        Open-source drug discovery is feasible—that is, no known scientific or economic barrier
        bars the way. But what are the risks? Experience with software collaborations highlights
        the main social and economic challenges. First, the project will have to find and motivate
        volunteers. Based on existing software collaborations, we estimate a required minimum
        “critical mass” of a few dozen active members. Second, modest chemistry and biology
        experiments will be needed to increase the chances for success. Resources of several
        hundred thousand dollars per year—mostly in the form of in-kind donations of databases,
        laboratory access, and computing time—would make open-source drug discovery much more
        powerful. By most standards, such risks are real but acceptable.
        The largest uncertainties are scientific. Can a volunteer effort based on computational
        biology and modest experiments produce the high-quality drug candidates that Virtual Pharma
        needs? A successful program must (1) make a significant contribution toward supplying the
        genomic insights that tropical disease research needs to move forward, and (2) make useful
        drug candidates available for development and production under open-source licenses.
        Open-source drug discovery looks feasible. The only way to be sure is to do the
        experiment—and we invite you to join us.
        
          To learn more about TDI or to volunteer, go to
          http://www.tropicaldisease.org
        
      
    
  

  
    
      
        Diabetes: Epidemiology and Complications
        Treatment for, and the prognosis of, type-1 diabetes mellitus (T1DM) has progressed
        dramatically during the last century, but the disease remains a major cause of morbidity
        and mortality. Although precise figures are not available, over 1 million United States
        citizens currently live with the disease, with approximately 30,000 new cases diagnosed in
        the US each year. The total number of people with diabetes worldwide is expected to rise to
        366 million in 2030, up from 171 million in 2000 [1].
        The exact etiology of the disease remains uncertain, but extensive research suggests an
        interaction between genetic predisposition and environment. In fact, for unknown reasons,
        the incidence of T1DM is increasing [2]. Diabetes continues to have a tremendous societal
        impact; it is both difficult and expensive to treat and is associated with a number of
        long-term complications, including kidney failure, blindness, nerve damage, and premature
        mortality (predominately due to cardiovascular problems).
      
      
        Insulin's Impact
        Banting and Best's discovery of insulin in the early 1920s revolutionized diabetes
        treatment and greatly improved the prognosis for what had previously been a rapidly fatal
        disease. As shown by the Diabetes Control and Complications Trial and the more recent
        Epidemiology of Diabetes Interventions and Complications trial, insulin therapy has made
        such considerable advances (with better insulin formulations and delivery systems) that
        many patients can maintain their blood sugar levels within a tight range and thereby reduce
        their risk for the disease's long-term complications [3,4,5]. In addition, improved
        treatment of other associated conditions such as hypertension and hyperlipidemia have
        helped reduce, or at least delay, many of the long-term sequelae of diabetes [6]. However,
        problems with insulin-based treatment regimens persist. For the patient, treatment is
        expensive and difficult, requiring strict attention to blood glucose monitoring, insulin
        dosing, diet, and exercise. Further, good glycemia control is not easily achieved by all
        patients, and even for those able to achieve this goal, the treatment is not always
        completely effective.
      
      
        Promising Directions
        Just as financial investors balance a portfolio, with some risky investments and others
        that are more secure, researchers will undoubtedly continue to further refine “secure”
        insulin-based regimens to help patients achieve even better glycemia control. At the same
        time, scientists are pursuing more high-risk, high-payoff approaches to revolutionize
        diabetes care. One such approach is the closed-loop insulin pump (i.e., a pump that
        continuously monitors blood glucose and concurrently converts that data into appropriate
        insulin dosing), which offers the potential to serve as a mechanical pancreas. However,
        such a mechanical system would need be fail-safe in order to avoid devastating effects
        (e.g., if the monitor were to register a falsely elevated blood glucose and thereby trigger
        an inappropriately high insulin dose). In other, similar scenarios with no tolerance for
        error, NASA (for instance) sets up systems in which two independent monitoring systems must
        come up with similar measurements before an action is taken. Perhaps the engineering
        obstacles that currently limit the closed-loop insulin pump can be overcome.
        Other research groups are investigating whether the insulin-producing cells within the
        pancreas (so-called ß cells), might be promoted to regenerate (in vitro or in vivo) to
        replace the pool of insulin-producing cells reduced by autoimmune destruction. Another
        promising approach for creating cells capable of physiologically regulated insulin
        secretion is to “coax” stem cells—undifferentiated cells with self-regenerative capacity—to
        differentiate into ß-like cells. Gene therapy approaches may overcome present obstacles and
        result in cells capable of physiologically regulated insulin secretion [7]. Lastly, the
        recent completion of the Human Genome Project suggests that the genetics of diabetes may
        eventually become clearer and may direct appropriate preventative approaches.
        While such potential therapies remain experimental, pancreas transplantation is
        currently performed in patients with complicated diabetes. However, a recent report that
        shows benefit for patients with both diabetes and kidney failure who receive a combined
        pancreas and kidney transplant also found that an isolated pancreas transplant (for
        patients with preserved kidney function) actually worsened survival [8]. The main point is
        that as we develop new therapies, we must maintain humility and recognize that newer
        approaches may have great promise, but they also have the potential for harm.
      
      
        History of Islet Transplantation
        Islet transplantation has recently received considerable interest as a potentially
        definitive treatment for diabetes. The concept of islet transplantation is not
        new—investigators as early as the English surgeon Charles Pybus (1882–1975) attempted to
        graft pancreatic tissue to cure diabetes. Most, however, credit the recent era of islet
        transplantation research to Paul Lacy's studies dating back more than three decades. In
        1967, Lacy's group described a novel collagenase-based method (later modified by Dr.
        Camillo Ricordi, then working with Dr. Lacy) to isolate islets, paving the way for future
        in vitro and in vivo islet experiments [9]. Subsequent studies showed that transplanted
        islets could reverse diabetes in both rodents and non-human primates [10,11] (Figure 1). In
        a summary of the 1977 Workshop on Pancreatic Islet Cell Transplantation in Diabetes, Lacy
        commented on the feasibility of “islet cell transplantation as a therapeutic approach [for]
        the possible prevention of the complications of diabetes in man” [12]. Improvements in
        isolation techniques and immunosuppressive regimens ushered in the first human islet
        transplantation clinical trails in the mid-1980s. Yet despite continued procedural
        improvements, only about 10% of islet recipients in the late 1990s achieved euglycemia
        (normal blood glucose). In 2000, Dr. James Shapiro and colleagues published a report
        describing seven consecutive patients who achieved euglycemia following islet
        transplantation using a steroid-free protocol and large numbers of donor islets, since
        referred to as the Edmonton protocol [13]. This protocol has been adapted by islet
        transplant centers around the world and has greatly increased islet transplant success.
      
      
        Current Limitations of Islet Transplantation
        While significant progress has been made in the islet transplantation field [14], many
        obstacles remain that currently preclude its widespread application. Two of the most
        important limitations are the currently inadequate means for preventing islet rejection,
        and the limited supply of islets for transplantation. Current immunosuppressive regimens
        are capable of preventing islet failure for months to years, but the agents used in these
        treatments are expensive and may increase the risk for specific malignancies and
        opportunistic infections. In addition, and somewhat ironically, the most commonly used
        agents (like steroids, calcineurin inhibitors, and rapamycin) are also known to impair
        normal islet function and/or insulin action. Further, like all medications, the agents have
        other associated toxicities, with side effects such as oral ulcers, peripheral edema,
        anemia, weight loss, hypertension, hyperlipidemia, diarrhea, and fatigue [15]. Perhaps of
        greatest concern to the patient and physician is the harmful effect of certain widely
        employed immunosuppressive agents on renal function. For the patient with diabetes, renal
        function is a crucial factor in determining long-term outcome, and calcineurin inhibitors
        (tacrolimus and cyclosporin) are significantly nephrotoxic. Thus, while some patients with
        a pancreas transplant tolerate the immunosuppressive agents well, and for such patients
        diabetic nephropathy can gradually improve, in other patients the net effect (decreased
        risk due to the improved blood glucose control, increased risk from the immunosuppressive
        agents) may worsen kidney function. Indeed, Ojo et al. have published an analysis
        indicating that among patients receiving other-than-kidney allografts, 7%–21% end up with
        renal failure as a result of the transplant and/or subsequent immunosuppression [16].
        Looked at another way, patients with heart, liver, lung, or kidney failure have a dismal
        prognosis for survival, so the toxicity associated with immunosuppression is warranted (the
        benefits of graft survival outweigh the risks associated with the medications). But for the
        subset of patients with diabetes and preserved kidney function, even those with
        long-standing and difficult-to-control disease, the prognosis for survival is comparatively
        much better. In addition to the immunosuppressive toxicities, other risks are associated
        with the islet transplant procedure itself, including intra-abdominal hemorrhage following
        the transplant, and portal vein thromboses. The fact that there is already a good
        alternative to islet transplantation (i.e., the modern intensive insulin regimen) forces us
        to regard any newer, riskier interventions with a critical eye.
        Like all transplantation therapies, islet transplantation is also handicapped by the
        limited donor pool. The numbers are striking; at least 1 million Americans have T1DM, and
        only a few thousand donor pancreata are available each year. To circumvent this organ
        shortage problem, researchers continue to look for ways to grow islets—or at least cells
        capable of physiologically regulated insulin secretion—in vitro, but currently only islets
        from cadaveric donors can be used to restore euglycemia. Further exacerbating the problem
        (and unlike kidney, liver, and heart transplants, where only one donor is needed for each
        recipient) most islet transplant patients require islets from two or more donors to achieve
        euglycemia. Lastly, the current methods for islet isolation need improvement, since only
        about half of attempted isolations produce transplant-ready islets.
        While islet transplantation research has made important progress and the success stories
        are encouraging, the long-term safety and efficacy of the procedure remain unclear. Other
        concerns relating to the field include questions about the impact of having
        insulin-producing foreign cells within the hepatic parenchyma, the long-term consequences
        of elevated portal pressures resulting from the islet infusion, and the fact that islet
        recipients can be sensitized against donor tissue types, making it more difficult to find a
        suitable donor should another life-saving transplant be required in the future. Also, very
        few islet transplant recipients have remained euglycemic without the use of any exogenous
        insulin beyond four years post-transplant. Thus, while most islet recipients achieve better
        glycemia control and suffer less serious hypoglycemia, islet transplantation continues to
        fall short of the definitive diabetes cure.
      
      
        Is Islet Transplantation Ready for Widespread Use?
        While no one suggests that the therapy is ready for widespread clinical application,
        another way of highlighting current problems is to focus on cost. Assuming present hurdles
        were cleared, islet transplantation costs approximately $150,000 per patient per
        transplant. With over 1 million Americans dealing with T1DM, it would cost over $100
        billion to give each patient a single islet transplant, with little assurance as yet of any
        long-term benefit. In contrast, the annual direct cost of a proven therapy like intensive
        insulin treatment is about $3,500 per patient [17].
        The limitations of islet transplantation force us to recognize that the therapy remains
        experimental, and that many questions must be answered before it is incorporated into
        general clinical practice. At the present time, we urge a focus on the selection of only
        those patients for whom this procedure offers the greatest likelihood of benefit. Most
        people with diabetes can, with diligence and perseverance, implement an insulin regimen
        that maintains tight glucose control while avoiding dangerous hypoglycemia. However, there
        are some patients who continue to have tremendous difficulty managing their disease despite
        optimal care and effort. Even the statement “despite optimal care and effort” is difficult
        to define, and we advocate that all patients being considered for an islet transplant first
        be referred for several months to specialty teams that are committed to diabetes care.
        Since such patients whose diabetes is the most difficult to control have a poor quality of
        life, islet transplantation offers potential benefit. Even a low baseline level of insulin
        production by the transplanted islets may lower the amount of insulin required, while
        reducing the number and severity of hypoglycemic events. We also believe the islet
        transplant risk-benefit ratio is favorable for those with both T1DM and kidney failure who
        are listed for a life-preserving kidney transplant; such patients will have to take
        immunosuppressive agents after transplant to preserve the kidney allograft function, so the
        islets can be added without too much additional risk.
        Where do we go from here? Just as early studies showed islet transplantation's promise,
        research must now overcome the hurdles revealed by the recent islet transplant experience.
        New immunomodulatory agents offer the greatest hope of revolutionizing the field. New drug
        regimens capable of inducing tolerance to the transplanted islets would allow recipients to
        maintain their grafts without general immunosuppression and its associated toxicities.
        While many targets are currently under investigation, none are ready for clinical use. We
        advocate that such immunomodulatory approaches be tested first in controlled models where
        the results can be appropriately attributed to the agent itself.
      
      
        Conclusion
        Less than a century ago, T1DM was invariably a fatal disease. With the advent of
        insulin, the prognosis changed overnight, and we have continued to witness improvements in
        diabetes care and outcomes. Pancreatic islet transplant has offered renewed hope to many
        patients with diabetes, who envision a life free of glucose checks and insulin injections.
        Some transplanted patients have enjoyed “success” and are pleased with their decisions;
        unfortunately these results are not universal. Researchers must continue to look for ways
        to improve the procedure while protecting the welfare of each individual patient. The field
        has come a long way, but we must remain cautious, as we are treating a non-fatal disease
        for which there is a very effective standard therapy.
      
    
  

  
    
      
        
        Over the past several years, there has been growing concern about selective publication
        of clinical trial results [1,2]. The debate has intensified since New York State Attorney
        General Elliot Spitzer filed suit against GlaxoSmithKline on June 2, 2004, alleging that
        the company was hiding data regarding the efficacy and safety of selective serotonin
        reuptake inhibitors in pediatric patients with depression [3].
        The two most frequently suggested remedies for the selective reporting of clinical
        trials results have been to register all clinical trials and to make their results publicly
        available. Registries have been called for at least as far back as 1974; hundreds have in
        fact already been established [4]. Shortcomings of registries include the fact that they
        are often not coordinated and that participation is often voluntary and—in cases where they
        are mandated by legislation—difficult to enforce. For example, ClinicalTrials.gov, a
        registry authorized by the Food and Drug Modernization Act of 1997, appears not to be
        comprehensive. One study found that, of 127 cancer protocols sponsored by pharmaceutical
        companies that met criteria for inclusion, only 48% were in fact submitted to the registry
        [5]. Thus, one can check a number of registries and still have little assurance that all
        the relevant trials of interest have been included.
        Increasing the pressure on pharmaceutical companies to include more trials in
        registries, the International Committee of Medical Journal Editors has announced that, as a
        condition of considering a trial for publication, member journals will require its
        registration in a public trials registry [6]. Further, at the American Medical Association
        (AMA) Annual Meeting of the House of Delegates in June 2004, the AMA called on the
        Department of Health and Human Services to establish a comprehensive national registry. In
        September 2004, an AMA trustee testified in a United States Congressional hearing,
        outlining elements necessary to make such a registry effective [7]. Momentum for a
        comprehensive clinical trials registry is also building internationally [8].
        In this essay, I argue that a highly valuable but underused registry and results
        database for US trials already exists within the Department of Health and Human Services,
        specifically within the Food and Drug Administration (FDA).
      
      
        New Drug Applications
        Before a pharmaceutical company can conduct a US trial that it intends to use in support
        of a new drug application (NDA), it must first register that trial with the FDA. Because
        the NDA forms the basis for marketing approval, it seems likely that the percentage of
        industry-sponsored trials that are registered with the FDA is very high. This registration
        takes the form of an investigational new drug (IND) application [9]. The IND contains a
        trial protocol; protocols for additional studies within the same clinical trials program
        are submitted as amendments to the IND. Later, when the sponsor has completed its clinical
        trials program and wishes to apply for marketing approval, it submits its NDA.
        The FDA then begins the NDA review process, during which a physician, a statistician,
        and a pharmacologist, among others, generate lengthy review documents [10]. These reviews
        not only address the sponsor's analyses of the data on pivotal studies, but they often also
        include reanalyses by the reviewers using raw data obtained from the sponsor. These
        analyses are conducted in adherence to the statistical methods set forth a priori in the
        original trial protocols. (By contrast, with most journal publications, it is usually not
        possible for the reader to verify whether what is presented as the main finding is
        consistent with the original hypothesis or whether it was a post hoc finding.) After the
        primary reviewers have written their reviews, shorter reviews are written by their
        superiors, with the process culminating in a decision about whether to approve the drug for
        the proposed indication.
      
      
        A Semi-Public Database
        This process occurs entirely outside of the public domain. However, in the interest of
        making the FDA more “transparent,” and in accordance with the Electronic Freedom of
        Information Act [11], the FDA has, for the past several years, posted selected NDA reviews
        for approved drug–indication combinations on the FDA Web site Drugs@FDA
        (http://www.accessdata.fda.gov/scripts/cder/drugsatfda/index.cfm). These NDA review
        documents are much more detailed than the resulting package insert and often more detailed
        than corresponding journal publications.
        For example, while the clinical trials section of the package insert is typically a few
        paragraphs long, the efficacy portion of the clinical review usually runs tens of pages.
        Because the FDA is made aware of all studies that the sponsor plans to use in support of
        the NDA before they are conducted, and thus before there can be any selection based on
        outcome, these reviews cover not only studies that are positive (and more likely to be
        published in journals), but also studies whose outcome was negative or indeterminate. The
        sidebar gives an example of how NDA review documents at the FDA give valuable information
        about paroxetine for anxiety disorders.
      
      
        FDA Reviews for All Approved Drugs Should Be Made Public
        In the examples discussed in the sidebar, our having access to the FDA review documents
        allows us to become aware of, and see beyond, apparent publication bias. It is in the best
        interest of the public health for the FDA to make as many reviews available as possible.
        According to the FDA Web site, “As FDA continues to be one of the world's leading agencies
        in its emphasis on openness and transparency, it is aware that making even more information
        available to the public will further the Agency's mission to protect and promote public
        health and improve its credibility. For example, FDA has aggressively implemented the
        Electronic Freedom of Information Act…” [11].
        Unfortunately, the availability of review documents on Drugs@FDA is sporadic. To take
        additional examples from psychiatry, NDA reviews have been posted on Drugs@FDA for some
        approved drug–indication combinations, such as fluoxetine for pediatric depression, and
        aripiprazole and quetiapine for schizophrenia. However, NDA reviews for many other
        drug–indication combinations have not been posted: the Prozac Weekly formulation of
        fluoxetine, clozapine for suicidal behavior in patients with schizophrenia or
        schizoaffective disorder, and quetiapine for mania, among others. A review on paroxetine
        for pediatric depression, the subject of Elliot Spitzer's suit against GlaxoSmithKline, is
        not posted. This is probably because this drug–indication combination was not approved; in
        fact, it is possible that GlaxoSmithKline did not file an NDA to be reviewed. However, I do
        not understand why, in cases where NDAs were both submitted and approved, such as the ones
        listed above, some reviews are posted while others are not.
        I therefore suggest that we increase access to the clinical trials registry and results
        database that already exist within the FDA. The agency could expand its implementation of
        the Electronic Freedom of Information Act and make all NDA reviews, at least for approved
        NDAs, available in the public domain. The act is written into the FDA portion of the Code
        of Federal Regulations as follows: “The Food and Drug Administration will make the fullest
        possible disclosure of records to the public, consistent with the rights of individuals to
        privacy, the property rights of persons in trade secrets and confidential commercial or
        financial information” [12].
      
      
        Obstacles and Limitations
        There would surely be obstacles. The pharmaceutical industry would vigorously invoke
        Exemption 4 of the Freedom of Information Act, the exemption for trade secrets and
        confidential business information [13]. However, the FDA Freedom of Information Office
        already deals with confidential and proprietary information by redacting or editing it out
        of the review documents before making them available. Within the FDA's Freedom of
        Information Office, staffing would need to be greatly increased. Some oversight might be
        necessary to ensure that the taxpaying public has been granted the fullest possible access
        and that unwarranted redaction does not occur. Unless the Freedom of Information Act is
        modified, access would still likely be limited to approved NDAs. Data would remain
        unavailable for trials that did not lead to an approved NDA.
        It should be clarified that this resource does not compete with proposals by the AMA and
        other groups for clinical trial registries—rather, it complements them. The AMA has
        proposed the creation of a registry that is comprehensive in scope. The FDA's registry and
        results database are restricted to those trials aimed at supporting US marketing approval
        or a change in labeling in the US. While data from many studies conducted abroad are
        submitted to the FDA for this purpose, this is not the case for drugs for which the sponsor
        has elected not to seek approval for marketing in the US. Nor does the FDA review data from
        most trials funded by other US government agencies, such as the National Institutes of
        Health, or by foundations. And drug companies fund investigator-initiated trials that are
        often not registered with the FDA.
        To make the FDA review data more accessible and user-friendly, simple formatting changes
        would be needed. For those (few) reviews that are currently posted on Drugs@FDA, one can
        determine the indication being evaluated only after opening the document and paging through
        it. (Descriptive titles would be helpful, and these could be linked to ClinicalTrials.gov.
        Further, the trials reviewed could be identified with a unique international identifier, as
        promoted by the World Health Organization [14].) Despite the fact that the reviews are
        created in Microsoft Word and converted to PDF, the versions that appear on the Web site
        are no longer in a searchable text format. While the reviews tend to be well organized, the
        posted versions are difficult to navigate because there is no hyperlinked table of
        contents. In addition to having these formatting issues addressed, clinicians and patients
        might benefit from brief summaries, the writing of which might require the addition of new
        FDA staff.
      
      
        Conclusion
        Despite the limitations of the FDA's database, making it public is a strategy that could
        be implemented both rapidly and easily by building upon existing infrastructure. While we
        await the creation of a clinical trials registry and results database that is truly
        comprehensive, we already have at our disposal one that could serve as a trove of in-depth
        and unbiased information on many, if not most, drugs currently marketed in the US.
      
    
  

  
    
      
        
        There is a well-documented relationship between short sleep duration and high body mass
        index (BMI). In the largest study, a survey on sleep duration and frequency of insomnia in
        more than 1.1 million participants, increasing BMI occurred for habitual sleep amounts
        below 7–8 hours [1]. A recent prospective study found an association between sleep
        curtailment and future weight gain [2]. The mechanism linking short sleep with weight gain
        is unknown, but Mignot and colleagues' study in this month's 
        PLoS Medicine [3] adds to the growing evidence implicating leptin and
        ghrelin, the two key opposing hormones involved in appetite regulation.
      
      
        Hormones That Regulate Appetite
        Leptin, a peptide hormone secreted from white adipocytes, is implicated in the
        regulation of food intake and energy balance. The hormone acts on the central nervous
        system, in particular the hypothalamus, suppressing food intake and stimulating energy
        expenditure. Leptin production is primarily regulated by insulin-induced changes in
        adipocyte metabolism—its secretion levels correlate with adipocyte mass and lipid
        loads.
        Leptin promotes inflammation. The hormone provides an interesting link between obesity
        and pathophysiological processes such as insulin resistance and atherosclerosis, and
        disorders such as autoimmune and cardiovascular diseases and the metabolic syndrome.
        Increased serum leptin levels in obesity and metabolic syndrome support the view that these
        disorders are in fact low-grade systemic inflammatory diseases, characterized by increased
        concentrations of proinflammatory cytokines like interleukin-6, tumor necrosis factor-α and
        leptin. Leptin's proinflammatory role suggests that it may link energy homeostasis to the
        immune system [4,5].Ghrelin is a peptide hormone that stimulates appetite, fat production,
        and body growth—leading to increased food intake and body weight. It is secreted into the
        circulation from the stomach, but is also synthesised in a number of other tissues,
        including the kidney, pituitary, and hypothalamus, suggesting that the hormone has both
        distant and local (endocrine and paracrine) effects. These effects include stimulating the
        secretion of growth hormone, prolactin, and adrenocorticotropic hormone, and a diabetogenic
        effect on carbohydrate metabolism [6].
      
      
        The New Study
        In this study of 1,024 participants in the population-based Wisconsin Sleep Cohort Study
        [7], Mignot and colleagues found that in persons sleeping less than 8 hours, increased BMI
        was proportional to decreased sleep [3]. The researchers also found that shorter sleep
        times were associated with increased circulating ghrelin and decreased leptin, a hormonal
        pattern that is consistent with decreased energy expenditure and increased appetite and
        obesity.
        These findings confirm earlier clinical reports on the effects of sleep deprivation and
        extend them to include naturalistic sleep in a large, community-based population. The study
        provides an exciting addition to the growing literature showing relationships between sleep
        curtailment, metabolic hormones, and metabolic disorders (including obesity). The data have
        important implications for our understanding of obesity and related disorders in the
        general population, with one caveat: the study population was enriched with snorers, making
        the results less applicable to a general population.
        Mignot and colleagues' data are in accord with human and animal studies that show that
        experimental curtailment of sleep leads to lower levels of leptin [8,9,10,11] and increased
        ghrelin [12]. The new study therefore lends some support to the interpretation that reduced
        sleep levels cause the hormonal changes.
        But there is also evidence of opposite effects—that is, that administration of leptin
        [13] and ghrelin can alter sleep. Ghrelin administration has been found to increase non-REM
        sleep in humans and mice, possibly via its interactions with the sleep-inducing peptide
        growth hormone releasing hormone (GHRH). Ghrelin is an endogenous ligand of the growth
        hormone secretagogue receptor, making it a candidate for an endogenous sleep-promoting
        factor [14]. Mignot and colleagues' study is congruent with the idea that inadequate sleep
        enhances ghrelin secretion, which in turn acts as an endogenous sleep factor in humans.
        This is an important new area of research that could conceivably lead to more physiological
        sleep aids than are currently available, with profound implications for improved public
        health.
        Overall, the available studies suggest the presence of reciprocal interactions between
        metabolic hormones and sleep, relationships that are poorly understood at present. Does
        sleep interact with metabolic hormones directly or via intervening factors such as
        sleep-related breathing disorders? Patients with obstructive sleep apnea have impaired
        sleep and higher ghrelin levels than BMI-matched controls, and treatment with continuous
        positive airway pressure reduces ghrelin to control levels [15]. Although sleep-disordered
        breathing (SDB) was measured in the present study, the SDB analyses were not shown, making
        it difficult to evaluate the influence of SDB on ghrelin and leptin in this population.
        There is a clear need for well-controlled, population-based studies that allow us to
        examine multiple relevant factors simultaneously. The present study highlights the
        importance of shortened sleep in relation to obesity, leptin, and ghrelin, a good start
        toward this goal.
      
      
        Sleep and Public Health
        Many other important questions remain, such as the roles that other hormones, cytokines,
        and SDB play in obesity. Many of the unanswered questions have important implications for
        public health. For example, diabetes, visceral obesity, hypertension, and hyperinsulinemia
        commonly aggregate together in large populations, and are considered a “metabolic syndrome”
        that has been linked to SDB [16] and to inflammatory disorders [17]. To what extent does
        long-term sleep curtailment contribute to these and related public health issues?
        The possible role of sleep restriction in autoimmune and inflammatory disorders is of
        particular interest in light of recent findings linking immune function with ghrelin and
        leptin. Ghrelin and its receptor are expressed in human T-lymphocytes, where they can
        inhibit cytokine activation, including interleukins, tumor necrosis factor-α and leptin
        [18]. Conversely, leptin stimulates cytokine activation and immune-cell proliferation, an
        effect that predisposes to inflammatory conditions [4]. Is it possible, then, that
        sleep-related changes in leptin and ghrelin influence the development of metabolic and
        immune disorders? Can biologically restorative sleep reverse disease progression? Can
        biologically restorative sleep be defined on the basis of metabolic hormone responses?
        Future research may answer some of these and other questions, further elucidating the role
        of sleep in public health.
      
    
  

  
    
      
        Introduction
        In population studies, a dose-response relationship between short sleep duration and
        high body mass index (BMI) has been reported across all age groups [1–7]. In the largest
        studied sample, elevated BMI occurred for habitual sleep amounts below 7–8 h [2]. A
        U-shaped curvilinear relationship between sleep duration and BMI was observed for women,
        but for men, there was a monotonic trend towards higher BMI with shorter sleep duration.
        Importantly, a recent prospective study identified a longitudinal association between sleep
        curtailment and future weight gain [6]. How sleep curtailment may interact with body weight
        is unknown, but hormones regulating appetite and energy expenditure may be involved.
        A number of hormones may mediate the interactions between short sleep duration,
        metabolism, and high BMI. We hypothesized that the two key opposing hormones in appetite
        regulation, leptin and ghrelin [8,9], play a significant role in the interaction between
        short sleep duration and high BMI. Leptin is an adipocyte-derived hormone that suppresses
        appetite [10]. Ghrelin is predominantly a stomach-derived peptide that stimulates appetite
        [9,11]. Other mediators of metabolism that may contribute include adiponectin and insulin.
        Adiponectin is a novel hormone secreted by adipocytes and is associated with insulin
        sensitivity [12,13]. We investigated the associations among sleep duration (acute and
        habitual), metabolic hormones, and BMI in the population-based Wisconsin Sleep Cohort Study
        [14].
      
      
        Methods
        
          Overview
          The institutional review board of the University of Wisconsin Medical School approved
          all protocols for the study, and informed consent was obtained from all participants. The
          Wisconsin Sleep Cohort Study is an ongoing longitudinal study of sleep habits and
          disorders in the general population [14]. Briefly, to construct a defined sampling frame,
          all employees aged 30–60 y of four state agencies in south central Wisconsin were mailed
          a survey on sleep habits, health, and demographics in 1989. Mailed surveys were repeated
          at 5-y intervals. A stratified random sample of respondents was then recruited for an
          extensive overnight protocol including polysomnography at baseline. Stratification was
          based on risk for sleep-disordered breathing (SDB), with an oversampling of habitual
          snorers to ensure an adequate distribution of SDB. Analyses were adjusted for the
          weighted sampling when appropriate. Recruitment for baseline studies was staggered to
          conduct seven studies per week; study entry and follow-up time thus varied within the
          cohort. Exclusion criteria included pregnancy, unstable cardiopulmonary disease, airway
          cancers, and recent upper respiratory tract surgery. The baseline response rate was 51%,
          with most refusals due to the inconvenience of sleeping away from home. Follow-up studies
          have been conducted at 4-y intervals, with up to three follow-up studies to date.
          Collection of morning, fasted blood was added to the protocol in 1995. Extensive survey
          and other data available from the sampling frame have been used to evaluate the potential
          for response and drop out biases.
          Figure 1 provides an overview of the study population and the data collected. The
          sample comprised 1,024 participants with an overnight study and blood sample. A 6-d diary
          of sleep duration was added as part of a protocol to assess daytime sleepiness after the
          initiation of the cohort study; these data were available for 721 participants.
        
        
          Data Collection
          This investigation is based on Wisconsin Sleep Cohort Study data collected from the
          mailed sleep surveys, overnight studies, and 6-d sleep diaries. Overnight studies were
          conducted in laboratory bedrooms, with participants setting their own sleep and rise
          times. After informed consent was obtained, questionnaires on lifestyle and health
          history were administered, and height and weight measured. A blood sample was collected
          shortly after awakening from overnight polysomnography.
          
            Polysomnography
            An 18-channel polysomnographic system was used to assess sleep states, and
            respiratory and cardiac variables [14]. Sleep was studied using electroencephalography,
            electro-oculography, and chin electromyography (Grass Instruments, Quincy,
            Massachusetts, United States). Continuous measurement of arterial oxyhemoglobin
            saturation by pulse oximetry (Ohmeda, Englewood, Colorado, United States), oral and
            nasal airflow, nasal air pressure, and thoracic cage and abdominal respiratory motion
            (Respitrace Ambulatory Monitoring, Ardsley, New York, United States) were used to
            assess SDB [14]. Each 30-s interval of the polysomnographic record was scored for sleep
            stage and SDB using standard criteria [14]. The average number of apneas and hypopneas
            per hour of measured sleep, the apnea-hypopnea index (AHI), was the measure for SDB.
            For analyses including AHI as a covariate, participants were excluded if they had sleep
            studies with less than 4 h of usable polysomnography, or if they were receiving
            treatment for SDB.
          
          
            Polysomnographic measures of acute sleep
            Polysomnographic measures of sleep duration just prior to blood sampling were used
            to evaluate degree of “acute sleep restriction.” “Total sleep time” was total hours of
            polysomnographically defined sleep. “Wake after sleep onset” (WASO) was hours of wake
            time after three epochs of sleep had occurred. “Sleep efficiency” was total sleep time
            divided by time from lights out until arising in the morning.
          
          
            Self-reported sleep measures of chronic sleep
            Two variables were used to evaluate degree of “chronic sleep restriction” by
            estimating average nightly sleep duration: (i) “usual sleep” (from questionnaires) and
            (ii) “average nightly sleep” (from sleep diaries).
          
          
            Questionnaire
            Usual sleep was estimated from the following questions: how many hours of sleep do
            you usually get in (a) a workday night? (b) a weekend or non-work night? These
            questions were included in all mailed surveys and were added to questionnaires
            completed at the overnight study in 1998. For participants studied after 1998, data
            from questionnaires administered at the overnight study were used (58%); for the
            remainder of the sample, data from the mailed survey closest in time to the overnight
            study were used. Usual sleep was calculated as (5 × workday sleep + 2 × weekend
            sleep)/7.
          
          
            Sleep diary
            Average sleep duration was also estimated using a 6-d sleep diary, kept by 721
            participants as part of an added protocol to measure daytime sleepiness. The median
            time between the blood collection and completion of the diary was 18 d. Almost all
            diaries (97%) were completed within 6 mo of blood sampling. In diaries, participants
            recorded the time they went to bed and arose each day, and the duration of any naps.
            “Average nightly sleep” was calculated as the sum of the hours between bedtime and
            arising divided by six. “Average nightly sleep plus naps” added naps to the above
            sum.
          
        
        
          The Relationship between BMI and Sleep Duration
          For the analysis of the association of BMI and sleep duration, the sample comprised
          1,040 participants with at least one 6-d sleep diary. Of these, 4-, 8-, and 12-y
          follow-up studies had been completed by 397, 179, and 11 participants respectively (1,828
          visits), providing repeated measures data for greater analytic efficiency and
          precision.
        
        
          Hormone Assays
          Following overnight fasting, serum was collected soon after awakening and stored at
          −70 °C. All samples were assayed in duplicate. It was not possible to assay samples from
          all participants in all assays because of the volume of serum available; this
          particularly affected the ghrelin assay, which required the most volume. Leptin and
          insulin were determined using enzyme-linked immunoassays (ELISA; Linco Research, St.
          Charles, Missouri, United States). Total ghrelin and adiponectin were measured by
          radioimmunoassay (Linco Research). Sensitivity for the leptin and insulin enzyme-linked
          immunoassays was 0.5 ng/ml and 2 μU/ml, respectively. Sensitivity for the ghrelin and
          adiponectin radioimmunoassays was 10 pg/ml and 1 ng/ml, respectively. Intra- and
          inter-assay variations were all less than 5% and less than 10%, respectively. The
          quantitative insulin sensitivity check index (QUICKI) was 1/(log (
          I ) + log (
          G )), where 
          I is fasting insulin and 
          G is fasting glucose [15,16].
        
        
          Statistical Analysis
          All analyses were cross-sectional and performed using SAS/STAT 8.2. Leptin, ghrelin,
          and adiponectin were square-root transformed and insulin log transformed based on the
          distribution of residuals from the multivariate regression models. We evaluated the
          relationship of age, sex, BMI, and time of storage of blood sample on hormones using
          multiple regression. Partial correlations adjusted for age, sex, and BMI were calculated
          for hormones and QUICKI, with and without control of other potential confounders. The
          relationships between hormones and sleep were evaluated using multiple linear regression
          after control for potential confounders including age, sex, BMI, SDB, and morningness
          tendencies (as measured using the Horne-Ostberg questionnaire, an indirect surrogate of
          earlier rising time). In all analyses involving insulin, glucose, and QUICKI (but not
          leptin, ghrelin, and adiponectin), participants with diabetes (self-reported diagnosis,
          or currently taking insulin or diabetic medications, or with glucose >300 mg/dl) were
          excluded. Participants with SDB were not removed from the analyses shown. When
          controlling for AHI in models, participants who used continuous positive airway pressure
          or who had inadequate sleep were excluded. Because controlling for AHI did not
          significantly change the sleep-hormone regression coefficients, these analyses are not
          shown. The relationship of BMI with average nightly sleep was evaluated using a quadratic
          fit. This was examined using multiple visits (
          n = 1,828) from 1,040 participants with sleep diary data available.
          Mixed modeling techniques were used to account for within-subject correlation for
          participants with multiple visits. SAS procedure mixed was used for modeling and
          hypothesis testing using robust standard errors and a compound symmetric within-subject
          correlation structure. All reported 
          p values are two-sided. For illustrative purposes, changes in leptin,
          ghrelin, and BMI for different sleep amounts were calculated at the average values and
          sex distribution of the relevant sample.
        
      
      
        Results
        Table 1 shows the characteristics of the sample, unadjusted for the weighted sampling
        scheme. Figure 2 shows the mean BMI for 45-min intervals of average nightly sleep after
        adjustment for age and sex. We found a significant U-shaped curvilinear relationship
        between average nightly sleep and BMI after adjustment for age and sex (average nightly
        sleep coefficient = −2.40, 
        p = 0.008; (average nightly sleep)
        2 coefficient = 0.156, 
        p = 0.008; the two coefficients define a curve). The minimum BMI was
        predicted at 7.7 h of average nightly sleep. The most striking portion of the curve was for
        persons sleeping less than 8 h (74.4% of the sample), where increased BMI was proportional
        to decreased sleep. An increase in BMI from 31.3 to 32.4 (+3.6%) corresponded approximately
        to an average nightly sleep duration decrease from 8 h to 5 h, as estimated at the mean age
        (53.1 y) and sex distribution (54.4% male) of the sample with available sleep diary
        data.
        Table 2 shows the association of each of the hormones, glucose, and QUICKI with age,
        sex, and BMI. Serum ghrelin, leptin, adiponectin, insulin, and glucose were significantly
        correlated with BMI and sex. Storage time had significant effects on some but not all
        variables; in all cases, however, effect size was small, and the effect was corrected for
        in all calculations. Adiponectin and glucose were correlated with age. Table 3 shows
        partial correlations among the measured and calculated variables, adjusted for age, sex,
        and BMI. All correlations agree with previous studies and validate our assays and
        population sample. We also examined several potential confounders to be later controlled
        for, if needed, in our models. Identified relationships included: ghrelin with high-density
        lipoprotein (HDL), alcohol intake, and creatinine; leptin with diastolic blood pressure,
        smoking, and blood urea nitrogen (BUN); adiponectin with HDL, triglycerides, uric acid, and
        BUN; insulin with HDL, triglycerides, uric acid, and smoking; glucose with HDL,
        triglycerides, uric acid, alcohol intake, BUN, and creatinine; QUICKI with HDL,
        triglycerides, uric acid, smoking, and BUN. When diabetics (diagnosed) and participants
        with high glucose (glucose >300 mg/dl) were removed from this analysis, all
        relationships remained significant except for the correlation between ghrelin and
        adiponectin, and ghrelin and insulin.
        Using polysomnography to measure objective sleep immediately prior to blood sampling,
        ghrelin correlated significantly with total sleep time, sleep efficiency, and WASO (Table
        4). Using questionnaire and diary data estimating chronic sleep, significant correlations
        were found between leptin and average nightly sleep (with and without naps) and usual sleep
        amounts (Table 4). A significant correlation was also observed between ghrelin and average
        nightly sleep plus naps. These relationships were consistently found when other possible
        confounding factors such as medications, hypertension, AHI, and factors listed above were
        included in the statistical model (analysis not shown). In unadjusted models, leptin was
        significantly correlated with total sleep time and average weekly sleep with naps, and
        ghrelin was significantly correlated with sleep efficiency and WASO. Figure 3A shows the
        mean leptin levels for half-hour increments of average nightly sleep after adjustment for
        age, sex, BMI, and time of storage (see Table 2). In the multiple regression model (see
        Table 4), there was a significant increasing trend in leptin for average nightly sleep
        duration (
        p = 0.01). When evaluated at the average values and sex distribution of
        our sample, a decrease from 8 to 5 h in average nightly sleep was associated with a
        predicted 15.5% decrease in leptin. Figure 3B shows the mean ghrelin levels for half-hour
        increments of total sleep time after adjustment for age, sex, BMI, and time of storage (see
        Table 2). In the multiple regression model (see Table 4), there was a significant
        decreasing trend in ghrelin with total sleep time (
        p = 0.008). When evaluated at the average values and sex distribution of
        our sample, a decrease from 8 to 5 h of polysomnographically defined total sleep time was
        associated with a predicted 14.9% increase in ghrelin. There was no significant correlation
        between sleep duration (acute or chronic) and serum adiponectin, insulin, glucose, or
        QUICKI. Results of our analyses were unchanged after adjusting for the weighted sampling
        scheme.
      
      
        Discussion
        We found that habitual sleep duration below 7.7 h was associated with increased BMI,
        similar to findings in other studies including children [1,17], adolescents [5], and adults
        [2,3]. We also report a significant association of sleep duration with leptin and ghrelin
        that is independent of BMI, age, sex, SDB, and other possible confounding factors (analysis
        not shown for SDB and other confounders). Short sleep duration was associated with
        decreased leptin and increased ghrelin, changes that have also been observed in reaction to
        food restriction and weight loss and are typically associated with increased appetite.
        These hormone alterations may contribute to the BMI increase that occurs with sleep
        curtailment.
        Previous studies have shown that both acute sleep deprivation [18] and chronic partial
        sleep deprivation (sleep restriction) [19] can cause a decrease in serum leptin
        concentrations. These studies, however, were performed under highly controlled laboratory
        circumstances. Our results validate the association of decreased leptin with decreased
        sleep time in a large sample of adults under real-life conditions and, now, indicate a role
        for ghrelin. Leptin deficiency increases appetite and produces obesity [8,20]. Leptin
        administration suppresses food intake and reduces energy expenditure [21,22]. Importantly,
        low leptin as observed with sleep loss has a greater impact on appetite than high leptin
        levels, which are associated with leptin resistance, as occurs with obesity [8].
        Levels of ghrelin, a potent stimulator of appetite [23,24,25], were higher in those with
        shorter sleep. Ghrelin levels are also positively associated with hunger ratings [26], but
        decrease with increased BMI (see Table 2). In one study, after 3 mo of dietary supervision,
        a reduction in BMI of approximately 5% was associated with a 12% increase in ghrelin and a
        15% decrease in leptin [27]. These changes, in participants of similar BMI to our sample
        and presumably producing increased appetite, are comparable to those observed with sleep
        loss of 2–3 h/night. With sleep loss, however, relatively high ghrelin and low leptin
        levels are associated with increased BMI. These changes can be hypothesized to play a
        contributory, rather than compensatory, role in the development of overweight and obesity
        with sleep restriction.
        Our findings are strengthened by the large and well-characterized population-based
        sample, attention to bias and confounding factors, and in-laboratory polysomnographic data.
        The changes in hormones with sleep duration were consistent and of significant magnitude.
        They also represent the first demonstration of a correlation between peripheral hormone
        levels and both self-reported (questionnaire and diary data) and polysomnographically
        determined sleep amounts in a general population sample. While these data are more
        comprehensive than previous studies on this topic, some misclassification error may exist
        because of intra-person variability or limitations of polysomnographic measurement. Little
        is known about the stability of self-reported sleep duration and polysomnographic measures
        of sleep duration over time. We examined the stability of the self-reported sleep duration
        data, and found these measures to be stable. For 860 participants who completed three
        surveys, the mean (standard deviation) of intra-person differences in usual sleep for two
        5-y periods was 0.10 (0.47) h. For 190 participants with at least three sleep diaries, the
        mean (standard deviation) of intra-person differences in average nightly sleep for two 4-y
        intervals was 0.09 (0.41) h. Furthermore, the subjectively reported hours of usual sleep
        and the diary-derived average nightly sleep values were highly correlated (
        r = 0.55, 
        p <0.001). One-night polysomnographically defined total sleep time
        had a similar intra-person mean difference (0.10 h), with a somewhat larger standard
        deviation (0.68) for 713 participants with at least three sleep studies.
        Elevated ghrelin mainly correlated with acute sleep loss as measured by polysomnography
        immediately prior to blood sampling (see Table 4; Figure 3B), while reduced leptin
        correlated with chronic sleep restriction indicated by self-reported sleep measures (see
        Table 4; Figure 3A). Measures of usual and one-night polysomnographically defined sleep
        time were only weakly, but statistically significantly, correlated (
        r = 0.12, 
        p <0.001), supporting the concept that these measures reflect
        long-term and short-term changes in sleep amounts, respectively. Our findings are in
        agreement with the current view that leptin is important in signaling long-term nutritional
        status while ghrelin has a more significant role in acute hunger. The changes in leptin and
        ghrelin with sleep restriction could, therefore, provide a powerful dual stimulus to food
        intake that may culminate in obesity.
        Longitudinal and intervention studies will be necessary to define further the link
        between sleep curtailment and increased BMI. Only total ghrelin was measured, since active
        octanoylated ghrelin is unstable. Although both total and active ghrelin appear to be
        regulated in a similar and parallel manner, future studies will need to focus on
        measurement of the biologically active form. Other potentially important appetite
        regulatory hormones, such as PYY 3–36 [28], were not measured. Measures of appetite were
        not included in the Wisconsin Sleep Cohort Study overnight protocol; therefore, a direct
        examination of the relationship between the observed hormone changes with sleep duration
        and alterations in appetite was not possible.
        Hormone measurements were all performed on a single fasted, morning sample and may not
        reflect the 24-h profile. It is possible that participants with shorter sleep woke up
        earlier and that hormone differences may be partially related to circadian time. Leptin and
        ghrelin levels rise slightly during the night [29], and this could result in higher hormone
        levels in short sleepers. This may be an issue for ghrelin, as levels increased with acute
        sleep restriction. It is, however, unlikely to play a role in the leptin finding, since
        lower levels were found with chronic but not acute sleep restriction. Additionally, studies
        have shown a high correlation between morning, fasting leptin and ghrelin levels and 24-h
        mean profile [29,30]. We also found that the ghrelin and leptin changes were unaffected by
        morningness tendencies. The fact that studies investigating the diurnal profile of these
        hormones found similar hormonal changes over the entire 24-h period after experimental
        sleep restriction also corroborates our results [18,19]. The robustness of our findings and
        similar observations from smaller controlled studies [18,19] also suggest that our
        statistically significant results are unlikely to be a reflection of the number of analyses
        carried out.
        Animal studies have suggested a link between sleep and metabolism [31,32]. In rats,
        prolonged, complete sleep deprivation increased both food intake and energy expenditure.
        The net effect was weight loss and, ultimately, death [33]. Rats fed a high
        protein-to-calorie diet had accelerated weight loss, compared to sleep-deprived rats fed
        calorie-augmented (fatty) diets [32]. Food consumption remained normal in sleep-deprived
        rats fed protein-rich diets, but increased 250% in rats fed calorie-rich diets. Preference
        for fatty foods has also been reported anecdotally in sleep-deprived humans [32,34]. Sleep
        deprivation may thus increase not only appetite but also preference for lipid-rich,
        high-calorie foods. Animal experiments that have found weight loss after prolonged sleep
        deprivation have to be interpreted in the context of a stressful procedure producing
        intense sleep debt [35,36], which may interfere with adequate food intake. From our study,
        we hypothesize that the moderate chronic sleep debt associated with habitual short sleep is
        associated with increased appetite and energy expenditure. In societies where high-calorie
        food is freely available and consumption uncontrolled, after milder chronic sleep
        restriction, the equation may be tipped towards food intake for high-calorie food rather
        than expenditure, culminating in obesity. Short sleepers may also have more time to
        overeat.
        Sleep loss from a baseline of 7.7 h was associated with a dose-dependent increase in
        BMI. This was the predominant effect in a population increasingly curtailing sleep [37].
        Sleep greater than 7.7 h, however, was also associated with increased BMI. Patients with
        SDB (a pathology associated with increased BMI) may spend a longer time in bed to
        compensate for fragmented sleep; however, controlling for AHI did not change the
        curvilinear BMI–sleep association. Another possibility is that in long sleepers, reduced
        energy expenditure due to increased time in bed has a greater impact than reduced food
        intake. In favor of this hypothesis, long sleepers exercise less [38]. In our data, we
        found that the odds ratio of high levels of self-reported exercise (>7 h/wk), based on a
        single survey question, decreased with increased sleep time, but controlling for this
        variable also did not change our findings (analyses not shown).
        Insulin resistance with sleep deprivation has been reported in a laboratory study of
        young, healthy volunteers [39]. When controlling for BMI, we found no significant
        correlation between insulin, glucose, or adiponectin levels and various measures of sleep
        duration. Also, there was no significant correlation between QUICKI (or the homeostatic
        model assessment HOMA [16]; data not shown) and sleep duration. This may be due to
        difficulties in detecting small effects on glucose tolerance under less-controlled
        conditions of large population studies.
        Our results demonstrate an important relationship between sleep and metabolic hormones.
        The direct effect of chronic partial sleep loss on food intake, energy expenditure, and
        obesity now needs to be explored. Altering sleep duration may prove to be an important
        adjunct to preventing and treating obesity.
      
    
  

  
    
      
        Introduction
        Antiretroviral therapy (ART) has been a milestone in the treatment of HIV infection.
        Current treatment guidelines for HIV-1 infection in the United States recommend the
        initiation of ART in patients with CD4 T cell counts of less than 350 cells/μl [1]. In
        implementing these guidelines, health-care providers face the ongoing challenge of
        developing treatment strategies that minimize drug-related toxicity and adverse effects
        while retaining effective control of viral replication. Furthermore, treatment-associated
        costs (particularly in resource-poor areas), difficulty in maintaining long-term optimal
        adherence [2], and the emergence of viral resistance [3,4,5] have limited the feasibility
        of life-long ART-mediated viral suppression, increasing the need for alternative treatment
        strategies. Intermittent therapy strategies, consisting of alternating cycles on and off
        ART, have increasingly emerged as a potential intervention to address limitations of
        continuous ART [6,7,8,9]. Therapy interruption (TI) studies in ART-treated patients with
        suppressed HIV infection [10] have addressed the general questions as to whether such
        strategies can achieve greater viral control through increased antiviral responses
        (autoimmunization hypothesis) or simply serve as a strategy to reduce cost of long-term
        therapy and drug-associated toxicity. While pilot studies and uncontrolled (or incomplete)
        trials in patients with chronic HIV infection have addressed viral and immune outcomes of
        fixed-length TI and fixed on-drug cycles [11,12,13,14,15,16], no completed randomized,
        controlled trial has yet addressed by intent-to-treat analysis the outcome during an
        open-ended TI of sequential TIs versus continuous treatment in patients with confirmed
        suppression. The largest study to date in this area is the prospective single-arm
        Swiss–Spanish Intermittent Trial (SSITT) conducted in 133 recruited patients undergoing
        sequential 2-wk TIs and showing a lack of impact of this strategy on achieving sustained
        viral loads of less than 5,000 copies/ml off therapy in those that completed the study
        [11]. However, the lack of a control arm in this study has left unanswered questions about
        the impact of multiple TIs on time to rebound, immune reconstitution, therapy failure, and
        viral resistance when analyzed against a randomized control arm of continuous treatment
        followed for equal time before a single open-ended interruption.
        We completed a randomized, controlled trial on the outcome of repeated 2- to 6-wk TIs in
        patients with chronic infection in which the comparator group maintained continuous therapy
        and then an open-ended interruption period was applied in both treatment groups. The study
        addressed the potential for repeated interruptions of therapy to delay time to viral
        rebound as a primary outcome and analyzed secondary outcomes regarding study-defined safety
        criteria, viral suppression and resistance, and retention of immune reconstitution.
      
      
        Methods
        
          Participants
          Between August 2000 and December 2003, we enrolled 42 patients infected with HIV who
          were older than 18 y and on ART; eligibility criteria included CD4 counts of greater than
          400 cells/μl on ART with a nadir of no less than 100 cells/μl, ART-mediated suppression
          (< 500 copies/ml) for more than 6 mo and less than 50 copies/ml at recruitment on any
          antiretroviral regimen. Approval of the study protocol was obtained from the
          institutional review board (IRB) of the Philadelphia Field Initiating Group for HIV
          Trials (Philadelphia, Pennsylvania, United States). Written informed consent was obtained
          from all patients. Human experimentation guidelines of the United States Department of
          Health and Human Services and of the authors' institutions were followed. The study
          protocol, including the patient consent form, the CONSORT form, and the IRB approval, can
          be found in Protocols S1–S4.
        
        
          Randomization and Study Design
          Forty-two eligible patients from the Jonathan Lax Immune Disorder Clinic in
          Philadelphia, Pennsylvania, were randomized via sealed envelopes in a 1:1 fashion to a
          first phase (phase I) of either (1) three successive TIs of 2, 4, and 6 wk, respectively,
          or (2) maintenance of ART for 40 wk before a final interruption of therapy in both arms
          (phase II) subject to therapy reinitiation criteria as described below. Phase II
          consisted of an open-ended interruption to allow for virological and immunological
          comparisons between the groups off therapy. Study visits were every 2 wk for the repeated
          interruptions group and every 4 wk for the continuous ART group during phase I. Both
          groups were followed every 2 wk during phase II. We followed a study design with
          step-wise increases in the length of TI cycles to address potential safety concerns
          (resuppression was confirmed after shorter TIs before longer interruptions were
          initiated) and the hypothesis that sequential viral replication intervals would stimulate
          viral control and a delay in time to viral rebound.
          Phase I procedures for the repeated interruptions group included the following. (1)
          Interruption of therapy was individually timed to occur after two HIV RNA measurements of
          less than 50 copies/ml without any viral load measurements greater than 400 copies/ml in
          between; these interruptions increased from 2 to 4 to 6 wk sequentially. (2) If a 0.5-log
          or greater reduction in viral load did not occur by 6 wk of reinitiated therapy or less
          than 50 copies/ml was not achieved within 20 wk of reinitiated therapy, patients were
          withdrawn as therapy failures and a resistance test was performed. (3) Patients were also
          withdrawn as therapy failures if (a) the CD4 cell number declined by more than 45% of the
          baseline CD4 count, (b) participants developed an opportunistic infection, even if
          retaining required CD4 count levels, or (c) a viral load of greater than 500,000
          copies/ml occurred once, with or without development of acute retroviral syndrome as
          defined by fever, skin lesions, and pharyngitis.
          Phase I procedures for the continuous therapy arm included the following: (1) patient
          monitoring if detected viremia was between 50 and 999 copies/ml, with the patient
          withdrawn if their viral load did not return to less than 50 copies/ml immediately prior
          to phase II, and (2) patient study withdrawal as therapy failure if during the 40-wk ART
          period viral load rebounded to more than 1,000 copies/ml at two consecutive time
          points.
          Phase II procedures for both arms included the following: (1) monitoring for patient
          study withdrawal criteria as described in phase I, (2) determining time to primary end
          point of a viral load greater than 5,000 copies/ml, (3) monitoring until the time of
          therapy reinitiation at a viral load greater than 30,000 copies/ml for three consecutive
          time points, and (4) after reinitiation of therapy, follow-up on therapy to confirm
          resuppression to less than 50 copies/ml at 6, 10, and 14 wk on therapy. Clinical and
          laboratory parameters (CD4 count and viral load) were monitored at each visit, and venous
          blood was collected for additional secondary outcomes during selected study visits.
          In both phase I and II, participants taking non-nucleoside reverse-transcriptase
          inhibitors (NNRTIs) were instructed to stop them a day earlier than the remaining drugs
          in the regimen.
        
        
          Primary and Secondary Outcomes
          The primary outcome was time to confirmed virological rebound during phase II. Rebound
          was defined as first time point with greater than 5,000 copies/ml. Viral replication
          magnitude as defined by mean HIV-1 plasma RNA area under the curve (AUC
          HIV RNA ) was measured as a secondary outcome at weeks 12 and 20 of
          phase II based on reinitiation-of-therapy criteria outlined above.
          Additional secondary outcomes included (1) safety outcomes (serious adverse events
          [SAEs] and patient withdrawal based on criteria defined above), (2) retention of
          ART-mediated immune reconstitution, and (3) detection of viral resistance. Retention of
          immune reconstitution was analyzed by (1) same-day whole blood flow-cytometry-based
          analysis of CD4 and CD8 T cells, including total and naïve (CD62 l/CD45RA) and memory
          (CD45RO) subsets as described [17], and (2) same-day recall response analysis of
          peripheral blood mononuclear cell lymphoproliferative responses to 
          Candida albicans as described [17]. Viral resistance mutations
          were retrospectively analyzed on cryopreserved plasma samples by genotyping of first
          available sample with viral load greater than 100 copies/ml following each interruption
          using the TruGene Assay (Visible Genetics, Toronto, Canada) at the Gladstone Institute of
          Virology and Immunology (San Francisco, California, United States) as previously
          described [18,19].
        
        
          Sample Size
          The sample size required was calculated using PS [20] software, and based on a type I
          error of 0.05, with 90% power, to detect a difference of 4 wk or more in time to viral
          rebound between arms. Eighteen patients per group resulted in sufficient power (18 for
          90%, 13 for 80%) to determine a difference of 4 wk or greater between groups in time to
          rebound of virus during the open-ended interruption. Assuming a loss to follow-up of 15%,
          we targeted 21 patients per group, or 42 total.
        
        
          Statistical Analysis
          The primary analysis was an intent-to-treat analysis in which dropouts were assigned a
          week 0 rebound time (e.g., maximum failure to delay rebound). In secondary analyses,
          these dropouts were excluded. The log-rank test was used to test the null hypothesis of
          no difference between arms in the number of weeks from initiation of the open-ended TI to
          reaching viral rebound as defined. Patients not reaching end point at 26 wk after the
          beginning of the open-ended TI were censored. Wilcoxon rank sum tests were used to
          compare baseline and week 0 of the open-ended interruption between groups. Wilcoxon
          signed rank tests were used to test for no change from baseline to week 0 of phase II.
          Finally, Wilcoxon rank sum tests were employed to test between groups for equality of the
          mean AUC
          HIV RNA up to 12 and 20 wk. In all cases, a two-sided alpha level of
          0.05 was used to define statistical significance. Unless otherwise stated, results are
          presented as median (interquartile range) in text and tables.
        
      
      
        Results
        
          Patient Flow and Discontinuations
          Trial patient flow is summarized in Figure 1. Between August 2000 and December 2003,
          42 patients at the Jonathan Lax Immune Disorder Clinic at the Philadelphia Field
          Initiating Group for HIV Trials were enrolled, randomized, and followed as shown in
          Figure 2. In the continuous therapy/single interruption arm, 16 of 21 patients reached
          the open-ended interruption. Reasons for study discontinuation in this arm were loss to
          follow-up (
          n = 1; patient moved away) and virological failure during continuous
          therapy (
          n = 4; further discussed below). In the repeated interruptions arm, 18
          of 21 patients reached the open-ended interruption following three TIs of 2, 4, and 6 wk
          duration, with median peak rises in viral loads of 136 (50–2,590), 13,651 (180–222,589),
          and 18,887 (3,893–96,101) copies/ml, respectively. Median time to less than 50 copies/ml
          after resumption of therapy was 2 (0–4), 3 (1.8–12), and 9.5 (2–12) wk, respectively,
          with 9, 18, and 20 wk as the maximum time needed to achieve suppression in 100% of
          patients before reaching the open-ended interruption. Study discontinuation in the
          repeated interruptions arm was due to protocol violation (
          n = 1; patient restarted therapy during interruption out of protocol),
          loss to follow-up (
          n = 1; patient imprisoned), and virological failure during on-therapy
          period (
          n = 1; further discussed below).
        
        
          Baseline Criteria and Follow-Up
          The demographic and clinical characteristics of the two groups at baseline are
          summarized in Table 1. Seventy-five percent of participants were on their second to
          fourth regimen while 25% were in their first regimen
          . No significant difference was found in baseline parameters between
          arms, with 33%–47% of patients on protease-inhibitor-containing and 61%–71% on
          NNRTI-containing regimens. Owing to the high participation of patients on NNRTI-based
          regimens and concerns about TI and safety in general, patient outcomes and treatment
          failure were reviewed monthly by the IRB of this study during the first 8 mo of study,
          quarterly for the following 4 mo, and semi-annually thereafter. Figure 2 shows study
          design for both arms, with a median follow-up of 41 (41–42) wk during phase I for the
          continuous therapy/single interruption arm and 42 (30–51) wk for the repeated
          interruptions arm. Follow-up during phase II had a median duration of 27 wk in both arms
          (continuous therapy/single interruption arm, 27 [8.75–47]; repeated interruptions arm, 27
          [16.5–35]). Following reinitiation of therapy after phase II, patients suppressed viral
          replication to less than 50 copies/ml by a median time of 10 (6–12) wk in both arms,
          excluding for two patients in the continuous therapy/single interruption arm who elected
          to stay off ART indefinitely and one patient from the repeated interruptions arm who
          reported nonadherence following regimen reinitiation yet reached 52 copies/ml before
          withdrawing from additional follow-up.
        
        
          Primary Outcome
          An intent-to-treat analysis of the time to viral rebound (>5,000 copies/ml) in the
          open-ended interruption showed no difference between groups (continuous therapy/single
          TI, median = 4 [1–8] wk, 
          n = 21; repeated TI, median = 5 [4–8] wk, 
          n = 21; 
          p = 0.36). Figure 3 (top panel) shows the probability of plasma HIV-1
          RNA remaining less than 5,000 copies/ml for the two groups (
          n = 21 per group). Exclusion of drop-outs in an as-treated analysis did
          not alter conclusions (single TI, median = 5 [4–9] wk, 
          n = 18; repeated TI, median = 6 [4–8] wk, 
          n = 16; 
          p > 0.05). Additional secondary analysis of the magnitude of viral
          load as shown in Figure 3 (second panel) showed similar viral replication as determined
          by mean AUC
          HIV RNA analysis at week 12 (single TI, median = 124,621
          [23,326–262,348] AUC
          HIV RNA ; repeated TI, median = 100,400 [47,221–365,731] AUC
          HIV RNA ; 
          p > 0.05) or week 20 (single TI, median = 114,550 [31,829–362,628]
          AUC
          HIV RNA ; repeated TI, median = 153,097 [67,427–515,421] AUC
          HIV RNA ; 
          p > 0.05).
        
        
          Secondary Outcomes SAEs and patient discontinuation
          No patient discontinuation in either group was due to study-defined changes in CD4
          cell count (reviewed further below) or due to study-associated SAEs (disease progression
          or acute retroviral syndrome). However, four non-study-related SAEs occurred: two
          patients from the continuous therapy/single interruption arm were hospitalized, one for a
          cholecystectomy and one for acute rectal bleeding, during the 40-wk ART period; a patient
          from the repeated interruptions arm died of liver cancer during week 26 of the open-ended
          interruption after previously reaching a viral load greater than 5,000 copies/ml yet
          electing to stay off ART; and a patient from the repeated interruptions arm developed a
          transient ileitis.
        
        
          Immune reconstitution
          No significant difference was observed between groups in CD4 T cell counts at the
          start of phase II, as illustrated in Figure 4. In addition, no difference in the
          percentage of naïve CD4 cells or decrease of recall response to 
          C. albicans was observed, confirming the absence of
          significant differences in the retention of baseline immune reconstitution correlates
          between arms. However, a significant decrease in the abundance of CD4 cells relative to
          other T cell types as summarized in CD4% (but not in absolute CD4 count ) was present in
          the repeated TI arm, corresponding to a significant increase in CD8 T cell count. In
          spite of fluctuations in CD4 T cell count levels between the start and end of each
          monitored TI, a recovery of CD4 count levels was achieved upon resuppression following
          each TI in conjunction with a retention of lymphoproliferative responses against 
          C. albicans before, during, and after each TI, as illustrated
          in Figure 5.
        
        
          Viral resistance mutations and therapy failure
          An intent-to-treat analysis of the combined number of patients per arm with detected
          resistance mutations irrespective of therapy failure in phase I and during the final TI
          in phase II showed no significant difference between arms (continuous therapy/single TI,
          7/21; repeated TI, 10/21; 
          p > 0.05).
          Study-defined criteria for therapy failure of a previously suppressive regimen were
          met by 4/21 patients in the continuous therapy/single interruption arm (patients S37,
          S47, S52, and S59) in association with self-reported nonadherence to therapy and
          detection of resistance mutations in phase I, as listed in Table 2. One patient in the
          repeated interruptions arm (1/21; patient S56) failed therapy after 20 wk following the
          third TI by maintaining a viral load between 50 and 999 copies/ml in the presence of
          previously undetected resistance mutations.
          In patients who reached phase II in the absence of therapy failure, a total of 12
          patients were identified to have resistance mutations at the first viremic time point
          (continuous therapy/single TI, 3/16; repeated TI, 9/18; 
          p = 0.06). A greater number of resistance mutations was detected in the
          repeated interruption arm, as summarized in Table 3. In ten out of these 12 patients, a
          change in resistance patterns was observed when comparing the first viremic time point to
          the last. All 11 of 12 patients in Table 3 who reinitiated therapy retained suppressive
          ability of their respective regimens, as did all other patients who did not show
          resistance mutations in phase II. In the repeated interruptions arm, analysis of newly
          detected resistance mutations in phase II, as defined by a lack of detection during
          viremic time points in phase I, identified 3/18 patients (patients S4, S22, and S43) with
          this pattern (see notations in Table 3).
        
      
      
        Discussion
        Earlier reports on TI strategies in patients with chronic HIV infection include multiple
        pilot or single-arm study designs centered on the effects on viral control by comparison
        with pre-therapy periods, detection of resistance mutations without parallel follow-up of a
        continuously treated arm, and inclusion of variable criteria regarding viral resuppression
        before proceeding with repeated TIs [11,12,14,16]. In contrast, our strategy mandated
        resuppression of viral replication to less than 50 copies/ml before each TI and presents
        the first comparison of viral replication during a final open-ended interruption of therapy
        between patients randomized to complete three sequential TIs or stay under continuous
        therapy. Our data, based on intent-to-treat analysis, did not show that repeated TIs
        resulted in a clinically significant virological benefit as measured by the time to viral
        rebound to more than 5,000 copies/ml (see Figure 3). Secondary as-treated analysis on viral
        replication magnitude also indicated a lack of difference between arms. Consistent with the
        findings of SSITT [11], analysis of our data by the categorical classification of a
        “responder” as a patient with viral load less than 5,000 copies/ml at week 12 off therapy
        showed no significant difference in this frequency between arms (single TI, 5/18; repeated
        TI, 5/16), suggesting the presence of “responders” irrespective of previous
        protocol-mandated TIs.
        Based on secondary outcome measures, the incidence of adverse events (SAEs, therapy
        failure, and patient discontinuation) or clinical disease progression (as indicated by CD4
        count on therapy or opportunistic infections) was not observed to be different between
        arms. Prospective safety outcomes in our study are in accordance with reports from a
        retrospective analysis of 1,290 patients who interrupted treatment at least once (< 3
        mo) without an increased risk of HIV-associated morbidity or mortality (with the exception
        of patients in Center for Disease Control and Prevention stage C during first interruption
        only) [21]. In regards to immunological outcomes, a concern associated with interruption of
        suppressive therapy is the potential for irreversible, viral-mediated CD4 T cell loss
        leading to disease progression [6,22]. We did not observe a decrease in CD4 cell numbers or
        lymphoproliferative responses against 
        C. albicans when measured between arms before the open-ended TI
        (see Figure 4), nor following resuppression after monitored TI reinitiation cycles in the
        repeated interruptions arm (see Figure 5). The latter is consistent with observations by
        others and does not support an immediate immunological “cost” to short-term TIs
        [12,14,15,16,23]. However, we do show that monitoring CD4 cell numbers by percentage could
        lead to misinterpreting a significant loss of CD4 cells as a result of a significant
        increase in CD8 count following TIs, even though absolute CD4 count numbers remained
        unchanged (see Figure 4). Interestingly, the increase in CD8 T cell number also
        corresponded with an increase in HIV-specific responses as measured by interferon-gamma
        expression (data not shown), which in light of an absence of effect on viral load between
        arms further supports that TI strategies alone may not significantly alter the pre-existing
        balance between viral replication and host antiviral responses [14,16,23,24].
        Importantly, no evidence for an increase of viral resistance in association with therapy
        failure was present in the repeated interruptions arm (See Table 2). We did not observe a
        greater clinical failure of NNRTI-based regimens in the repeated interruption arm due to
        “single drug” periods as predicted by recently redefined drug half-life estimates and the
        presence of viral replication during each interruption [25,26,27]. However, the percentage
        of patients with resistance mutations detected in this study in the repeated interruption
        arm (47%) is higher than the 17% observed in the SSITT cohort [11], in which patients with
        prior treatment failures were excluded [28]. We interpret this difference to mean that the
        resistance detected off drug in both our and their cohorts is likely associated with the
        greater number of drug-experienced patients in our cohort (75%) and the detection of prior
        archived resistance mutations as supported by Metzner et al. [29], who documented in 14/25
        (56%) SSITT patients the presence of minor populations of M184V occurring at least once off
        drug during interruption of therapy.
        In spite of the lack of difference in the total number of patients with resistant
        mutations detected on therapy during phase I and off therapy in phase II (7/21 [33%] versus
        10/21 [47%], respectively) in both arms, we do report in similarity to others a greater
        detection of resistance mutations in the TI arm when restricting analysis to the last
        off-drug period only [29,30] as three of 16 (18%) had mutations detected off drug in the
        continuous therapy/single interruption arm compared to nine of 18 (50%) in the repeated
        interruption arm. However, based on the lack of association between viral resistance
        detected off-drug shortly after TI and resuppression by the same regimen in all patients,
        it remains undetermined to what extent TIs favor the detection of archived mutations in
        chronically suppressed patients and to what extent these mutations are a signal for a
        future therapy failure. The latter is best exemplified by the data we collected on patients
        on NNRTI-based regimens in the repeated interruptions arm where two patients (S19 and S43)
        showed K103N detection (only during the off-drug periods) in the absence of therapy failure
        while maintaining the same regimen after each TI, including post-study follow-up (Table
        S1). On the other hand, virological failure in the continued presence of an NNRTI-based
        regimen in phase I was associated with detection of K103N, as observed in one patient (S56)
        in the repeated interruption arm and three patients (S37, S52, and S59) in the continuous
        therapy arm with self-reported non-adherence.
        Drug resistance that occurs during virological drug failure predicts virological
        responses to salvage treatment [31,32,33]. In contrast, the clinical implications of drug
        resistance mutations that appear shortly after TI in chronically suppressed patients are
        not clear. Case reports in this cohort of patients have demonstrated that drug-resistant
        variants that appeared during TIs may not persist in subsequent time points even after
        repeated use of the same antiretroviral regimen [19,34]. We now observe that drug
        resistance appearing during TIs can be transient since 50% and 33% of patients listed in
        Table 3 showed complete and partial reversion to wild type, respectively, when comparing to
        resistance at the last available viremic time point in phase II (See Table 3). Further, we
        observed durable resuppression of plasma viral RNA level in many patients who had
        drug-resistance mutations off therapy that would otherwise be expected to affect part of
        their treatment regimen when reinitiated (see Table S1). Virus populations that expand
        shortly after TI may lack all of the adaptations required to achieve high levels of plasma
        viremia in the presence of drug during continuous treatment. These adaptations may include
        the resistance-associated mutations, which were detected, as well as secondary mutations
        that may increase the viral replication capacity [35,36] or envelope adaptations required
        to escape concurrent humoral immune responses [37,38]. It is of interest to note that
        despite the large amount of research activity on TIs in patients with suppressed chronic
        infection and the hundreds of monitored interruptions studied to date, only limited cases
        of development of clinical resistance (as evidenced by a lack of viral resuppression
        following therapy reinitiation) have emerged, in contrast to the multiple reports of
        detection of viral sequences off ART associated with resistance as shown in this study and
        others [11,19,29,30,39,40].
        Taken together, while our data show no clinically significant benefit for repeated TIs
        of less than 1.5 mo in patients with CD4 counts greater than 400 on therapy with regard to
        viral control as defined by time to rebound, secondary outcomes document no significant
        difference in levels of retention of immune reconstitution between arms and no increased
        incidence of virological failure as a consequence of TIs. While our data indicate that this
        TI strategy should not be pursued outside of a clinical trial setting, we argue that it
        will be important to collect additional data on the potential benefits of drug-sparing
        regimens (such as reduced long-term toxicity and reduced cost) and to define long-term
        outcomes in comparison with continuous therapy.
      
      
        Supporting Information
        Registration of randomized trial at clinicaltrials.gov under identifier NCT00051818.
      
    
  

  
    
      
        Introduction
        Most primary malignancies spread systemically via lymphatic dissemination [1]. For
        example, the finding of axillary nodal metastases predicts a much shorter disease-free
        survival in breast cancer [2]. The total nodal tumor burden (number of affected nodes and
        metastatic tumor volume) affects prognosis even more severely [3]. Accurate lymph node
        staging also remains a cornerstone in choosing the most appropriate therapy for a given
        stage. Therapeutic intervention of metastatic lymph nodes [4], prophylactic radiation of
        frequently affected drainage routes [5], and systemic therapies [6] all have been shown to
        improve survival. Genetic profiles identifying metastatic tumors [7], serum biomarkers, and
        proteomic profiles are currently being developed to identify patients at risk [8,9]. No
        direct genetic profile, however, has been demonstrated to date to accurately predict the
        presence of human nodal metastases in a given patient. Rather, surgical approaches, such as
        sentinel lymph node biopsy or lymph node dissection, are still commonly used. Careful
        histological analysis includes mapping, bisectioning, and rapid staining in the frozen
        tissue laboratory. Higher diagnostic accuracies can be achieved by serial sectioning (50
        μm) and by immunohistochemical staining [10,11].
        Noninvasive imaging studies are commonly used during the workup of primary malignancies.
        Typically, lymph nodes are diagnosed by tomographic techniques (computed tomography [CT],
        magnetic resonance imaging [MRI]) as malignant when their short axis is >10 mm in size
        [12]. Such size criteria, however, have been shown to be unreliable [13]. Similarly, the
        detection of cancer in nonenlarged (occult) nodes is often quite low by positron-emission
        tomography (PET) and single photon emission computed tomography imaging. For example, small
        nodal metastases (< 5 mm) are often missed by PET imaging in patients with breast cancer
        [14]. More recently, it has become possible to image anatomic regions at submillimeter
        resolutions by MRI, with excellent spatial coverage and reduced motion artifacts. The
        development [15,16] and clinical introduction of lymphotropic magnetic nanoparticles has
        been shown to significantly improve diagnostic accuracies of MRI for nodal staging (LMRI)
        in prostate cancer [17]. These nanoparticles serve as probes for lymphatic anatomy and
        function and enhance tumor detection through abnormal distribution patterns in malignant
        nodes [17,18].
        Despite the advances of LMRI for cancer staging, image analysis has been challenging and
        occasionally controversial. Traditional analysis has been based on a reader's
        identification of certain structural abnormalities that can be variable, given differences
        in acquisition parameters and interpretation criteria [19,20,21]. Furthermore, it has been
        challenging to quickly and accurately analyze large datasets generated by LMRI.
        The goal of the current study was to develop and test technologies that would vastly
        improve the accuracy of current LMRI nodal staging. Specifically we set out to (a)
        determine whether unique magnetic parameters existed and could be used for semiautomated
        image analysis and (b) whether the technique could be applied to different primary cancers.
        Here we provide the first comprehensive analysis of tissue parameters validated against
        histopathology as an end point.
      
      
        Methods
        
          Study Design
          The Institutional Review Board approved the current study and all patients signed
          informed consent. The study was divided into a learning (
          n = 97 lymph nodes with known histopathology) and a test dataset (
          n = 216 lymph nodes with known histopathology; Table 1). Assignment
          into datasets was done in temporal fashion. The learning dataset represented
          retrospective cases at outset of the study, and the test dataset represented prospective
          cases collected during a 1-y interval. In the learning set, 55% of the nodes were benign,
          and 45% of the nodes were malignant. The learning dataset was obtained from 36 patients
          (24 male, 12 female, age 28–85 y, mean 59.7 y) with histologically proven primary
          genitourinary malignancies (prostate, 21; bladder, 9; testes, 5; ureter, 1). All patients
          completed the MRI study and then underwent surgical resection (
          n = 26) and/or nodal biopsy (
          n = 10). The investigated nodes had a mean short axis diameter of 10.5
          mm (range 3–39 mm).
          The test dataset was obtained from 34 patients (25 male, nine female, age 30–82 y,
          mean 58.9 y) with histologically proven malignancies from different primaries (Table 1),
          including prostate (
          n = 18), breast (
          n = 7), penile (
          n = 4), bladder (
          n = 2), testes (
          n = 2), and colon (
          n = 1). Seventy-nine percent of the nodes were benign and 21% of the
          nodes were malignant. The nodes in the test dataset had a mean short axis diameter of
          10.0 mm (range 3–39 mm). Both datasets included the full spectrum of normal nodes to
          completely replaced nodes.
        
        
          MRI
          MRI was performed at 1.5 T (System 9X, General Electric Medical Systems, Milwaukee,
          Wisconsin, United States) using phased-array coils. All images were archived on DICOM
          PACS servers (MIPortal, CMIR and Siemens Medical Systems, Erlangen, Germany; and Impax RS
          3000, AGFA Technical Imaging Systems, Richfield Park, New Jersey, United States) for
          subsequent analysis. Images of the pelvis (
          n = 56) extended from the pubic symphysis to just above the level of
          aortic bifurcation. In patients with primary testicular cancers (
          n = 7) imaging was extended superiorly to include the renal hilum and
          retroperitoneum. In patients with breast cancer (
          n = 7) we obtained MR images of the bilateral axillae, including the
          internal mammary and supraclavicular regions. All patients were imaged with identical
          pulse sequences and timing parameters. Imaging was performed before and 24 h after
          intravenous ferumoxtran-10 administration (Combidex, Advanced Magnetics, Cambridge,
          Massachusetts, United States; 2.6 mg Fe/kg diluted in normal saline and infused over a
          20-min period using a 5-μm filter).
          The acquired pulse sequences included (a) axial T2-weighted fast spin-echo (TR/TE,
          4500/80; flip angle, 90°; field of view, 24–28 cm; slice thickness, 3 mm; matrix, 256 ×
          256; number of excitations, 2; in-plane resolution, 1.2 mm); (b) a T1-weighted
          two-dimensional gradient-echo sequence obtained in different anatomical planes (TR/TE
          175/1.8; flip angle, 80°; field of view, 22–30 cm; slice thickness, 4 mm; matrix, 128 ×
          256; in-plane resolution, 2.0 mm); (c) an axial T2-weighted dual TE gradient-echo (TR/TE
          2100/14–24; flip angle, 70°; field of view, 26–28 cm; slice thickness, 3 mm; matrix, 160
          × 256; in-plane resolution, 1.7 mm); and (d) a three-dimensional (3D) T1-weighted
          gradient echo sequence; TR/TE 4.5–5.5/1.4; flip angle, 15°; field of view, 24–28 cm;
          slice thickness, 1.4 mm; matrix, 256 × 256; in-plane resolution, 1.0 mm).
          The above listed imaging sequences and parameters had previously been optimized to
          reduce motion artifacts, maximize signal-to-noise ratio (SNR), and provide diagnostically
          useful images of the pelvis, abdomen, and chest within clinically acceptable time limits.
          The T2-weighted fast spin-echo sequence, in (a) above, was primarily used for qualitative
          nodal detection, and hence a square pixel with more than one acquisition was obtained.
          The two-dimensional axial T1-weighted gradient-echo sequence, in (b) above, was chosen to
          achieve adequate anatomical coverage within a short imaging time. The axial dual-echo
          gradient-echo sequence, in (c) above, was developed specifically for this project to
          provide artifact-free datasets for quantitative image analysis. A matrix size of 160 ×
          256 was chosen for this sequence to achieve a balance between the upper limits for
          imaging time while reducing image noise. Finally, a 3D T1-weighted sequence was obtained,
          in (d) above to provide a dataset for vascular maximum intensity projection (MIP)
          reconstructions.
        
        
          Quantitative Image Analysis
          All image analysis was performed on archived DICOM images using different software
          packages (e.g., custom-built software such as CMIR-Image, MGH, Boston, Massachusetts,
          United States; Syngo, Siemens Medical Systems; Advantage Windows, General Electric
          Medical Systems). Lymph nodes were identified by readers who manually placed kernels onto
          each node for automated boundary detection and calculation of nodal dimensions and
          volumes. The thus identified regions of interest (ROIs) encompassed the entire lymph node
          (not only portions of it) and were used for quantitative signal-intensity (SI)
          measurements (see Table 2). Serial measurements of nodal dimensions on different pulse
          sequences or time points varied less than 2%.
          A number of quantitative tissue parameters were calculated either as differences
          between pre- and postcontrast scans (δ) or as single-value analysis on postcontrast scans
          (see Table 2). The lymph node/muscle (LNM) ratio was calculated by dividing signal
          intensities of an entire lymph node by that of adjacent muscle using a similar-sized ROI,
          drawn manually. The nodal SI change was calculated by obtaining SI before and after
          contrast administration. The nodal SNR was calculated by obtaining SD/SD
          noise . The T2* was calculated in nodal ROIs on dual TE images using
          CMIR-Image. T2* maps were constructed by performing fits of a standard exponential
          relaxation model (S = Ke
          –TE/T2* ) to the data on a pixel-by-pixel basis. Only pixels with
          intensity greater than a threshold level (2X of noise) were considered during the fitting
          process. Pixel variance was obtained from post-MR images. Comparative visual analysis
          included short axis measurements, and identification of heterogeneity, large focal
          defects, and central hyperintensity, according to criteria previously established
          [12,17].
          To determine the diagnostic accuracy of the different tissue parameters in the
          learning dataset, we determined sensitivity, specificity, and predictive values for each
          parameter alone and in combination (Table 3). The most discriminatory parameters were
          then applied to the test dataset (Table 4).
          In the final set of semiautomated image analysis, 3D reconstructions were obtained for
          nodal mapping onto vascular anatomy using MIP projections. While the MIP projections do
          not aid in the differentiation between malignant and benign lymph nodes, they are
          invaluable in providing anatomic content to the dozens of lymph nodes identified. In
          particular, MIP images were generated interactively from postcontrast, fat-saturated,
          volumetric interpolated breath-hold images to outline vascular anatomy. The evaluated
          lymph nodes characterized as benign or malignant (by T2*/variance analysis) were then
          superimposed on the volumetric 3D images, using customized software (Advantage Windows,
          General Electric Medical Systems).
        
        
          Statistical Analysis
          Data were expressed as mean ± standard deviations (SD) and medians. All statistical
          testing was performed using GraphPad Prism (GraphPad Software, San Diego, California,
          United States). The significance between two individual groups was determined using the
          nonpaired Student's 
          t -test (e.g., benign and malignant datasets in Figure 1). For the more
          discriminatory datasets alternative-free-response receiver operating characteristic
          curves were plotted. Ratios for cut-off single-value parameters were defined to yield
          highest sensitivity and specificity. Accuracy for a given parameter was expressed as the
          area under the curve (A
          z ), and values are summarized in Table 4.
        
        
          Histology
          All lymph nodes were sampled histologically within 2 wk of the MRI (mean: 6 d; range:
          2–14 d). The analysis was done in surgically resected lymph nodes (
          n = 55; both benign and malignant nodes) or in fine needle aspirates
          and core biopsies (
          n = 15; malignant nodes only), implementing careful mapping procedures
          to correlate nodes. Surgically excised nodes were sectioned at 10–20 μm intervals after
          bihalving and were stained with hematoxylin-eosin.
        
      
      
        Results
        
          Learning Dataset
          The learning dataset consisted of 97 histologically validated lymph nodes from 36
          patients with different primary malignancies (see Table 1). The mean short axis diameter
          was 10.5 mm (range 3–39 mm) with 56 of the 97 nodes (58.3%) measuring less than 10 mm,
          that is, below the traditional imaging cutoff for malignancy (“occult nodes”). Table 2
          summarizes the incidence of different visual, comparative (before and after contrast
          administration), and semiautomated (postcontrast administration only) parameters in the
          two different groups. Figure 1 is a graphical representation of overlaps between
          malignant and benign groups for different parameters listed in Table 2. Table 3
          summarizes sensitivities, specificities, and predictive values for the different
          quantitative imaging parameters. Sensitivities of metastasis detection by visual image
          analysis ranged from 50%–94%, however, often with lower specificities. Volumetric
          measurements, in particular, were insensitive markers of malignancy in nonenlarged nodes
          (see Table 3).
          In contradistinction, image analysis of pre- and postcontrast image sequences resulted
          in higher specificities and sensitivities (see Table 3). Comparative differences between
          benign and malignant nodal groups were highest for T2* and pixel variance measurements
          (see Table 3). Of all the semiautomated parameters tested alone, T2* measurements showed
          the highest sensitivity (93%; 95% confidence interval: 82%–98%) and specificity (94%; 95%
          confidence interval: 84%–99%) in the learning dataset (see Figure 1 and Table 3).
          Of all the semiautomated parameters tested in combination, T2* measurements combined
          with pixel variance analyses postcontrast showed the highest sensitivity (98%; 95%
          confidence interval: 88%–99%) and specificity (94%; 95% confidence interval: 82%–98%) in
          the learning dataset (Figure 1E). Using the dual-value analysis, there was one malignant
          outlier in the benign dataset (the lymph node was 3 mm in overall size, with few
          malignant cells seen on histology, and probably too small for analysis) and two benign
          outliers in the malignant dataset (both these nodes showed hyalinosis replacing more than
          50% of the nodal architecture).
        
        
          Test Dataset
          To determine whether feature extraction would be accurate for prospective nodal
          staging, we utilized the above criteria against a larger test dataset encompassing 216
          validated lymph nodes from 34 patients, including different primaries (see Table 1). The
          sensitivity, specificity, and predictive values of the most discriminatory parameters of
          this prospective analysis are summarized in Table 4. We primarily focused on
          semiautomated image analysis of postcontrast scans because of the high sensitivity and
          specificity determined in the learning dataset. T2* measurements showed a sensitivity of
          (93%; 95% confidence interval: 82%–99%) and a specificity of (91%; 95% confidence
          interval: 85%–96%). Combined T2* and pixel variance analysis achieved a sensitivity of
          98% (95% confidence interval: 88%–99%) and a specificity of 92% (95% confidence interval:
          87%–96%) comparable to that of the learning set and much superior to currently used size
          criteria.
          Using the dual-value analysis, there were two malignant outliers in the benign dataset
          (both of these nodes were less than 3 mm in overall size and probably too small for
          analysis—similar to the learning dataset) and three benign outliers in the malignant
          dataset (two of these nodes had hyalinosis replacing more than 50% of the nodal
          architecture and one had macrocalcifications). More important, all the misclassified
          nodes occurred in individual patients rather than in the same patient and, hence, did not
          affect the overall nodal staging on a patient-by-patient basis in this dataset.
        
        
          Image Reconstruction
          Utilizing semiautomated feature extraction to identify lymph nodes and image analysis
          (based on T2* and pixel variance), we subsequently proceeded to map individual lymph
          nodes onto vascular anatomy in the different anatomic drainage patterns. Figure 2
          summarizes the different steps in image analysis. Figure 3 and Video 1 shows an example
          of a 45-y-old patient with colorectal cancer undergoing semiautomated nodal staging. In
          this particular patient, MRI identified six positive lymph nodes (< 10 mm each),
          reconstructed as a 3D dataset, whereas all positive lymph nodes were missed by PET scans.
          Figure 4 and Video 2 show reconstructions and analyses from a patient with a breast
          cancer primary with bilateral nodal metastases. Note the high spatial resolution allowing
          the detection of a 3-mm nodal metastasis.
        
      
      
        Discussion
        We show that it is feasible to extract various quantitative tissue parameters to predict
        the likelihood of nodal metastases in vivo. These results are highly relevant in cancer
        staging because they provide evidence that (a) quantitative tissue parameters enable
        diagnosis of lymph node metastases while reducing interobserver variability and (b) that
        semiautomated reconstructions allow spatially more extensive mapping than is currently
        possible.
        Metastases to lymph nodes occur during growth of most primary malignancies, and their
        presence mandates the need for more extensive and systemic therapy. Nodal cancer staging
        currently relies on invasive procedures (surgical lymph node dissection, sentinel lymph
        node resection, biopsy) with significant morbidity and cost [22,23], or insensitive
        tomographic imaging methods [24]. For example, detection sensitivities using size criteria
        with state-of-the-art multislice CT are as low as 50%, whereas PET imaging of nonenlarged
        nodes has equally low sensitivities [14]. Based on the observation that nanoparticulate
        solutions accumulate in nodal macrophages upon systemic injections [25,26], lymphotropic
        superparamagnetic preparations have been developed [16]. In earlier clinical trials (using
        lower spatial resolution sequences), metastases of 1–2 mm have been detected [17], whereas
        as few as 1,000 tumor cells have been detected in nodes in experimental mouse models [18].
        Despite these advances, it has been difficult to acquire images of sufficiently high
        resolution and to derive parameters to automate diagnosis. The data presented here indicate
        that unique magnetic parameters allow identification of nodal metastases and accurate 3D
        reconstructions, including surgically inaccessible lymph nodes.
        The significance of the above findings is 3-fold. First, the ability to directly and
        noninvasively monitor nodal tumor involvement represents a powerful diagnostic tool for
        cancer. Accurate staging represents the cornerstone for triaging patients to either
        localized or to more aggressive and systemic therapies. Second, the method described here
        was sensitive for the limited subsets of primary cancers tested. It is reasonable to
        hypothesize that such analysis could be applied to staging of other common primaries. In
        particular, lung, colorectal, genitourinary, and head and neck cancers could benefit from
        this staging procedure. In addition to nodal staging, the nanoparticle-enhanced MRI can
        also be used to measure microvascularity in primary tumors [27] and to improve the
        detection of liver metastases [28]. Third, our results are significant because the
        semiautomated staging method is highly accurate and reduces variability in visual image
        analyses between different observers.
        The LMRI staging technique is believed to be clinically relevant in several key areas.
        First, LMRI may play a significant role in avoiding unnecessary surgeries, that is, those
        in node-positive patients. Second, since LMRI can detect lymph nodes outside traditional
        surgical fields, this information may influence surgical approaches. In colorectal cancer,
        LMRI may provide a “sentinel-node-like” guide to staging. Third, it is likely that LMRI
        would be useful to identify appropriate patients to receive neoadjuvant chemotherapy prior
        to surgery. Currently, neoadjuvant therapy is often reserved for postoperative patients,
        once the nodal status has been determined. Fourth, LMRI may be particularly useful to guide
        radiation therapy by mapping the complete nodal status onto bony and vascular landmarks.
        Finally, LMRI could be used to avoid invasive diagnostic procedures, which are not part of
        therapy. For example, LMRI could replace lymphangiography, mediastinoscopy, or endoscopic
        ultrasound for nodal staging.
        Our findings have a number of direct implications for technology development and in
        clinical care. Accurate measurements of T2* relies on motion artifact-free multiecho pulse
        sequences that are not routinely available on clinical scanners at spatial resolutions
        required for nodal staging. Such sequences will have to be implemented and combined with
        postprocessing tools to simplify and semiautomate analysis. Similar software approaches are
        already used routinely in lung nodule characterization [29] or screening for breast cancers
        [30]. We predict that in the case of LMRI, such automation routines will be highly
        specific, given the unique mechanism of image contrast. As a proof-of-principle, we
        implemented approaches to identify, segment, analyze, and display nodal information. While
        the current technology is already highly accurate, we anticipate further improvements with
        hardware and software advances. We hope that this will ultimately translate into clinical
        practice and replace unnecessary intervention.
      
    
  

  
    
      
        
        Accurate staging of cancers is one of the most important parts of the work up of
        patients for both prediction of prognosis and determination of the most appropriate
        treatment. And an essential part of this work up is assessing whether or not there has been
        lymphatic spread. Current methods include surgical removal of nodes for examination and
        various types of imaging, ranging from ultrasound to newer technologies such as magnetic
        resonance imaging (MRI). All these methods have problems; some are very invasive, others
        are very time consuming, and none are completely reliable.
        However in one of the more exciting crossovers from chemistry into medicine, researchers
        have developed nanoparticles to improve the diagnostic accuracy of MRI. The nanoparticles
        contain a central superparamagnetic iron oxide core and are covered by dextran, imparting
        long circulation times and biocompatibility. When injected intravenously, the nanoparticles
        localize to lymphoid tissue, and are internalized into macrophages. There is then a
        decrease in signal intensity on T2- and T2*-weighted images, and when metastases are
        present there is a recognizably abnormal pattern on MRI scans.
        In a previous paper published in the 
        New England Journal of Medicine , Ralph Weissleder and colleagues
        described using these nanoparticles to assess lymphoid spread in patients with prostate
        cancer. Now, in a paper published in this month's 
        PLoS Medicine , they have gone further by extending the analysis to
        patients with different types of cancer, and producing an algorithm that allows
        semiautomation of the procedure.
        The authors developed the algorithm in a training group of 36 patients and then
        validated it in a group of 34 patients. The results are encouraging: the analysis showed a
        sensitivity of 98% (95% confidence interval, 88%–99%) and a specificity of 92% (95%
        confidence interval, 87%–96%). The advantages of automating this procedure are substantial,
        not least because it can remove the problem of different observers assessing data
        differently.
        And what is more, once the data have been collected and assessed it is possible to
        reconstruct a virtual picture of the patient's lymph nodes, thus potentially allowing
        accurate surgical removal of the nodes.
      
    
  

  
    
      
        
        Some of us, when awake in the middle of the night, feel an urge to visit the kitchen.
        This could explain results of previous studies that have shown a link between short sleep
        duration and high body mass index (BMI). But a study by Emmanuel Mignot and colleagues
        suggests that it's not just the additional snacking opportunities that make short sleepers
        more likely to be overweight.
        Intrigued by the connection between sleep and BMI, and by recent studies showing that
        sleep deprivation in laboratory settings can cause a decrease in serum levels of leptin, a
        hormone known to control appetite, Emmanuel Mignot and colleagues set out to study the
        levels of various hormones known to regulate appetite and energy expenditure under “real
        life” conditions.
        They took advantage of the Wisconsin Sleep Cohort Study, an ongoing longitudinal study
        of sleep habits and disorders in the general population. The study began in 1989, when
        researchers mailed state employees aged 30–60 years a survey on sleep habits, health, and
        demographics. Mail surveys were repeated at 5-year intervals, and some of the respondents
        were recruited to sleep a night in the laboratory and undergo various tests. A number of
        participants were also asked to keep a sleep diary for 6 days. The study has already shown
        connections between sleep apnea and hypertension, and between menopause and
        sleep-disordered breathing.
        For their study, Mignot and colleagues measured sleep duration (habitual and immediately
        prior to blood sampling), BMI, and pre-breakfast blood hormone levels in 1,024
        participants. Consistent with previous studies, they found that in individuals who sleep
        less than 8 hours (74% of all participants), BMI was inversely proportional to sleep
        duration. In addition, short sleep was associated with low leptin and high ghrelin levels
        (ghrelin is a hormone thought to stimulate food intake).These hormonal differences are
        likely to increase appetite, which could be responsible for the increased BMI in short
        sleepers.
        These findings could explain, at least in part, why societies in which excess calories
        are much easier to come by than a good night's sleep are more prone to obesity. Mignot and
        colleagues plan to test this in intervention studies where they make people sleep more and
        measure the effects on body mass. “Good sleep, healthy eating habits, and regular exercise
        each may have important roles in fighting obesity in modern society,” suggests Mignot.
      
    
  

  
    
      
        
        The Icelandic population is now a part of a unique epidemiological study, which has
        involved investigating the genetic heritage of many of them. The reason that this
        experiment can be done is because of the remarkable records that exist in Iceland. Not only
        is there almost complete genealogical information dating back to the 18th century on all
        current (288,000) and many previous Icelanders (more than 600,000 in total), but in
        addition the country has an almost complete cancer registry dating from 1955. A company,
        deCODE Genetics, was set up to mine health-care data in Iceland, and to use it to assess
        the effect of genetics on health. Initially, the company attracted criticism, with some
        questioning the ethics of providing access to health-care data for many disease projects to
        a for-profit company. But the company has been supported by many Icelanders themselves,
        demonstrated by Icelanders donating blood samples with informed consent for research on
        multiple diseases, and now the project's scientific value is becoming apparent.
        One such analysis is the subject of a paper by Laufey Amundadottir and colleagues in
        this month's 
        PLoS Medicine that assesses how much genetic factors contribute to cancer
        risk across the whole Icelandic population.
        The paper looked at 27 different types of cancers (all those with more than 200 cases)
        that had been registered between 1955 and 2002 and analysed the frequency of close and
        distant relatives also having that cancer, or another kind of cancer. Of the 27 cancers, 16
        showed significant “familiality,” and for some this risk even extended to distant (that is,
        third- to fifth-degree) relatives. The seven cancers with the highest increased familial
        occurrence both in close and distant relatives were breast, prostate, stomach, lung, colon,
        kidney, and bladder cancers. And, interestingly, three cancers—stomach, lung, and colon
        cancer—were also seen more frequently in mates of patients, indicating a shared
        environmental risk factor. And for some cancers there was a familial association with other
        cancers, for example, relatives of individuals with stomach, colon, rectal, or endometrial
        cancer were more likely to have any of these cancers.
        Cathryn Lewis, the academic editor for the paper comments on the study's strengths.
        “This level of family relationship and clinical diagnosis is rarely available from
        interviewing patients and family members. The size of the study (over 600,000 individuals,
        with 32,000 cancer cases) and the high quality of data enables the authors to detect subtle
        effects across distant relationships.”
        How robust are these data, and what do they mean for the biological understanding of
        cancer? As Lewis says, “Although the current study is impressive in its size and scope,
        even here, the sample size becomes an issue, with the most convincing results seen in the
        most common cancers.” Certainly not all the findings are surprising; some rare cancers are
        already known to be associated with particular genetic defects, and syndromes that
        predispose to multiple cancers have been described, for example, that of Hereditary
        Nonpolyposis Colorectal Cancer. Other associations are more intriguing—the cluster of
        related cancers that include prostate, kidney, and bladder could possibly have a
        developmental origin, since all arise from the same part of the embryo.
        So, by highlighting these subtle links, the study's particular value may become
        apparent: deciding future avenues of investigation in the complex interrelationships that
        interact to produce cancer.
      
    
  

  
    
      
        
        Adverse side effects, viral resistance, and the high cost of antiretroviral therapies
        remain obstacles in the way of turning HIV/AIDS into a manageable chronic disease.
        Structured treatment interruptions (STIs) in individuals who have good viral control on
        therapy have been proposed as a strategy for overcoming these obstacles. The initial hope
        that STIs would help patients achieve greater viral control has so far not been supported
        by data from clinical trials, but interrupting treatment has also been proposed as a
        strategy to reduce the cost of long-term therapy and drug-associated toxicity.
        Luis Montaner and colleagues now report results from a randomized trial of 42
        participants (75% on their second to fourth regimen, 66% on regimens containing
        non-nucleoside reverse-transcriptase inhibitors) who received either continuous therapy for
        40 weeks or three successive treatment interruptions of two, four, and six weeks, followed
        by a final open-ended interruption for both groups.
        The study was designed to be able to detect a difference of four weeks or greater
        between the two groups for the time to viral rebound during the open-ended interruption—the
        primary outcome. No difference between the two groups was seen (median time for the group
        on continuous treatment was four weeks, and for the STI group was five weeks).
        Secondary outcomes included serious adverse events (disease progression, acute
        retroviral syndrome, therapy failure, or opportunistic infections at any point in the
        study), changes in CD4 count on therapy, immune reconstitution changes (CD4 recall
        responses and CD4 naïve/memory T cell distribution), and detection of viral mutations There
        were no study-related serious adverse events in either group and no increase of therapy
        failure in the STI arm. CD4 counts fluctuated between the start and end of each monitored
        treatment interruption, but levels recovered after resuppression of virus, with retention
        of recall responses throughout. Viral resistance was detected in both groups (in seven of
        21 patients in the continuous treatment group and ten of 21 patients in the STI group), but
        it was more commonly detected (50% versus 18%) in the STI group during the open-ended final
        interruption, even though all subjects suppressed virus upon reinitiating the same
        therapy.
        Possible risks and benefits of STIs remain controversial, but data from this and other
        published trials do not support short-term clinical benefits of treatment interruptions.
        However, because they do not see increased therapy failure and find preservation of immune
        function in the STI group, the authors conclude that, in light of the possibility of
        reducing costs and drug-related toxicity, additional trials of STIs are warranted.
        Particularly important in the debate over the safety of STIs is whether the detection of
        resistant mutants should be of concern. The authors point out that all participants were
        able to resuppress the mutant virus when they resumed their previous drug regimens but
        state that it remains undetermined to what extent resistant mutations are a signal for
        future therapy failure. Moreover, viral replication and rebound—which eventually occurred
        in all participants—is seen by some researchers as inherently detrimental, and these
        experts argue that treatment interruptions are unsafe and their use should be
        discontinued.
        What seems clear is that STIs have no place outside controlled clinical trials and that
        questions regarding long-term safety remain unanswered. At least a dozen additional trials
        that examine STIs are currently recruiting patients and will help answer these
        questions.
      
    
  

  
    
      
        
        Last year the European Parliament and Council formed the european and Developing
        Countries Clinical Trials Partnership (EDCTP). The aim of this new funding body, which has
        a budget of &euro;400 million spread over five years, is a noble one: to fund research
        in developing countries, particularly in Africa, that contributes to the development of
        affordable prophylactics and drugs for HIV/AIDS, tuberculosis, and malaria.
        Unfortunately, the organization has not got off to an auspicious start. Its executive
        director, Piero Olliaro, was ousted from power at the first EDCTP annual forum at the end
        of September. There have been rumblings of discontent among grant applicants who say that
        the first round of grant assessments was administered poorly. And not-for-profit
        organizations that would like to partner with EDCTP have been left in the dark regarding
        whom to speak to at the organization.
        This omission is significant because partnership is one of the key tenets of the EDCTP.
        European research agencies are slowly beginning to realize that they need to cooperate with
        each other if they are to be competitive with the United States. The history of many
        European countries is such that Europe has much stronger ties with Africa than does the
        United States, so it makes political sense for the European Union to fund research that
        provides a springboard for European researchers to compete effectively with US
        scientists.
        Crucially, the EDCTP was also set up to enable European and African scientists to work
        together as equal partners. There is increasing recognition that the paternalistic,
        colonial attitude that pervaded “tropical medicine” in the past just will not do. The EDCTP
        hoped to change that by having a Partnership Board that contains equal numbers of African
        and European representatives. However, the EDCTP Assembly, which contains a representative
        from each of 14 EU member states but none from African countries, has the power to veto the
        decisions of the Partnership Board, which is supposedly the scientific decision-making
        authority.
        Doing clinical trials in Africa is far from easy. There are too few adequately resourced
        research centers, and those that do consistently perform well are oversubscribed.
        Therefore, there is a clear need for "capacity building"—development of a research
        infrastructure, in terms of both equipment and personnel, that is capable of coping with
        the challenges of clinical trials. The EDCTP hopes to contribute to this essential endeavor
        by funding clinical trials that are sustainable in the long term. In particular, it
        believes that the best way to train a new generation of African scientists is by teaching
        them on the job, that is, involving them fully in the planning and execution of the trial,
        rather than flying in European experts who leave as soon as the trial is finished.
        A commitment from European researchers to be engaged for the long term is essential for
        the success of these projects. In addition, partnerships need to be brokered with national
        programs in Africa to ensure that the new capacity can be sustained over time. The end goal
        is to produce centers of excellence that are run by Africans doing internationally
        recognized research that conforms to Good Clinical Practice guidelines. But this will only
        happen if African researchers are treated as equal partners and are allowed to be fully
        engaged in the projects that are taking place in their countries.
        So can the EDCTP work, or is it doomed to failure? In many ways the organization has a
        great deal going for it. Although the budget of 3400 million spread over five years is tiny
        considering the combined burden of HIV/AIDS, tuberculosis, and malaria, it is important to
        remember that it is the biggest single European project for clinical trials in Africa. In
        many ways the EDCTP is a demonstration project: if some success can be achieved it is very
        likely that additional funds will follow. The project is certainly strengthened by the
        involvement of Pascoal Mocumbi, the former prime minister of Mozambique, as High
        Representative of the project. Mocumbi is highly respected by the global-health community
        and carries considerable weight with African politicians. Mobilization of political will
        within Africa will be essential if research capacity is to be sustained for the long
        term.
        On the downside, it seems clear to most insiders that the management structure needs to
        be radically changed and partnership with other organizations needs to be improved. The
        EDCTP Assembly met on October 28 and 29 to discuss these issues and to elect a new leader.
        At the time this editorial went to press, there was still no public announcement of the
        outcome of this meeting. In addition, the political infighting that pervades European
        politics at all levels needs to be controlled, or at least managed effectively. This might
        be a tall order, but it is essential if this worthwhile and high-profile project is to
        succeed.
      
    
  

  
    
      
        
        It has been more than a century since coccidioidomycosis was first recognized as a
        serious disease, and its etiology and epidemiology have been well documented. But the
        disease remains an enigma to many, and it often goes undiagnosed, even in endemic areas. As
        management of this chronic disease remains problematic, new preventive or therapeutic
        options are needed.
      
      
        Etiology and Epidemiology
        Coccidioidomycosis is a fungal disease found only in the Western Hemisphere. It is
        caused by two nearly identical species, 
        Coccidioides immitis and 
        C. posadasii , generically referred to as the “Californian” and
        “non-Californian” species respectively [1]. The fungus grows in a mycelial phase (see Box
        1) in the soil within a geographically delineated area of the United States known as the
        Lower Sonoran Life Zone [2]. This semiarid zone encompasses the southern parts of Texas,
        Arizona, New Mexico, and much of central and southern California (Figure 1).
        Endemic regions for coccidioidomycosis have long been identified in semiarid areas in
        Mexico [3], and smaller endemic foci have been described in areas of Central and South
        America [4,5]. More recently, Brazil has also been found to contain endemic areas in the
        semiarid northeastern states of the country [6]. The climatic conditions and flora of these
        states are similar to those in endemic regions in North, Central, and South America. In
        Latin America, Mexico has the largest number of reported cases, with the prevalence of
        infection in northern Mexico reported to be between 10%–40% [7,8]. 
        C. posadasii is thought to be the predominant species in Mexico
        [3].
        As the soil dries or nutrients become limiting, the fungus reproduces asexually by
        disarticulating the hyphae into small, environmentally-resistant arthroconidia
        (reproductive spores) (Figure 2). These are easily aerosolized when the soil is disturbed
        by wind or human activities. Consequently, it is the inhalation of the dust-borne
        arthroconidia that leads to infection by this pathogenic fungus in both humans and domestic
        or wild mammals. Upon inhalation, the fungus converts to a unique life cycle of alternating
        spherules and progeny endospores, which comprises the parasitic phase of this dimorphic
        fungus (Figure 2) [9]. Mycelial elements are only occasionally found in diseased tissue
        [10]. Coccidioidomycosis is not contagious; reports of human-to-human spread are extremely
        rare. Hence, primary exposure to contaminated dust is the sole risk factor for the
        acquisition of this disease.
        It is estimated that upwards of 100,000 primary coccidioidal infections occur in humans
        each year in the endemic areas of the United States [11]. In recent years, the incidence of
        the disease has increased in California and Arizona, which may be partially due to the
        rapid immigration of previously unexposed persons from states outside the endemic areas (in
        other words, the pool of susceptible people has increased) [12]. In the United States,
        diagnosis in patients who have symptoms is established by serodiagnosis in conjunction with
        patient history. In previous decades, a coccidioidal skin test antigen was a useful adjunct
        in the diagnosis, but it became unavailable in the 1980s [13]. The incidence of primary
        pulmonary disease outside the United States is not established; most reports are limited to
        disseminated or unusual cases [14]. Diagnosis in Latin America is usually based on
        microbiologic findings, as serology is not always available [14].
      
      
        Clinical Features
        In their pioneering epidemiologic studies, Smith and colleagues found that about 60% of
        exposures to the fungus result in asymptomatic infection [15]. In the 40% of patients who
        have symptomatic disease, there are protean manifestations. These range from a primary, or
        benign, pulmonary infection (commonly known as “Valley Fever”) to a progressive pulmonary
        or extrapulmonary disease involving the skin, bones and/or joints, the central nervous
        system, and other organ systems. Fortunately, most patients with primary disease recover
        spontaneously and retain lifelong immunity to exogenous reinfection. Chronic and
        disseminated disease is estimated to occur in up to 5% of infected individuals, with
        comparatively more cases occurring in older individuals and in males [12]. The most
        dangerous form of the disease is meningeal infection, which occurs in about 0.15%–0.75% of
        extrapulmonary coccidioidomycosis cases and requires treatment for life [16].
        In regions where tuberculosis rates are high, the two diseases may occur together.
        Tuberculosis and coccidioidomycosis share common epidemiological, clinical, radiographic,
        and even histopathological features, making a correct diagnosis extremely difficult in
        cases where both diseases coexist. In areas where both diseases are endemic, the pertinent
        studies for diagnosing both conditions should be performed in every patient with compatible
        clinical features. The diagnosis of one of them does not exclude the possible existence of
        the other [17].
      
      
        Treatment
        Historically, patients with the primary respiratory form of the disease were not treated
        because the vast majority recovered on their own. Instead, such patients were given
        supportive care and were monitored, often with radiographs, until the disease resolved. In
        recent years, however, an increasing number of physicians are prescribing azole antifungals
        in cases of primary disease, both because drugs like fluconazole have a good safety record,
        and because there is a perception that treatment may prevent progression to more serious
        forms of the disease. This latter presumption, however, is not supported by controlled
        trial data.
        All cases of chronic or disseminated disease call for antifungal therapy, but the choice
        of drugs, route, and duration of therapy is highly dependent on the form of the disease,
        the severity and site(s) of infection, and the immune status of the patient. Galgiani and
        colleagues have published clinical practice guidelines on the choice of drug and duration
        of therapy for a given form of the disease [18].
        There are only two classes of antifungal therapy routinely used for treatment of
        coccidioidomycosis. The first class is the polyenes, with amphotericin B desoxycholate and
        the newer lipid formulations used for the more serious forms of disease. The second class
        is the azoles, with ketoconazole, fluconazole, itraconazole, and the newer analogue
        voriconazole as available options. Voriconazole, in particular, is being used more and more
        often in life-threatening mycoses, and was found to be better than amphotericin B in the
        primary therapy of invasive aspergillosis [19]. According to available reports, treatment
        in Latin America usually consists of one of the azoles (fluconazole or itraconazole) and/or
        amphotericin B desoxycholate; lipid formulations are too costly to be accessible[20].
        Treatment of the more serious or aggressive forms of the disease is typically of long
        duration and often results in less than complete resolution of disease; relapse is common
        [21]. Unfortunately, information on the treatment of coccidioidomycosis is limited, due to
        the small numbers of controlled trials performed for what is perceived to be a niche
        market. Clearly, newer, more powerful drugs are needed.
        In addition to drugs, surgery is sometimes indicated to remove focalized infections,
        such as pulmonary cavities, or to debride osseous forms of the disease [22].
      
      
        Immunology and the Basis for a Vaccine
        Acquired resistance to coccidioidomycosis strongly correlates with the development of a
        delayed-type hypersensitivity skin test response to coccidioidal antigens [23] and the
        production of T-helper-1 (Th1)-associated cytokines to coccidioidal antigens, such as
        interferon-gamma (IFN-γ) and Interleukin-2 (IL-2) [24]. Humoral immunity plays no known
        role in overcoming infection.
        Although all humans are equally susceptible to initial infection, there is evidence of
        genetic predisposition to dissemination, independent of socioeconomic or environmental
        factors, particularly among African-Americans and Filipinos [25]. Pregnancy is also a risk
        factor.
        In cases of marked immunosuppression, either in advanced AIDS or other forms of
        depressed cellular immunity, the management of coccidioidomycosis is particularly
        challenging and requires aggressive treatment [26].
        As previously mentioned, recovery from disease confers lifelong immunity to reinfection,
        and is a rationale for the development and implementation of a vaccine for the prevention
        of symptomatic or serious forms of the disease. The combination of increasing incidence of
        disease, a growing population in the endemic area, and the lack of a highly effective drug
        treatment justifies efforts to prevent (rather than treat) this disease. To that end, a
        university-based consortium, the Valley Fever Vaccine Project (www.valleyfever.com), has
        identified and cloned immunogenic proteins that have proven effective in the prevention of
        deaths and fungal burdens in mouse models of coccidioidomycosis. This suggests that a
        vaccine for use in humans could be created [27]. A candidate vaccine comprised of a fusion
        protein based on two antigens has been selected and is currently in pharmaceutical
        development under the sponsorship of this project, with the goal of evaluating the safety
        and immunogenicity in humans.
      
      
        Conclusion
        Although the vast majority of infected individuals emerge from coccidioidomycosis
        without complications, an unlucky minority are faced with a debilitating disease that lacks
        adequate drug options for rapid and completely effective treatment. In the absence of newer
        therapeutics, discoveries that lead to immunologic intervention [28] or prevention by
        vaccines may ultimately bring a measure of relief.
      
    
  

  
    
      
        
        Meningococcal meningitis in western Africa shows recurrent seasonal patterns every year.
        Epidemics typically start at the beginning of February and last until May. We can try to
        explain the observed patterns on the basis of some seasonally varying environmental factor
        that favors disease transmission. Air dryness produced by strong dust winds is the most
        likely candidate.
        But while there are qualitative “stories” of this kind in the literature for many
        seasonal phenomena, convincing quantitative evidence to support them remains largely
        elusive. Instead, we tend to see weak associations between environmental and transmission
        variables when measured by simple, linear correlations. The study of meningococcal
        meningitis in Mali by Sultan and colleagues in this issue of 
        PLoS Medicine is a remarkable exception [1]. The study reports a strong
        association between the yearly onset of epidemics and a large-scale regional index for
        atmospheric circulation related to the Harmattan winds in Sahelo-Sudanian Africa.
      
      
        The Importance of Seasonality
        Why is a focus on the seasonality of infectious diseases and its variation from year to
        year so important? Isn't it more important for us to instead understand the effects of
        long-term climate change on human health?
        At first sight, understanding seasonal patterns seems disconnected from understanding
        the impact of long-term climate change. However, seasonal patterns are one major pathway
        for the subtle but potentially drastic effects of climate change on disease dynamics.
        Long-term climate change affects seasonal patterns through the lengthening of the
        transmission season and the crossing of environmental and demographic thresholds that
        underlie seasonal outbreaks [2]. Thus, identifying the specific environmental factors
        underlying seasonal transmission is a critical step towards predicting and understanding
        how long-term environmental trends in mean climate and their variability will impact human
        health.
      
      
        The Problem of Scale
        One important difficulty in uncovering seasonal drivers of infectious diseases is to
        identify the appropriate scale of analysis. The relationship between disease and climate
        described by Sultan and colleagues only becomes apparent at large spatial scales. The
        authors argue that these large scales are necessary to eliminate “idiosyncratic”
        variability in the relationship between cases and climate at the local level. In other
        words, there are only weak correlations between seasonal variations and climate variables
        at small scales because of the multiple other factors that play a local role and act as
        noise.
        But we should be cautious about the suggestion that appropriate larger scales will
        always resolve the problem of local variability and present strong linear associations
        between climate and disease. Public health measures might require predictions not only at
        national and regional scales, but also at a variety of smaller scales.
        Moreover, one important source of variation in how infectious diseases respond to
        climate is the fraction of susceptible individuals in the population. This fraction varies
        over time as the result of immunity acquired by previous infection, and by the input of
        births and migrants into the pool of susceptible people. The constant waxing and waning of
        this pool of hosts underlies the intrinsic potential of the population dynamics of
        infectious diseases to oscillate and create epidemic outbreaks. The tendency of these
        intrinsic cycles to go up and down in synchrony at different locations in space will
        determine whether susceptibility levels act as noise at small scales or, alternatively,
        whether their effect must be considered in conjunction with climate at larger scales.
        Because the number of susceptible individuals is a hidden variable in most epidemiological
        analyses, recently proposed methods for its reconstruction from data on cases must be
        combined with studies on climate variation if we are to understand the interaction between
        susceptibility levels and climate variation [3,4,5].
        The problem of scale also arises when we need to identify the appropriate timing (the
        temporal window) to detect strong associations between disease outbreaks and environmental
        covariates. This is particularly important when strong couplings between environment and
        transmission occur only transiently. This seems to be the case for cholera in Bangladesh,
        where couplings are strong during El Niños, but considerably weaker the rest of the time
        [6]. Intermittent couplings provide insight into how the system might behave if pushed into
        specific dynamic regions by a change in climate. Intermittent couplings also suggest the
        existence of thresholds in the response to climate, an area of research that remains in
        need of quantitative approaches.
      
      
        Seasonal Drivers May Be Elusive
        Besides scale, specific seasonal drivers are often elusive because of the simpler reason
        that in nature seasonality is ubiquitous. Multiple and covarying drivers have been proposed
        for the seasonal nature of cholera, including temperature, rainfall, and plankton blooms
        [7]. Yet the specific roles of these drivers in the bimodal seasonal cycle of cholera, and
        particularly in the second peak in endemic regions in south Asia, have not been
        convincingly shown (Figure 1) [8]. We still don't have predictive explanations of the
        geographic variation in seasonal patterns. We won't find such explanations by considering
        the average seasonal pattern; instead, we must consider the anomalies in amplitude and
        onset of the peaks that occur in different years.
        Ecologists have considered seasonality in mathematical models of the population dynamics
        of infectious diseases. Models of populations with seasonally forced, dynamic interactions
        (births, deaths, aggregation, or disease transmission) reveal an array of possible
        responses, from simple yearly cycles, through cycles that repeat with longer periods, to
        irregular chaotic fluctuations. Some models also predict intermittent switching between
        different dynamic infectious disease behaviors. But typical models consider only simple
        seasonal forcing functions (mathematical functions that are periodic in time and therefore
        describe in a generic way the seasonal variation in the transmission rate or some other
        seasonal parameter—a sine wave is an example). There are some important exceptions to
        this—some models do incorporate more complicated seasonal forcing functions that describe
        the actual processes underlying the seasonal drivers of transmission. Examples are models
        of childhood diseases that describe the regular stopping and starting of school terms
        [9,10,11], and recent malaria models that include the seasonal dynamics of mosquito births
        and pathogen incubation as functions of temperature and rainfall [12].
        The explicit way in which models treat seasonal environmental drivers may be critical in
        addressing the links between within-year seasonal cycles and those of longer period that
        are observed in many infectious diseases. For meningococcal meningitis, we still need to
        examine the connection between the seasonal association described by Sultan and colleagues
        and the previously proposed role of humidity in inter-annual cycles [13].
      
      
        The Complexity of Infectious Disease Dynamics
        Sultan and colleagues' study is exceptional in that it illustrates a clear relationship
        between an external environmental variable and the initiation of disease outbreaks. In
        contrast, many studies seeking environmental drivers are plagued by the many confounding
        factors, particularly the impact that other components of global change have on the
        transmission dynamics of infectious diseases. Thus, when we examine datasets for malaria,
        we must also consider the evolution of drug resistance and a growing human population that
        is increasingly forced to live in areas that are marginal for agricultural production but
        optimal for malaria transmission.
        Given this complexity, a serious limiting factor to quantitative analyses and predictive
        models of ecological and disease patterns is the lack of long-term disease records with
        similar data collected over a network of spatial locations. The handful of extremely
        valuable records that have allowed progress in understanding long-term patterns in disease
        dynamics pale in comparison to the spatiotemporal coverage available for climate studies
        and modeling. The need to resolve these issues of scale and confounding variability only
        underscores the urgency and importance of maintaining and developing systematic
        surveillance programs for infectious diseases around the world.
      
    
  

  
    
      
        
        Physicians now commonly advise older adults to engage in mentally stimulating activity
        as a way of reducing their risk of dementia. Indeed, the recommendation is often followed
        by the acknowledgment that evidence of benefit is still lacking, but “it can't hurt.” What
        could possibly be the problem with older adults spending their time doing crossword puzzles
        and anagrams, completing figural logic puzzles, or testing their reaction time on a
        computer? In certain respects, there is no problem. Patients will probably improve at the
        targeted skills, and may feel good—particularly if the activity is both challenging and
        successfully completed.
        But can it hurt? Possibly. There are two ways that encouraging mental activity programs
        might do more harm than good. First, they may offer false hope. Second, individuals who do
        develop dementia might be blamed for their condition. When heavy smokers get lung cancer,
        they are sometimes seen as having contributed to their own fates. People with Alzheimer
        disease might similarly be viewed as having brought it on themselves through failure to
        exercise their brains.
      
      
        What Does the Evidence Show?
        Three types of evidence are cited to support the idea that mental exercise can improve
        one's chances of escaping Alzheimer disease.
        
          Epidemiological studies
          Having more years of education has been shown to be related to a lower prevalence of
          Alzheimer disease in cross-sectional, population-based studies [1] and to a lower
          incidence of Alzheimer disease in cohorts followed longitudinally [2]. Typically, the
          risk of Alzheimer disease is two to four times higher in those who have fewer years of
          education, as compared to those who have more years of education. Other epidemiological
          studies, albeit with less consistency, have suggested that those who engage in more
          leisure activities, especially activities that are mentally stimulating, have a lower
          prevalence and incidence of Alzheimer disease [3,4]. Additionally, longitudinal studies
          have found that older adults without dementia who participate in more intellectually
          challenging daily activities show less decline over time on various tests of cognitive
          performance [5].
          In epidemiological studies, people cannot be randomly assigned to different levels of
          education, or to different kinds and levels of participation in leisure activities.
          Consequently, researchers must try to identify confounders and take them into account
          analytically. However, uncertainties remain. Both education and leisure activities are
          imperfect measures of mental exercise. For instance, leisure activities represent a
          combination of influences. Not only is there mental activation, but there may also be
          broader health effects, including stress reduction and improved vascular health—both of
          which may contribute to reducing dementia risk [6]. It could also be that a third factor,
          such as intelligence, leads to greater levels of education (and more engagement in
          cognitively stimulating activities), and independently, to lower risk of dementia.
          Research in Scotland, for example, showed that IQ test scores at age 11 were predictive
          of future dementia risk [7].
          Another problem with these epidemiological studies is that reverse causation could be
          involved—in other words, that incipient dementia could be causing reduced engagement in
          leisure activities, although some prospective studies have been particularly attentive to
          controlling for this possibility [8]. Clinical trials are needed to test the hypotheses
          that emerge from the best epidemiological research. Moreover, because the onset of
          Alzheimer disease can be hard to pinpoint, and early changes may occur years before the
          disease is diagnosed, conclusions must be based on large samples, followed over a long
          period of time.
        
        
          Randomized clinical trials
          Many studies support the possibility of enhancing memory and other cognitive
          performance, or of slowing cognitive decline in older adults without dementia [9]. The
          most effective programs teach mnemonic strategies, provide practice, and give supportive
          feedback. Mnemonic strategies include the organization of items into meaningful groups,
          the use of imagery, and the method of loci (visualizing items to be remembered in a
          sequence of specific, well-learned locations). Comprehensive programs can also include:
          encouraging memory aids (such as appointment books), teaching relaxation techniques, and
          providing instruction about memory changes in normal aging. However, improvements are not
          found in all studies. When improvements are found, they are often modest, may not be
          maintained over time, and do not generalize beyond the skill being trained. Often, the
          subjective gains rival the objective ones; for example, participants do tend to report
          fewer complaints about their memory.
          These limitations are evident in one of the largest randomized controlled trials of
          cognitive training with older adults, a large, multisite study named ACTIVE (Advanced
          Cognitive Training for Independent and Vital Elderly) [10]. Participants were assigned to
          receive training in one of three cognitive skills: memory, reasoning, or speed of
          processing. Tests of cognitive abilities given immediately after training showed large
          improvements on the particular cognitive skill on which the individual had been trained,
          but no transfer to the other two cognitive domains. Additionally, for the control group
          that received no training, simply taking the test battery at pre-test led to improvement
          on the post-test. The effects of training were maintained over a two-year follow-up.
          However, the cognitive training program had no significant effect on measures of everyday
          functioning. Finally, for participants in ACTIVE or in other memory training programs, it
          remains unknown whether eventual rates of Alzheimer disease will be reduced.
        
        
          Neurobiology studies
          The third type of evidence suggesting that mental exercise may help to prevent
          Alzheimer disease comes from neurobiology studies that show greater brain complexity in
          those with higher levels of mental activity. Many such studies, done with animals, show
          greater neural complexity after having been exposed to an enriched environment that
          provides lots of stimulation, for example by including wheels, tunnels, toys, and gnawing
          sticks [11]. One human study with magnetic resonance spectroscopy showed changes in the
          hippocampus in elderly memory training participants compared to controls [12]. Another
          report found changes on positron emission tomography scanning following two weeks of a
          comprehensive memory program that included memory training, special diet, physical
          exercise, and stress reduction [13].
        
      
      
        Mental Exercise and Cognitive Reserve
        The concept of cognitive reserve is often used to explain why education and mental
        stimulation are beneficial. The term cognitive reserve is sometimes taken to refer directly
        to brain size or to synaptic density in the cortex. At other times, cognitive reserve is
        defined as the ability to compensate for acquired brain pathology. This definition
        encompasses coping skills as well as recruitment of other brain areas, with cognitive
        reserve thus accounting for individual differences in severity of cognitive dysfunction
        when there are pathological neural changes. People with a higher level of education have
        greater cognitive reserve. In some studies, education or occupation are even used as proxy
        measures of cognitive reserve, while others are beginning to measure neural substrates that
        correspond to reserve [14].
        Taken together, the evidence is very suggestive that having greater cognitive reserve is
        related to a reduced risk of Alzheimer disease. But the evidence that mental exercise per
        se can increase cognitive reserve and stave off dementia is weaker. Epidemiological studies
        suggest that individual differences in cognitive reserve may actually be lifelong. In
        addition, people with greater cognitive reserve may choose mentally stimulating leisure
        activities and jobs, leading to a chicken-and-egg dilemma for the interpretation of the
        relationship between mentally stimulating activities in adulthood and dementia risk.
        Cognitive training has demonstrable effects on performance, on views of self, and on brain
        function—but the results are very specific to the skills that are trained, and it is as yet
        entirely unknown whether there is any effect on when or whether an individual develops
        Alzheimer disease. Further, the types of skills taught by practicing mental puzzles may be
        less helpful in everyday life than more prosaic “tricks,” such as concentrating, or taking
        notes, or putting objects in the same place each time so that they won't be lost.
      
      
        Conclusion
        So far, we have little evidence that mental practice will help prevent the development
        of dementia. We have better evidence that good brain health is multiply determined, that
        brain development early in life matters, and that genetic influences are of great
        importance in accounting for individual differences in cognitive reserve and in explaining
        who develops Alzheimer disease and who does not. At least half of the explanation for
        individual differences in susceptibility to Alzheimer disease is genetic, although the
        genes involved have not yet been completely discovered [15]. The balance of the explanation
        lies in environmental influences and behavioral health practices, alone or in interaction
        with genetic factors.
        For older adults, health practices that could influence the brain include sound
        nutrition, sufficient sleep, stress management, treatment of mood or anxiety disorders,
        good vascular health, physical exercise, and avoidance of head trauma. But there is no
        convincing evidence that memory practice and other cognitively stimulating activities are
        sufficient to prevent Alzheimer disease; it is not just a case of “use it or lose it.”
      
    
  

  
    
      
        Ross McKinney's Viewpoint: Universities Should Be Allowed, Provided the Trial Is
        Approved by an External Review Board
        One of the principal missions of an academic health center is to advance the
        understanding and treatment of disease through clinical research. In this pursuit, there is
        a need for checks and balances. When Jesse Gelsinger, a relatively healthy young adult,
        died in Philadelphia during a clinical trial of a novel adenovirus-based genetic therapy
        for ornithine transcarbamylase deficiency, it was a tragedy [1]. In retrospect, there were
        many clues that there were problems with the adenovirus vector, clues that neither the
        investigator nor the institution pursued.
        Attorney Alan Milstein made the case that the investigator and institution were both
        blinded to these problems by their heavy financial investment in the technology, an
        investment worth millions of dollars [2]. Though the legal case was settled out of court,
        it created a de facto standard that institutions with commercial rights in a new drug or
        technology should not be allowed to pursue clinical trials involving that new technology. I
        do not believe that such a blanket prohibition is necessary.
        At its core, the issue revolves around conflicts of interest. In clinical research, the
        investigator should be primarily an advocate for the patient or volunteer. The core reason
        to perform clinical research is to create generalizable knowledge about a therapy, patient
        population, or a disease process with the long-term intent of improving human health. The
        interests of the patient and investigator should be fully aligned. However, most physicians
        in clinical research have other, more personal motivations, intermixed with the desire for
        progress. Successful research projects can lead to publications, promotions, grant
        renewals, and per case clinical trial enrollment fees. Some investigators have intellectual
        property rights that may have very substantial financial value if the drug or device
        reaches the level of approval by the United States Food and Drug Administration. These
        investigators stand to gain personally if the clinical trial is successful, a situation
        that has the potential to distort the investigator's objectivity, and may lead to a less
        honest relationship with study volunteers.
        In order to ensure that investigators are honest with potential research volunteers, the
        system of institutional review boards (IRBs) evolved. The IRB approves the informed consent
        document, which should describe the clinical experiment in a clear and dispassionate way to
        patients and their families. IRBs are largely made up of faculty and staff from the
        institution, although there are also public members and nonscientists on most IRB panels.
        The IRB must remain autonomous and be able to hold up or stop an investigation. There is an
        obligation that the IRB first and foremost think about patient rights and safety.
        The passage of the Bayh–Dole Act in 1980 enabled universities to license inventions for
        commercial development [3]. The closer to Food and Drug Administration approval the drug or
        technology is at the time of licensure, the more valuable it becomes. Therefore,
        universities have an incentive to advance the clinical development of inventions by their
        faculty. In this regard, they are very much like corporate sponsors of research, subject to
        the same Food and Drug Administration oversight as corporations.
        In terms of performing clinical trials using new technologies in which it has a
        financial interest, how is a university different from a corporate sponsor? In regard to
        patient safety, one primary distinction rests with the IRB. The corporate sponsor will
        present the research protocol to an independent commercial IRB, the university to its own
        IRB. Yet in both cases, there are potential conflicts of interest. The university IRB
        members will have a conflict of interest between the investments of their employer and the
        rights of the research volunteers. Independent commercial IRBs depend on pleasing corporate
        customers for their continued existence, and there is an unstated expectation that they
        will both be fast and produce rulings consistent with corporate expectations (which in most
        cases include a desire to do the research ethically).
        At the university level a logical and conservative solution to the problem of
        institutional conflicts is to require that an IRB from outside the institution become the
        IRB of record when such a conflict arises. This external IRB could be either an independent
        commercial IRB or one of another university. The key is to grant the IRB independence and
        the authority to provide real oversight.
        There are other elements of conflicts of interest that need to be considered when the
        institution has a commercial interest, but most have more to do with the management of
        personal conflicts than institutional conflicts. The institution needs to assure the
        presence of an independent data safety monitoring board, thorough audits of good clinical
        practice, and a publications committee that will ensure submission of all meaningful study
        results, whether positive, negative, or neutral. Anyone subjectively evaluating patient
        data should be as free of conflicts as possible. These steps can be formulaically required,
        which should allow for performance of clinical research despite the presence of
        institutional commercial interests.
      
      
        David Korn's Viewpoint: Academic Biomedical Research Must Be Free from the Taint of
        Financial Compromise
        United States research universities, and especially their academic medical centers, have
        greatly benefited from their uniquely privileged status in our society. That status is
        rooted in public confidence and trust that these institutions and their faculties will be
        independent and impartial in fulfilling both their academic mission to create, transmit,
        and preserve knowledge, and their duty to the general society to serve as credible,
        trustworthy arbiters of knowledge. One important mark of this status has been the
        remarkably consistent generosity of public support for biomedical research. Another has
        been the noteworthy deference of the federal government to university autonomy, and the
        light hand with which the sponsoring agencies historically have overseen the conduct of
        university research.
        When federal interposition occurred, it typically responded to widely publicized
        episodes of research misconduct, sometimes intertwined with egregious financial self
        interests of investigators; these episodes legitimately questioned the effectiveness of
        institutional oversight. Nevertheless, regulations consistently focused more on defining
        the metes and bounds of the permissible than on prescriptive mandates, and their
        implementation was effected largely through the mechanism of “assurances”—commitments that
        institutions would faithfully safeguard the specified perimeters of acceptable conduct.
        Awardee institutions thus bear primary responsibility for assuring the credibility and
        integrity of federally sponsored research. Public confidence in the trustworthiness of
        these institutions is critical, and yet nowhere is it more fragile than in biomedical
        research involving human participants. That confidence eroded in the 1980s and 1990s
        because of reports of scientific misconduct and of individual and institutional financial
        self interests in clinical trials. Scathing reports from federal oversight agencies and
        angry congressional hearings questioned whether financially self-interested institutions
        could any longer be trusted to guard the welfare of research participants or the integrity
        of clinical research.
        In 2001, the Association of American Medical Colleges convened a task force to examine
        and make recommendations on individual and institutional financial self interests in
        clinical research. The task force began by recognizing four important trends over the past
        three decades. First, the nature and culture of academic biomedical research have changed,
        bringing the potential of commercial relevance even to the most fundamental of scientific
        discoveries. Second, there has been enormous growth in the extent and depth of interactions
        between research universities and industry, especially in biomedicine. Third, the public
        has become increasingly impatient that its extraordinary investments in research yield more
        effective disease preventions and therapies. Fourth, the involvement of academic
        researchers in the translation of their discoveries has been essential in bringing those
        discoveries to market and to the benefit of public health.
        But the task force, in its two reports, asserted that both individual and institutional
        financial conflicts of interest in clinical research could be problematic [4,5]. It
        recommended urgent and substantial refinement and strengthening of institutional policies
        and practices for monitoring, managing, and—when necessary—extinguishing such
        conflicts.
        Both reports rest on a common set of core principles. The most important is that
        institutions should regard all significant financial interests in research involving human
        participants as potentially problematic. Where such interests exist, there should be a
        rebuttable presumption that the concerned individual or institution should not conduct the
        research, absent compelling circumstances. Importantly, the task force, after intense
        debate, rejected categorical prohibitions lest they unintentionally impede the translation
        of research discoveries into tangible public benefits.
        The task force acknowledged that the issue of institutional financial self interests is
        extraordinarily complex and sensitive, since it touches the very core of institutional
        autonomy. But the fact that an institution has a financial interest per se should raise a
        strong presumption against its participation in the clinical testing of that product.
        Public accountability and scientific integrity require that all research results emanating
        from academic medicine be as free as possible from the taint of financial compromise.
        Adding human participants to the research mix should raise the barrier to the highest level
        and require compelling justification for any participation by a financially self-interested
        institution.
        The task force did not define “compelling,” believing that each institution should make
        that determination based on disinterested scrutiny of the facts and circumstances of each
        case. For example, there may exist in a given institution a unique capability, without
        which the proposed research involving human participants could not be conducted as
        effectively or safely, or at all. In these instances, the public and science deserve access
        to that capability, provided the necessary safeguards are put in place to mediate the
        conflicting interests. In all such instances, protection of scientific integrity and the
        welfare of research participants must remain the foremost priority of both investigator and
        institution.
        This narrow window avoids absolute prohibition while striving to prevent institutional
        participation where credible alternatives exist. Only by such stringent self-policing can
        we sustain the trustworthiness and credibility of biomedical research, researchers, and
        their institutions, while continuing vigorously to promote the translation of biomedical
        discovery for the public's benefit.
      
      
        Ross McKinney's Response to David Korn's Viewpoint
        The public has every right to expect that academic institutions are working first for
        the public's interest. This value is even codified in the laws granting these institutions
        tax-exempt status. The public also expects that new, more effective therapies will be
        developed swiftly as a consequence of its support for academic research.
        The inventor of a new technology is always more motivated to see it through to
        widespread use than anyone else. This motivation, which may be as simple and benign as
        curiosity or as easy to understand as a financial incentive, is a powerful force driving
        human research. This force can be disciplined and controlled by the IRB and policies on
        conflicts of interest. Personal investment in research is, nevertheless, an important
        driver of scientific progress.
        When society makes an unnecessarily broad assumption that nearly all research with
        financial implications for investigators or their institution is potentially corrupted, a
        brake is placed on progress. Society will be better served by establishing clear guidelines
        and formalizing oversight of the research process than by rigidly limiting clinical
        research affected by conflict of interest. As examples, clinical trials should have
        independent data safety monitoring boards charged to review the study design, execution,
        data analysis, and publication of results. The IRB system should be strengthened in its
        independence through the use of community members. And, to be certain that institutional
        conflict of interest is avoided, an IRB from outside the institution in most cases will be
        preferable
        Society wants better treatments. The fact that an inventor has mixed motives for
        developing a new treatment has always been acknowledged. The need to carefully manage the
        experimental process in human studies has always been understood. However, when rules
        minimize the role of inventors at academic centers, by forcing trials of their new ideas to
        go to outside institutions, society loses more than it gains. The incremental gain in
        safety is likely to be small (particularly if oversight is well established), while the
        decrease in speed of development will be significant.
      
      
        David Korn's Response to Ross McKinney's Viewpoint
        There are many similarities between the position espoused by Ross McKinney and my
        position. Most saliently, we both recognize the critically important role played by
        academic biomedical scientists in making discoveries and in facilitating their efficient
        translation into beneficial products. Neither of us proposes that academic investigators,
        or their institutions, should be flatly prohibited from trying to foster that translation
        in the presence of financial self interests.
        But there is an important difference. McKinney's approach for dealing with institutional
        conflicts of interest depends critically on the engagement of external agents to monitor
        closely both scientific integrity and the welfare of human participants. That, in my view,
        would require such deep interposition of those agents into the conduct of academic research
        as to be not only unprecedented but unfeasible. Beyond that, the approach falls short with
        respect to the maintenance of institutional trustworthiness and protection of public
        trust.
        Routine clinical assessment of technologies by financially interested institutions
        fosters public cynicism and distrust of the motives of academic biomedical researchers. The
        protective mechanisms recommended by McKinney are opaque to the public and reflect a
        “business as usual” image that fails fully to account for the markedly changed
        circumstances and perceptions of academic biomedical research. Most important, the
        mechanisms appear to be aimed primarily at protecting institutions' financial
        interests.
        By contrast, the American Association of Medical Colleges formulation urges that any
        institutional involvement in clinical research involving human participants in the presence
        of financial conflicts must be predicated on the presence or absence within the institution
        of demonstrably unique capability. This approach offers a much higher and more credible
        standard that aims to protect not only participant well-being and scientific integrity, but
        also institutional trustworthiness and public trust.
      
    
  

  
    
      
        Introduction
        The protective effects of antibody to pneumococcal capsular polysaccharides have been
        appreciated since the development of serum therapy, in which passively transferred,
        serotype-specific antipneumococcal serum reduced mortality from pneumococcal pneumonia by
        half [1]. The development of pneumococcal polysaccharide vaccines for adults [2] and the
        efficacy of pneumococcal polysaccharide–protein conjugate vaccines in infants and children
        [3,4] have confirmed that active immunity to the polysaccharide can provide excellent
        protection against invasive disease from pneumococci of the same serotype, and in some
        cases protection against cross-reacting serotypes within the same serogroup.
        While the ability of passive or vaccine-induced anticapsular antibodies to protect
        against pneumococcal disease is clear, less is known about the natural development of
        immunity to pneumococcal disease in unimmunized persons. In unimmunized populations, the
        incidence of invasive disease follows a well-known age distribution, peaking in the first 2
        y of life, declining by more than an order of magnitude by the second and third decades of
        life, and then rising at an accelerating pace, with incidence in persons over 70 y
        approaching that in infants [5]. The reason for the decline in incidence has not been
        conclusively determined, yet it is often suggested that the acquisition of anticapsular
        antibodies plays a critical role in this decline [6,7]. Indeed, it has been proposed that
        the human immune system sees each serotype of 
        Streptococcus pneumoniae as a distinct, independent pathogen
        [8].
        The hypothesis that protection from invasive pneumococcal disease is caused by the
        acquisition of anticapsular antibodies directed against each of the pneumococcal serotypes
        yields two simple predictions about the age-specific epidemiology of pneumococcal disease.
        First, it predicts that the age-specific timing of the decline in invasive disease should
        be different for different serotypes: those that are rare, poorly immunogenic, or both
        should decline later in life than those that are common and immunogenic. Second, it
        predicts that protection against invasive disease from a given serotype should coincide
        temporally with the acquisition of anticapsular antibody to that serotype, both at an
        individual level and at a population level. We tested these predictions using data from the
        United States, Finland, and Israel.
      
      
        Methods
        
          United States Dataset
          Incidence of invasive pneumococcal disease was measured in eight sites around the
          United States participating in the Centers for Disease Control and Prevention's Active
          Bacterial Core Surveillance between 1994 and 1999. The data used here are restricted to
          those periods during which serotyping was routinely performed: 1994–1999 for the Georgia
          site, 1995–1999 for the Minnesota site, and 1998–1999 for all other sites [5]. Data were
          not available on the timing of anticapsular antibody acquisition in these same
          populations, but we compared the timing of the decline in pneumococcal disease against
          previously published data on age-specific prevalence of anticapsular antibody levels
          greater than 0.2 mcg/ml [9].
        
        
          Israel Dataset
          Antibody concentrations were measured by enzyme-linked immunosorbent assay (ELISA)
          (with absorption by cell wall polysaccharide but not by 22F polysaccharide) in blood
          samples that were obtained from 130 toddlers at enrollment and at approximately 12 and 24
          mo after enrollment in a double-blind, controlled trial of a nine-valent pneumococcal
          conjugate vaccine. The toddlers analyzed for this study were those in the control group,
          which received meningococcal C conjugate vaccine; the details of the trial [10] were
          previously described. Preliminary analyses of these data confirmed previous findings [11]
          that ELISA measurements were highly correlated (and therefore likely revealed
          cross-reactions) for all pairs of serotypes, except for type 14, for which correlations
          were minimal, consistent with previous findings of little cross-reaction. For this
          reason, we chose to analyze age trends only in serotype 14 antibodies.
        
        
          Finland Dataset
          Mandatory reporting from all microbiological laboratories in Finland to the National
          Register of Infectious Disease (http://www3.ktl.fi/stat/) identified all blood and
          cerebrospinal fluid isolates of 
          S. pneumoniae obtained in the years 1995–2001. Incidence
          within 6-mo age groups was calculated using population denominators obtained from
          Statistics Finland (Helsinki, Finland). Since the primary purpose of examining incidence
          in Finland was to compare age-specific rates against published distributions of antibody
          concentrations for the same age groups [12], we restricted our attention to serotype 14
          and serogroup 6, for which subsequent investigations suggested antibody measurements in
          Finland were relatively unaffected by cross-reactions [13] (ELISA measurements for
          published data from Finland used a special type 6B polysaccharide that was found to
          minimize cross-reactions [13]).
        
      
      
        Results
        
          United States Findings
          Figure 1 shows the age-specific incidence of invasive pneumococcal disease, by
          capsular serogroup, obtained from population-based active surveillance in the United
          States prior to the introduction of the conjugate vaccine. Figure 2 shows age-specific
          incidence by type of infection, for the same age range.
          Incidence peaks between the ages of 9 and 15 mo, and falls in an approximately
          parallel fashion thereafter, for each of the seven most important serogroups (which are
          those included in the seven-valent conjugate vaccine) and for the remaining serogroups
          put together. The same pattern is observed for both pneumonia and bacteremia. For each
          serogroup, incidence by age 24 mo is approximately half that in the peak age group, and
          by 36 mo, incidence for each serogroup has fallen to 10%–25% of its peak.
          The consistent timing of the pattern across multiple serogroups argues for a common
          mechanism, rather than for independent acquisition of immunity to each serogroup as a
          separate event. Since most individuals do not suffer from invasive pneumococcal disease
          in this age range, carriage or mucosal disease (otitis media) from pneumococci may be the
          immunizing event for anticapsular antibodies in the general population [12] (although in
          principle immunity to some serogroups could be generated in response to cross-reacting
          antigens from other bacterial species or other sources [14]). Different serogroups have
          vastly different frequencies among pneumococci isolated from carriage [12,15,16,17] and
          otitis media [12,18]; for example, serogroups 4 and 18 and the non-vaccine serogroups are
          isolated far less commonly than several of the other pneumococcal types identified in
          Figure 1. One could postulate that these differences in frequency of carriage are offset
          by differences in immunogenicity; however, there is little evidence that serotypes 4 or
          18C are more immunogenic than other, far more common serotypes [4,15]. One could also
          postulate that the frequency of isolation of serotypes from carriage depends on duration
          as well as incidence, so that the serotypes for which carriage appears rare are simply
          carried for a shorter duration. While the data to address this speculation are limited,
          the duration of carriage of types 4 and 18C seems to be comparable to that of other, more
          frequently carried serotypes [15,16]. Thus, the most parsimonious interpretation of the
          data on the timing of the decline in age-specific susceptibility is that one or more
          common mechanisms are responsible for the decline in disease from all serotypes.
          Testing the second prediction against data is hampered by the fact that, to our
          knowledge, no study has characterized the age-specific distribution of antibody
          concentration in a large population using the currently accepted methodology, which
          includes absorption with both cell wall polysaccharide and serotype 22F polysaccharide
          [13,19]. Analyses by Soininen and colleagues have found that antibodies measured by
          standard ELISA in unimmunized children are highly cross-reactive between different
          serotypes, and that cross-reactive antibodies lack opsonophagocytic function and often
          appear in the absence of any documented exposure to a given capsular serotype. As a
          result, age-specific antibody concentration data for any given serotype are
          “contaminated,” to a greater or lesser degree, by cross-reactive antibodies with other
          specificties.
          The most important exception to this problem occurs for antibodies to serotype 14, for
          which cross-reaction is minimal [11]. A recent publication describes the age-specific
          proportion of children in the United States with anti-type-14 polysaccharide antibody
          concentration exceeding the putative protective concentration of 0.2 μg/ml (Figure 3 of
          [9]). At 12 mo, 90%–95% of the population falls below this level, and at 24 mo, 80%–85%
          remains below it—despite a 40%–50% drop in disease incidence from 12 mo to 24 mo. At 36
          mo, 75% of children remain below the putative protective level, although by this age
          incidence has fallen more than 80% from its 12-mo peak. In summary, if the 0.2-μg/ml
          concentration were truly the threshold for “protection,” the 20%–30% reduction in the
          unprotected population between ages 12 and 36 mo would be inadequate to explain the 90%
          decline in disease incidence. Clearly, 0.2 μg/ml is not a precise dividing line between
          being “protected” and “unprotected,” a threshold that (if it exists) may vary by
          serotype, but given the available data, there is reason to doubt that anti-type-14
          antibody alone is responsible for the decline in disease in this age range.
        
        
          Israel Findings
          To assess whether the limitations of the United States antibody described above—i.e.,
          the availability of only one cutoff point for antibody concentrations—might be providing
          an incomplete picture of the distribution of antibody levels by age, we examined an
          additional dataset from Israeli toddlers. For the reasons described above, we examined
          only antibodies to serotype 14, for which the distribution of concentrations by age is
          shown in Figure 3. These data indicate that between the ages of 12–17 and 36–41 mo, the
          median antibody concentration increases by about 2-fold. These data are broadly
          consistent with those published for the United States; antibody levels rise very
          gradually, though detectably, during the second and third years of life. It is difficult
          to believe—albeit not impossible—that the dramatic declines in disease incidence over
          these years are explained simply by this small rise in antibody concentrations.
        
        
          Finland Findings
          Incidence of serogroup 6 and serotype 14 invasive pneumococcal disease by 6-mo age
          groups in Finland, shown in Figure 4, is broadly similar to that found in the United
          States, albeit with lower absolute incidence for both serogroups. Peak incidence occurs
          in the 12–17-mo age group, and incidence declines to 25%–30% of its peak rate by 24–29 mo
          of age. This decline in incidence may be compared against the cumulative distributions of
          antibody concentrations in Finnish toddlers shown in Figure 2 of [12]. Between ages 12
          and 24 mo, there is a discernible increase in the concentration of antibodies in the
          population, but the median concentration increases by only about 2-fold in this period.
          Moreover, the proportion of the population with antibody concentration below any
          particular threshold that may indicate protection changes little in this period. For
          example, the proportion of the population with anti-type-14 antibody concentrations less
          than 0.2 μg/ml declines from approximately 55% to approximately 40%, and the proportion
          with less than 0.5 μg/ml is reduced from about 95% to about 80%. Similar patterns are
          seen in the Finnish antibody data for type 6B [12]. Thus, as in the Israeli data, only a
          very small shift in the distribution of type-specific anti-polysaccharide antibody
          concentration is observed during the second year of life, yet incidence of invasive
          disease from the serotypes in question declines substantially.
        
      
      
        Discussion
        We have assessed two lines of epidemiological evidence, analyzed ecologically, that bear
        on the role of anticapsular polysaccharide antibody as the determinant of protection
        against invasive disease that develops during the second and third year of life. The
        simultaneous and approximately parallel nature of the decline in disease incidence for the
        seven most important serogroups in the United States suggests that one mechanism, rather
        than seven independent mechanisms, account for the declines in invasive disease from these
        serogroups. Moreover, only a slight increase in anticapsular antibody concentration is
        measurable in Finnish, United States, and Israeli toddlers during the same age range. As we
        discuss below, each of these lines of evidence is subject to caveats, but we believe that,
        taken together, these observations make a strong case for the importance of one or more
        factors other than acquisition of anticapsular antibodies in the development of protection
        against pneumococcal disease.
        There are several possible candidates for mechanisms that could explain this age-related
        decline in pneumococcal disease. These include the following: acquisition of antibodies or
        cellular immune responses to noncapsular pneumococcal “species” antigens; age-related
        changes in host biology that are not related to acquired immunity, such as maturation of
        the innate immune system or changes in anatomy or receptors for pneumococcal attachment;
        changes in other risk factors, such as exposure; or changes related to other
        microorganisms, including changes in the resident flora or changes in the incidence of
        viral infections.
        Systemic antibodies to several pneumococcal protein antigens, which are conserved across
        pneumococcal strains and serotypes, develop following pneumococcal carriage and otitis
        media and are present by the beginning of the second year of life [20,21]. In both Finland
        [20] and Kenya [21], there is an increase in the concentration of antibodies to the
        pneumococcal proteins pneumolysin and pneumococcal surface protein A over the first two or
        more years of life. In Kenya, antibodies to another conserved protein, pneumococcal surface
        adhesin A, showed similar distributions in the first, second, and subsequent years of life,
        while in Finland, levels of these antibodies were already high (equivalent to adult levels)
        in the first year of life, and increased above these levels in the second year. In mice,
        either passively transferred human serum IgG against pneumococcal surface protein A or
        vaccine-induced antibodies to pneumococcal surface protein A and/or pneumolysin are
        protective against invasive disease. Such data are consistent with the hypothesis that
        antibodies to these, or perhaps other, conserved pneumococcal proteins are in part
        responsible for the decline in invasive disease in the second and subsequent years of
        life.
        A number of investigators have tested the hypothesis that antibodies to the pneumococcal
        teichoic acid, known as cell wall polysaccharide (CWPS), are capable of protecting
        individuals against pneumococcal invasive disease. While studies in animals [22] and humans
        [23] have failed to find a protective effect of antibodies to CWPS or its components, a
        recent study showed that passive transfer of human IgG against phosphorylcholine, a
        component of CWPS, could protect mice against invasive pneumococcal infections [24].
        Notably, such antibodies might be elicited by a number of bacteria in addition to
        pneumococci, such as 
        Haemophilus influenzae, which also produce phosphorylcholine. We are
        unaware of studies on the timing of acquisition of anti-CWPS antibodies.
        We have recently shown that mice that are exposed thrice at weekly intervals to
        intranasal colonization with encapsulated pneumococci are protected against subsequent
        carriage, that this protection is effective for heterologous as well as homologous capsular
        types, and that it is effective even in MuMT mice, which lack the ability to produce
        antibodies (Malley R, Trzcinski K, Srivastava A, Thompson CM, Anderson PW, et al.,
        unpublished data). We have also shown that intranasal immunization with unencapsulated,
        killed pneumococci protects against nasopharyngeal colonization, in a fashion that is
        independent of antibody but requires CD4+ T cells at the time of challenge. The relevance
        of cellular immune mechanisms in protecting humans against pneumococcal colonization or
        disease is not known.
        Another candidate for a factor that may be changing with age is susceptibility to viral
        infections, especially influenza, which may predispose to pneumococcal colonization [25] or
        disease [26,27]. Recent evidence from clinical trials of pneumococcal conjugate vaccines
        shows that the vaccines can reduce the incidence of infections such as bronchiolitis that
        are usually associated with viruses [28] and of documented, virus-associated pneumonia
        [27]. These findings raise the possibility that the decline in pneumococcal disease with
        age reflects, in part, a decline in the incidence or severity of viral infections, so that
        fewer such infections lead to secondary pneumococcal disease.
        Exposure to pneumococci probably changes in some fashion over the first 5 y of life.
        However, for changes in exposure to account for the sharp drop in disease incidence
        following the first birthday, it would be necessary for exposure also to drop severalfold
        per year over this age range. Studies of pneumococcal carriage do show gradual changes in
        the prevalence and serotype composition of the nasopharyngeal flora in these years, but the
        prevalence of carriage changes much more gradually than the incidence of invasive disease
        [29].
        We are not aware of data that bear strongly on the plausibility of other possible
        mechanisms for the age-related decline in pneumococcal disease, such as changes in anatomy,
        physiology, receptor expression, or resident bacterial flora. However, factors other than
        antibody—such as innate or acquired cellular immune responses, age-related anatomical
        changes, or changes in exposure to pneumococci—cannot be ruled out, and more than one
        factor may be involved. Indeed, the peak of pneumococcal meningitis incidence in the 3–6-mo
        age group (see Figure 2) suggests that the mechanism of protection against meningitis may
        differ from those against pneumonia and bacteremia.
        Although we suggest that anticapsular antibody is not primarily responsible for the
        age-specific decline in invasive pneumococcal disease, there is no question that the
        capsule is an important virulence factor that interacts with the innate and acquired immune
        system in a number of ways. It is clear that the pneumococcal capsule interferes with
        various host clearance mechanisms [30]. It would be unsurprising if different capsular
        types were differentially effective in permitting pneumococci to evade phagocytosis and
        other host defenses [31] (M. Melin, H. Jarva, S. Meri, and H. Käyhty, unpublished data). If
        this were the case, then one could envision that certain capsular types might in fact
        follow a different age-specific incidence. In particular, recent analyses suggest that
        serotypes 1 and 5 have relatively stable incidence over a range of age groups (W. P.
        Hausdorff, D. R. Feikin, and K. P. Klugman, unpublished data).
        The evidence adduced here is subject to several limitations. With respect to the
        relative timing of acquisition of protection against different serotypes, one could
        postulate that because some of the most common pneumococcal serotypes, such as 6B, 19F, and
        23F, are also among the least immunogenic [12], the effective exposure of the immune system
        is more consistent across serogroups than it appears from serogroup frequency alone.
        However, this pattern is not general; for example, serotype 14 is both very common and
        highly immunogenic [12]. With respect to the absolute timing of protection relative to the
        acquisition of antibody, one could argue that low levels of anticapsular antibody, perhaps
        of low affinity, may be present and even active at levels below those that can be reliably
        detected by current assays, or that B cell memory may be present and protective at an
        earlier age than that at which high levels of antibody are measurable. Inferences about
        protective antibody concentrations from animal studies and from concentrations achieved by
        vaccines suffer from several uncertainties. Making allowances for all of these limitations,
        we nonetheless believe the data suggest that mechanisms other than anticapsular antibody
        are primarily responsible for the age-specific decline in pneumococcal invasive disease
        that starts at the age of 1 y.
        The likelihood that mechanisms other than anticapsular antibody confer immunity to
        pneumococcal disease has important implications with respect to vaccine design. As
        experience with conjugate pneumococcal vaccines in children unfolds, it is becoming
        increasingly clear that such a strategy suffers from several limitations, including the
        possibility of serotype replacement (already confirmed in several clinical trials), a
        modest effect on nasopharyngeal colonization, limited serotype coverage, cost, and
        difficulties in production that have led to shortages since licensure. A better
        understanding of the mechanisms that underlie natural immunity to pneumococcus could pave
        the way for the development of more effective, species-specific pneumococcal vaccines.
      
    
  

  
    
      
        Introduction
        In June 2001, heads of state and government convened a United Nations Special Session on
        HIV/AIDS and adopted unanimously the “Declaration of Commitment on HIV/AIDS” [1]. In
        preparation for that session, Schwartländer et al. published an estimate of the resource
        needs for an expanded global response to the epidemic, which called for around US$10
        billion for the fight against HIV/AIDS in 2005 [2]. In 2002, on the occasion of the 14th
        International AIDS conference in Barcelona, Spain, Stover et al. showed that such an
        immediate and expanded response in low- and middle-income countries could reverse the
        course of the HIV/AIDS epidemic and avert nearly 30 million infections through 2010 [3].
        Today, more resources are available for the fight against HIV than ever before, but global
        efforts to confront the epidemic continue to disappoint. Worldwide in 2004, more people
        were living with HIV and more people died of AIDS than in any previous year [4]. In
        sub-Saharan Africa, home to two-thirds of all people living with HIV/AIDS, and three out of
        four people dying from AIDS [4], only one in 50 persons with advanced disease had access to
        life-saving medicines at the beginning of 2004 [5].
        The theme of the 15th International AIDS Conference in Bangkok last summer was timely
        and relevant. “Access for All” calls for extending to all of those in need both sufficient
        resources and a set of proven interventions to prevent new infections and save lives
        through effective treatment. Recent developments in HIV treatment, with simple combination
        therapies priced at less than US$150 per year—unthinkable just a short time ago—were a
        major driver of discussions during the conference. Widespread access to effective
        antiretroviral therapy (ART) for people living with HIV/AIDS is now conceivable even in
        countries with severely limited resources.
        The World Health Organization and its partners in the Joint United Nations Programme on
        HIV/AIDS have defined an ambitious “3 by 5” target of 3 million people on ART—half of those
        in most urgent need—by the end of 2005. The potential epidemiologic impact of large-scale
        roll-out of treatment programs, however, remains uncertain. Experience to date is limited,
        and comes mostly from Western countries and Brazil. While declines in AIDS mortality in the
        industrialized world have been impressive [6,7,8], many of these success stories have been
        accompanied by a resurgence in HIV incidence due to increasing risk behavior as emphasis
        shifted from prevention to treatment in the 1990s [9,10,11]. Will the extension of ART to
        millions who suffer from AIDS in developing countries be the long-awaited breakthrough in
        the response to HIV, or will the emphasis on treatment detract from prevention efforts, and
        thus hamper AIDS control in the medium and long term? The experience in high-income
        countries underscores the potential perils of failing to adapt prevention strategies to an
        environment in which life-saving treatment becomes available on a large scale; however,
        more favorable outcomes in some settings [12,13] indicate that rising risk behavior is not
        an inevitable outcome of increased treatment access.
        In our previous analysis of the potential benefits of a comprehensive package of
        preventive interventions [3], we noted that these prevention effects would be achieved only
        in the presence of wide-scale treatment and political support. The two intervening years
        have seen a dramatic rise in both momentum and financial resources for ART scale-up, but
        the potential epidemiologic impact of treatment in the context of a broader strategy for
        HIV/AIDS control has not yet been examined. In this paper, we quantify the opportunities
        and potential risks of large-scale treatment roll-out. The results of this analysis will be
        informative for all regions and countries, independent of the level and stage of the
        epidemic. However, since three of four deaths from AIDS occur in sub-Saharan Africa,
        successes and failures in rolling out treatments immediately will have the most dramatic
        effects in this region. We therefore focus our analyses and discussions in this paper on
        the HIV epidemics in sub-Saharan Africa
        .
      
      
        Methods
        
          Projections of HIV Epidemics in Sub-Saharan Africa
          Baseline projections of HIV epidemics in sub-Saharan Africa have been developed by the
          Joint United Nations Programme on HIV/AIDS and the World Health Organization based on the
          most current data available, and in collaboration with epidemiologic experts and analysts
          within the countries assessed [4]. These “business as usual” forecasts from 2004 to 2020
          are characterized by the absence of behavioral change or ART scale-up in future years.
          Combined with the natural dynamics of the epidemic, these assumptions result in a
          relatively stable HIV prevalence rate.
          To simulate the effects of prevention and treatment on HIV/AIDS incidence, prevalence,
          and mortality, we first adapted the analytic approach used in the previously described
          Goals model [3] to allow explicit modeling of treatment effects, and calibrated the model
          to the baseline projections for three African regions (East, West/Central, and Southern)
          (see Protocol S1 for more details). In line with the predominant epidemiologic pattern in
          sub-Saharan Africa of HIV spreading through heterosexual contact, the model divides the
          sexually active population into five different interacting risk groups: single men,
          single women, married men, married women, and female sex workers. In sub-Saharan Africa
          HIV is transmitted via other modes at comparatively low levels, and these modes were
          therefore not considered in our analyses.
          The model includes underlying regional demography, acquisition of HIV and other
          sexually transmitted infections (STIs), progression from HIV to AIDS, and progression
          from AIDS to death. Annual risks of HIV infection in each risk group depend on the number
          of partnerships, the number of sex acts per partnership, HIV prevalence among partners
          and condom use. These risks are magnified by the presence of other STIs [14] and also
          vary as a function of the time since infection, with the highest risks during acute
          infection, followed by lower levels that persist until viral loads rise with the onset of
          clinical AIDS [15,16,17].
          The regional models were calibrated as follows: first, plausible ranges were specified
          for model parameters governing sexual behavior and biological factors (e.g., transmission
          risks and cofactor effects of other STIs) based on review of published studies and survey
          results; second, multiple simulations were undertaken by sampling values from each of the
          ranges and recalculating the model for each set of sampled parameter values; third, model
          fit was assessed by comparing modeled prevalence for adult males and females separately
          to baseline projections through 2020; and fourth, the best-fitting parameter set in each
          regional model was selected for the purpose of scenario analysis (see Protocol S1).
        
        
          Alternative Scenarios for Prevention and Treatment
          Potential impacts of prevention efforts at a given coverage level were based on
          previously published estimates [3] for a comprehensive package of 12 interventions that
          included mass media campaigns, voluntary counseling and testing, peer counseling for sex
          workers, school-, youth- and workplace-based programs, condom promotion and distribution,
          treatment for STIs, and prevention of mother-to-child transmission. The comprehensive
          package described by Stover and colleagues also included interventions such as harm
          reduction for injecting drug users and peer outreach for men who have sex with men, which
          we have not modeled for sub-Saharan Africa. Impacts were captured in terms of changes in
          condom use, sexual partnerships, treatment-seeking for STIs, and age at first sex. The
          impacts of treatment included increased survival by a median of 3 y, reductions in
          transmission probabilities given contact with an infected partner, and behavior
          change.
          We examined a range of alternative scenarios based on various levels and effectiveness
          of prevention interventions, with and without successful attainment of the 3 by 5
          treatment target for sub-Saharan Africa:
          
            Baseline (“business as usual”).
            Risk behaviors are maintained at current levels, and no treatment scale-up occurs.
            This is simply the baseline scenario that produces a relatively stable prevalence rate
            over the duration of the projection, with the number of people living with HIV and the
            number of new infections rising slowly over time because of population growth.
          
          
            Treatment-centered response.
            In two alternative scenarios, the 3 by 5 target of 50% coverage of those in need of
            treatment by the end of 2005 is attained, and scale-up continues to reach 80% ART
            coverage of those in need by 2010, maintained at 80% thereafter. In an “optimal ART
            effects” scenario, we assumed that treatment reduces transmissibility by 99%, and that
            those under treatment have 50% lower annual partnership numbers and two times higher
            condom use than other adults. With a response that focuses primarily on treatment, it
            is assumed that behavior in the general community of infected and uninfected adults is
            unchanged from the baseline. In an alternative “mixed ART effects” scenario, less
            optimistic assumptions were made: that treatment reduces transmissibility only to the
            same levels as in asymptomatic infected individuals (two-thirds reduction from no
            treatment), and that behavior in treated patients is the same as in other adults. To
            capture the possibility of behavioral disinhibition in response to treatment
            availability, we assumed that condom use declines by 10% in both treated patients and
            the general community, with other behaviors unchanged. The potential for disinhibition
            is suggested primarily by experience in some developed countries, where condom use
            increased dramatically in the populations at highest risk prior to the introduction of
            ART but then declined; the likelihood and magnitude of reductions in condom use in
            sub-Saharan Africa, where such prevention-induced changes generally are much less
            prominent today, might be questioned. We therefore considered in sensitivity analyses a
            variant of this scenario that excludes disinhibition but preserves all other
            assumptions.
          
          
            Prevention-centered response.
            In the absence of wide availability of treatment, reflecting weaker political and
            social support for HIV control efforts, we modeled a scenario in which the
            comprehensive prevention package described previously [3] has only partial
            effectiveness at the population level, and no ART scale-up occurs. As evidence about
            the magnitude of treatment–prevention interactions remains limited, we considered a
            reduction of 50% from the full impact as a base case and examined a range of reductions
            from 25% to 75% in sensitivity analyses.
          
          
            Combined response.
            We examined two scenarios combining treatment and prevention efforts, reflecting
            either optimistic or pessimistic possibilities. In the optimistic scenario, treatment
            strengthens prevention efforts. ART coverage is the same as in the two
            treatment-centered scenarios, with optimal assumptions about treatment impact on
            transmissibility and patient behavior. It is assumed that widespread availability of
            treatment enables the full impact of prevention efforts to be attained as described by
            Stover et al. [3]. In a more pessimistic scenario, an emphasis on treatment leads to
            less effective implementation of prevention. This scenario includes the mixed
            assumptions about ART effects (excluding disinhibition in the general community), and
            assumes only 25% attainment of the maximum potential impact of prevention efforts.
            Additional scenarios could include pessimistic assumptions about limited ART
            scale-up levels and timing, emergence of large-scale drug resistance resulting from low
            adherence, or other possible unintended outcomes of wider treatment. Certainly,
            large-scale treatment efforts will demand close monitoring of adverse effects. However,
            experience with treatment programs in developing countries has been encouraging thus
            far, with reported adherence levels that are at least as high as those in developed
            countries [18,19].
          
        
      
      
        Results
        In the baseline projections for sub-Saharan Africa, the annual number of new adult HIV
        infections rises from 2.4 to 3.7 million between 2004 and 2020, and adult AIDS mortality
        rises from 1.8 to 2.6 million (Figure 1). If scale-up of ART reaches the 3 by 5 target and
        eventually expands to 80% coverage, without any behavior change in the broader community
        (treatment-centered response/optimal ART effects), the annual number of new infections
        could be reduced by up to 6% compared to baseline by 2020. Mortality would initially
        decline by 33% but long-term trends would converge toward the baseline. We note that total
        annual death numbers indicate broad trends in mortality but mask more subtle health gains
        in the form of years added to individuals' lives.
        With less optimistic assumptions (treatment-centered response/mixed ART effects), the
        number of new infections rises, to 4.3 million per year by 2020 (a 14% increase); mortality
        trends are similar to the optimistic scenario in the short term, but worse in the long
        term, even compared to the baseline. Excluding the assumption of reduced condom use through
        disinhibition from the treatment-centered/mixed effects scenario has minimal effect on the
        results, lowering the number of new infections in 2020 by only 2% compared to the scenario
        that includes disinhibition.
        A prevention-centered response would have greater impact on the number of new
        infections, lowering annual incidence by more than half by 2020. The long-term mortality
        trend is more favorable in the prevention-centered scenario than in the treatment-centered
        scenario because of reduced incidence, but prevention would produce negligible mortality
        benefits in the near- and mid-term future in comparison to strategies that include ART.
        Alternative assumptions regarding overall effectiveness in a prevention-centered response
        produce results that scale as expected, with reductions in annual incidence of 34% to 64%
        and reductions in annual mortality of 20% to 42% by 2020.
        If treatment and effective prevention are scaled up jointly in a combined response, the
        benefits in terms of both infections and deaths averted could be substantially higher. In
        an optimistic scenario in which treatment programs support expanded prevention, the annual
        number of new infections would be 74% lower and annual mortality would be 47% lower by
        2020, compared to baseline. It is worth noting that the long-term decline in AIDS deaths is
        driven more by prevention of new infections than by direct survivorship benefits from ART.
        In a pessimistic scenario in which a more narrow treatment focus limits effective
        prevention, the overall benefits are much more modest, with 26% and 16% reductions,
        respectively, in new infections and mortality by 2020 compared to the baseline.
        Prevalence rises by 7% in the optimal and by 27% in the mixed treatment-centered
        scenarios by 2020, as longer survival for treated patients offsets reductions in new
        infections through reduced transmissibility (and risk reductions among treated patients in
        the more optimistic scenario) (Figure S1). In scenarios that include prevention efforts,
        prevalence declines by 41% in the prevention-centered scenario, by 53% in the optimistic
        combined response, and by 6% in the pessimistic combined response by 2020.
        The total number of infections averted through a combined response would be 29 million
        over the period 2004 to 2020 if treatment enhances prevention, a benefit that is ten times
        greater than that of a strategy which focuses on treatment only, even with optimal
        assumptions, and 51% greater than that of a strategy which focuses on (less effective)
        prevention alone (Table 1). If a treatment focus limits the effectiveness of prevention, on
        the other hand, the total number of averted infections between 2004 and 2020 would be 9
        million. Similarly, the benefits of a combined response in terms of mortality reductions
        are considerably higher under optimistic circumstances than the benefits of either
        treatment only or prevention only, with 10.1 million deaths averted (27%) through 2020 when
        treatment enhances prevention, compared to 5.0 million (13%) in the optimal-effects
        treatment only scenario, 3.5 million (9%) in the mixed-effects treatment only scenario, and
        4.8 million (13%) with prevention only. Under more pessimistic assumptions about
        treatment–prevention interactions, the combined response would avert 5.8 million deaths
        (16%). Table 1 also reports total benefits of the various strategies over the shorter term,
        in which the ranking of alternatives is similar with regard to the total number of
        infections averted, but mortality reductions are attributable almost exclusively to
        treatment.
        Combining treatment with prevention efforts will reduce the resource needs for treatment
        substantially in the long term (Figure 2). In the various scenarios the numbers of people
        being treated in 2020 ranges from 9.2 million in the treatment only (mixed effects)
        scenario, to 4.2 million in the optimistic combined response scenario.
      
      
        Discussion
        In this paper, we have examined the potential epidemiologic impact of global HIV/AIDS
        control efforts under a range of alternative scenarios reflecting varying implementation of
        strategies for prevention and treatment. Although we focus in particular on population
        health outcomes and epidemiologic trends, we recognize that there are numerous other
        social, economic, and individual health effects of interventions including ART that are
        beyond the scope of this analysis. We also restrict our focus in this paper to sub-Saharan
        Africa, where the overwhelming majority of people living with and dying from HIV/AIDS
        reside; however, our findings have broader applicability and more general implications in
        the worldwide fight against HIV/AIDS, which we highlight here.
        
          The Potential for Treatment to Enhance Prevention Must Be Exploited
          Effective prevention requires more than having sufficient funds to offer information
          and services. It also requires an environment that encourages people to internalize
          messages about risky behavior and to adopt actual behavior change, and allows people to
          utilize services such as testing and counseling without fear of stigma or discrimination.
          Stoneburner and Low-Beer have argued that the supportive social and political environment
          in Uganda allowed people to discuss AIDS with family members and close friends, which led
          to greater behavior change than in Kenya or Zambia where most people received information
          from mass media only [20]. People have little reason to seek HIV testing when a positive
          result brings only negative consequences, whereas widespread availability of treatment
          provides a major incentive for people to learn their serostatus.
          Involving communities and family members in the delivery of treatment—for example, as
          treatment monitors—offers unique entry points for effective prevention activities and a
          lever for population-wide behavior change. Experience with community roll-out of
          treatment programs has shown, for example, that uptake of voluntary counseling and
          testing increased by 300% in one year of roll-out in Haiti, and by a factor of 12 in
          Khayelitsha, South Africa, after treatment introduction [21,22]. A study targeting nine
          commuter sites in South Africa found the highest levels of condom use, willingness to use
          a female condom, and willingness to have an HIV test in Khayelitsha, a difference that
          may be attributed largely to the availability of ART and comprehensive AIDS care [23]. If
          increased uptake of voluntary counseling and testing is indicative of broader prevention
          effectiveness where ART is available, we estimate that over 50% more new infections and
          more than twice as many deaths could be avoided through a combined response compared to
          prevention alone. In contrast, if a narrow treatment scale-up leads to reduced
          effectiveness of prevention, short-term mortality reductions will come at the expense of
          longer-term progress in stemming the tide of the epidemic.
          During most of the past 15 years, efforts to address the AIDS epidemic in sub-Saharan
          Africa have focused on prevention. There have been successes in some countries, but
          overall these efforts have not achieved their goals. The advent of vastly expanding
          treatment programs in the coming years, if opportunities to capitalize on broadened
          political support and community mobilization can be seized, offers the potential to
          enhance prevention effectiveness and avert many new infections and deaths.
        
        
          Only Effective Prevention Will Make Treatment Affordable in the Long Run
          While prevention programs are unlikely to achieve full impact in the absence of
          treatment, so too is the impact of treatment programs reduced if vigorous prevention
          efforts are absent. Without effective prevention, the number of people requiring care and
          treatment will grow each year. As more and more people are kept alive with ART the
          treatment burden will become enormous unless effective prevention reduces the number of
          people becoming newly infected.
          Without effective prevention programs, we project that the number of people receiving
          treatment will grow to 6.3 million by 2010 and up to 9.2 million by 2020 in Africa alone
          to achieve 80% coverage of those in urgent need. Meeting this need would require a
          tremendous increase in financing, human capacity and infrastructure that might not be
          attainable.
          If effective prevention programs are combined with treatment programs, the same level
          of 80% ART coverage would be achieved by treating 5.8 million in 2010 and 4.2 million in
          2020. In other words, the same goal could be attained at a far lower treatment cost and
          with a much greater chance of sustainability.
        
        
          A Successful Global Response Cannot Rely on Either Prevention or Treatment
          Alone
          Over the long term, it is effective prevention that will reduce the burden of illness
          due to AIDS and the number of people in need of ART. The lessons learned in the
          industrialized world have to be taken on board. The availability of treatment and the
          shift in focus away from very effective prevention programs has led to increases in
          unsafe sexual behavior, STIs, and HIV transmission in some settings [9,10,11]. There is
          no doubt that effective therapy can extend and improve the quality of life for those who
          are treated, but it also must be integrated into a comprehensive community response to
          HIV so that it can enhance the effectiveness of prevention efforts. Long-term, sustained
          progress in the fight against AIDS demands more than an exclusive focus on either
          prevention or treatment alone. Prevention makes treatment affordable, and treatment can
          make prevention more effective.
          Countries in sub-Saharan Africa are faced with the most devastating epidemic of our
          times. We now have the unique opportunity to derive the maximum impact from available
          resources. The results from our analyses show how potential synergies between prevention
          and treatment could be translated into considerable health benefits at the population
          level. But synergies do not mean simply that prevention and treatment are pursued in
          parallel. When whole communities become involved in the scale-up of treatment access—as
          will be necessary to achieve the ambitious treatment targets defined by the 3 by 5
          campaign—crucial opportunities can be created for increasing their involvement in
          prevention activities. Only if interactions with patients, family, and community members
          occasioned by the provision of treatment are also used to reinforce prevention, and only
          if prevention workers have an opportunity to refer those in need to care and treatment,
          will we move at last from slogans to impact.
        
      
      
        Supporting Information
      
    
  

  
    
      
        Introduction
        Genes of the 
        ERBB family encode receptor tyrosine kinases that mediate cellular
        responses to growth signals. Somatic mutations in the tyrosine kinase domains of two 
        ERBB genes, 
        epidermal growth factor receptor (EGFR) and 
        HER2, have been found in a proportion of lung adenocarcinomas [1,2,3,4].
        For 
        EGFR, mutations are associated with sensitivity to the small-molecule
        kinase inhibitors gefitinib (Iressa) [1,2,3] and erlotinib (Tarceva) [3].
        ERBB signaling pathways include downstream GTPases encoded by 
        RAS genes. Some 15%–30% of lung adenocarcinomas contain activating
        mutations in the 
        RAS family member 
        KRAS . These mutations are most frequently found in codons 12 and 13 in
        exon 2 [5,6], and may be associated with unfavorable outcomes [7]. Interestingly, 
        EGFR and 
        KRAS mutations are rarely found in the same tumors, suggesting that they
        have functionally equivalent roles in lung tumorigenesis ([8]; M. Meyerson, personal
        communication). Furthermore, 
        EGFR mutations are common in tumors from patients who have smoked less
        than 100 cigarettes in their lifetimes (“never smokers”) [3], while 
        KRAS mutations more commonly occur in individuals with a history of
        substantial cigarette use [9].
        We sought to determine whether 
        KRAS mutations could also be used to predict primary sensitivity or
        resistance to gefitinib or erlotinib. We systematically evaluated 60 lung adenocarcinomas
        from patients with known responses to either of these drugs for the presence of mutations
        in 
        EGFR (exons 18 through 21) and 
        KRAS2 (exon 2). Here, we show that mutations in 
        KRAS are associated with primary resistance to single-agent gefitinib or
        erlotinib. Our results suggest that a determination of mutational status for both 
        EGFR and 
        KRAS may help define which patients are likely to benefit from receiving
        gefitinib or erlotinib.
      
      
        Methods
        
          Tissue Procurement
          Tumor specimens were obtained through protocols approved by the institutional review
          board of Memorial Sloan-Kettering Cancer Center, as previously described [3] (see
          Protocols S1–S3). Tumor material, obtained from patients prior to kinase inhibitor
          treatment for lung cancer, was collected retrospectively for patients on gefitinib, who
          received 250 mg or 500 mg orally once daily (
          n = 24), and prospectively for patients on erlotinib, who received 150
          mg orally once daily (
          n = 36). The latter cohort of patients was part of a clinical trial of
          erlotinib for patients with bronchioloalveolar carcinoma. The analysis presented here
          includes specimens we previously reported on (
          n = 17 for gefitinib and 
          n = 17 for erlotinib) [3].
          All specimens were reviewed by a single reference pathologist (M. F. Z.). Imaging
          studies were assessed by a single reference radiologist (R. T. H.), who graded responses
          according to Response Evaluation Criteria in Solid Tumors (RECIST) [10]. Both observers
          were blinded to patient outcomes.
          Eight of nine patients with tumors sensitive to gefitinib had objective partial
          responses as defined by RECIST, i.e., at least a 30% decrease in the sum of the longest
          diameters of target lesions, taking as reference the sum measured at baseline. The ninth
          patient had marked clinical improvement, as ascertained by two independent reviewing
          physicians and manifested by lessened dyspnea and cancer-related pain. However, this
          individual had radiographic lesions (pleural and bone metastases) that were deemed
          nonmeasurable by RECIST criteria. As erlotinib-treated patients were all in a clinical
          trial, all had disease measurable using RECIST guidelines. For both drugs in this study,
          tumors were considered refractory if they did not undergo sufficient shrinkage to qualify
          for partial response. This definition includes patients whose “best overall response” was
          either progression of disease (
          n = 26) or stable disease (
          n = 12) as defined by RECIST. No patients had a complete response.
        
        
          Mutational Analyses of EGFR and KRAS in Lung Tumors
          Genomic DNA was extracted from tumors embedded in paraffin blocks, except for tumor
          109T, which was a fresh-frozen tumor specimen. Primers for 
          EGFR analyses (exons 18–21) were as published [3]. For 
          KRAS analyses, the following nested primer sets for exon 2 were used:
          huKRAS2 ex2F, 5′- GAATGGTCCTGCACCAGTAA-3′; huKRAS2 ex2R, 5′- GTGTGACATGTTCTAATATAGTCA-3′;
          huKRAS2 ex2Fint, 5′- GTCCTGCACCAGTAATATGC-3′; and huKRAS2 ex2Rint, 5′-
          ATGTTCTAATATAGTCACATTTTC-3′.
          For both 
          EGFR and 
          KRAS, PCR was performed using the HotStarTaq Master Mix Kit (Qiagen,
          Valencia, California, United States), as per manufacturer's instructions. Use of this
          method often obviated the need for nested PCR sets. All sequencing reactions were
          performed in both forward and reverse directions, and all mutations were confirmed by PCR
          amplification of an independent DNA isolate.
          In 12 cases, exon 19 deletions were also studied by length analysis of fluorescently
          labeled PCR products on a capillary electrophoresis device, using the following primers: 
          EGFR -Ex19-FWD1, 5′- GCACCATCTCACAATTGCCAGTTA-3′, and 
          EGFR -Ex19-REV1, 5′-Fam- AAAAGGTGGGCCTGAGGTTCA-3′. Using serial
          dilutions of DNA from the H1650 non-small-cell lung cancer cell line (exon 19
          deletion-positive [11]), this assay detects the mutant allele when H1650 DNA comprises 6%
          or more of the total DNA tested, compared to a sensitivity of 12% for direct sequencing.
          These same cases were also screened for the exon 21 L858R mutation by a PCR–restriction
          fragment length polymorphism assay, based on a new Sau96I restriction site created by the
          L858R mutation (2,573T→G). The Sau96I-digested fluorescently labeled PCR products were
          analyzed by capillary electrophoresis, and the following primers were used: 
          EGFR -Ex21-FWD1, 5′- CCTCACAGCAGGGTCTTCTCTGT-3′, and 
          EGFR -Ex21-REV1, 5′-Fam- TCAGGAAAATGCTGGCTGACCTA-3′. Using serial
          dilutions of DNA from the H1975 cell line (L858R-positive [11]), this assay detects the
          mutant allele when H1975 DNA comprises 3% or more of the total DNA tested, compared to a
          sensitivity of 6% for direct sequencing (Q. Pan, W. Pao, and M. Ladanyi, unpublished
          data).
        
        
          Statistics
          Fisher's Exact Test was used to calculate 
          p- values, and confidence intervals were calculated using 
          Statistics with Confidence software [12].
        
      
      
        Results
        We identified 60 lung adenocarcinomas from individual patients with tumors shown to be
        sensitive or refractory to single-agent gefitinib or erlotinib and evaluated these tumors
        for mutations in 
        EGFR and 
        KRAS . Collectively, nine of 38 (24%) tumors refractory to either kinase
        inhibitor had 
        KRAS mutations, while zero of 21 (0%) drug-sensitive tumors had such
        mutations (
        p = 0.02) (Table 1). The 95% confidence intervals (CIs) for these
        observations are 13%–39% and 0%–16%, respectively. Conversely, 17 of 22 (77%) tumors
        sensitive to either kinase inhibitor had 
        EGFR mutations, in contrast to zero of 38 (0%) drug-resistant tumors (
        p = 6.8 × 10
        −11 ). The 95% CIs for these observed response rates are 57%–90% and
        0%–9%, respectively. All 17 tumors with 
        EGFR mutations responded to gefitinib or erlotinib, while all nine tumors
        with 
        KRAS mutations did not (
        p = 3.2 × 10
        −7 ).
        Correlation of 
        EGFR and 
        KRAS mutational status with drug and treatment response is detailed in
        Table 1. The spectrum of 
        KRAS mutations is shown in Figure 1 and Table 2. Results with gefitinib
        and erlotinib were similar overall. However, the incidence of 
        KRAS mutations in the patients treated with erlotinib was low, probably
        because of the fact that all patients treated with this drug had bronchioloalveolar
        carcinoma, which rarely has 
        RAS mutations [13]. Alternatively, our analyses involving only exon 2 of 
        KRAS2 may have missed some 
        RAS mutations. However, in our analysis of the exonic regions encoding
        the first 100 amino acids of 
        KRAS in 110 surgically resected early-stage non-small-cell lung cancers,
        we have found 18 mutations, and all were in either codon 12 or codon 13, encoded by exon 2
        (W. Pao, R. Wilson, H. Varmus, unpublished data). Another possibility is that the
        erlotinib-treated tumors have mutations in other 
        RAS genes, since a minority of 
        RAS mutations in lung cancer have been reported to occur in 
        N- or 
        HRAS [5,6].
      
      
        Discussion
        These results have important clinical implications. First, they extend previous data
        from our group and others showing that lung adenocarcinomas containing 
        EGFR mutations are associated with sensitivity to gefitinib or erlotinib
        (17 of 17 in this series; 100% observed response rate; 95% CI, 82%–100%). Second, these
        data show that tumors with 
        KRAS exon 2 mutations (
        n = 9) are associated with a lack of response to these kinase inhibitors
        (0% observed response rate; 95% CI, 0%–30%). Third, no drug-sensitive tumors had 
        KRAS exon 2 mutations (
        n = 21). Whether 
        KRAS mutational status can be used to predict responses to gefitinib or
        erlotinib in patients whose tumors have wild-type 
        EGFR sequence is still under investigation: our analysis comparing
        response rates for tumors with neither 
        EGFR nor 
        KRAS mutations versus tumors with wild-type 
        EGFR but mutated 
        KRAS does not reach statistical significance (five of 22 versus zero of
        nine; 
        p = 0.29). Nevertheless, these findings suggest that patients whose lung
        adenocarcinomas have 
        KRAS mutations will not experience significant tumor regression with
        either drug.
        The incidence of 
        EGFR mutations in tumors responsive to EGFR kinase inhibitors has varied
        from 71% to 100% ([1,2,3] and this paper). Thus, at this point, patients whose tumors test
        negative for 
        EGFR mutations should not necessarily be precluded from treatment with
        either gefitinib or erlotinib. Data presented here suggest that clinical decisions
        regarding the use of these agents in patients with lung adenocarcinomas might be improved
        in the future by pre-treatment mutational profiling of both 
        EGFR and 
        KRAS . These findings warrant validation in large prospective trials
        using standardized mutation detection techniques.
        
          Supporting Information
        
        
          Accession Numbers
          The LocusLink (http://www.ncbi.nlm.nih.gov/LocusLink/) accession number for the 
          KRAS2 sequence discussed in this paper is 3845; the GenBank
          (http://www.ncbi.nlm.nih.gov/Genbank/) accession number for the 
          KRAS2 sequence discussed in this paper is NT_009714.16.
        
      
    
  

  
    
      
        Introduction
        Alzheimer disease is the leading cause of dementia among the elderly and is
        characterized by accumulation of extracellular and vascular amyloid in the brain [1].
        Amyloid deposits are composed of the amyloid-β peptide (Aβ), a 4-kDa peptide released
        during “amyloidogenic” proteolytic processing of the Alzheimer Aβ precursor protein (APP)
        [2]. APP can also be cleaved by the nonamyloidogenic α-secretases, a disintegrin and
        metalloproteinase 10 (ADAM-10) and ADAM-17 [3], in a reaction that is believed to occur
        primarily on the plasma membrane [4] and is known as “ectodomain shedding.”
        α-Secretase-type ectodomain shedding divides the Aβ domain of APP, thereby generating
        α-secretase-cleaved soluble APP ectodomain (sAPP
        α ) [4]. This reaction can be stimulated by activation of protein kinase
        C (PKC) or extracellular-signal-regulated protein kinases (ERKs) [5,6,7] or by inactivation
        of protein phosphatase 1 or 2A [5].
        Reports from retrospective analyses suggest that the statin class of
        cholesterol-lowering HMGCoA reductase inhibitors may lower the risk for Alzheimer disease
        by as much as 70% [8,9,10,11]. Studies in wild-type guinea pigs and in plaque-forming
        transgenic mice have demonstrated that chronic statin treatment can attenuate cerebral
        amyloidosis [12,13], suggesting that statins may exert their risk-reducing effects, at
        least in part, by modulating APP metabolism. In cell culture, lovastatin and simvastatin
        decrease the release of Aβ by rat hippocampal neurons [12,14] while activating
        α-secretase-type ectodomain shedding [15,16]. However, the molecular mechanisms by which
        statins modulate ectodomain shedding remain to be elucidated [17,18].
        Statin effects on APP metabolism are, to some extent, attributable to cholesterol
        lowering, but statin actions on APP may also involve cholesterol-independent actions [19].
        Reduction in synthesis of mevalonate leads to decreased generation of a number of
        isoprenoid lipid derivatives. Isoprenoids, such as farnesyl pyrophosphate and
        geranylgeranyl pyrophosphate, are 15- or 20-carbon lipid moieties. Through the action of
        farnesyl transferases and type I geranylgeranyl transferases, isoprenoids are attached to
        the amino acid sequence Cys-Ala-Ala-Xaa (“CAAX”) at the C-terminus of the Rho family of
        GTPases [20]. These posttranslational lipid modifications are essential for attachment of
        the GTPases to the cytosolic face of intracellular vesicles and/or to the cytosolic leaflet
        of the plasma membrane, thereby specifying subcellular targets for GTPase action(s).
        Some members of the Rho GTPase family exert their actions through modulation of protein
        kinase activities. One of the best characterized is Rho-associated protein kinase 1 (ROCK1,
        also called ROKβ). ROCK1 is a serine/threonine kinase with an apparent mass of 160 kDa that
        can be activated by either RhoA or RhoB [21,22,23]. Structurally, the ROCK1 N-terminus
        contains the protein kinase domain, while the C-terminus has both a Rho-binding domain and
        a pleckstrin homology domain, either of which can modulate protein–protein interactions. In
        the inactive state, the Rho-binding domain and the pleckstrin homology domain form an
        autoinhibitory loop by binding and blocking the kinase domain at the N-terminus of the
        molecule. Activation of ROCK1 occurs when a Rho protein binds to the Rho-binding domain,
        causing a conformational change that opens the kinase domain for the phosphorylation of
        downstream effectors [23]. Once activated, ROCK1 phosphorylates several substrates,
        including myosin light chain phosphatase, LIM kinases (Lin11, Isl1, and Mec3), and
        ezrin-radixin-moesin proteins [24,25,26,27]. ROCK1 has recently been implicated in
        modifying the site of substrate cleavage by APP γ-secretase [28], perhaps acting via
        ROCK1-dependent phosphorylation of a component of the γ-secretase enzyme complex.
        In the current study, we demonstrate that activation of sAPP
        α shedding from cultured cells by atorvastatin or simvastatin involves
        isoprenoid-mediated protein phosphorylation. Treatment of cells with a farnesyl transferase
        inhibitor or expression of a dominant negative (DN) ROCK1 molecule led to enhanced sAPP
        α shedding, supporting the notion that shedding is modulated by the
        isoprenoid pathway. Transfection with the cDNA for a constitutively active (CA) ROCK1
        molecule led to inhibition of statin-activated sAPP
        α shedding. These results raise the possibility that the apparent
        beneficial effect of statins in the prevention of Alzheimer disease could be, at least in
        part, mediated by isoprenoid modulation of APP metabolism.
      
      
        Methods
        
          Reagents
          The APP C-terminal specific polyclonal antibody 369 [29] was used to detect
          full-length APP and its C-terminal fragments. Monoclonal antibody 6E10 against residues
          1–16 of human Aβ (Signet, Dedham, Massachusetts, United States) was used to detect human
          holoAPP or sAPP
          α . Anti-ROCK1 antibody was purchased from Chemicon (Temecula,
          California, United States). Streptavidin-antibody HRP-conjugated C-Myc antibody 9E10,
          mevalonic acid, arachidonic acid, and phenylarsine oxide were purchased from Sigma (St.
          Louis, Missouri, United States). Atorvastatin was obtained from Pfizer (Groton,
          Connecticut, United States), and simvastatin was obtained from LKT Labs (St. Paul,
          Minnesota, United States). N2 supplement was obtained from Gibco (Carlsbad, California,
          United States). Sulfo-NHS-LC-Biotin was purchased from Pierce (Rockford, Illinois, United
          States). CA and DN Myc-tagged ROCK1 vectors were generated as previously described
          [30,31] and were generous gifts from Liqun Luo (Stanford University). Fugene 6 was
          purchased from Roche (Basel, Switzerland). Farnesyl transferase inhibitor 1 (FTI-1) was
          obtained from Biomol (Plymouth Meeting, Pennsylvania, United States). Tumor necrosis
          factor α (TNFα) protease inhibitor 2 and Y-27632 were purchased from Calbiochem (San
          Diego, California, United States). Protein concentration assay kit was purchased from
          Biorad (Hercules, California, United States). LIVE/DEAD Viability/Cytotoxicity Assay Kits
          and Amplex Red Cholesterol Assay Kits were purchased from Molecular Probes (Eugene,
          Oregon, United States).
        
        
          Culture Methods and Sample Preparation
          N2a mouse neuroblastoma cells stably transfected with the Swedish mutant form of APP
          (SweAPP N2a cells; APP695, 595–596 KM/NL) (gift from G. Thinakaran and S. Sisodia,
          University of Chicago, Chicago, Illinois, United States) were maintained in DMEM, 10%
          FBS, and 200 μg/ml G418 in the presence of penicillin and streptomycin [32]. For the 24 h
          prior to pharmacological treatments, the culture media were changed to N2-supplemented
          lipid-free medium. In some studies, cells were transfected in N2-supplemented FBS-free
          medium 48 h before pharmacological treatments. Transfections were carried out using the
          Fugene reagent, according to the manufacturer's instructions. All treatments were
          performed in the presence of 1 μM mevalonic acid, unless otherwise specified.
          Cells were lysed in 1% Triton-X/PBS buffer containing 1 X complete proteinase
          inhibitor cocktail (Roche), sonicated twice for 30 s, and centrifuged at 5,000
          g for 5 min. Protein concentration in the supernatant was determined
          using the Biorad Protein Assay kit, following the manufacturer's instructions. For the
          detection of holoAPP and sAPP
          α , samples were separated in 7.5% polyacrylamide gels, transferred to
          nitrocellulose, and the proteins detected with either 369 (1:3,000 for holoAPP and
          C-terminal fragments) or 6E10 (1:1,000 for holoAPP or sAPP
          α ), followed by incubation of the transfers with appropriate
          secondary anti-rabbit or anti-mouse antibodies. For the detection of transfected ROCK1
          proteins, samples were immunoprecipitated with 2 μg of anti-Myc antibody, separated in 5%
          polyacrylamide gels, transferred, and the proteins detected with anti-ROCK1 antibody
          (1:1,000 dilution).
        
        
          Cell-Surface Biotinylation
          Cells were plated in a 100-mm dish at a concentration of 5 × 10
          6 cells/dish. After treatment, media were harvested, and sAPP
          α levels were evaluated by immunoblotting as described above. Cells
          were washed twice in PBS and then incubated with Sulfo-NHS-LC-Biotin for 30 min at 4 
          o C. Biotinylation reactions were terminated by one wash in Tris
          followed by two washes in PBS. Cells were lysed in 1% Triton-X/PBS buffer containing
          protease inhibitor cocktail as indicated above. Lysates were immunoprecipitated with 3 μl
          of whole 369 antibody serum and 30 μl of protein A beads. After washing twice with 1%
          Triton/PBS, and then twice with PBS, samples were boiled in sample buffer for 3 min,
          separated in a 7.5% polyacrylamide gel, and transferred to nitrocellulose. The
          biotinylated proteins were detected using streptavidin HRP polymer (1:10,000
          dilution).
        
        
          Viability/Cytotoxicity Assays
          Cells were plated in an eight-well slide at a concentration of 1 × 10
          4 cells/well. After treatments as indicated, LIVE/DEAD assays were
          performed following the manufacturer's instructions (Molecular Probes).
        
        
          Cholesterol Assays
          Cholesterol levels in cell lysates were measured using Amplex Red following the
          manufacturer's instructions (Molecular Probes). We have previously demonstrated that
          standard doses of either simvastatin or atorvastatin reduce cholesterol levels in N2a
          cells by 65%–67% [16].
        
        
          Quantification and Statistical Analysis
          Quantification of protein bands was performed using the UVP Bioimaging System, and
          statistical analysis was performed on paired observations using the Student's 
          t test.
        
      
      
        Results
        
          Atorvastatin Activates sAPPα Shedding at a Subcellular Site Upstream of Endocytosis
          from the Plasma Membrane
          We confirmed our previous observation [16] that atorvastatin produces an increase in
          sAPP
          α shedding that is dose-dependent, reaching a maximum effect at 5 μM.
          The increase in sAPP
          α shedding is accompanied by a corresponding increase in levels of the
          nonamyloidogenic APP α-C-terminal fragment (C83; data not shown).
          In order to refine our localization of the subcellular target of statin action, we
          evaluated the steady-state levels of cell-surface APP (csAPP) in the absence or presence
          of statins. After drug treatments, cells were subjected to the surface biotinylation
          protocol. Cells and media were harvested, and levels of sAPP
          α , holoAPP, and csAPP were measured. Treatment with atorvastatin
          increased csAPP by approximately 1.6-fold, similar to the effect of the drug on holoAPP
          (Figure 1), while sAPP
          α shedding was increased by approximately 7-fold (
          p < 0.05). Since csAPP levels were only slightly raised in the same
          statin-treated cells in which sAPP
          α shedding was dramatically increased, we interpret this disparity to
          indicate that the effector of statin-stimulated shedding is probably intrinsic to the
          plasma membrane. In other studies, the plasma membrane has been proposed to be, or to
          contain, the statin target. For example, statins have been proposed to cause
          co-localization of α-secretase and APP within lipid rafts [15,33]; statins might also
          induce modification of the structure and activity of a protein in the plasma membrane
          α-secretase complex, perhaps in an action similar to how statins bind and “lock”
          cell-surface integrins [34].
          A pulse-chase protocol was also used to study post-transcriptional regulation of APP
          metabolsm by statins (data not shown). This protocol avoids any confound that might arise
          because of altered APP transcription. Pulse-chase studies were performed using a 10-min
          pulse with [
          35 S]methionine followed by various chase times from 0 to 120 min.
          Typical maturation and half-life of mature cellular holoAPP were observed, as was
          subsequent release of sAPP
          α [5,29]. In the presence of either atorvastatin or simvastatin, the
          time course of maturation and release perfectly paralleled that observed in the absence
          of either drug, except that the fractional content of cellular mature holoAPP was
          approximately 2-fold greater in the presence of drug (i.e., at 15 or 75 min chase, mature
          APP in the presence of statin was approximately 310% of the level of immature APP at 
          t = 0 versus a control [vehicle treatment] of 150% of the level of
          immature APP at 
          t = 0; also, at 
          t = 30 min, the relative percent values for drug versus vehicle were
          380% and 200%, respectively). Fold increases in released sAPP
          α in the same experiments were approximately 3- to 4-fold (2.0
          arbitrary units versus 5.5–8.0 arbitrary units at 120 min for atorvastatin and
          simvastatin, respectively). Secretory maturation toxicity is one possible mechanism for
          elevated levels of intracellular mature holoAPP and causes retarded conversion of mature
          holoAPP to sAPP
          α . This pattern was not observed following statin treatment,
          excluding maturation toxicity as a mechanism underlying the altered levels of mature
          cellular holoAPP.
          Instead, the pattern that we observed raises the possibility that statins, presumably
          via isoprenoids (given the reversibility with mevalonate), as discussed in the next
          section, may alter sorting of cellular holoAPP, diverting holoproteins away from terminal
          degradation in the endosomal/lysosomal pathway and into the constitutive secretory
          pathway that generates sAPP
          α . However, the fold effect on reduced intracellular turnover in the
          endosomal/lysosomal pathway (or sorting out of the endosomal/lysosomal pathway and into
          the constitutive secretory/shedding pathway) is apparently insufficient to explain the
          fold effect on sAPP
          α generation (2-fold for the former, vs 3- to 4-fold for the latter),
          indicating a contribution from a downstream site in the processing pathway. When these
          results are taken together with independent work on regulated shedding of transforming
          growth factor α (TGFα) [35,36], a parsimonious explanation is that an important target
          for activation of ectodomain shedding is probably located at the plasma membrane or
          downstream of APP residence at the plasma membrane.
          The identification of the regulatory components of the ectodomain shedding machinery
          have been long-sought in other studies employing phorbol esters to stimulate shedding of
          sAPP
          α or TGFα [4,35,36,37]. Munc-13 has recently been implicated as a
          phorbol target in regulated shedding [38]. In our opinion, this molecule is rather
          unlikely to play a major role in shedding regulation, given the specificity of Munc-13
          effects for phorbols and the generalization of the regulated shedding phenomenon to
          include activation by protein phosphatase inhibitors and neurotransmitters. Neither of
          these would be predicted to act via the phorbol-binding C1 domain of Munc-13.
          Sisodia and colleagues [39] demonstrated that arrest of APP endocytosis from the
          plasma membrane by deletion of its 
          NPXY clathrin-coated vesicle targeting sequence [40,41] can
          dramatically stimulate sAPP
          α shedding, presumably by extending the half-life of co-localized
          α-secretase and APP on the plasma membrane. In order to exclude the possible contribution
          of altered endocytosis to statin-stimulated shedding, we evaluated the effect of
          phenylarsine oxide (PO), an inhibitor of endocytosis, on statin-stimulated shedding
          (Figure 2). Treatment with either atorvastatin, simvastatin, or PO alone increased sAPP
          α shedding, as expected. Co-treatment of cells with PO plus either
          atorvastatin or simvastatin caused stimulation of sAPP
          α shedding to levels greater than the maximal levels of shedding
          achievable with inhibition of endocytosis using PO alone or with maximal doses of either
          statin alone (
          p < 0.05). The additivity of statin- and PO-stimulated shedding is
          consistent with the hypothesis that statins act at or near the plasma membrane, prior to
          internalization of csAPP. Under all circumstances, stimulated sAPP
          α shedding was completely blocked using TNFα protease inhibitor 2, a
          standard α-secretase/metalloproteinase inhibitor (Figure 2). We interpret this as an
          indication that the statin-induced α-cleavage of APP is probably mediated by one of the
          molecules usually associated with the phenomenon, i.e., ADAM-10 or ADAM-17 [16].
        
        
          Compounds that Modulate Isoprenoid Levels Activate sAPPα Shedding
          As discussed above, there is an established relationship between statins and
          isoprenoid-modulated protein phosphorylation. We therefore tested the effects of FTI-1 on
          statin-stimulated sAPP
          α shedding. FTI-1 increased the shedding of sAPP
          α , but the combination of a statin plus FTI-1 increased sAPP
          α shedding to levels greater than those achievable by using either
          compound alone (Figure 3; 
          p < 0.05). In the same experiment, levels of holoAPP were modestly
          increased but, again, to an extent insufficient to account for the increase in shed sAPP
          α (Figure 3).
          To test whether statin-activated shedding might be attributable to a metabolite
          downstream of HMGCoA reductase, cells were treated with 1 μM simvastatin, 5 μM FTI-1, and
          a series of concentrations of mevalonic acid. Since FTI-1 acts downstream of HMGCoA
          reductase, the stimulatory effect of FTI-1 on sAPP
          α shedding would not be predicted to be modified by mevalonate
          supplementation. Low doses (<1 μM) of mevalonic acid did not affect statin-induced
          sAPP
          α shedding, but complete inhibition of statin-activated sAPP
          α shedding was achieved with higher doses of mevalonic acid (100 μM).
          As predicted, the shedding observed following treatment with FTI-1 was not inhibited by
          any of the concentrations of mevalonic acid tested (Figure 4). These data are consistent
          with a role for isoprenoids in statin control of APP metabolism in cultured cells.
        
        
          Expression of ROCK-Related Molecules Modulates sAPPα Shedding in a Bidirectional
          Manner
          Since many isoprenoid-mediated Rho effects converge on ROCKs, we next transfected N2a
          cells with cDNAs encoding either green fluorescent protein (GFP) (control), CA ROCK1, or
          DN ROCK1 (Figure 5). Simvastatin caused a typical activation of sAPP
          α shedding from GFP-transfected cells. When CA ROCK1 was introduced,
          however, shedding of sAPP
          α from both untreated and simvastatin-treated cells was diminished
          (Figure 5; 
          p < 0.05 versus GFP control). Conversely, DN ROCK1 alone activated
          shedding of sAPP
          α . Cellular levels of holoAPP were not affected by transfection
          (Figure 5; 
          p < 0.05 versus GFP control). In studies aimed at independent
          confirmation of the involvement of ROCK activation in sAPP
          α shedding, we treated SweAPP N2a cells with arachidonic acid, an
          activator of ROCK. As shown in Figure 6, arachidonic acid reduced the shedding of sAPP
          α without altering levels of holoAPP. Based on this series of results,
          we concluded that both basal and activated sAPP
          α shedding from cultured cells are controlled by ROCK activity.
          In some experiments, cells were treated with Y-27632 (10 nM to 50 μM), a compound that
          can inhibit ROCKs. Y-27632 showed no effect on basal sAPP
          α release and blocked statin-activated sAPP
          α shedding (data not shown). This result was unexpected in light of
          the effects of DN ROCK1. Given the internally consistent actions of DN ROCK1 and CA
          ROCK1, as well as the results employing either FTI-1 or arachidonate, we concluded that
          the Y-27632 result might be due to inhibition by Y-27632 of protein kinases other than
          ROCK1 [23]. The possibility was also considered that cytotoxicity of Y-27632 for the
          central vacuolar pathway might explain the disparity between the effects of DN ROCK1 and
          those of Y-27632, but neither impairment of intracellular APP maturation nor increased
          apoptosis as measured by LIVE/DEAD assay were apparent following Y-27632 treatment (data
          not shown). Ultimately, we were unable to document any explanation for the disparate
          results of Y-27632 and DN ROCK1.
        
      
      
        Discussion
        The isoprenoid pathway involves lipid modification of various members of the Rho family
        of small GTPases by the addition of either farnesyl or geranylgeranyl moieties [20,42].
        Isoprenylation serves to target the GTPases to the proper organelle membrane, where their
        actions often relate to cytoskeletal dynamics and/or vesicle trafficking [20,42]. ROCKs are
        important downstream targets of Rho (Figure 7), catalyzing the phosphorylation of effector
        phosphoprotein substrates [23]. The foregoing data indicate that statin-induced activation
        of APP shedding in cultured cells involves the Rho/ROCK pathway. More specifically, the
        data indicate that ROCK1 activation blocks the effects of statins on APP ectodomain
        shedding, while ROCK1 blockade alone can mimic the effect of statins on APP shedding. By
        extension, these data predict that application of statins to neurons might directly or
        indirectly inhibit ROCK1 activity. Evaluation of this possibility will be the subject of
        future investigation.
        The first evidence that APP might be a substrate for ectodomain shedding was provided by
        Weidemann et al. [43] who identified sAPP
        α in the cerebrospinal fluid and blood. This aspect of APP metabolism
        bears resemblance to the proteolytic signal transduction pathways involved in processing
        pro-TGFα [35,36] and Notch [44]. In the case of Notch, the process is set in motion by the
        binding of a ligand to the Notch ectodomain, triggering its release (shedding). For APP,
        intracellular signal transduction appears to be more important [2,29,45]. In early studies,
        the existence of the shed ectodomain of APP was used to deduce the existence of the
        proteolytic activity designated α-secretase, which has the unusual specificity of cleaving
        its substrates at a proscribed distance from the extracellular leaflet of the plasma
        membrane [4,39]. Ultimately, the integral cell-surface metalloproteinases ADAM-10 and
        ADAM-17/TACE were found to underlie α-secretase-type ectodomain shedding [3,46].
        Why is APP a substrate for ectodomain shedding? To answer this question requires
        contemplation of the physiological function of APP. APP is a type 1 integral protein that
        is subjected to a host of post-translational processing events, including N- and
        O-glycosylation, tyrosyl sulfation, phosphorylation, and proteolysis [39,43,47,48]. Most
        (60%–80%) newly synthesized APP is subjected to terminal intracellular degradation that
        generates no discrete fragments [5]. A smaller fraction of APP molecules (approximately 20%
        in PC12 cells under basal conditions [5]) undergoes ectodomain shedding catalyzed by either
        the α-secretase (nonamyloidogenic) or β-secretase (potentially amyloiodgenic) pathway. When
        PKC is activated, the stoichiometry of shed sAPP
        α rises from 2 mol shed per 10 mol synthesized to 4 mol shed per mole
        synthesized. Most of this shedding is catalyzed by the α-secretase pathway, but a trace
        amount (<5% [49]) is catalyzed by β-secretase/β-site APP cleaving enzyme [50]. sAPP
        α and sAPP
        β differ by the inclusion in sAPP
        α of the first 16 residues of Aβ. Unlike sAPP
        α , which is generated at the plasma membrane, most sAPP
        β is probably generated by cleavage within the 
        trans -Golgi network and endocytic pathway vesicles. HoloAPP levels are
        likely limiting at one or more sorting steps in the late secretory pathway, since activated
        sAPP
        α shedding is apparently accompanied by diminished generation of sAPP
        β [51].
        What is the function of shed sAPP
        α ? Again, from other molecules, we know that shedding can serve
        important cellular functions by releasing diffusible ligands from their membrane-bound
        precursors (e.g., TGFα and TNFα) or by terminating intercellular signaling (e.g., Notch). A
        popular model holds that sAPP
        α may function as a neurotrophic and/or neuroprotective factor, and may
        promote neurite outgrowth [52]. More recent evidence suggests that released APP derivatives
        modulate efficacy of neurotransmission at the synapse [53]. Targeted deletion of APP has
        not revealed a striking phenotype [54], presumably because of functional redundancy
        supplied by APP-like proteins [55]. Mice with double and triple null mutations in various
        combinations of APP, APP-like protein 1, and APP-like protein 2 are now being created, in
        search of evidence for a definitive function for APP.
        Cao and Sudhof [56] have recently discovered that the APP C-terminal fragment generated
        by α- or β-secretase is itself cleaved to release Aβ and an APP intracellular domain (AICD)
        that diffuses into the nucleus, possibly acting there as a transcription factor. The
        pathway leading to AICD must be initiated by ectodomain shedding: holoAPP cannot directly
        give rise to AICD. Therefore, one important function for α- and/or β-secretase processing
        of APP may be the eventual generation of AICD. Our results suggest that Rho/ROCK signaling
        provides modulation of basal and stimulated α-secretase activity. It will now be important
        to dissect pathways upstream of Rho/ROCK signaling in order to identify the intracellular
        and intercellular events that participate in Rho/ROCK regulation of α-secretase under
        physiological and pathological conditions.
        The potential role of cholesterol in α-secretase-mediated shedding was discovered by
        Bodovitz and Klein [57] who used β-cyclodextrin to lower cellular cholesterol. Kojro et al.
        [15] confirmed this observation, using not only β-cyclodextrin but also lovastatin to lower
        cellular cholesterol. These investigators proposed that elevated ADAM-10 activity and
        protein levels contributed to these effects. These basic observations dovetailed with
        emerging epidemiological evidence that administration of statins might lead to a diminished
        incidence of Alzheimer disease [8,9,10,11]. Despite this, however, the association of
        statins and cholesterol levels with activated α-secretase-mediated shedding of the APP
        ectodomain was unexpected and not readily explicable by existing knowledge regarding
        regulation of α-secretase activity. The best characterized regulation of α-secretase
        processing typically involves protein phosphorylation via PKC [5,29] or ERKs [7] or protein
        dephosphorylation by protein phosphatase 1 or 2A [29]. We recently excluded the possibility
        that either PKC or ERK plays a role in statin-activated shedding [16], raising the
        possibility that other protein phosphorylation signaling pathways might link statins and/or
        cholesterol to α-secretase activation.
        Maillet et al. [58] implicated the Rho pathway in modulation of α-secretase activity
        while dissecting the activated shedding process that accompanies serotonergic signal
        transduction. These investigators discovered that Rap1 acts through Rac to modulate
        α-secretase processing of APP. Soon thereafter, ROCK1 was discovered by Zhou et al. [28] to
        modulate a downstream processing step in APP metabolism that involves
        presenilin/γ-secretase-mediated proteolysis of APP C-terminal fragments C99 and C83. These
        investigators discovered that activation of ROCK1 may account for how nonsteroidal
        anti-inflammatory drugs specify the scissile bond within the APP transmembrane domain that
        is cleaved by presenilin/γ-secretase to generate the C-terminus of Aβ. Based on these
        reports, we asked whether the Rho/ROCK pathway might play a role in controlling shedding of
        sAPP
        α following statin application.
        CA ROCK1 and DN ROCK1 molecules yielded direct and complementary evidence that ROCK1 was
        indeed a candidate for modulation of statin-activated α-secretase action. Further, we were
        able to demonstrate that α-secretase activity could be modulated by molecules further
        upstream in the isoprenoid pathway (see Figure 7). FTI-1, an inhibitor of farnesyl
        transferase also known as L-744,832 [59], mimicked and potentiated statin-activated
        shedding, presumably by blocking transfer of isoprenoid moieties to a Rho protein by
        farnesyl transferase, and thereby decreasing Rho activity. However, FTI-1 treatment can
        also increase the level of geranylgeranylated isoforms of certain Rho proteins, e.g., the
        inhibitory geranylgeranylated RhoB protein [60]. In further support of a role for
        isoprenoids, we were able to demonstrate that supplementation of cells with mevalonate
        abolished statin-activated shedding (see Figure 4). Statins block HMGCoA reductase
        generation of mevalonate from 3-hydroxy-methyl-glutarate (see Figure 7). Therefore, the
        addition of mevalonate would be predicted to antagonize statin action via the isoprenoid
        pathway, by relieving statin-induced mevalonate deficiency. As predicted by this model, we
        observed that statin-activated shedding was abolished by adding mevalonate. Taken together,
        these results suggest the existence of a reciprocal relationship between
        isoprenoid-mediated Rho/ROCK signaling and sAPP
        α shedding, i.e., activation of ROCK1 blocks basal and stimulated
        shedding while ROCK1 inhibition apparently relieves a tonic negative influence exerted on
        shedding by ROCK1 activity.
        As in PKC- and ERK-activated shedding, the ROCK1 substrate effector molecule or
        molecules that regulate proteolysis by ADAMs remain to be identified. The cytoplasmic
        domains of both APP and ADAM-17 have been evaluated as candidates for important targets of
        protein phosphorylation during the regulated shedding process, but neither “substrate
        activation” nor “enzyme activation” appears to explain the phenomenon, i.e.,
        phosphorylation of neither APP nor ADAM-17 dramatically increases the efficiency of
        α-secretion [61,62], indicating that activation is more indirect.
        Our data using statins and PO localize the mediator of statin-activated shedding to the
        plasma membrane, upstream of endocytosis, as appears to be the case for PKC-activated
        shedding [36,63,64,65]. Similar conclusions were drawn by Bosenberg and colleagues [36] who
        used streptolysin-porated cells and N-ethylmaleimide to demonstrate that reconstitution of
        activated shedding of TGFα from CHO cells does not require membrane trafficking and
        apparently occurs on the plasma membrane. These results suggest that a tightly
        membrane-associated regulatory subunit of the α-secretase complex is likely to be the key
        phosphoprotein that mediates α-secretase activity as a function of its state of
        phosphorylation by PKC and perhaps also ERK and ROCK1. The molecular identity of this
        phosphoprotein remains unknown.
        α-Secretase activation is a potential therapeutic strategy for modifying cerebral
        amyloidosis in Alzheimer disease [66]. This proposal is supported by recent evidence that
        either genetic modification of ADAM-10/α-secretase activity [67] or administration of
        bryostatin, a PKC activator [68], can modulate levels of brain Aβ in plaque-forming
        transgenic mice. α-Secretase activation may explain how statins lower the risk for
        Alzheimer disease [69], since atorvastatin diminishes Aβ burden in plaque-forming
        transgenic mice [13]. If α-secretase stimulation is to be truly viable as a human clinical
        intervention, it will be essential to assess the possibility that enhanced APP ectodomain
        shedding might incur mechanism-based toxicity (analogous to the concerns currently
        surrounding γ-secretase inhibitors). Along this line, extension of this work to other shed
        proteins will be important to determine the impact of enhanced shedding via ADAM
        proteinases on other substrates of those proteinases, including Notch, pro-TGFα, pro-TNFα,
        and CD44 [3].
        Preliminary results from a pilot proof-of-concept using atorvastatin in a human clinical
        treatment trial are consistent with the proposed beneficial effects of this class of
        compounds [70]. Since atorvastatin has low blood–brain barrier permeability [71], this
        beneficial effect, if attributable to Aβ lowering, must be due to altered Aβ metabolism in
        the periphery. Reduction in levels of free Aβ in the circulation has been demonstrated to
        lead to diminution in brain plaque burden following active or passive immunization [72]. It
        is conceivable that, if statins lower circulating Aβ, this effect could secondarily cause
        diffusion of central nervous system interstitial Aβ down its concentration gradient and
        into the cerebrospinal fluid and circulation, from which it is cleared. To date, however,
        this mechanism is not supported by data from human clinical trials, where statin
        administration has shown no consistent effect on levels of circulating or cerebrospinal
        fluid Aβ [73].
        The results reported here point to several areas for additional investigation. As
        described above, the key substrate or substrates linking cytoplasmic protein
        phosphorylation to intralumenal or cell-surface protelysis remain to be identified.
        Nonetheless, α-secretase activation has been validated as a viable therapeutic strategy for
        modulating cerebral amyloidosis [67]. Identification of the role of the Rho/ROCK pathway in
        regulating α-secretase provides a new avenue for its therapeutic activation, even though
        the potential relevance of atorvastatin-mediated ROCK1 inhibition in neurons may not
        explain the apparent clinical benefits of the drug. Still, if the reported
        disease-modifying effect of atorvastatin is confirmed in the National Institute on Aging's
        large, multi-center trial of simvastatin, one or more compounds of this class may be among
        the first disease-modifying compounds approved by the Food and Drug Administration for
        slowing the progression of Alzheimer disease.
      
      
        Supporting Information
        
          Accession Numbers
          The GenBank (http://www.ncbi.nlm.nih.gov/Genbank/) accession numbers for the proteins
          discussed in this paper are ROCK1 (NP_005397) and APP695, 595–596 KM/NL (NP_958817).
        
      
    
  

  
    
      
        PRESENTATION of CASE
        A 43-y-old white female presented to the hospital in July 2004 with pain in the left eye
        and left upper lid ptosis. She did not perceive any difference in perspiration between the
        two halves of her face. She was a nonsmoker and denied any history of head or neck trauma,
        or ocular, cardiac, vascular, or neurologic disease. Neuro-ophthalmological examination was
        normal except for 1 mm of left upper eyelid ptosis (drooping of the eyelid), miosis
        (constriction of the pupil), and mild enophthalmos (recession of the eyeball into the
        orbit) consistent with classic left-sided Horner syndrome (Figure 1). There was no
        carotidynia (a neck pain syndrome associated with tenderness to palpation over the carotid
        bifurcation) or carotid bruit. A chest radiograph obtained to rule out an underlying left
        apical superior sulcus tumor was normal. Magnetic resonance imaging/magnetic resonance
        angiography of the brain with cross-sectional imaging of the neck was obtained, which
        revealed extracranial left internal carotid artery dissection (Figures 2 and 3). The
        patient was treated with unfractionated heparin and coumadin and made an uneventful
        recovery. The patient was seen in the clinic a few months later and did not have any
        complications at follow-up.
      
      
        DISCUSSION
        Horner syndrome—characterized by the constellation of miosis, ptosis, anhidrosis (lack
        of sweating), enophthalmos, and anisocoria (unequal pupil size)—is present in up to 58% of
        internal carotid artery dissections [1]. Most patients experience neck, facial, and head
        pain ipsilateral to the lesion because of ischemia or stretching of the trigeminal pain
        fibers surrounding the carotid arteries [2]. Ophthalmic manifestations have been reported
        to occur in up to 62% of patients with internal carotid artery dissection [2]. Common
        findings in descending order of frequency are painful partial Horner syndrome (due to
        disruption of the third-order neuron oculosympathetic fibers) as seen in our patient,
        transient monocular vision loss, and permanent visual loss [2].
        De Bray et al. studied the prognosis of 90 cases of isolated Horner syndrome due to
        internal carotid artery dissection [3]. They found that 91% of cases of Horner syndrome due
        to internal carotid artery dissection were painful. The risk of an early ischemic stroke
        within the first 2 wk was high (around 17%) without initial antithrombotic treatment
        [3].
        Internal carotid artery dissection is a potentially life-threatening condition and
        carries a substantial risk of disabling stroke [4]. Carotid dissection is under-recognized
        as a cause of Horner syndrome and can be missed [5]. It is important to diagnose dissection
        because anticoagulation can prevent carotid thrombosis and embolism [5]. The investigation
        of choice is magnetic resonance imaging and angiography scan of the head and neck [5].The
        treatment advocated for dissection is anticoagulation for 3–6 mo [5].
      
    
  

  
    
      
        
        
        Streptococcus pneumoniae is a common bacterium that is present
        in the nasopharynx of many children and some adults, where it causes no harm to its carrier
        but can be transmitted to others. If it moves beyond the nasopharynx, however, it can cause
        ear infections or invasive disease, such as pneumonia or meningitis. Invasive disease from
        this organism occurs especially in children, the elderly, and individuals with weakened
        immune systems.
        The protective effect of antibodies against bacterial pneumonia has been appreciated
        since the 1930s, when it was shown that serum therapy—the transfer of serum from an
        immunized animal to a patient with acute disease caused by the same bacterial strain—could
        reduce mortality from pneumococcal pneumonia by half. Subsequent development of vaccines
        based on the bacterium's polysaccharide capsule, which could protect against infection,
        confirmed that an endogenous antibody response can provide protection against invasive
        disease.
        One challenge for vaccine development has been the existence of many different serotypes
        (the same species of bacteria but with different composition of the polysaccharide
        capsule). As protection usually doesn't extend to different serotypes, vaccination with
        capsule components from different serotypes is necessary to ensure broad protection. Such
        vaccines have been shown to be efficient and safe. They are now recommended in many
        countries for infants and toddlers, and for people over 65—the two age groups in which
        invasive disease is most common—and for others who are at increased risk of pneumococcal
        disease (e.g., patients with heart, kidney, liver, or lung disease, or who have had a
        splenectomy).
        Even without vaccination, however, most exposed individuals will never get invasive
        disease. Instead, they develop natural immunity against the different serotypes, though
        this immunity gradually declines with old age. Marc Lipsitch and colleagues wanted to
        understand the immunological basis of this natural immunity, and specifically whether it
        was due to anticapsular antibodies.
        If protection from invasive disease is due to acquiring anticapsular antibodies against
        each of the pneumococcal serotypes, they argued, this would lead to two predictions about
        the age distribution of disease caused by the different serotypes in the non-vaccinated
        population. First, for serotypes that are more common and therefore encountered earlier in
        life, children should develop immunity more quickly, causing disease from these types to
        drop off earlier in life than disease from the less common types. Second, protection
        against invasive disease from a particular serotype should coincide with the acquisition of
        antibodies against that serotype, on both the individual and population level.
        Neither prediction was borne out by the actual data the researchers analyzed, suggesting
        that there is more to natural immunity against pneumococcal disease than just anticapsular
        antibodies. The study doesn't demonstrate what the additional components are, but
        additional research might not just teach us about our immune system but also provide clues
        for further vaccine development. As the authors say, “A better understanding of the
        mechanisms that underlie natural immunity to pneumococcus could pave the way for the
        development of more effective, species-specific pneumococcal vaccines.”
      
    
  

  
    
      
        
        Tyrosine kinases regulate signaling pathways that control cell growth, proliferation,
        motility, and other critical cellular processes. Mutations in tyrosine kinase genes can
        lead to abnormal kinase activity, and some tumors become dependent upon this activity for
        growth and survival. Thus, kinases are attractive targets for anti-cancer drugs. Examples
        of new kinase inhibitors include gefitinib and erlotinib, which have recently shown promise
        in treating non-small-cell lung cancer. Unfortunately, gefitinib and erlotinib work only in
        a subset of patients, and they can have severe side effects, albeit infrequently. So
        researchers have been trying to find ways to predict who will benefit from therapy with
        these drugs and who won't.
        Following the work of Lynch et al. (N Engl J Med 350: 2129–2139) and Paez et al.
        (Science 304: 1497–1500), William Pao and colleagues have previously shown that the
        epidermal growth factor receptor (EGFR), a tyrosine kinase, is often mutated in
        non-small-cell lung cancers, and that tumors that harbor such mutations are sensitive to
        gefitinib and erlotinib.
        In this new study, they focused on a signaling protein called KRAS, which functions
        downstream of many tyrosine kinases, including EGFR. The KRAS gene is also often mutated in
        lung cancers, but very few cancers have mutations in both EGFR and the KRAS gene. To find
        out whether KRAS mutations could help to predict which patients would respond to gefitinib
        or erlotinib, the researchers looked for mutations in EGFR and KRAS genes in 60 tumors for
        which sensitivity to either drug was known.
        They extended their earlier findings that EGFR mutations (which were found in 17 of the
        tumors) were associated with sensitivity to the kinase inhibitors, and found that tumors
        that had mutations in KRAS (a total of nine) were refractory (i.e., did not respond) to
        either drug.
        These results need to be validated in larger and prospective trials that use
        standardized mutation detection techniques. If they are confirmed, knowing the mutation
        status of EGFR and KRAS in tumors could help physicians decide which patients should
        receive gefitinib and/or erlotinib. As Inoue and Nukiwa state in a Perspective that
        accompanies the article, “By combining all the factors that relate to response or
        resistance, patients who will benefit from treatment can hopefully be identified.
        Undoubtedly we have taken a great step forward in molecular therapy for lung cancer
        treatment.”
      
    
  

  
    
      
        
        Epidemiological studies suggest that statins, a class of cholesterol-lowering drugs, may
        lower the risk for Alzheimer disease. The mechanism for this effect is unclear. Alzheimer
        disease is characterized by accumulation of amyloid deposits in the brain. These deposits
        are composed of amyloid-beta (Aβ) peptide, a protein fragment that is cleaved off from the
        amyloid precursor protein APP. APP can be cleaved in two different ways. Amyloidogenic
        (“amyloid generating”) cleavage by an enzyme called beta-secretase yields “sticky” Aβ
        peptides that aggregate to form deposits, whereas non-amyloidogenic cleavage by
        alpha-secretases generates soluble peptides that do not form deposits. Studies in animal
        models and cell culture suggest that statins might modulate APP processing and shift the
        balance toward “healthy” (non-amyloidogenic) cleavage.
        In their quest to understand how statins affect APP processing, Sam Gandy and colleagues
        focused on a molecule called ROCK1, a kinase enzyme that had recently been implicated in
        APP processing. The theoretical link between statins and ROCK1 goes as follows: statins
        inhibit the isoprenoid pathway, isoprenoids are regulators of the enzyme Rho, and Rho in
        turn activates ROCK1. And while such potential connections could be drawn for any number of
        molecules, Gandy and colleagues went on to test whether statins exert their effect on APP
        cleavage by interfering with the isoprenoid/Rho/ROCK1 pathway.
        Working in mouse neuroblastoma cells, they confirmed that two different statins
        increased healthy cleavage of APP. When they directly interfered with the
        isoprenoid/Rho/ROCK1 pathway by adding a drug that inhibits Rho activation, they saw
        effects similar to those of the statins (i.e., an increase in healthy cleavage). The same
        effects were seen when they transfected the cells with a dominant negative form of ROCK1
        (which inactivates the normal ROCK1 molecules in the cell); this outcome shows that the
        pathway can influence APP cleavage. Most conclusively, when they added a version of ROCK1
        that was constitutively (always) active, they reduced basal levels and abolished
        statin-stimulated levels of healthy cleavage.
        Taken together, these results suggest that statins influence APP processing, at least in
        part, by modulating the isoprenoid pathway and inactivating the ROCK1 kinase. Future
        studies are necessary to determine whether this mechanism is actually responsible for the
        apparent clinical benefits of statins. Another question worth exploring further is whether
        ROCK1 might be a suitable target for therapeutic interventions that aim to decrease
        harmful, and promote healthy, cleavage of APP.
      
    
  

  
    
      
        
        The HIV epidemic is continuing to grow, year by year. According to the Joint United
        Nations Programme on HIV/AIDS (UNAIDS), in 2004 there were more people living with the
        virus than ever before, and in the same year more people than ever before died of it. So,
        although in the developed world HIV/AIDS is a controllable disease, one with which a
        treated person might expect to have a near normal lifespan, in much of the rest of the
        world AIDS is still a death sentence. Despite the fact that the cost of AIDS medicine has
        come down to around $150 per year in the developing world—a much lower cost than
        previously—the drugs are still unavailable to the vast majority of patients. What is more,
        every infected person has the chance of infecting many others. Although huge sums of money
        have been poured into combating HIV/AIDS—around US$4.7 billion in 2003—UNAIDS estimates
        this amount is less than half of what is required by 2005, and only a quarter of what will
        be required by 2007, to mount a comprehensive response to AIDS in low-income and
        middle-income countries. One of the real dilemmas, therefore, of HIV/AIDS policy is
        deciding whether it is better to concentrate resources on prevention of infections or on
        treatment of infected individuals. Each approach has ramifications for the other, as shown
        by the experience in some developed countries, where an increase in availability of
        treatment has been accompanied by an increase in risk behavior.
        An analysis by Joshua Salomon and colleagues in this month's 
        PLoS Medicine suggests that trying to concentrate on one or the other of
        these alternatives is a false dichotomy, and that not integrating the two approaches could
        have a catastrophic effect on the global toll of HIV/AIDS by 2020. In this theoretical
        paper the authors analyze the epidemic in sub-Saharan Africa (where three-fourths of deaths
        from AIDS occur). With no change in current levels of prevention and care, it is predicted
        that there will be 3.7 million new HIV infections and 2.6 million adults dying of AIDS in
        this region each year within the next two decades. The authors predicted that concentrating
        on prevention alone could decrease yearly infections by half, and that concentrating on
        treatment could decrease yearly infections by 6%. However, combining both approaches could
        yield substantially greater benefits than the sum of the two alone—lowering projected new
        infections by 74% and projected annual mortality by half. These percentages translate into
        29 million new infections and 10 million deaths averted between 2004 and 2020.
        The challenge now is obviously how to put these policy suggestions into practice. The
        current World Health Organization treatment target of having three million people on
        antiretroviral therapy by the end of 2005 (the “3 by 5” objective) provides a yardstick for
        only one part of the equation. The authors comment that the mobilization of communities
        that will be needed to achieve the 3 by 5 objective should also be harnessed for
        prevention, and that those who teach prevention must also be allowed to get care for those
        infected. As the authors say, only by doing so “will we at last move from slogans to
        impact.”
      
    
  

  
    
      
        
        Many different things combine to cause epidemics of disease. Among these factors are the
        characteristics of the infecting organism, the resistance of the host, and, as is
        increasingly realized, climatic conditions.
        El Niño, the best known climatic disturbance, is caused by a warming of the Pacific
        Ocean, which then affects the climate globally. Previous work has suggested that this
        recurring phenomenon can have a profound effect on the incidence of many diseases,
        including dengue, malaria, and diarrheal diseases.
        In a paper in this month's 
        PLoS Medicine , Sultan and colleagues from a climate research institute
        and an infectious diseases center in France looked at the relation between climate and
        meningitis outbreaks in Mali in West Africa, a region that every year between February and
        May sees devastating epidemics of meningococcal meningitis affecting up to 200,000 people.
        The most important recurring climatic event in this region is a dry wind, known as the
        Harmattan, that blows throughout the winter, causing a drop in humidity and the production
        of vast quantities of dust.
        What the authors found was that over the years 1994–2002, the week of the onset of the
        yearly meningitis epidemic came at around the same time as the peak of one measure of the
        wind—the sixth week of the year. As Pascual and Dobson say in their Perspective article on
        this study, “Sultan and colleagues' study is exceptional in that it illustrates a clear
        relationship between an external environmental variable and the initiation of disease
        outbreaks.”
        How do climatic changes influence disease? In some cases, such as the role of flooding
        in spreading a waterborne disease, the causes are perhaps obvious, but why should a dry
        wind affect disease incidence? Previous works have suggested that the climate can work in a
        number of ways, by influencing the life cycle of both disease vectors and the
        disease-causing organism, and, as here perhaps, by affecting the resistance of the host.
        Sultan and colleagues speculate that the drying effects of the wind on the mucous membranes
        could increase the chances of the organism getting established in the human host. Whatever
        the causes, one very useful feature of climate is that, once the patterns are understood,
        they can often be predicted.
        A way of predicting these meningitis epidemics could be enormously useful. Sultan and
        colleagues looked at only a few years, but if these findings are confirmed over a longer
        time period, they could make preparing for an epidemic much more efficient.
      
    
  

  
    
      
        
        The study by Planche et al. [1] provides important new information addressing
        intracellular volume depletion in children with severe childhood malaria, but does not
        address the question of whether intravascular volume depletion (hypovolemic shock) is
        present. Using sophisticated methodology to determine total body water and extracellular
        water, they demonstrate a 6.7% deficit in total body water and an 11.7% deficit of
        intracellular water, providing an important indication of the volumes of fluid that may be
        required to optimize hydration. The data, however, do not address the degree of filling of
        the intravascular compartment, nor should they be used to answer the question about the
        state of tissue and organ perfusion. Indeed, we believe that these new data present no
        conflict with our previously reported findings. Using methods to study critical illness
        physiology that are widely employed within pediatric intensive care units for
        interpretation of circulatory status, we have demonstrated evidence for hypovolemia in 53
        Kenyan children with severe malaria complicated by metabolic acidosis [2]. Our children
        were younger, had longer capillary refilling times (>3 s), lower central venous
        pressures (mean 2.9 cm H
        2 O) and higher creatinines (>80 µmol/l): all features of compensated
        hypovolemic shock. Furthermore, hypotension (systolic BP < 80 mm Hg) was present in 44%
        of children with severe acidosis (base deficit >15). These findings also indicate
        important baseline differences in two cohorts of children studied. We agree that
        reconsideration of guidelines for acute fluid management is warranted, particularly when
        current recommendations await an adequate evidence base. Nevertheless, conflicting opinions
        on the question of volume status in children with severe malaria can be satisfactorily
        resolved only through prospective randomized trials that include both fluid resuscitation
        and control groups. While the design and conduct of such trials will involve considerable
        challenges, optimal fluid management will never be resolved on the basis of theoretical
        consideration alone.
      
    
  

  
    
      
        
        Dr. Gerberding outlines critical steps for arresting the HIV/AIDS epidemic [1]. She
        suggests moving ahead with “ABCs” and with “D” for diagnosis and “R” for responsibility.
        These are good suggestions—with increased HIV testing and individuals taking responsibility
        for their role in HIV spread, the epidemic might be slowed. We could continue to add
        incrementally to the alphabet soup of public health. But instead, we could choose to
        immediately implement the mainstays of public health—universal testing and contact tracing
        [2,3,4]. Every sexually active individual and every individual at risk for HIV deserves to
        know their HIV status. Thus, every HIV-infected individual must be called upon to be
        accountable for preventing HIV transmission. Contact tracing should be instituted for HIV
        just as it is for other infectious diseases. Those who have been exposed to HIV have a
        right to know how to protect themselves and if they too are infected, to be offered
        treatment [5]. HIV testing has too often focused on testing of women in a perinatal setting
        rather than universal testing in routine clinical care. Without universal voluntary HIV
        testing and contact tracing, we will see the continued tilt of the epidemic toward women,
        now at 55% of all HIV infections and in all likelihood at 75%–80% in another 8 to 10 years
        [6,7]. For too long the debate has been that contact tracing will result in physical abuse
        of women. Confining our definition of abuse of women to physical abuse alone is to have too
        narrow an ethical focus—HIV infection itself is an abuse of women or of anyone else.
        Universal HIV testing and contact tracing adds an essential comprehensive public health
        approach to the epidemic that will be successful in reducing the ever-escalating numbers of
        new infections.
      
    
  

  
    
      
        
        Despite the significant progress in increasing our understanding of the immune
        mechanisms of multiple sclerosis (MS), in improving clinical classification and brain
        imaging, and in developing new treatments, the factors that determine the course of the
        disease are mostly unknown [1]. Currently, it is nearly impossible to predict the course of
        MS, its severity in terms of disability progression, or when a relapse will happen.
        The most commonly used disease-modifying therapies are interferon β (IFNβ) [2] and
        glatiramer acetate [2,3]. Despite initial excitement, these therapies have beneficial
        effects in some, but not all, patients [2,3]. Because of the potential favorable effects of
        these therapies, it has been suggested that they should be initiated as early as possible
        to maximize neuroprotection [4]. Additionally, it has been recommended that patients should
        be monitored closely to determine whether and when it is necessary to modify treatment in
        order to maximize the benefit [5]. The recommended monitoring is based on annual rate of
        relapses, neurological deterioration, and evidence of disease activity on brain magnetic
        resonance imaging scans. However, given the destructive nature of the disease, if we rely
        solely on clinical or radiological manifestations (such as a relapse or a new lesion on a
        scan) to determine a patient's response to therapy, we will probably be responding too
        late.
      
      
        Gene Expression Patterns in Affected Organs
        The diagnosis and management of disease could be transformed thanks to the completion of
        the human genome project, the availability of sequence information for nearly every gene,
        and the advent of novel high throughput technologies (microarrays—see Glossary) that allow
        parallel profiling of thousands of genes. By definition, nearly every aspect of a disease
        phenotype should be represented in gene expression signatures of multiple genes in the
        affected organ. Indeed, studies that analyze affected tissues (mostly in cancer) clearly
        show that it is possible to predict prognosis, to identify new classes of diseases, and
        potentially to determine response to therapy [6,7,8].
        In diseases that do not require tissue resection for diagnosis or therapy, it is rare to
        obtain tissues for analysis. This problem is even more pronounced in diseases like MS, in
        which the target organ is the very inaccessible brain and spinal cord. Despite these
        limitations, several groups used microarrays to analyze brain tissues obtained posthumously
        from patients who had MS and identified genes that characterized either acute or chronic
        lesions [9,10,11]. However, although these studies identified some potential genes that may
        be involved in the local pathogenesis of the disease, they did not produce any information
        that could be used for identifying biomarkers associated with disease activity.
      
      
        Diagnostic Peripheral Blood Mononuclear Cell Gene Expression Signatures
        In MS, looking for markers of disease activity in the much more accessible peripheral
        blood does not require a significant leap of faith. MS is an autoimmune disease, and it is
        possible that some of the cells involved in the pathogenesis of the disease will be found
        in the bloodstream. Abnormal T cell populations have repeatedly been observed in the
        peripheral blood of patients with MS [12,13,14]. While these results supported looking at
        the easily accessible peripheral blood mononuclear cells (PBMCs) for potential markers that
        reflect the disease, some doubts persisted. These revolved around two very strong
        arguments. The first argument was that if the signal comes from a minority of the cells
        within the bloodstream it will be too low to be detected. The second was that interpersonal
        variability, added to the inherent noisy nature of gene expression data, will make the data
        impossible to reproduce.
        Fortunately, recent observations suggest that these doubts are unfounded. Bomprezzi et
        al. [15] determined that gene expression patterns can distinguish patients with MS from
        controls and suggested that at least some of the differences identified were derived from
        activated T cells. Achiron et al. [16] analyzed the expression of 12,000 genes in patients
        with relapsing–remitting MS. Gene expression patterns clearly distinguished patients with
        MS from controls as well as relapse from remission. Mandel et al. [17] compared patients
        with systemic lupus erythematosus and MS, and identified a common autoimmunity signature as
        well as disease-specific gene expression signatures. Interestingly, similar findings were
        recently described for pulmonary arterial hypertension [18].
      
      
        Could PBMC Gene Expression Signatures Be Used for Predicting Response to
        Therapy?
        Weinstock-Guttman et al. [19] analyzed the acute transcriptional response of 4,000 genes
        in peripheral blood lymphocytes to IFNβ. They identified increases in known
        interferon-inducible genes, and in genes involved in antiviral activity and interferon
        signaling. Using complementary DNA (cDNA) arrays, Sturzebecher et al. [20] identified gene
        expression signatures that distinguished IFNβ responders from nonresponders.
        And now, in a new study published in last month's 
        PLoS Biology , Baranzini et al. [21] provide compelling evidence that
        these PBMC gene expression signatures can be used to predict response to therapy (Figure
        1). They studied the expression of 70 genes selected for their presumed biological function
        in 52 patients with MS, followed up for at least two years after initiation of IFNβ
        therapy. Instead of using microarrays that carry probes for thousands of genes, they chose
        to use real-time PCR. This method is highly sensitive, specific, and reproducible across
        different laboratories. It is often used to verify microarray findings. Baranzini et al.
        identified MX1 (interferon-inducible protein p78), a known interferon-inducible gene, as
        the marker of treatment with IFNβ. They did not find overall differences between responders
        and nonresponders, but they did, using supervised classification methods, identify triplets
        of genes that distinguish IFNβ responders and nonresponders.
        Interestingly, individual and pairs of genes did not perform that well, and all three
        genes in a triplet were required for the highest accuracy (about 80%–90%). The minimal
        combinatorial number of genes that contains the most predictive information is not
        available since combinations of more than three genes were not performed. Although the
        results were not tested on an independent dataset, as is frequently requested [22], the
        authors applied an array of cross-validation strategies that convincingly suggested that
        the identified predictive signal was robust.
      
      
        Implications of the Study
        What could Baranzini and colleagues' findings mean? Clearly, the most obvious conclusion
        is that the lack of response did not result from the deactivation of IFNβ. The effect of
        IFNβ on MX1, IFNAr1, and STAT2 was observed for two years in all patients, suggesting that
        the response did not depend on IFNβ bioavailability. Considering that PBMCs represent an
        admixture of multiple cell types, the most plausible explanation is a simple lack of shift
        in subcellular populations.
        However, the importance of Baranzini and colleagues' study lies not in its mechanistic
        insights, but in its clinical relevance. The careful design of the experiment, the use of
        reproducible real-time PCR instead of microarrays, the meticulous analysis, and the
        previous observations [15,16,17,19,20] support the notion that PBMCs express clinically
        relevant gene expression signatures in MS and probably in other organ-confined diseases. To
        further prove this notion will require a significant investment in large studies that
        prospectively test the utility of these signatures in guiding the management of MS. Only
        when direct evidence shows that therapy guided by markers expressed in PBMCs improves
        patient outcome will PBMC gene expression patterns take their place as biomarkers at the
        center stage of monitoring MS progression and response to therapy.
      
    
  

  
    
      
        
        Over the last hundred years, there have been major triumphs in medicine related to
        public health, vaccination, and the introduction of new medicines. However, over the same
        period, several diseases have increased in prevalence and/or severity. In some cases, the
        causes of the increase have become obvious—the increases in lung cancer, coronary artery
        disease and type 2 diabetes, for example, are not considered to be a mystery. On the other
        hand, a large group of diseases related broadly to “inflammation” have also increased. For
        these, a wide range of hypotheses about causation have been proposed. Type 1 diabetes,
        rheumatoid arthritis, and inflammatory bowel disease have increased since 1980 [1]. Some
        analyses of the increase in hay fever and asthma would suggest a similar time course, and
        this parallelism of the time frame has been taken to suggest that there could be a common
        cause. Indeed, there is a proposal that these diseases are all related to some changes in
        “cleanliness” or “hygiene” that have resulted in decreased activation of a common control
        mechanism. Specifically, this control has been ascribed to T regulator cells, which produce
        interleukin-10 (IL-10) or transforming growth factor-beta.
      
      
        Change in Living Conditions
        The major changes in rural areas, tropical villages or in Europe pre-1900 that could be
        related to the change in immune responses include: decreases in helminth infection;
        physical proximity to farm animals [2]; exposure to those mycobacteria that are commonly
        found in the soil; bifidobacteria colonization of the gut; as well as decreased prevalence
        of Hepatitis A infection [3]. The implication in each case is that asthma has increased
        secondary to an increase in inflammation or an increase of the allergic response that is
        closely associated. This assumes that hay fever, asthma, and other diseases have increased
        in parallel, which is probably not true.
        Hay fever became a problem in Northern Germany and England as early as 1900. Clear
        evidence for this view comes from Noon's description of the development of immunotherapy
        for hay fever in 1911, the studies on hay fever prevalence by Ratner and Silverman in New
        York in 1935, and the recognition that hay fever was a major community problem in New York
        in 1946. More recently, Harold Nelson analyzed all the studies on hay fever published in
        the United States and found a prevalence of ~15% in 1960, with no convincing evidence of an
        increase since then (H. Nelson, personal communication).
        If seasonal hay fever was epidemic in 1960, then the subsequent increase in asthma has
        to be seen in a different light. The best estimates for the start of the asthma epidemic
        are around 1960 for such diverse populations as army recruits in Finland [4], school
        children in Birmingham in the United Kingdom, and African American children in Charleston,
        South Carolina, United States [5]. In each of these studies, the increase in asthma
        symptoms or disease has been greater than tenfold. However, the absolute values of the
        change have been dramatically different in New Zealand and Scotland (from ~2% up to 20%)
        compared with Finland (from 0.2% to 4%) [6]. Furthermore, some countries have experienced
        much greater increases in hospitalization than others. Clear evidence for increases in
        mortality has only come from the United Kingdom, New Zealand, and the United States,
        countries with a high prevalence of symptoms and hospitalization.
        Several hypotheses have been proposed to explain the increase in asthma (Box 1). At this
        point, our questions about the increase in asthma are: 1) Is an increase in allergy or hay
        fever a necessary precursor for the increase in asthma, a parallel event, or separate? 2)
        Why did the increase in asthma have such a consistent time course throughout the Western
        world in countries where changes in infectious diseases have occurred very differently? and
        3) Why is asthma more common and more severe in some countries than in other countries that
        have had an equal scale of increase (i.e., tenfold) over the same time course?
      
      
        The Persistent Association between Specific IgE, Total IgE, and Asthma
        Long before the first case control or prospective study, the association between allergy
        and asthma was obvious in case series. These earlier studies reported skin testing with an
        amorphous extract called “house dust,” but it was not until the identification of dust
        mites that the association was clarified [7]. Indeed, as late as 1978, there were
        significant doubts that allergens played a role in asthma. Using extracts of 
        Dermatophagoides pteronyssinus (dust mites), the strong
        association between this allergen and asthma was established in many parts of the world,
        with odds ratios as high as 6 or even 10 [7,8]. The possibility of a causal relationship
        was further supported by bronchial challenge studies and avoidance experiments [9]. The
        cohort in Poole, Dorset was, until recently, the only study with household measurements of
        allergen and the results strongly suggested that exposure in the child's own house was the
        primary determinant of sensitization [8]. Subsequently, studies from other parts of the
        world provided evidence about other indoor allergens, particularly cats, dogs, and the
        German cockroach [10,11]. These studies showed that perennial exposure to allergens was an
        important cause of inflammation in the lungs and associated nonspecific bronchial
        hyperreactivity (Figure 1). In most of the case-control and prospective studies,
        sensitization to seasonal pollens has not been significantly associated with asthma [7].
        This is an important issue; if allergy is associated with asthma for genetic reasons or
        because of some common immunological feature, it is not clear why the association should be
        with perennial allergens only.
        Since assays for total serum IgE became available in the 1970s, it has been clear that
        patients with asthma have, on average, higher total IgE than patients with hay fever or no
        allergy. Indeed, by 1980 this was considered an established fact in textbooks of
        immunology. It was assumed that the increased total IgE related to allergen-specific
        responses. However, in some studies, the association between total IgE and asthma was
        stronger than the association between asthma and specific IgE. In 1989, Burrows et al. went
        further and suggested that specific IgE correlated with hay fever, while total IgE
        correlated with asthma [12]. The implication was that IgE has a complex relationship with
        asthma that is not dependent on specific allergens.
        The strength of the association between asthma and total IgE raises questions that have
        not been resolved. Do specific IgE antibody responses contribute to or even push total IgE?
        If so, do the IgE antibody responses to some allergens have more effect than others? This
        question is relevant both to attempts to explain major differences in total IgE between
        countries and to studies on acute asthma. In emergency room and hospital studies, the
        geometric mean total serum IgE of patients with asthma is often greater than 200 IU/ml
        higher than values found in population-based studies. Recent work from Heymann et al. and
        Green et al. on patients hospitalized for asthma has suggested that the interaction between
        rhinovirus and allergy occurs predominantly among patients with total IgE > 200 IU/ml
        [13]. Thus, the different properties of allergens could influence both the prevalence and
        severity of asthma. However, the properties of the dominant allergens do not explain the
        overall increase in prevalence, which has occurred in countries with very different houses,
        climates, and traditions of domestic pet ownership.
      
      
        The Paradoxical Relationship between Cat Ownership and Sensitization: Significance
        for Prevalence or Severity
        Antigen exposure is considered to be a primary requisite for immune responses, and
        allergen-specific responses are no exception. There are many examples of allergens that are
        not significant in areas where the allergen is not encountered. For example, the pollen of
        olive trees is not relevant in northern countries, dust mite allergens are not significant
        in the northern part of Scandinavia or the mountain states of the United States, and
        cockroach allergens are not significant in suburban areas of the United States, the United
        Kingdom, or New Zealand.
        For dust mites, there is a wide range of evidence that increased exposure increases
        sensitization. Homes in Sweden, Berlin, the United Kingdom, and New Zealand have
        progressively higher concentrations of mite allergen and progressively higher prevalence of
        sensitization to mite allergens [7]. But there is now evidence that increasing exposure to
        cats does not lead to a higher prevalence of allergy [14,15,16]. On a population basis, the
        effect may be profound; sensitization to cats among school-age children is generally ~10%,
        while mite sensitization is often as high as 30% (Figure 2). This effect cannot be ascribed
        to inadequate exposure, since all estimates of the quantity of cat allergen inhaled are
        higher than for dust mites. Furthermore, the quantity of cat allergen found in schools or
        even in houses without cats is sufficient to sensitize at-risk children [11].
        In fact, children raised in a house with a cat were less likely to be sensitized to cats
        [15]. Initially, it seemed possible that the effect was an example of reverse causation.
        However, the effect has been observed in countries where a large proportion of families
        keep cats, and very few families report choosing not to own a cat because of asthma in the
        family. Furthermore, the presence of a cat in a house in New Zealand does not decrease IgE
        antibody response to dust mites—in other words, tolerance to cats can be cat-specific [17].
        Understanding how the response to cat allergen is controlled could provide an insight into
        how both the prevalence and the titre of IgE antibody responses in general are (or should
        be) controlled. It seems inevitable that the primary control is by T cells specific for cat
        allergens. Indeed, there is already excellent evidence that injection of peptides derived
        from the cat allergen Fel d 1, which give T cell responses, can be used for immunotherapy
        [18]. Studying overlapping peptides of Fel d 1, we identified a striking response to two
        peptides at the terminal end of Chain 2. Furthermore, both allergic and tolerant
        individuals respond to these peptides by producing IL-10 and interferon gamma [19]. The
        implication is that Fel d 1 inherently induces control, and that this control influences
        both allergic and non-allergic responses to cat allergen (Figure 3). In keeping with this,
        IgE antibody responses to cat allergen are not quantitatively as high as those to dust
        mites [17].
        One of the central assumptions of the cleanliness hypothesis is that regulation of
        immune responses is, at least in part, non-specific. It is assumed that helminth infection,
        mycobacteria, Hepatitis A, endotoxin, and early-life infections can create a milieu that
        leads to a decrease in allergic responses in general. Likewise, we might expect that
        exposure to high concentrations of cat allergen, which can induce IL-10 producing cells,
        should have a general effect. There is some evidence for this hypothesis. In Detroit, the
        presence of two or more animals in the house tended to reduce allergy in general. In
        Sweden, the presence of a cat in the house is associated with decreased sensitization to
        cat, and also to birch and dog. However, in the United States and New Zealand, the presence
        of a cat in the home has no effect on the prevalence or titre of IgE antibody to dust mites
        [15,17]. Thus, it is clear that under some circumstances the tolerance to cats can occur in
        highly atopic individuals and be cat-specific. This phenomenon is not in keeping with any
        version of the cleanliness hypothesis. It has been proposed that animals in the home could
        have a non-specific effect because they shed or encourage endotoxin. However, recent
        studies on airborne endotoxin found that the presence of cats had no effect, and that there
        were significantly lower endotoxin levels in houses with cats compared to dogs.
      
      
        The Possible Role of Lifestyle Changes
        Over the last half of the 20th century, there have been major changes in diet and
        physical activity. The most obvious result of these changes is an increase in obesity,
        which has reached epidemic proportions in the United States. Since 1994, there have been
        multiple reports of an association between elevated body mass index (BMI) and asthma [20].
        In 1996, we suggested that changes in physical activity could be related to bronchospasm
        [21]. Thus, there are three distinct but strongly interrelated aspects of lifestyle that
        could be relevant to the prevalence and severity of asthma: diet, physical activity, and
        obesity.
        It is much easier to document BMI than diet or physical activity, and although some of
        the obesity data are convincing, they are not consistent and certainly not comparable to
        the association between obesity and diseases such as type 2 diabetes in childhood. In a
        typical study, Camargo et al. found that the prevalence of wheezing was 13% among the
        heaviest quintile and 7% among the lowest quintile for BMI [22]. Comparable values for type
        2 diabetes would be 5% and 0.1%.
        Studies using questionnaires have attempted to ask whether there is a relationship
        between wheezing and physical activity. However, the track record for questionnaires on
        this subject is very poor. Westerterp and his colleagues have reported two observations:
        first, that general activity contributes more to energy consumption than “aerobic exercise”
        does and second, that many subjects who initiate an exercise program (such as a twice
        weekly visit to the gym) overcompensate so that they actually decrease overall activity
        [23]. Recently, we have documented a decrease in activity among children (age ~4 years) in
        the United States Head Start program (a child-development program that aims to increase the
        school-readiness of young children in low-income families) who have a history of wheezing
        [24]. Although there are several possible explanations for this result, it seems clear that
        decreased activity can be present before elevated BMI.
        The next question to ask is whether physical activity would have a beneficial effect on
        asthma? Fredburg concluded that full expansion of the lungs had a more potent effect on
        bronchial smooth muscle than isoprenaline [25]. Togias and his colleagues have shown that
        prolonged shallow breathing (=20 minutes) can lead to increased non-specific bronchial
        reactivity [26]. It is obvious that expansion of the lungs is decreased during prolonged
        periods of sitting down. However, periodic expansion of the lungs occurs with sighs. Recent
        data from our group shows that “sigh rates” while seated are very variable but are
        significantly lower when watching a video than while reading [27]. Thus there is a real
        possibility that some forms of childhood behavior—TV, videos, computer games, etc.—might be
        associated with sigh rates low enough to increase non-specific bronchial
        hyperreactivity.
        A different explanation for the effects of physical activity comes from evidence that
        physical activity can be “anti-inflammatory.” This evidence relates to several different
        models though not at present to the lungs. In some studies, obesity appears to be a risk
        factor for wheezing among non-allergic children. However, in most studies, the association
        between allergen sensitization and asthma has been found in obese and non-obese children
        equally [28]. Obviously, some obese individuals are unfit and become breathless on
        exercise. In addition, these individuals may have sleep-disordered breathing. Thus, there
        are other conditions that are easily confused with exercise-induced asthma or nocturnal
        asthma. Taken together, there are excellent reasons for asking whether lifestyle changes
        have contributed to the increased prevalence or severity of asthma. However, it seems
        unlikely that this effect occurs on normal lungs, so the hypothesis has to be that
        decreased physical activity in patients who are allergic can allow persistent or increased
        severity of wheezing.
      
      
        Conclusions
        Although the explanation for the increase in asthma is not yet clear, it is possible to
        put forward a model that includes elements of each of the three main hypotheses. Children
        raised in the tropics, on farms, or in villages such as those in Africa or Papua New Guinea
        have exposure to endotoxin or infections sufficient to interfere with the development of
        allergen-specific IgE antibody responses. Once water supplies are clean, and major
        infectious diseases have been controlled, allergic diseases will appear.
        However, asthma appears to be associated with perennial, i.e., indoor exposure, and may
        be more common or more severe in countries where mites or cockroaches are the major source
        of allergens. Even with indoor allergen exposure, wheezing may remain transient or mild,
        provided prolonged outdoor play is normal. It is the combination of the control of
        infectious diseases, prolonged indoor exposure, and a sedentary lifestyle that is the key
        to the asthma epidemic and, in particular, the key to the rise in severity. Using this
        analysis, the severity of asthma in North American cities becomes much easier to explain.
        Children in New York, Atlanta, Philadelphia, and Washington, D.C. spend long hours indoors,
        have high exposure to mite, cockroach, and/or rodent allergens, and have very low levels of
        physical activity.
        In conclusion, it appears that these combined factors are the key to the asthma epidemic
        and, in particular, the key to the rise in severity. We clearly need to develop ways to
        increase prolonged physical activity, both among patients with asthma and in the general
        population. We also need to investigate whether prolonged moderate activity is beneficial
        in the treatment of asthma and/or is “anti-inflammatory.” What is equally clear is that the
        current obsession of the medical profession with the pharmaceutical management of asthma
        (as well as other lifestyle-related diseases) does not address the reasons why the disease
        has become so common and so severe.
      
    
  

  
    
      
        
        In 1997, United States President Bill Clinton announced the challenge to develop an AIDS
        vaccine by 2007. Since 1997, the AIDS Vaccine Advocacy Coalition (AVAC) has published
        annual reports on the global status of the effort to meet Clinton's deadline. Last year's
        report, entitled “AIDS Vaccine Trials—Getting the Global House in Order,” officially ends
        the countdown. Saying that “we are on a long term mission,” AVAC concludes that there will
        not be a safe and efficient vaccine in 2007, and that we need to “focus on the long haul
        and set an agenda for sustained and sustainable action that stretches well beyond 2007.” It
        is not that there are no vaccine candidates in clinical trials, but there is little hope
        that any of the current candidates will turn out to be a cheap and safe vaccine that
        affords long-term protection.
        Among notable developments over the past 12 months, the AVAC report highlights the
        Global HIV/AIDS Vaccine Enterprise as an effort to improve coordination within the AIDS
        vaccine field. The Enterprise was announced in June 2003 and now shares its scientific
        strategic plan with everyone affected by the AIDS pandemic—that is, all of us—by publishing
        it in 
        PLoS Medicine (DOI: 10.1371/journal.pmed.0020025).
        In its plan the Enterprise presents itself as a global endeavor and emphasizes the need
        for integration and capacity building around the world. It is not “a discrete organization
        with a pool of money” but a “coordinating group of individual funding agencies that will
        support specific areas of research using their own mechanisms, according to their own
        practices and policies, and following the Enterprise's principles.” These principles
        include collaboration, standardization, and coordination among international researchers
        and agencies. The plan focuses on specific scientific roadblocks that need to be overcome,
        but also looks ahead and mentions the need to build capacity for product manufacturing and
        clinical trials, and to address regulatory issues.
        These are noble goals, and the fact that they are stipulated jointly by many of the
        leaders in the field will generate excitement and expectations, even though much of what is
        said has been said before. The plan stresses collaboration and coordination; there are
        clear benefits from a concerted effort. But might a level of competition, rather than
        collaboration, be healthy, and, if so, what level of competition would work best? The
        Enterprise members seem to have wrestled with that question. The plan mentions an
        “appropriate balance between productive competition and effective collaboration,” and
        suggests that certain incentives could be provided by “the funders with greatest
        flexibility.” As long as it remains unclear where scientific breakthroughs will come from,
        diversity and flexibility should be encouraged and not stifled. David Ho, in his
        Perspective on the plan (DOI: 10.1371/journal.pmed.0020036), mentions the danger of “group
        think,” and the Enterprise must not fall into that trap.
        Notably absent from this initial plan is any mention of a timeline or milestones. The
        remit of the plan's authors was not to prescribe specific research but “to stimulate both
        researchers and funders to explore new, more collaborative, cooperative, and transparent
        approaches…in addition to continuing the productive, high-quality approaches already
        underway.” However, without a timeline, the plan fails to convey a sense of urgency. This
        is problematic, as any delays in developing a vaccine will increase the burden from
        HIV/AIDS in the parts of the world that can least afford it. To accelerate vaccine
        development, the plan urgently needs to be supplemented with a list of specific tasks,
        responsible individuals, necessary resources, and allocated time.
        The next document from the Enterprise must provide specifics on project management,
        although one problem with putting a time frame on HIV vaccine development is a fundamental
        one: we do not know whether it is actually possible to develop a safe and effective
        vaccine. (One assumes the Enterprise members agree, though there is no explicit
        acknowledgement of this uncertainty in the plan.) Moreover, provided it can be done, it is
        impossible to predict when the necessary scientific advances will happen. That said,
        without a list of specific projects, project leaders, and a time frame for achieving or at
        least evaluating specific goals, it will be impossible to define success and failure,
        review progress, and assure internal and external accountability.
        There is another reason why a best-guess timeline is essential: realistic expectations
        about an AIDS vaccine would stress the urgency of combating the AIDS pandemic over the next
        decade—and maybe longer—in the absence of an effective vaccine. The potential benefits of a
        vaccine cannot be overestimated, and its development has to be one top priority for the
        global scientific community. But its success cannot be taken for granted and will come too
        late for millions. Therefore, parallel efforts to prevent or reduce transmission and to
        treat infected individuals need to be accelerated now.
        The Enterprise's plan should be hailed as a crucially important outline for vaccine
        development, but the goodwill surrounding it won't last unless it is quickly followed up
        with a set of milestones, and a transparent process by which progress will be measured and
        course corrections implemented.
      
    
  

  
    
      
        
        The scientific strategic plan of the Global HIV/AIDS Vaccine Enterprise, published in
        this month's 
        PLoS Medicine , is a clear and cogent document describing how major
        funders and stakeholders in HIV vaccine development should move forward in a collaborative
        fashion [1]. There is no doubt that this roadmap will be regarded as a useful instrument to
        bring greater cohesion and coordination to the field. The individuals who championed this
        effort should be commended for providing a great service to the scientific community. It is
        an excellent start to a continuing dialogue of utmost importance.
      
      
        The Challenge
        Why is it that we still do not have a protective vaccine against HIV 22 years after its
        initial identification? Many possible explanations come to mind.
        In the natural course of HIV infection, the virus wins 99% of the time, showing that
        specific immunity in an infected person is unable to completely clear the virus. We have
        also known for over a decade that primary HIV isolates are relatively resistant to antibody
        neutralization, probably because of a “protective shield” on the viral envelope
        glycoproteins, consisting of variable loop sequences and extensive N-linked glycosylations.
        Another explanation is the extreme plasticity of HIV that allows new viral variants to
        evade immune recognition in the same way that they escape from drugs. Moreover,
        superinfection by a second viral strain has been documented in a number of individuals who
        have already mounted immune responses to the initial HIV infection. Yet another problem is
        that the AIDS research community has yet to uncover the correlates of immune protection in
        vivo. Lastly, proven vaccine approaches from the past have either failed (whole killed
        virus and subunit vaccines) or faced seemingly insurmountable regulatory hurdles (live
        attenuated virus vaccine).
        Given these daunting obstacles, why have so many continued in the long struggle to
        develop an HIV vaccine? The answer must lie, in part, in the noble cause at hand. Yet there
        are also some encouraging clinical and experimental observations (Figure 1). Rare patients
        do control HIV infection spontaneously. Certain people remain virus-negative despite
        repeated exposures. That superinfection is not more commonly found supports the notion of
        immune control. Vaccine-mediated protection against simian immunodeficiency virus is indeed
        possible using live viruses attenuated by specific mutations or by pharmacological
        interventions. Finally, and perhaps most importantly, HIV transmission by sex in the
        natural setting is typically inefficient (and thus easier to block), unlike most
        experimental challenge systems employed in monkey studies to date. Collectively, these
        findings provide a ray of hope to push on.
      
      
        The Enterprise
        The scientific strategic plan of the Enterprise is spot-on in identifying the major
        roadblocks in HIV vaccine development, as well as in establishing the key scientific
        priorities as we see them today [1]. It rightly recommends the formation of a growing
        alliance of organizations to foster a better collaborative spirit that could lead to, among
        other things, stronger political support and increased funding. The proposed greater
        coordination and management, sharing of information, technologies, and reagents, and
        harmonization of standards, assays, and approaches could only add to our overall
        efforts.
        One might ask, however, whether there are potential downsides to the plan. In the name
        of continuing this important dialogue, I would like to offer one general concern. Arguably,
        the reason for the lack of an effective HIV vaccine today is rooted in the basic problems
        posed by the virus itself. What we need foremost are new scientific solutions, although a
        prim and proper “process and structure” in our approach will be helpful. The needed
        breakthroughs to develop a vaccine will likely emerge from the creativity of scientists
        doing fundamental research that is free of preconceived biases. It is my contention that
        great new ideas are as likely to come from curiosity-driven basic studies as from the
        mission-oriented approach that is represented by the new proposal. Therefore, the
        leadership of the Enterprise must safeguard against the kind of “group think” that is so
        pervasive in large collaborative endeavors of this nature. The views of a small number of
        researchers, no matter how smart or accomplished, must not supersede the collective wisdom
        of the scientific community at large.
        No doubt important contributions will be made by scientists working outside of the
        Enterprise. Measures should be taken to ensure that their views and approaches, even if
        deemed unconventional, are not stifled by the newly established system. Likewise, their
        research support should not be compromised because the creation of the Enterprise
        concentrates the funding into the hands of a relatively small number of designated
        scientists. To me this is a serious risk given the current “flat funding” at the National
        Institutes of Health.
      
      
        The Future
        The authors of the “The Global HIV/AIDS Vaccine Enterprise: Scientific Strategic Plan”
        have laid out a timely and insightful plan to address perhaps the greatest public-health
        need of the millennium. This document and its later revisions will serve as useful
        guideposts for the AIDS vaccine development effort for years to come. To be successful in
        this mission, our research community will ultimately need a specific “scientific blueprint”
        for making an HIV vaccine. That day will come only after we get another shot in the arm,
        infusing us with new knowledge and know-how. Is there any doubt that we need to redouble
        our investment in basic research on the challenges posed by HIV?
      
    
  

  
    
      
        
        There has been a recent and dramatic rise in global funding for HIV/AIDS, from US$2.1
        billion in 2001 to US$6.1 billion in 2004 [1], thanks to several new funding mechanisms
        (Box 1). These funds, coupled with reduced drug costs, make it feasible to roll out
        antiretroviral therapy (ART) even in resource-poor settings. Nevertheless, the total number
        of people living with HIV rose in 2004 to reach its highest level ever: an estimated 39.4
        million people are living with the virus, including 4.9 million who acquired it in 2004
        [1]. Therefore, the debate over the appropriate distribution of money between prevention
        efforts (such as voluntary counseling and testing [VCT], or behavior change) and treatment
        efforts (the provision of ART) is now more topical than ever.
      
      
        Balancing Prevention and Treatment
        The scale of the proposed increase in the number of patients receiving ART raises
        numerous questions about the treatment itself. Which drugs will be used? How much will it
        cost? How will their quality be monitored and assured? How will they be distributed? Who
        will be eligible? How will the desired level of treatment be sustained? Is there adequate
        infrastructure and human resources to support the expanded services?
        The commitment of substantial funding to treatment in resource-poor countries also has
        implications for the prevention efforts in those same countries. In many Western countries
        and Brazil (the sources of the majority of the available data on the subject), the
        impressive drop in mortality due to HIV following increased access to ART is coupled with a
        disheartening rise in the number of new cases of HIV, as emphasis and funding are shifted
        from prevention to treatment [2]. Countries in which this pattern has been seen are
        evidence of the pitfalls of failing to adapt prevention efforts once life-extending
        treatment becomes widely available.
        Of course, prevention and treatment are not mutually exclusive. Successful prevention
        efforts mean fewer patients will need the costly drug treatment programs, helping extend
        the sustainability of ART. In turn, the success of ART in prolonging healthy living helps
        prevention efforts by reducing the stigma associated with self-education and responsible
        behaviors.
      
      
        Measuring Prevention and Treatment Effects
        In their study in the January 2005 issue of 
        PLoS Medicine , “Integrating HIV Prevention and Treatment: From Slogans
        to Impact,” Salomon and colleagues use mathematical modeling to assess the epidemiologic
        impact of treatment and prevention efforts, and to quantify the opportunities and potential
        risks of large-scale treatment roll-out. Using a variety of different scenarios, they
        propose methods for establishing the most effective balance between spending on prevention
        and spending on treatment.
        Modeling is a technique used by many scientists, including epidemiologists and
        statisticians, to create a mathematical equation that can be used to determine which
        variables affect an outcome of interest, and to what extent. Once the influential variables
        are determined, a baseline model is established that includes those variables and reflects
        their relative importance to the outcome. The effect of changing the value of any of these
        variables, or several of them, can then be tested, and new outcomes projected. HIV modeling
        is inexact and requires far better data but can nevertheless provide important
        insights.
        Salomon and colleagues used mathematical modeling to assess the effect of changing
        aspects of the HIV/AIDS “equation” on the future course of the HIV/AIDS epidemic. First, a
        baseline model was created to fit expected HIV/AIDS projections for the year 2020 if there
        were to be no change in the current epidemiologic trends—no ART scale-up, and no changes in
        prevention efforts or behavior. Heterosexual contact is the predominant mode of HIV
        transmission across Africa, and Salomon and colleagues' study modeled the disease only
        within the heterosexual population. The model was also tailored to take into account
        epidemiologic, demographic, and sociologic patterns in the eastern, central/western and
        southern regions of Africa. Using the baseline models tailored to each region, the effects
        of prevention and treatment efforts were then measured.
        Two treatment-centered scenarios were tested in which the World Health Organization's “3
        by 5” initiative (see Box 1) was achieved. In these treatment-centered scenarios, the
        reduction of transmissibility, the number of partners of each patient, and condom use were
        either optimal (reduced transmissibility, reduced partners, and increased condom use) or
        less than optimal. The prevention-centered scenario tested the impact of a comprehensive
        package of 12 prevention tools (such as VCT and peer counseling for sex workers), modeling
        only partial effectiveness at the population level, to reflect weaker political and social
        support for HIV control efforts. Finally, combined response scenarios were tested. In the
        first scenario, treatment efforts strengthened prevention efforts as, for example, when the
        availability of ART increases people's willingness to undergo testing. In the second, an
        emphasis on treatment led to less effective implementation of prevention efforts.
        Baseline projections in Salomon and colleagues' study showed that without any behavioral
        change or ART scale-up, the HIV/AIDS prevalence rate would remain relatively stable, but
        the number of new infections would increase by 52.3 million by 2020. Treatment-centered
        scenarios reduced the total number of new infections through 2020 by a maximum of 3
        million, or 6%, while indicating that the number of AIDS deaths through 2020 would decline
        by 13%, to 32.4 million. A prevention-centered strategy would provide greater reductions in
        incidence (36%) and similar mortality reductions by 2020, but more modest mortality
        benefits over the next five to ten years.
        The scenarios in which all of these statistics were most improved, however, were those
        that combined both prevention and treatment efforts. In the scenario in which treatment
        enhanced prevention, Salomon and colleagues projected 29 million averted infections (55%)
        and 10 million averted deaths (27%) through the year 2020. However, if a narrow focus on
        treatment scale-up leads to reduced effectiveness of prevention efforts, the benefits of a
        combined response would be considerably smaller—9 million averted infections (17%) and 6
        million averted deaths (16%) (Figure 1).
        Combining treatment with effective prevention efforts could reduce the resource needs
        for treatment dramatically in the long term. In the various scenarios the numbers of people
        being treated in 2020 ranges from 9.2 million in a treatment-only scenario with mixed
        effects, to 4.2 million in a combined response with positive treatment–prevention
        synergies.
      
      
        Moving Forward
        The authors have demonstrated through mathematical modeling that the integration of
        treatment and prevention is epidemiologically sound. However, an integrated and
        comprehensive program (Figure 2) is not only logical but makes sense from the service
        delivery point of view: it can be cost-effective and ideal for the community.
        
        Effective prevention makes treatment more affordable and
        sustainable. Effective prevention can lead to a substantial reduction in the number of
        new infections and therefore ultimately will lead to a reduction in the number of people
        who will need treatment. The reduction of adult HIV/AIDS prevalence in Uganda from 18.5% to
        6% over the last several years has reduced the number of those eventually needing treatment
        by nearly 68% [3]. Unless the incidence of HIV is sharply reduced, HIV treatment will not
        be able to keep pace with all those who will need therapy [4]. Salomon and colleagues'
        reaffirmation that only effective prevention will make treatment affordable is critically
        important.
        
        Successful treatment and care can make prevention more acceptable and
        effective. Widespread access to treatment could bring millions of people into
        health-care settings, providing new opportunities for health-care workers to deliver and
        reinforce HIV prevention messages and interventions [4]. Improved access to HIV testing
        provides an entry point to both prevention and treatment services and provides a unique
        opportunity to identify and target the infected, vulnerable, and uninfected with more
        appropriate interventions. All health-care settings, including HIV treatment sites, should
        deliver HIV prevention services [4].
        
        Prevention can make treatment more accessible. The early establishment of
        community-based prevention services in rural Ghana was instrumental in reducing the stigma
        of AIDS and improving the knowledge and attitude of the community prior to the development
        of ART and VCT services (K. Torpey, personal communication). This process also made it
        easier for community and implementing agencies to identify and refer patients needing
        treatment services.
        
        Expanded care and prevention activities have synergistic
        effects. Continued effective treatment, care, and prevention programs will reduce the
        number of orphans and vulnerable children, reduce mother to child transmission of the
        virus, and improve the lives of families and the strength of communities.
        
        Integration ensures that prevention activities are not neglected. The
        world has a unique opportunity, as ART services are launched and expanded, to
        simultaneously bolster prevention efforts [4]. Experience in the United States indicates
        that availability of treatment can lead to increased risk behavior [5]. In addition, the
        improvement in the health, well-being, and longevity of people living with AIDS could
        increase the opportunities for HIV transmission. Integration can help reduce these
        potential negative impacts of treatment.
        
        Integration can provide opportunities to address vulnerable groups more
        effectively. A commitment to providing large-scale treatment helps to focus attention
        on communities at greatest risk, particularly in lower prevalence contexts. This provides
        an opportunity to address the prevention and treatment needs of vulnerable groups more
        effectively.
        
        Treatment resources can help improve infrastructure for prevention and other
        health services. The training of health providers and improvements in laboratory
        services, pharmacy, logistics, commodity management, and health information systems can
        benefit both treatment and prevention services. Further, in many countries, a large number
        of health-care workers are themselves infected. Treatment can help to preserve the lives
        and productivity of these critically needed AIDS prevention and treatment workers, as well
        as those of other health professionals.
        
        A long-term decline in AIDS deaths may be preventing new infections. The
        short-term decline in AIDS deaths is driven by effective care and treatment programs, but a
        long-term decline may be driven by the prevention of new infections. Integrated and
        comprehensive strategies are more likely to lead to affordable, sustainable programs.
        
        Success requires dramatic expansion of both ART and prevention. Globally,
        fewer than one in five people at high risk of infection have access to proven HIV
        prevention interventions [6] and less than 10% have access to ART [1]. Unless there is a
        substantial increase in commitment and resources for both prevention and ART, efforts to
        control HIV/AIDS and mitigate its impact will only meet with partial and limited success.
        In addition, to increase resources, intensified commitment is required to ensure every
        opportunity is taken to integrate prevention and treatment. Future analysis and debate
        should move from comparisons of prevention and treatment priorities to a sustained analysis
        of how we can reciprocally integrate and strengthen prevention and care and use every
        opportunity provided by one to reinforce the other. We must focus on the development of
        training, monitoring, and quality assurance systems that ensure that prevention and care
        are integrated whenever possible.
        
        The results of Salomon and colleagues' model need to be
        validated. Further operational research is needed to validate the findings of this
        study.
      
    
  

  
    
      
        DESCRIPTION of CASE
        A 31-year-old white male with no significant past medical history is referred by his
        workplace to a primary care physician for an elevated blood pressure (BP). He presents to
        the clinic with no complaints. His mother and grandmother both have diabetes, and his
        father has hypertension. He has had a 15-pound (lb) weight gain over the last year and has
        become more sedentary.
        His BP is 142/90 mm Hg, pulse is 88 beats per minute (bpm), weight is 209 lb, and height
        is 5′ 11″. On examination he displays moderate central obesity, but otherwise the
        examination is normal. His fasting cholesterol is 228 mg/dl (to convert milligrams per
        deciliter of cholesterol [total, HDL or LDL] to micromoles per liter, divide by 39),
        low-density lipoprotein (LDL) is 166 mg/dl, high-density lipoprotein (HDL) is 32 mg/dl,
        triglycerides (TG) are 223 mg/dl (to convert mg/dl of triglycerides to mmol/l, divide by
        89), and fasting glucose is 114 mg/dl (to convert mg/dl of glucose to mmol/l, divide by
        18).
        
          What Is the diagnosis?
          This patient meets the diagnostic criteria for the metabolic syndrome as defined by
          the National Cholesterol Education Program Adult Treatment Panel III guidelines [1]. Any
          three or more of the criteria make this diagnosis (see Table 1). Intensive lifestyle
          modifications such as exercise and weight loss should be made to improve cholesterol,
          blood pressure, and other cardiovascular disease (CVD) risk factors [2]. It may be timely
          to address the prevention of diabetes in patients with metabolic syndrome since these
          patients are at high risk for development of type 2 diabetes. Lifestyle changes delay the
          onset or prevent the incidence of type 2 diabetes in patients with glucose intolerance, a
          key feature of metabolic syndrome [3]. The patient is started on an exercise and weight
          loss program, sent for nutritional counseling, and scheduled for a return clinic
          appointment for three months later.
        
        
          Two Years Later
          The patient returns to the clinic two years later. He presents with complaints of
          increasing frequency of urination and episodes of blurry vision. He has nocturia and has
          lost 5 lb in the last week. Otherwise, his review of systems is unremarkable. His blood
          pressure is 146/88 mm Hg, pulse 80 bpm, and weight 216 lb. His fundoscopic examination is
          normal. He continues to have moderate central obesity. Current medications are a thiazide
          diuretic, 12.5 mg once daily (QD), started one year prior. A non-fasting blood sugar is
          267 mg/dl.
          
            Can a diagnosis be made?
            There are three criteria for the diagnosis of type 2 diabetes as defined by the
            American Diabetes Association (ADA), of which any one is sufficient to make the
            diagnosis (see Box 1). This patient meets the criteria for type 2 diabetes. He does not
            need to have a fasting blood sugar done because a random glucose greater than 200 mg/dl
            with symptoms of diabetes meets the first criterion. Failing to comply with lifestyle
            modification, his weight has increased 7 lb in two years and likely contributes to his
            development of diabetes. Of note, his recent weight loss is presumably due to overt
            hyperglycemia and glycosuria, further underestimating his true weight increase.
            His additional investigations are as follows: fasting glucose, 215 mg/dl; hemoglobin
            A1c (HbA
            1c ), 8.6%; and urine albumin-to-creatinine ratio, 2.0 mg/mmol
            (normal is <2.5 mg/mmol in men and <3.5 mg/mmol in women). LDL is 176 mg/dl, HDL
            32 mg/dl, and TG 292 mg/dl. His electrocardiogram is normal.
          
        
        
          What are the next steps in management at this time?
          Diabetes management should involve a multifaceted, goal-directed approach, which
          includes dietary modifications, diabetes education, assessment of blood sugar readings,
          and pharmacotherapy. The ADA recommends glycemic and other CVD risk factor goals (see
          Table 2), in addition to foot evaluation and screening for nephropathy and retinopathy,
          for all adults with diabetes [4]. The patient is started on metformin, 500 mg twice daily
          (BID) with meals. Therapy with metformin appears to decrease the risk of diabetes-related
          endpoints, including a reduction in cardiovascular events independent of glycemic
          control. There is also less weight gain and fewer hypoglycemic attacks than with insulin
          and sulphonylureas. Therefore, metformin may be an effective first-line pharmacotherapy
          of choice in these patients [5]. There are several oral hypoglycemic agents (i.e.,
          sulfonylureas, metformin, acarbose, and thiazolidinediones) that are effective
          monotherapy for reducing hyperglycemia.
          The patient is also started on low-dose aspirin, indicated for primary prevention of
          macrovascular disease in people with diabetes who have any risk factors for CVD [4], and
          a cholesterol-lowering agent, a statin, for his increased LDL cholesterol [6]. He is
          given a glucose meter, is scheduled to have diabetes education classes and diabetes
          nutritional counseling for a 1,800-calorie ADA diet, and is instructed to record his
          pre-meal blood sugars. Smoking cessation is another important aspect of diabetes
          management to address. He returns in three months for follow-up and has an HbA1c of 7.3%,
          at which time no additional therapy is started.
        
        
          Three Years Later
          The patient is now 37 years old and returns for a follow-up appointment. He states
          that he has felt “pins and needles” in his feet and fingertips. He has had difficulty
          with maintaining erections but has a normal libido. Blood sugars are 160–190 mg/dl in the
          mornings and 200–240 mg/dl in the evenings, and the patient reports no hypoglycemic
          events. He has diminished sensation to vibration over his right great toe and left toes
          and heel with intact monofilament sensation. The remainder of his examination is
          unchanged. His medications are metformin at 1 g BID, a thiazide diuretic at 25 mg QD, a
          statin QD, and an aspirin QD. He is 215 lb, BP is 142/86 mm Hg, and pulse is 76 bpm.
          Recent laboratory tests produced the following results: a HbA1c of 8.1%, a fasting
          glucose of 212 mg/dl, and normal electrolytes, creatinine, and liver enzymes. Fasting
          lipids are LDL 144 mg/dl, HDL 33 mg/dl, and TG 209 mg/dl.
          
            What additional diagnostic tests would be helpful at this time, and why?
            A spot urine albumin-to-creatinine ratio is 7.6 mg/mmol. This measurement technique
            is preferred because it has lower rates of false-positive and false-negative results
            than a spot urine microalbumin. Persistent microalbuminuria should be confirmed on two
            or three subsequent readings within a six-month period to rule out false-positive
            results. The elevated ratio of microalbumin in the urine signifies early nephropathy
            because microalbuminuria has been shown to progress to macroalbuminuria and eventual
            nephropathy in type 1 and type 2 diabetes. Any degree of albuminuria is a risk factor
            for cardiovascular events in individuals with or without diabetes; the risk increases
            with the level of absolute microalbuminuria [7]. Therefore, screening for
            microalbuminuria should be done annually in all people with type 1 and type 2 diabetes
            [8].
            Annual screening for diabetic retinopathy should be performed in all people with
            diabetes after an initial evaluation and reassessed more frequently if retinopathy is
            diagnosed. This patient remains free of retinopathy, but a significant number of
            patients with type 2 diabetes have retinopathy at the onset of diagnosis owing to the
            insidious nature of type 2 diabetes and the failure to diagnose type 2 diabetes early.
            Tight glycemic control can slow the progression of diabetic retinopathy (Figures 1–3)
            [9] and help prevent development of proliferative diabetic retinopathy.
          
          
            What additional pharmacotherapy should be started at this time?
            The patient has developed neuropathy and erectile dysfunction, both of which are
            complications of diabetes. He continues to have suboptimal glycemic control; therefore,
            additional therapy in the form of combinations is appropriate. The patient is started
            on a thiazolidinedione (TZD) QD. With continued elevated systolic BP >130 mm Hg and
            diastolic BP >80 mm Hg, an angiotensin-converting enzyme inhibitor (ACE-I) is
            started. An ACE-I at this time is appropriate for BP control and has the additional
            preventative effects of reducing progression to nephropathy and CVD events [10,11]. In
            addition, continued strict BP control is as effective as tight glycemic control in
            preventing macrovasular disease in diabetic patients and slowing the progression of
            diabetic nephropathy and retinopathy [12]. Erectile dysfunction is a complication
            associated with diabetes and can be an early sign of neuropathy and vascular disease,
            therefore a phosphodiesterase-5 enzyme inhibitor is an appropriate choice for patients
            not on vasodilators or with a history of significant CVD. The statin dose is increased
            to achieve a goal LDL of ≤100 mg/dl. Diabetic neuropathy is a significant cause of
            morbidity in diabetes, and its progression correlates directly with glycemic control.
            Tighter glucose control and proper foot care are effective. It is important to continue
            emphasis on dietary, exercise, and lifestyle modifications in addition to
            pharmacotherapy.
          
        
        
          Five Years Later
          The patient returns to clinic today after spending the last three years overseas and
          has not seen a physician in two years. He complains of fatigue, occasional blurry vision,
          awakening three to four times at night to urinate, and diarrhea at least once a week. He
          says that he has been compliant with his diabetes medications but has gained 15 lb in the
          last six months. His medications include metformin at 1 g BID, a TZD BID, and an ACE-I
          QD. His blood sugar is 289 mg/dl (fasting), BP is 130/90 mm Hg, pulse is 88 bpm, and
          weight is 221 lb. There are no foot sores or ulcers, but he has diminished sensation to
          monofilament on the plantar surfaces of both feet. The remainder of his examination is
          unchanged, including normal fundoscopy. His HbA1c is 9.6%, LDL is 143 mg/dl, and spot
          urine albumin-to-creatinine ratio is 15 mg/mmol. His creatinine and liver enzymes are
          normal. His pre-meal blood sugars average 210–250 mg/dl.
          
            What is the next most appropriate step in his medical management?
            He continues to have an elevated HbA1c, worsening neuropathy, and weight gain, which
            prompt a more effective treatment strategy. There are several options for
            pharmacotherapy available to choose from at this point. The patient could begin a third
            oral agent after maximizing the doses of metformin and TZD, or he could begin insulin
            injections with or without additional oral agents. Because of the significant cost
            associated with three oral medications and his need for further glycemic control,
            insulin would be an appropriate choice at this time. However, he should be advised of
            the side effect of additional weight gain when beginning insulin therapy.
          
        
      
      
        DISCUSSION
        This case presentation illustrates an otherwise healthy appearing patient who is found
        to have the metabolic syndrome and despite evidence-based management develops type 2
        diabetes. This patient likely represents the natural history of type 2 diabetes in most
        patients. Mild hypertension is often the only presenting sign of metabolic syndrome and
        prediabetes, allowing an opportunity for prevention of type 2 diabetes.
        There is an association between metabolic syndrome and the development of CVD and type 2
        diabetes [13]. This syndrome is characterized not only by the criteria given in Table 1,
        but also by a state of compensatory hyperinsulinemia [14]. However, a diagnosis of
        metabolic syndrome alone does not imply diabetes, as patients with metabolic syndrome can
        have a fasting plasma glucose less than 110 mg/dl. It is the body's ability to maintain
        glucose utilization and suppress endogenous glucose production in the setting of this
        compensatory hyperinsulinemia that separates metabolic syndrome from diabetes. The effect
        of this hyperinsulinemic state in metabolic syndrome is also believed to be involved in
        excess pro-inflammatory and pro-thrombotic markers associated with the development of
        diabetes and CVD [15]. These patients develop diabetes when tissues of the body fail to
        utilize glucose appropriately owing to increased resistance to insulin and concomitant
        beta-cell dysfunction of the pancreas [16].
        Metformin is in the class of biguanides and works by decreasing hepatic glucose output
        and increasing insulin action in tissues. Metformin has been suggested to help prevent the
        onset of diabetes but is less effective than diet and lifestyle changes [3]. Other
        medications shown to possibly delay or prevent the onset of type 2 diabetes are ACE-I and
        angiotensin II receptor blockers [17,18]. Patients treated with diuretics can progress to
        type 2 diabetes even though thiazide diuretics are proven effective in treating
        hypertension [19,20].
        Intensive therapy in patients with type 2 diabetes results in a decreased risk of
        microvascular complications; therefore, it is appropriate to use combinations of
        medications in patients with suboptimal glycemia [21]. The class of TZDs works to lower
        plasma glucose levels by increasing insulin sensitivity in muscle and liver [22]. TZDs
        lower mean HbA1c modestly when added to metformin as compared to metformin alone [23]. Side
        effects include weight gain and water retention, and patients with a history of New York
        Heart Association class III or IV heart failure should not use TZDs [24,25].
        The pathophysiology of type 2 diabetes involves, in part, a “relative” deficiency of
        insulin. Although a state of endogenous hyperinsulinemia occurs, the degree of tissue
        resistance causes a total decrease in “effective” endogenous insulin. Progression of
        disease is also attributed to worsening beta-cell dysfunction and decreased release of
        insulin [26]. Insulin is used in a variety of combinations and is individualized to patient
        lifestyle. A frequent starting dose consists of a long- or intermediate-acting insulin,
        such as NPH insulin, divided into morning and evening doses, or insulin glargine given QD,
        usually at bedtime. The patient whose case is described here was started on NPH at bedtime,
        which decreases overnight hepatic glucose production such that the patient begins the
        morning with near-normal glycemia for daytime oral therapy. There may be times when a
        post-meal surge in glucose requires extra insulin in addition to the intermediate-acting
        NPH. In such a case, using a short-acting (regular) insulin before meals provides insulin
        action that closely approximates normal insulin secretion (Figure 4). The rapid-acting
        lispro and aspart insulins have an even shorter half-life and quicker onset of action than
        regular insulin. Common empirical initiation doses range from 0.4–1.2 units of insulin per
        kilogram per 24 hours. Patients should be advised of hypoglycemia and weight gain as the
        main side effects of insulin therapy. Insulin and insulin-sensitizer combinations
        significantly improve hyperglycemia; however, there is an increased incidence of heart
        failure reported with this combination, prompting close monitoring of patients for signs
        and symptoms of heart failure [27].
        In summary, diabetes prevention and management is an important goal in practice. The
        morbidity and mortality from diabetes is a significant burden to health care, emphasizing
        the need for effective prevention and control of diabetes in improving outcomes.
      
    
  

  
    
      
        Introduction
        Diabetic nephropathy (DNP) is a serious and common complication of type 1 and type 2
        diabetes mellitus, leading to end-stage renal failure in up to 30% of individuals with
        diabetes. Early abnormalities of DNP affect glomeruli and include an increase in glomerular
        filtration rate, microalbuminuria, glomerular hypertrophy, and thickening of the glomerular
        basement membrane, followed by expansion of mesangial extracellular matrix and
        glomerulosclerosis [1,2]. As with most chronic degenerative kidney diseases, the gradual
        decline of renal function at later stages of DNP is invariably associated with tubular
        epithelial degeneration (TED), also called tubular atrophy, and interstitial fibrosis (IF),
        hallmarks of degeneration to end-stage renal failure [3]. Pathomechanisms that may initiate
        and/or mediate TED in DNP remain poorly understood. While glomerular lesions consistent
        with human DNP have been described in various mouse models of diabetes, TED and IF have not
        been described in diabetic mice [4].
        Combining detailed renal phenotype analysis with gene expression profiling of
        hyperglycemic mouse models of type 1 (streptozotocin [STZ]) and type 2 (db/db) diabetes, we
        recently reported that decreased mRNA levels of CD36 in kidneys were strongly correlated
        with albuminuria [5]. CD36 is a transmembrane protein of the class B scavenger receptor
        family and is involved in multiple biological processes [6]. CD36 is widely expressed and
        may interact with multiple extracellular ligands, including thrombospondin-1 (TSP-1),
        long-chain free fatty acids (FFAs), modified (oxidized) low-density lipoprotein (ox-LDL),
        advanced glycation end (AGE) products, and collagens I and IV [6]. CD36 mediates
        phagocytosis of apoptotic cells and malaria-parasitized erythrocytes [7]. Furthermore, CD36
        mediates antiangiogenic activity associated with endothelial cell apoptosis induced by
        TSP-1 through p38 MAP kinase (MAPK) and caspase 3 [8]. Hyperglycemia-induced synthesis of
        CD36 protein in macrophages has been associated with increased uptake of ox-LDL by
        macrophages and foam cell formation in atherosclerotic lesions in people with diabetes
        [6,9,10]. While diabetic cardiovascular complications are closely linked epidemiologically
        with albuminuria and DNP, a role for CD36 in DNP and renal pathophysiology has not to our
        knowledge been described to date.
        Here we report a novel functional role for CD36 scavenger receptor and AGE and FFA
        palmitate (PA) in tubular epithelial apoptosis associated with TED and progression of DNP.
        Specifically, we show that glucose stimulates CD36 cell surface expression in proximal
        tubular epithelial cells (PTECs), and increased CD36 renders PTECs susceptible to both AGE-
        and PA-induced PTEC apoptosis by mediating sequential activation of src kinase,
        proapoptotic p38 MAPK, and caspase 3. Based on these findings, we propose a new two-step
        metabolic hit model for TED in the progression of DNP.
      
      
        Methods
        
          Animals
          Kidneys were obtained from 28-wk-old C57BLKS/J-lepr
          db/db , STZ-treated C57BL/6J, or STZ-treated 129SvJ mice and from
          age-matched control C57BLKS/J-lepr
          db/m , C57BL/6J, and 129SvJ mice as described [5].
        
        
          Cell Culture
          Human proximal tubular cell line HK-2 and murine collecting duct cell line M1 were
          purchased from American Type Culture Collection (Manassas, Virginia, United States) and
          cultured according to the vendor's instructions. Mouse proximal tubular cell line MCT was
          provided by Fuad Ziyadeh (University of Pennsylvania, Philadelphia, Pennsylvania, United
          States). Transfections were performed with Fugene 6 (Roche Diagnostics, Indianapolis,
          Indiana, United States) according to manufacturer's protocol. CD36-containing plasmid was
          a kind gift of Nada Abumhrad (SUNY at Stony Brook, New York, United States). Cells were
          also co-transfected with EGFP (Clontech, Franklin Lakes, New Jersey, United States) to
          assess transfection efficiency. Cells were serum starved in 0.2% serum containing DMEM (1
          g/l glucose) for at least 24 h prior to stimulation with AGE–bovine serum albumin (BSA),
          glucose, or FFA.
        
        
          Quantitative Real-Time PCR
          Quantitative real-time PCR analysis of mouse and human CD36, HPRT1, and beta actin was
          performed as described previously [5]. The following primers were used: mouse CD36 5′
          TGCTGGAGCTGTTATTGGTG and 3′ CATGAGAATGCCTCCAAACA, mouse beta actin 5′
          ACCGTGAAAAGATGATGACCCAG and 3′ AGCCTGGATGGCTACGTACA, mouse HPRT1 5′ TGTTGTTGGATATGCCCTTG
          and 3′ TTGCGCTCATCTTAGGCTTT, human CD36 5′ GCTCTGGGGCTACAAAGATG and 3′
          TAGGGAGAGATATCGGGCCT, human beta actin 5′ GATGAGATTGGCATGGCTTT and 3′
          CACCTTCACCGTTCCAGTTT, and human HPRT1 5′ AAAGGACCCCACGAAGTGTT and 3′
          TCAAGGGCATATCCTACAACAA.
        
        
          Immunostaining and Immunoblotting
          Primary antibodies specific for the following proteins were used: monoclonal mouse
          anti-CD36 antibody, clone FA 6–152 (IgG) (Immunotech, Fullerton, California, United
          States), clone SMO (IgM) (Santa Cruz Biotechnology, Santa Cruz, California, United
          States), rabbit polyclonal anti-CD36 (Santa Cruz Biotechnology), rabbit polyclonal
          anti-aquaporin1, anti-aquaporin2, anti-Na/K/2Cl (Chemicon, Temecula, California, United
          States), rabbit polyclonal phospho38/MAPK and mouse monoclonal p38 (Cell Signaling
          Technology, Beverly, Massachusetts, United States), rabbit polyclonal p-src (Y418)
          (Biosource, Camarillo, California, United States), and mouse monoclonal anti-tubulin
          (Sigma, St. Louis, Missouri, United States). Immunostaining was performed on frozen
          sections with FITC- and Cy3-labeled secondary antibodies (Jackson Laboratories, USA), or
          on paraffin-embedded sections with immunoperoxidase, as described earlier [5].
          Immunoblotting was performed with 30 μg of protein isolated from cultured cells. Protein
          samples were resolved on a 10% SDS-PAGE and immunoblotted with primary antibody and
          revealed with horse radish peroxidase (HRP)-conjugated anti-mouse IgM, or anti-rabbit IgG
          (Jackson Laboratory, Bar Harbor, Maine, United States). Immuncomplexes were detected by
          enhanced chemiluminescence (Pierce, Rockford, Illinois, United States). The proximal
          tubular immunostaining was evaluated semi-quantitatively by two independent pathologists
          who were unaware of the diagnosis; distribution and intensity of staining was scored on a
          ten-point scale.
        
        
          Fluorescence Flow Cytometric Analysis
          Cells were incubated in 0.5 mM EDTA in PBS at 37 °C for 20 min, scraped, and then
          washed with 1% fetal bovine serum. Cells were then exposed to monoclonal anti-CD36 IgG
          FA6 (5 μg/ml), or control mouse IgG1 (5 μg/ml) (Sigma), for 45 min on ice in the presence
          of 10% fetal bovine serum then washed with PBS. This was followed by an incubation with
          phycoerythrin-conjugated goat anti-murine secondary antibody (Southern Biotechnology,
          Birmingham, Alabama, United States) 1:50 for 45 min on ice. Cells (1 × 10
          4 ) were analyzed by using a SCAN flow cytometer (BD, Franklin Lakes,
          New Jersey, United States), with appropriate gating. Flow cytometry data were analyzed
          using Cellquest (BD).
        
        
          Preparation of Glycated Albumin and Carboxymethyl-Lysine Albumin
          Briefly, to prepare AGE-BSA, essentially fatty-acid-free and endotoxin-free BSA (250
          mg/ml) was incubated at 37 °C for 2, 5, and 10 wk with D-glucose (500 mM) in a 0.4-M
          phosphate buffer containing EDTA, ampicillin, Fungazone, polymixin B, and protease
          inhibitors. Control preparations were treated identically except that glucose was
          omitted. Carboxymethyl-lysine (CML)–BSA was prepared as described earlier [11]. Briefly,
          BSA with minimal CML content (CMLmin-BSA) was prepared by incubation of BSA (0.66 mM)
          with glyoxylic acid (2.15 mM) in the presence of sodium cyanoboronydrate (56 mM) in 200
          mM sodium phosphate buffer (pH 7.8) at 37 °C under aseptic conditions. Finally,
          preparations were extensively dialyzed against phosphate buffer to remove free glucose.
          Preparations were then tested for the presence of LPS with a Quantitative Chromogenic LAL
          kit (Cambrex, East Rutherford, New Jersey, United States). The concentration of LPS was
          lower than 0.07 IU/mg protein in all preparations.
        
        
          Preparation of FFA
          Palmitic acid (P5585), oleic acid, and FFA-free low-endotoxin BSA (A8806) were
          purchased from Sigma. Palmitic acid was dissolved at 12 mM in PBS containing 11%
          fatty-acid-free BSA, sonicated for 5 min, shaken overnight at 37 °C, and sonicated for 5
          min again [12]. For control experiments, BSA in the absence of fatty acids was prepared,
          as described above. The effective concentration of PA was determined using a commercially
          available kit (Wako Chemicals, Neuss, Germany).
        
        
          Apoptosis Detection
          In situ detection of DNA fragmentation was performed using the ApoTag TUNEL assay
          following the manufacturer's protocol (Intergen, Purchase, New York, United States) [13].
          Apoptotic nuclei were detected using DAPI staining (1 μg/ml; 10 min) in cell cultures
          fixed with 4% paraformaldehyde, and analyzed via fluorescence microscopy to assess
          chromatin condensation and segregation. Caspase3 activity was detected by using the
          ApoAlert Caspase3 Fluorescent Detection system (BD) according to the manufacturer's
          protocol. Activity was normalized to total protein content. Z-DEVD-fmk, z-VAD-fmk,
          z-FA-fmk, and z-LEHD-fmk were purchased from BD.
        
        
          Human Kidney Biopsy Sample and Patient Characteristics
          Human kidney tissues (ten controls, ten with diabetic nephropathy, and ten with focal
          segmental glomerulosclerosis [FSGS]) were obtained from archived kidney biopsy samples or
          from discarded nephrectomy specimens. All diabetic samples were from patients with
          biopsy-proven advanced DNP with serum creatinine ranging from 1.7 to 5.6 mg/dl (151 to
          444 μM/l), heavy proteinuria (3+ by dipstick or 3–6 gr/d), and hypertension. All patients
          with FSGS were from patients with creatinine levels of 1.7 to 4.9 mg/dl (151 to 435
          μM/l), heavy proteinuria (3+ by dipstick), and hypertension. The diagnosis of FSGS was
          made on Periodic acid–Schiff staining in the absence of immunodeposits on electron
          microscopy. The diagnosis of diabetic nephropathy was based on the presence of diabetes,
          proteinuria, and the characteristic light microscopy findings. Institutional Review Board
          approval was obtained for procurement of kidney specimens at the Thomas Jefferson
          University Hospital.
        
        
          Statistical Methods
          Data are reported as mean and standard error of the mean (SEM) for continuous
          variables. All cell culture experiments were performed at least three times and
          summarized. Standard software packages (SPSS and Excel for Windows) were used to provide
          descriptive statistical plots (unpaired 
          t -tests). The Bonferroni correction was used for multiple comparisons.
          Significance for the quantification of the CD36 staining in human biopsy samples was
          calculated via the Wilcoxon Rank Sum Test.
        
      
      
        Results
        
          Increased Expression of CD36 Specifically in Proximal Tubules of Human Diabetic
          Kidneys Is Associated with TED
          Using microarray-based gene expression profiling on whole kidney RNA together with
          supervised clustering methods, we previously identified and validated gene expression
          patterns for molecular classification of diabetic mice with albuminuria and mesangial
          expansion [5]. Reduced renal mRNA levels of the class B scavenger receptor CD36 were
          characteristic for diabetic mice with albuminuria [5]. Here we examined patterns of CD36
          protein expression in kidneys of non-diabetic and diabetic mice and humans. CD36 protein
          was detectable in the thick ascending limb of loop of Henle and in the collecting duct,
          and absent in proximal tubules in both control and diabetic mouse kidneys (Figure 1A–1D).
          In contrast, CD36 was detectable only rarely in individual proximal tubular cells in
          sections from non-diabetic human kidneys (controls) (Figure 1E and 1H), but was markedly
          increased specifically in PTECs in human diabetic kidneys (Figure 1F and 1I). In
          addition, we did not observe increased proximal tubular CD36 expression in kidney biopsy
          samples from patients with FSGS (Figure 1J), that were matched with DNP samples for the
          severity of proteinuria (all in the nephrotic range) and renal insufficiency (all with
          elevated serum creatinine; 1.7–5.0 mg/dl). Semi-quantitative analysis of the distribution
          and intensity of CD36-positive PTECs (CD36 PTEC score), which was performed by two
          independent pathologists in a blinded manner, demonstrated that mean CD36 PTEC scores
          were not different between FSGS kidneys and normal human kidneys, but were significantly
          increased in DNP kidneys (Figure 1K).
          Periodic acid–Schiff–stained sections of kidneys from mice exposed to type 2 diabetes
          (db/db mice) for 20 wk (Figure 2A), or type 1 diabetes (STZ-treated C57BL/6J mice) for 20
          wk (data not shown) demonstrated moderate to advanced mesangial expansion and
          glomerulosclerosis (Figure 2A). Tubular abnormalities were not detectable in either model
          (Figure 2A). In contrast, TED and IF were associated with moderate to advanced mesangial
          expansion and glomerulosclerosis on kidney sections of human DNP (Figure 2B). These
          findings indicate that in humans with DNP, diabetes-induced upregulation of CD36
          expression in proximal tubules was associated with moderate to advanced stages of TED and
          IF. In contrast, in diabetic mice with albuminuria, mesangial expansion, and
          glomerulosclerosis, absence of CD36 expression was associated with normal appearance of
          the tubular epithelium and interstitial space. These findings suggest an association
          between diabetes-induced proximal tubular CD36 expression and TED.
        
        
          Coincidence of Increased CD36 Expression and Increased Tubular Epithelial Cell
          Apoptosis in Human DNP
          CD36 has been shown to mediate apoptosis signaling induced by TSP-1 in endothelial
          cells [8] and by ox-LDL in macrophages [14]. We examined whether the strong upregulation
          of CD36 protein in PTECs, observed specifically in human DNP, was associated with tubular
          epithelial cell apoptosis in vivo. TUNEL-positive tubular epithelial cells also stained
          positive for CD36 protein (Figure 2D) and aquaporin1 (Figure 2C), indicating that
          apoptosis and CD36 expression coincided in PTECs in human DNP. In contrast, CD36 was not
          detectable in TUNEL-positive PTECs in non-diabetic FSGS kidneys and in normal human
          kidney (data not shown). Statistical analysis showed that the rate of TUNEL-positive
          tubular cells was significantly increased in kidneys of human DNP compared with normal
          control human kidney (Figure 2E). In addition, tubular epithelial apoptosis was
          increased, but highly variable, in FSGS kidneys (data not shown). In contrast, tubular
          epithelial apoptosis rates were comparable between non-diabetic control and all diabetic
          mouse kidneys (Figure 2E). The diabetic mouse group included 24-wk-old STZ-treated
          diabetic C57BL/6J or 129SvJ mice (0.23 ± 0.1 TUNEL-positive cells per 100 tubular cells)
          and 24-wk-old lepr
          db/db mice (0.2 ± 0.1 TUNEL-positive cells per 100 tubular cells).
          Together, these findings indicate that CD36 expression in PTECs is associated with
          apoptotic events of proximal tubular cells and TED specifically in human DNP, but not in
          FSGS with matched functional and clinical abnormalities. These in vivo findings
          demonstrate a strong association of diabetes-induced CD36 expression and apoptosis in
          PTECs in human DNP, suggesting that CD36 may play a critical role in TED by mediating
          PTEC apoptosis in progressive human DNP.
        
        
          High Ambient Glucose Induces CD36 Expression in Human PTECs
          High ambient glucose has been shown to induce CD36 protein synthesis in macrophages
          [9]. Because CD36 protein was markedly increased in proximal tubules in human DNP, we
          examined the effects of high ambient glucose on CD36 mRNA and protein expression in the
          human PTEC line HK-2 (Figure 3). Exposure of cells to 30 mM D-glucose for 24 h, but not
          to control L-glucose, significantly increased levels of CD36 mRNA (Figure 3A), CD36 cell
          surface protein (Figure 3C), and CD36 protein expression in cell lysates (Figure 3D). In
          contrast, CD36 mRNA and protein were not detectable in the murine PTEC line MCT at either
          normal or high ambient glucose concentrations (data not shown). Interestingly, glucose
          stimulation decreased CD36 mRNA levels (Figure 3B) and CD36 cell surface protein (Figure
          3C) in the murine collecting duct cell line M1, consistent with our previously reported
          findings in diabetic mouse kidney [5]. Exposure of human HK-2 and murine M1 cell lines to
          defined preparations of FFA PA or AGE-BSA had no effect on CD36 mRNA and protein
          expression levels (data not shown). These findings demonstrate that high ambient glucose
          causes upregulation of CD36 mRNA and protein specifically in human, but not in mouse,
          PTECs. Together with our in vivo observations, these results suggest that hyperglycemia
          may induce upregulation of CD36 mRNA and protein selectively in proximal tubules in
          kidneys of human DNP, but not diabetic mice with albuminuria.
        
        
          AGE-BSA, CML-BSA, and FFA PA Induce Apoptosis in Human PTECs via CD36
          AGE albumin [15] and FFAs [16] have been implicated in the pathogenesis of diabetic
          complications, including tubular degeneration [17] and tubular epithelial-to-mesenchymal
          transition [18]. In addition, AGE albumin and FFA are known to interact with CD36
          [19,20]. However, it is not known whether AGE and/or FFA can activate CD36 signaling and
          apoptosis in tubular epithelial cells. Treatment with AGE-BSA for 2, 5, or 10 wk or with
          CML-BSA induced a significant increase in the number of apoptotic nuclei in CD36-positive
          HK-2 cells compared with control BSA-treated or untreated HK-2 cells (Figure 4A). In
          contrast, AGE-BSA and CML-BSA had no effect on the rate of apoptotic nuclei in
          CD36-negative murine MCT PTECs (data not shown). Because AGE-BSA glycated for 5 wk
          (AGE-BSA5) induced robust apoptosis at concentrations between 20 and 40 μM (Figure 4A),
          we chose this preparation and concentration for further analysis in all subsequent
          experiments. AGE-BSA5-induced apoptosis was blocked when cells were preincubated with
          neutralizing anti-CD36 antibody, while preincubation with control IgG antibody had no
          effect (Figure 4A). These results were confirmed by DNA laddering assay (data not
          shown).
          Among the most abundant glucose-modified proteins detectable in the plasma of diabetic
          individuals are CML proteins [21], which are typically present at 1.6 to 2.3 μM
          concentrations in the plasma and urine of diabetic individuals [22,23]. To use
          physiologically relevant CML proteins in our in vitro experiments, we prepared
          CMLmin-BSA, characterized by glycation of approximately 30% of lysine residues [21]. When
          applied to HK-2 PTECs at concentrations ranging from 0.5 to 10 μM, CMLmin-BSA increased
          apoptosis rates significantly (Figure 4B). The proapoptotic effect of CMLmin-BSA was
          blocked by CD36 neutralizing antibody, but not by control IgG (Figure 4B).
          CD36 has been shown to transport fatty acids in adipocytes [24] and in muscle cells
          [25]. Concentrations of FFAs may be substantially elevated, to levels of up to 700 μM, in
          individuals with type 2 diabetes or obesity [26]. Thus, we examined the effects of
          saturated FFA PA and monounsaturated FFA oleate on apoptosis of HK-2 PTECs in the absence
          or presence of anti-CD36 neutralizing antibody. PA significantly increased rates of
          apoptotic nuclei in a concentration-dependent manner in HK-2 PTECs (Figure 4C). Anti-CD36
          neutralizing antibody, but not control IgG, blocked PA-induced apoptosis (Figure 4C). In
          contrast, oleate did not induce apoptosis, even at concentrations as high as 300 μM
          (Figure 4C), neither did it prevent PA-induced apoptosis (data not shown). Of note, these
          experiments were performed using a total fatty acid:BSA ratio of 6.6:1, in order to
          closely model pathophysiologic states in which unbound FFA concentration is high [27].
          Taken together, our findings demonstrate that pathophysiologically relevant species of
          AGE-BSA and CML-BSA, as well as saturated FFA PA, induce apoptosis in human PTECs at
          concentrations previously observed in plasma and/or urine in humans with diabetes.
        
        
          AGE-BSA and PA Sequentially Activate src kinase, Proapoptotic p38 MAPK, and Caspase
          3 through CD36 Receptor
          CD36 has previously been shown to trigger the activation of p59fyn, p38 MAPK, and
          caspase 3 (GeneID: 836) in response to thrombospondin in endothelial cells [8]. Therefore
          we examined phospho-src, phospho-p38 levels and caspase 3 activation in HK-2 PTECs
          treated with AGE-BSA and PA in the absence or presence of anti-CD36 neutralizing
          antibody. Both AGE-BSA5 and PA increased phospho-src levels rapidly (after as little as 5
          min), and over a prolonged time interval (up to 3 h) (Figure 5A and 5B). Phosphorylation
          of src kinase was blocked by anti-CD36 neutralizing antibody (Figure 5A and 5B). This
          observation is consistent with previous findings demonstrating direct interaction between
          CD36 and p59fyn [8]; however, the involvement of other src kinases cannot be excluded. We
          also observed increased levels of phosphorylation of p38 MAPK beginning 1 to 2 h after
          treatment, and p38 activation was also completely blocked by anti-CD36 neutralizing
          antibody (Figure 5C and 5D). These findings indicate that CD36 activates proapoptotic p38
          MAPK possibly via src kinase activation in human PTECs when stimulated with AGE-BSA5 and
          PA. Chemical inhibition of p38 MAPK prevented the increase in the rate of apoptotic
          nuclei induced by both AGE-BSA5 and PA in HK-2 PTECs (Figure 5G), indicating that p38
          MAPK function is required for apoptosis induced by AGE-BSA and PA through CD36 receptor.
          AGE-BSA and PA significantly increased activity of effector caspase 3 in human PTECs
          (Figure 5E and 5F). Caspase 3 activation was blocked by anti-CD36 neutralizing antibody,
          but not by control IgG (Figure 5E and 5F). Pan-caspase inhibitor z-VAD-fmk and the
          specific caspase 3 inhibitor z-DEVD-fmk prevented apoptosis induced by PA and AGE-BSA,
          while the specific caspase 9 inhibitor z-LEHD-fmk had no significant inhibitor effect
          (Figure 5G). Together these findings indicate that CD36 receptor mediates sequential
          phosphorylation of src kinases and p38 MAPK, leading to activation of caspase 3 and
          apoptosis in human PTECs exposed to AGE-BSA and PA ligands. Interestingly, we did not
          observe phosphorylation of Smad2 and p42/44 ERK MAPK under these conditions, as
          previously reported for AGE binding to the RAGE receptor [28].
        
        
          CD36 Is Sufficient to Mediate Apoptosis Induced by AGE-BSA and FFA
          In contrast with CD36-positive human HK-2 PTECs, we found that treatment of
          CD36-negative mouse MCT PTECs with AGE-BSA had no effect on rates of apoptotic nuclei
          (data not shown). To test whether CD36 was sufficient to mediate AGE-BSA-induced
          apoptosis, we transfected CD36-negative mouse MCT PTECs with a plasmid expressing human
          CD36 or empty control vector, followed by treatment with control BSA or AGE-BSA5.
          AGE-BSA5 treatment had no significant effect on rates of apoptotic nuclei in MCT PTECs
          transfected with control vector (Figure 6). In contrast, AGE-BSA significantly increased
          apoptotic nuclei compared with unglycated BSA in MCT PTECs transiently transfected with
          CD36 expression vector (Figure 6). Nonglycated control albumin did not cause apoptosis.
          Thus, transgenic de novo expression of human CD36 in CD36-negative mouse PTECs was
          sufficient to mediate apoptosis induced by AGE-BSA.
        
      
      
        Discussion
        Advanced diabetic nephropathies in humans with type 1 or type 2 diabetes are uniformly
        characterized by TED, or tubular atrophy, and IF leading to renal failure [29,30]. Although
        TED and IF are the strongest predictors for progression of DNP [31], mechanisms that
        underlie TED in DNP remain poorly understood. Based on our in vitro and in vivo findings we
        propose a two-step metabolic hit model for TED in DNP. High ambient glucose, but not AGE or
        FFA, cause stimulation of CD36 expression in PTECs specifically in diabetic kidneys.
        Increased CD36 expression mediates sequential activation of src kinase, proapoptotic p38
        MAPK, and caspase 3 in PTECs in the presence of AGE and FFA PA, resulting in PTEC
        apoptosis. Proximal tubular epithelial apoptosis may be an initial mechanism for TED in
        DNP.
        Our conclusions are supported by several key observations. First, we identify a new
        functional role for CD36 as an essential mediator of proximal tubular epithelial apoptosis,
        inducible by AGE-BSA, CMLmin-BSA, and FFA PA. Previous reports demonstrated a role for CD36
        in mediating apoptosis induced by TSP-1 in endothelial cells and ox-LDL in macrophages
        [8,14]. In the present study, we show for the first time, to our knowledge, that CD36
        mediates apoptosis in differentiated epithelial cells that are exposed to AGE-BSA-,
        CMLmin-BSA-, and FFA-induced metabolic injury characteristic of the diabetic milieu.
        Interestingly, AGE albumins and CML are present in the urine of individuals with diabetes
        with albuminuria due to DNP and have been shown to bind proximal tubular epithelium
        [22,32]. While the presence or absence of FFAs in the urine of diabetics with DNP has not
        been determined to date, FFAs may cause tubular apoptosis [33]. It remains to be determined
        whether FFA interacts with CD36 to activate CD36 receptor signaling, or whether CD36
        mediates FFA uptake to activate src kinase and p38 MAPK signaling. Irrespective of the
        upstream mechanism of FFA and CD36 interaction, our results demonstrate very rapid
        activation of a well-characterized intracellular kinase cascade of proapoptotic signaling.
        Our finding that AGE-BSA and PA induce apoptosis through a CD36-mediated and p38- and
        caspase-dependent mechanism in tubular epithelial cells, similar to TSP-1 and ox-LDL in
        endothelial cells and macrophages, respectively, suggests that multiple, context-dependent
        extracellular stimuli of apoptosis may converge on CD36 scavenger receptor to activate src
        kinase and proapoptotic p38 MAPK pathway. In the context of the diabetic milieu and
        diabetic complications, our findings provide new molecular insights into diabetes-induced
        AGE- and FFA-dependent injury of renal epithelial cells.
        Almost all TUNEL-positive apoptotic tubular epithelial cells showed increased expression
        of CD36, suggesting a strong correlation between upregulation of CD36 expression and
        increased apoptosis in PTECs specifically in human diabetic kidney in vivo. Importantly,
        biopsy samples from cases of FSGS that were matched for degree of proteinuria, renal
        function, and hypertension were characterized by TED, IF, and increased tubular epithelial
        apoptosis; however, proximal tubular CD36 expression was similar to that in normal human
        control kidney. Therefore, CD36 expression in PTECs is specifically associated with the
        diabetic condition and appears to be independent of degree of proteinuria and renal
        failure. Indeed, increased CD36 expression in PTECs in human DNP in vivo may be caused by
        hyperglycemia, as we show that high glucose concentration stimulates CD36 expression in
        vitro. It is intriguing that CD36 expression was not detected in PTECs in diabetic mice
        with longstanding hyperglycemia in vivo, although underlying mechanisms for the
        species-dependent differential regulation of CD36 in PTECs in vivo and in vitro between
        mouse and human remain unclear at this time. Comparisons of human 
        CD36 and mouse 
        Cd36 genes indicate a high degree of sequence and structural similarity
        in both coding and regulatory regions, suggesting that the mechanism or mechanisms that
        underlie our findings are likely determined by sequence-independent, epigenetically
        distinct response patterns to the diabetic milieu that differ between these species. It is
        also possible that dietary or metabolic factors account for the differences in CD36
        expression, as mice were maintained on standard mouse chow characterized by significantly
        lower fat and cholesterol contents than typical western diets consumed by humans. However,
        dietary or other unknown environmental factors cannot explain the differential CD36
        regulation by glucose in human and mouse PTECs. Thus, we are exploring whether biochemical
        or functional differences between mouse and human PTECs in glucose metabolism or
        glucose-induced signaling can be identified. However, current lack of understanding of the
        observed differential regulation between human and mouse does not diminish the
        translational research significance of our findings, with their clear therapeutic
        implications. Thus, the present study identifies a new CD36-dependent molecular signaling
        pathway that mediates tubular epithelial apoptosis, and may underlie TED and IF, hallmarks
        of disease progression, specifically in human diabetic nephropathy.
        Third, to our knowledge, our report provides the first controlled study demonstrating
        increased apoptosis specifically in PTECs in DNP with TED and IF. These findings are
        consistent with a recent uncontrolled case series of five patients with DNP [34], and with
        previous reports demonstrating tubular apoptosis in kidneys of STZ-treated diabetic rats
        [35,36]. Interestingly, our study shows that tubular epithelial apoptosis was associated
        with TED and IF in human DNP, while normal appearance of tubular epithelium and
        interstitium was associated with baseline apoptosis rates in diabetic mouse models.
        Together, published observations from experimental diabetes models in mouse and rat, and
        human DNP, and our own findings in diabetic mouse models and human DNP, suggest a striking
        association of TED and tubular epithelial apoptosis. However, whether tubular epithelial
        apoptosis causes TED in DNP will require further investigation. Interestingly, acute and
        chronic chemical inhibition of caspase activity in a nephrotoxic serum nephritis model of
        chronic progressive glomerulonephritis with TED and IF reduced tubular apoptosis and TED
        [37]. Decreased tubular apoptosis and TED were associated with significantly reduced IF and
        decreased collagen synthesis in this model. This finding suggests that tubular epithelial
        apoptosis may trigger TED and IF in this model of chronic glomerulonephritis in rat, and
        supports our conclusions that diabetes-induced tubular epithelial apoptosis may underlie
        TED associated with IF in human DNP.
        In conclusion, we report a new functional role for CD36 scavenger receptor in tubular
        epithelial apoptosis associated with tubular degeneration and progression of DNP.
        Specifically, we show for the first time that both AGE and FFA PA induce PTEC apoptosis
        through CD36-mediated activation of src kinase, p38 MAPK, and caspase 3. Because high
        glucose stimulates CD36 expression in human PTECs and because CD36 expression is increased
        in apoptotic tubular epithelial cells in human DNP, we propose a two-step metabolic hit
        model relevant for TED, a hallmark of progression of human DNP.
      
      
        Supporting Information
        
          Accession Numbers
          The LocusLink (http://www.ncbi.nlm.nih.gov/LocusLink/) accession numbers for the gene
          products discussed in this paper are caspase 3 (GeneID: 836), CD36 (GeneID: 948), MAPK
          (GeneID: 1432), p42/44 ERK MAPK (GeneID: 50689), p59fyn (GeneID: 2534), and Smad2
          (GeneID: 4087).
        
      
    
  

  
    
      
        
        In free community science, where large numbers of scientists participate as volunteers
        in a single project, the ideal of scientific cooperation finds a new expression. Free
        community science was inspired by the free software movement, which itself was inspired by
        the application of the ideal of scientific cooperation, as it was applied to software
        development by the operating system developers of the Massachusetts Institute of Technology
        Artificial Intelligence Lab in the 1970s. This ideal has suffered for two decades from
        corporate pressure to privatize science, so it is very gratifying to see that the free
        software movement can today help reinvigorate the principle that inspired it.
        The ideal of scientific cooperation goes beyond the conduct of individual projects.
        Scientific cooperation is also being reinvigorated today through the open-access movement,
        which promotes the public's freedom to redistribute scientific and scholarly articles. In
        the age of the computer networks, the best way to disseminate scientific writing is by
        making it freely accessible to all and letting everyone redistribute it. I give a vote of
        thanks to the Public Library of Science for leading the campaign that is now gaining
        momentum. When research funding agencies pressure journals to allow free redistribution of
        new articles they fund, they should apply this demand to the old articles “owned” by the
        same publishers—not just to papers published starting today.
        Journal editors can promote scientific cooperation by adopting standards requiring
        internet publication of the supporting data and software for the articles they publish. The
        software and the data will be useful for other research. Moreover, research carried out
        using software cannot be checked or evaluated properly by other scientists unless they can
        read the source code that was used.
        A significant impediment to publication and cooperation comes from university patent
        policies. Many universities hope to strike it rich with patents, but this is as foolish as
        playing the lottery, since most “technology licensing offices” don't even cover their
        operating costs. Like the Red Queen, these universities are running hard to stay in the
        same place. Society should recognize that funding university research through patents is
        folly, and should fund it directly, as in the past. Meanwhile, laws that encourage
        universities to seek patents at the expense of cooperation in research should be
        changed.
        Another impediment comes from strings attached to corporate research funding.
        Universities or their public funding agencies should ensure private sponsors cannot block
        research they do not like. These sponsors must never have the power to veto or delay
        publication of results—or to intimidate the researchers. Thus, sponsors whose interests
        could be hurt by publication of certain possible results must never be in a position to cut
        the funding for a specific research group.
        The free software movement, the free redistribution policy of this journal, and the
        practice of free community science for developing diagnostic disease classifications [1]
        are all based on the same fundamental principle: knowledge contributes to society when it
        can be shared and developed by communities. All three face opposition from those who would
        like to privatize knowledge and charge tolls for its use. In the free software movement we
        have 20 years' experience in resisting this opposition, and we have built up considerable
        strength and momentum. We can give the other two movements a boost, so they can advance
        more quickly.
      
    
  

  
    
      
        
        Current journal requirements forcing clinical trials to be registered [1] are
        insufficient and are unlikely to solve the problem of negative trials never even making it
        to a journal. Most of the patients consenting to clinical trials do so out of altruism. It
        is a great betrayal of their trust to suppress clinical trial data. I suggest that
        institutional review boards refuse to allow human experimentation unless the protocol is
        filed in a central (online) repository. The primary data should also be required to be in
        the public domain (say, within 1–2 years after completion). Data obtained by appealing to
        altrusitic instincts, similar to money in public charities, are not proprietary
        information, nor can physicians cash out the trust of their patients. In reality, it is the
        pharmaceutical industry that stands to gain the most if data are made public as such data
        inform future research and help smaller, innovative companies avoid redundancy. Voluntarily
        sticking to higher standards of ethics will raise societal respect for the industry
        (currently being battered for greed) and attract a more talented workforce, and may even
        help the current efforts to reform the tort law.
      
    
  

  
    
      
        Introduction
        The HIV/AIDS epidemic is having a devastating impact in sub-Saharan Africa and other
        resource-constrained regions. Recently, the World Health Organization and other
        organizations have committed to expand access to antiretrovirals (ARVs) in the developing
        world, the United States government has pledged to provide $15 billion for AIDS in Africa
        and the Carribean, and drug prices have fallen [1]. However, even if these resources are
        provided for the global treatment of HIV, the number of individuals in need of treatment
        will far exceed the supply of ARVs [1]. Thus, difficult decisions will have to be made as
        to how to design HIV treatment strategies with these scarce resources. Resource allocation
        decisions can be made on the basis of many different epidemiological, ethical, or
        preferential treatment priority criteria. Many diverse groups have been suggested for
        treatment priority in resource-limited regions, including the following: only men, pregnant
        women, children, the sickest, the most economically productive, individuals in the
        military, or even individuals of the dominant ethnic group [2]. It has also been proposed
        that a lottery would be the only fair approach to allocating ARVs [3]. Only a limited
        number of ARVs will be available, and only a fixed number of health-care facilities (HCFs)
        can be used for ARV distribution. Thus, the resource allocation decisions that need to be
        made are extremely complex.
        Here, we use operations research to address this important resource allocation problem
        and to design ARV allocation strategies that are rational and equitable. The allocation
        decisions that we make here are based on ethical criteria, and not on epidemiological or
        preferential treatment priority criteria. Specifically, we determine the optimal allocation
        strategy that would ensure that each individual with HIV has an equal chance of receiving
        ARVs. We present a novel spatial mathematical model of treatment accessibility that we use
        in conjunction with an equity objective function to determine an optimal equitable
        allocation strategy (OEAS) for ARVs in a resource-constrained region. We quantify how
        changing the size of the catchment region surrounding each HCF, and the number of HCFs
        utilized for ARV distribution, alters the OEAS. Specifically, we use data from the detailed
        ARV rollout plan designed by the government of South Africa to determine an OEAS (based
        upon a variety of assumptions) for the province of KwaZulu–Natal. We also discuss how our
        proposed ARV allocation strategy differs from the currently proposed plan.
        Our current analysis is applied to the South African province of KwaZulu–Natal, although
        our methodology could be applied to any resource-constrained setting. KwaZulu–Natal is the
        largest province in South Africa with a population of approximately 9.4 million and has
        more people infected with HIV than any other province (approximately 21% of all cases in
        South Africa [4]). We use data from 51 communities (cities, towns, and villages) in the
        province of KwaZulu–Natal; we exclude communities with a population of less than 500
        people. Data are not available on the number of individuals with HIV in each specific
        community, and thus we use the estimated HIV prevalence in the region (approximately 13% in
        urban areas and 9% in rural areas [4]) to estimate the number of infected people in each
        community. See Figure 1 and Table 1 for the population sizes and spatial locations of each
        of the 51 communities used in our analysis. For our analysis the quantity of ARVs available
        for distribution to the HCFs is sufficient to treat 10% of the total number of infected
        people, which is a realistic level during the incremental scale-up of ARV therapy over the
        next few years. The government of South Africa has selected 17 HCFs to participate in the
        ARV rollout that began in April 2004. These 17 HCFs are distributed throughout the province
        (see Figure 1 and Table 2). Some communities are close to HCFs, whilst others are a great
        distance from any HCF, with a range of 0–90 km (Figure 2A). Hence, this spatial
        distribution of HCFs produces large heterogeneity in accessibility to treatment. Inequality
        in access to health care is a common characteristic of resource-constrained regions
        [5,6,7,8,9,10]. We explicitly consider heterogeneity in treatment accessibility in our
        analysis of ARV allocation strategies.
        We have developed a novel spatial mathematical model of treatment accessibility that we
        use to determine an OEAS for ARVs in a resource-constrained region. To the best of our
        knowledge, this is the first analysis to address how to deal with the extremely difficult
        problem of allocating a scarce supply of ARVs in order to design a rational and equitable
        allocation strategy. We model the “spatial diffusion of treatment” to the locations of
        disease, rather than modeling the “spatial diffusion of disease,” which is the conventional
        approach [11,12,13,14,15,16]. Our spatial model includes HCFs and the HIV-infected
        communities surrounding these HCFs; we refer to the region around each HCF as the catchment
        region. Thus, the radius of the catchment region specifies the approximate maximum distance
        that we assume infected people would be willing (or able) to travel for treatment. Each HCF
        can serve many communities, and some communities can access multiple HCFs; our model sums
        the number of people with HIV in each HCF's catchment region who could potentially travel
        to the HCF to receive ARVs (we define this number as the “effective demand” on that
        specific HCF). Thus, the “effective demand” on each HCF is a direct function of the number
        of individuals with HIV in the catchment region, weighted by their distance from the HCF.
        By including a weighting function we explicitly model heterogeneity in accessibility to
        treatment based on distance from the HCF. Here, the distance from a HCF becomes the main
        determinant influencing whether or not an individual with HIV has access to treatment.
        We developed an equity objective function to assess how the limited supply of ARVs
        should be allocated to each HCF to ensure that an equal proportion of infected people in
        each community receive treatment. To apply our theoretical framework to KwaZulu–Natal we
        model the specific location of the 17 HCFs and the 51 communities of 500 or more
        individuals (see Figure 1); for these conditions we determine an OEAS. We compared our OEAS
        with two other allocation strategies: (i) allocating ARVs only to Durban, the major urban
        area (i.e., concentrating ARVs where there is the best health-care infrastructure) and (ii)
        allocating ARVs equally to all 17 HCFs. We conduct our analysis assuming three different
        radii of catchment regions: 20 km, 40 km, and 60 km. We then extend this analysis and
        recalculate the OEAS assuming that more than 17 HCFs are available to distribute ARVs. This
        analysis is useful because there is a second potential pool of 27 ARV-implementation HCFs
        in the South African operational plan for ARV rollout [17]. We analyze this case, in which
        27 HCFs are utilized in the ARV rollout, and we also analyze how optimal ARV allocation
        would change if all 54 hospitals in KwaZulu–Natal were operational for the rollout of
        ARVs.
      
      
        Methods
        
          Calculating Demand and Treatment Access
          We assume that the number of people with HIV who will travel to a specific HCF is
          directly proportional to the number of individuals with HIV in that particular community,
          but that the probability of an individual traveling to receive ARVs (i.e., the treatment
          accessibility) decreases with distance from the HCF. We define 
          d
          i,j as the distance from community 
          i to HCF 
          j, f (
          d
          i,j ) as a weighting function that determines the treatment
          accessibility to a HCF based upon distance 
          d
          i,j , and 
          I
          i as the number of people with HIV in community 
          i . The distance, 
          d
          ij , between community 
          i and HCF 
          j is based on the longitude (lon) and latitude (lat) of each location
          and is determined by
          
          where 
          R is the radius of the earth, taken to be 6,371 km, and the angles are
          in radian measure. We calculate the “effective demand” of community 
          i on HCF 
          j to be the number of people with HIV in community 
          i that will travel to HCF 
          j for ARV regimes, namely, 
          f (
          d
          i,j )
          I
          i . Thus, demand on HCFs for ARVs is reduced by the treatment
          accessibility function. Our model is conceptually similar to the “gravity” models that
          have been used to predict retail travel [18], plan land use [19], and determine
          accessibility of primary care [20]. However, this is to our knowledge the first time this
          approach has been used to calculate ARV allocations. We use a Gaussian to model treatment
          accessibility, 
          f (
          d ) = exp(−
          kd
          2 ), where 
          k is a dispersal length scale parameter determining the radius of the
          catchment region. The size of the actual catchment regions is unknown, but based upon
          distances from communities to HCFs in KwaZulu–Natal (see Figure 2A) we assume that
          individuals are likely to travel a maximum distance of approximately 40 km to a HCF (
          k = 0.003786). We vary the catchment region by considering a 20-km
          radius (
          k = 0.0151) and a 60-km radius (
          k = 0.00168). The different catchment regions that we simulate (with
          radii of 20 km, 40 km, and 60 km) for each HCF are illustrated in Figure 2B–2D. The
          number of people with HIV throughout the province that have access to HCFs is
          approximately 86% of the total number of people with HIV for the case of a 20-km
          catchment region, 89% for a 40-km catchment region, and 93% for a 60-km catchment
          region.
        
        
          Modeling the Distribution of Treatment
          To determine how many ARVs should be allocated to each HCF, we first calculate how a
          given supply of ARVs will be distributed from each HCF to the surrounding communities in
          the catchment region. We calculate the “effective demand” on HCF 
          j, D
          j , to be
          
          which sums the “effective demand” of all communities on HCF 
          j (where there are 
          m communities). Then, we model the distribution of ARVs from a HCF to
          each community within the catchment region as the proportion of the “effective demand” on
          HCF 
          j that is contributed by the respective community. Accordingly, ARVs
          will be distributed from HCF, 
          j, to each community as the ratio
          
          Therefore, the number of people treated in community 
          i by the drug supply allocated to HCF 
          j is
          
          where 
          S
          j is the number of regimes allocated to HCF 
          j . Hence, the total number of people with HIV treated in community 
          i,T
          i , summing over all 
          n HCFs is
          
        
        
          The Equity Objective Function
          We establish an equity objective function to determine the optimal equitable
          allocation of ARVs to each HCF so that all individuals with HIV have an equal chance of
          receiving treatment. To obtain the same fraction of treated individuals in each
          community, given that there are 
          A ARV regimes for a total of individuals with HIV, the resulting
          objective function to minimize (based on least squares) becomes
          
          Our goal is to minimize 
          E, by solving for the number of ARVs to be allocated to each HCF (
          S
          1 , 
          S
          2 ,…, 
          S
          n ), whilst enforcing the following three constraints: (i) ensure that
          the total number of ARVs available is equal to the sum of the supply allocated to all
          HCFs,
          
          (ii) ensure that only a positive number of ARVs are allocated to each HCF (
          S
          j ≥ 0, 
          j = 1…
          n ); and (iii) ensure that the number of people treated in each
          community is not greater than the number of people with HIV in the community (
          T
          i ≤ 
          I
          i , i = 1…
          m ). We note that if a different objective is required, then all of our
          preceding analysis still holds and only the functional form of the objective function
          needs to be altered. To solve the problem, and determine the OEAS, we used successive
          linear programming operations research techniques [21].
        
      
      
        Results
        The OEAS of ARVs in KwaZulu–Natal that we determined is complex (see Figure 3A and 3B).
        According to our OEAS, the majority of ARVs should be allocated to HCFs in Durban, and the
        remaining ARVs should be allocated to the other HCFs throughout the province (with two
        non-Durban HCFs receiving 5%–15% of the total ARVs and the remaining non-Durban HCFs each
        receiving less than 5% of the total ARVs available). We note that our OEAS does not produce
        perfect equality; however, our optimal strategy significantly improves equality in
        obtaining treatment over the two other allocation strategies that we analyzed for
        comparison: (i) ARVs allocated only to one HCF (in the largest city, Durban) (see Figure 3D
        and 3E), and (ii) equal quantities of ARVs allocated to each HCF throughout the province
        (see Figure 3G and 3H). For comparison of allocation strategies (in Figure 3) we used an
        effective catchment radius of 40 km (
        k = 0.003786). The proportion of infected individuals that are treated at
        each location is displayed graphically in Figure 3 for our OEAS (Figure 3C) and the two
        comparison allocation strategies (Figure 3F and 3I). The best achievable outcome, given the
        limited treatment resources available, is that 10% of people with HIV are treated in each
        community throughout the province, yielding the map shown in Figure 3C, 3F, and 3I, but
        with dark blue/magenta over the entire province. Whilst our OEAS does not fully achieve
        this, it is considerably better than both of the comparison ARV allocation strategies.
        Furthermore, the equity objective function evaluates to 
        E = 0.27 for our OEAS, compared with (i) 
        E = 0.50 and (ii) 
        E = 133.88 for the comparison allocation strategies. There is large
        diversity in the fraction of individuals with HIV treated per community when equal
        quantities of ARVs are given to each HCF, evidenced by an inter-quartile range of
        0.025%–41.746% compared with inter-quartile ranges of 0%–0% and 0.011%–9.982% for the first
        comparison strategy and our OEAS, respectively. Therefore, equal access is not obtained if
        equal quantities of ARVs are allocated to each HCF. Obviously, allocating to only one HCF
        (the first comparison strategy) could also be considered unequal because although the
        inter-quartile range is minimal, effectively only one community (Durban) receives ARVs. Our
        OEAS, while not perfect, achieves the best equality possible given the accessibility
        constraints and limited ARV supply.
        The catchment region for HCFs is a factor of large uncertainty. We considered three
        catchment region sizes: radii of 20 km, 40 km, and 60 km. We also simulated two additional
        cases with increased numbers and locations of HCFs (27 HCFs as suggested in South Africa's
        official ARV rollout operational plan [17]; and all 54 hospitals in KwaZulu–Natal). In
        Figure 4 we present box plots of the percentage of infected people that obtain treatment
        per community for the three sets of HCFs and the three catchment region sizes we simulate.
        For each specified condition we calculate the OEAS. It is apparent that equality in access
        to ARVs is improved substantially if the radius of each catchment region is increased
        and/or the number of HCFs is increased (Figure 4). Our results show that the number of HCFs
        utilized is of greater importance than the size of the catchment region. If 54 HCFs are
        used, then even a (small) catchment radius of 20 km results in the ideal median proportion
        of 10% of people with HIV in each community receiving ARVs. In the case of 27 HCFs, 88% of
        all people with HIV have access to HCFs for a 20-km catchment region, 91% for a 40-km
        catchment region, and 96% for a 60-km catchment region. In the case of 54 HCFs, 90% of all
        people with HIV in the province have access to HCFs for a 20-km catchment region, 94% for a
        40-km catchment region, and 99% for a 60-km catchment region. Therefore, increasing the
        number of HCFs available for an ARV rollout is effective in significantly increasing
        equality in treatment accessibility as shown in Figure 4. Furthermore, if catchment regions
        actually have a radius of 60 km, or can be increased to this size through improvements in
        transportation, this would enable access to HCFs for almost all people in the province, as
        shown in Figure 4. The actual HCF allocations determined by our model and optimization for
        the cases of 17, 27, and 54 HCFs (and for all catchment sizes we consider) are presented as
        pie charts in Figure 5. It is clear from our analysis that the equality criterion, such
        that each individual with HIV in KwaZulu–Natal has an equal chance of receiving ARVs, can
        best be satisfied by utilizing all 54 HCFs for ARV distribution and ensuring that each HCF
        serves a catchment region of 40 to 60 km.
      
      
        Discussion
        We have established an elegant and simple theoretical framework for determining an
        equitable and rational allocation of ARVs to HCFs in resource-constrained countries. To the
        best of our knowledge, this is the first analysis to address this very difficult problem.
        We determined that increasing the size of the catchment region of each HCF can improve
        access to HCFs considerably for rural populations. We suggest that studies be performed to
        collect data on the distance that individuals with HIV are willing and able to travel for
        treatment. This will facilitate discussions of this important issue, which must be
        considered in the making of policy decisions. A database consisting of such information has
        been proposed for South Africa [22]. In an effort to provide equal access to communities
        with relatively little access to ARV therapy, the concept of a mobile clinic that would
        travel between communities to take health-care workers and resources to the location of the
        demand is a new initiative in Nigeria (S. Agwale, personal communication) that could also
        be considered in other regions.
        We calculated the optimal allocation of ARVs to available HCFs so that all infected
        individuals will have as close as possible to an equal chance of obtaining treatment. We
        have shown that increasing the number of HCFs involved in ARV distribution can improve
        equality of access to ARVs substantially. The current plan in KwaZulu–Natal is to use only
        17 HCFs. However, our results clearly show that in order to achieve an optimal equitable
        allocation strategy, all existing infrastructure (i.e., all 54 HCFs) should be used. The
        strategy that we are advising may be fairly easy to accomplish at the policy level because
        the health-care infrastructure (specifically these HCFs) already exists, although
        consideration must be made for issues such as the training and transportation that is
        necessary, which may be costly. In contrast, increasing the size of catchment regions may
        be very difficult. Obviously, increasing both the number of HCFs and the size of the
        catchment region each services would substantially increase equality of access to health
        care in KwaZulu–Natal.
        Future modeling studies could extend our work by not making the simplifying assumption
        that all patients have similar ease of travel over the same distance and by including
        weighting functions on distance impedance for different communities (based on the quality
        of the road infrastructure, for example, and the availability of transportation) (D. P.
        Wilson, J. O. Kahn, S. M. Blower, unpublished data). Here, we have shown how to calculate
        optimal ARV allocation strategies based upon the principle of equity. Future research is
        necessary to compare ARV allocation strategies based upon the principle of efficiency
        (i.e., allocating ARVs to maximize epidemic reduction) in order to determine whether
        utilizing different principles for optimization would result in similar (or different)
        allocation strategies.
        The World Health Organization and the Joint United Nations Programme on HIV/AIDS have
        identified three core principles that should underlie the effort to fairly distribute ARVs,
        namely: urgency, equity, and sustainability [23]. They state that policy decisions for the
        fair distribution of ARVs should be based upon the following ethical principles: (i) the
        principle that like cases should be treated alike, (ii) the utilitarian principles of
        maximizing overall societal benefits, (iii) the egalitarian principles of equity
        (distributing resources, such as health care, equally among different groups), and (iv) the
        Maximin principle (which prioritizes individuals that are the least advantaged) [24]. Here,
        we investigated the level of decision-making associated with allocating ARVs to HCFs, and
        we have applied the egalitarian principle of equity with respect to access to health care.
        We suggest that allocating ARVs to HCFs to achieve equality in accessibility could be
        carried out, and then individual-level ethical considerations could be thought out at the
        next level of deliberation. Future research is necessary to identify alternative (and more
        detailed) ethical ARV allocation strategies.
        Although we have focused on one equitable strategy, there are many other ARV allocation
        strategies that are ethical. Uneven access to HIV treatment has the very real potential to
        fracture social and political structures and could lead to intrastate and/or interstate
        conflict [2]. Government decisions on ARV allocation have potentially socially
        destabilizing ramifications because essentially the decisions determine who lives and who
        dies. Resource allocation decisions will have to be made at a number of levels: it must be
        decided what proportion of the available ARVs should be allocated to each province; then it
        must be decided how many ARVs should be allocated to each HCF within each region; and
        finally, particular groups of individuals may be chosen to have treatment priority.
        Treatment priority decisions for individuals could be based on many different criteria,
        including disease progression (CD4 cell counts and viral load), socioeconomic status,
        ethnicity, and who is thought to have the greatest risk of transmitting infections (for
        example, pregnant women with HIV or female sex workers). Although it could be argued that
        behavioral core groups should be targeted to receive ARVs because this may have the
        greatest epidemiological impact, such an allocation strategy would be neither feasible nor
        practical to implement. For example, sex workers are an obvious behavioral core group, but
        many women would likely claim to be sex workers if they knew that ARVs were only available
        to sex workers. Additionally, the ethics of targeting such groups in favor of other
        societal groups must be questioned. It could also be argued that, to maximize the
        preventative effect of ARV therapy, ARVs should be concentrated in virological core groups
        (i.e., people with the highest viral load) [25,26]; this novel approach of targeting the
        virological core group has recently been proposed for controlling HSV-2 epidemics [27].
        Identifying individuals in the virological core group would be far easier than identifying
        individuals in the behavioral core group. These individuals are likely to be the sickest
        and those with evidence of disease-related symptoms. Treatment allocation strategies could
        also be designed based on reducing the future epidemic impact and disregarding treatment
        equality amongst currently infected people. Such strategies place different social value on
        currently infected people in comparison with future infected people; such strategies
        therefore may not be ethical even though they may be epidemiologically sound (also, it is
        important to note that any epidemic predictions have large uncertainty ranges [28,29]).
        Our model has been applied to the South African province of KwaZulu–Natal, but it can be
        applied by government health officials in any resource-constrained country. In many of the
        countries worst affected by the HIV pandemic, scarcity of resources will mean that not
        everyone that could potentially benefit from ARVs will be able to access them. Many of the
        decisions that must be made to develop an effective response to the HIV/AIDS epidemic are
        inevitably underpinned by ethical considerations. Leadership in most resource-constrained
        regions cannot avoid these decisions. Whilst there has been considerable attention given to
        South Africa, many other countries worldwide either have plans in place (e.g., Brazil,
        Thailand, and Botswana) or are in the process of developing national programs for ARV
        distribution through the public health system (e.g., Mozambique, Malawi, and Kenya) [1].
        Legitimate authorities in each nation must come to their own consensus on the priorities
        and objectives of an ARV rollout, which is not a trivial matter [1,30]. Our objective
        function and model can be used to calculate allocation strategies that provide equity in
        access (compensating for geographical isolation), but if authorities in a given nation
        prioritize a different goal for ARV rollout, then an objective function to optimize can be
        formulated to reflect the specific national policy goal. Our model can be used by policy
        makers to determine an optimal scientifically based allocation strategy, based upon the
        specific objective function. As the ARV rollout commences in KwaZulu–Natal, difficult
        decisions will have to be made as to how to allocate scarce resources. We have shown that
        it is possible to obtain a mathematical solution to an equity problem. We suggest that our
        novel approach could be used to determine optimal equitable allocation strategies for many
        other resource-constrained countries that are just beginning to receive ARVs [31].
      
    
  

  
    
      
        
        “Publishing results in traditional paper based way in a journal hides too much
        information.” This is the verdict of Markus Ruschhaupt and colleagues who, in a paper in 
        Statistical Applications in Genetics and Molecular Biology (3: article
        37), discuss a paradigm for the presentation of complex data—in this case, from microarray
        analyses. The title of the article, “A Compendium to Ensure Reproducibility in
        High-Dimensional Classification Tasks,” may not lend itself easily to a clinical audience,
        but the underlying message to clinicians could not be more important: that, currently,
        studies involving large datasets, especially ones that have a clinical outcome, are so
        poorly reported (or possibly so poorly done) that many are not reproducible. This problem
        was also the topic of a recent meeting in Heidelberg, “Best Practice in Microarray Studies”
        (http://www.biometrie.uni-heidelberg.de/workshops/bestpractice/index.htm).
        As microarrays have become mainstream research tools in biology and medicine, the large
        datasets and complex analyses from these studies have presented challenges: for authors in
        analyzing the data, for reviewers and editors in deciding on the suitability of papers for
        publication, for journals in determining how much data needs to be presented within the
        paper itself, for other researchers in reproducing the data, and, finally, for readers in
        deciding how to assess the data presented. The results from several high-profile papers
        have already proved difficult to reproduce, even by those with sufficient time and
        computing expertise.
        Where do such analyses leave the new science of molecular pathology? Ruschhaupt and
        colleagues comment that “the literature on the induction of prognostic profiles from
        microarray studies is a methodological wasteland.” Much the same could be said of other
        applications of molecular biology to clinical samples. A systematic review of molecular and
        biological tumor markers in neuroblastoma (Clin Cancer Res 10: 4–12) found that its
        conclusions were limited by “small sample sizes, poor statistical reporting, large
        heterogeneity across studies…and publication bias.” John Ioannidis and colleagues (Lancet
        362: 1439–1444) did a similar analysis of 30 microarray studies with major clinical
        outcomes in cancer. They showed that the studies were small—median sample size was 25
        patients, and validation was incomplete in most studies. They recommended that molecular
        prognostic studies be classified as phase 1 (early exploratory probing associations), phase
        2 (exploratory with extensive analyses), or phase 3 (large confirmatory studies with
        pre-stated hypotheses and precise quantification of the magnitude of the effect), and that
        only studies that had undergone phase 3 testing should be considered robust enough for use
        in clinical practice. Most current studies should be considered as phase 1 or, at best,
        phase 2.
        So, despite considerable hype, the published studies are far from the level of evidence
        that would be accepted for virtually any other medical test. In a review in 2003
        (Hematology [Am Soc Hematol Educ Program] 2003: 279–293), Rita Braziel and colleagues
        concluded, “rapid identification and neutralization of spurious results is essential to
        prevent them from becoming accepted facts.”
        But these problems are not new in medical research. In 1994 (BMJ 308: 283–284), Doug
        Altman, who was instrumental in developing the CONSORT guidelines for reporting of clinical
        trials, said that “huge sums of money are spent annually on research that is seriously
        flawed through the use of inappropriate designs, unrepresentative samples, small samples,
        incorrect methods of analysis, and faulty interpretation,” and “that quality control needs
        to be built in from the start rather than the failures being discarded.”
        So how can we ensure that the wealth of data pouring out of microarray and other
        molecular diagnostic studies is turned into meaningful knowledge? The Microarray Gene
        Expression Data Society has proposed a set of guidelines (MIAME) for the reporting of
        microarray data, and that all such data should be deposited in public databases. But as
        Ruschhaupt and others have shown, disclosure of results and data is not enough, since there
        is little consensus on the appropriate statistical analyses and many are developed on a
        case by case basis, which may not be reproducible, even by the authors. Some researchers
        advocate the use of standard statistical packages, which allows the reader to repeat an
        entire analysis quickly and, hence, assess the robustness of the results. Some authors have
        produced a transcript of their statistical analyses as a supplement to their articles
        (e.g., Nucleic Acids Res 32: e50). At the very least authors should have a protocol with a
        prespecified plan for patient selection and statistical analysis—accepted practice for
        clinical trials, but not yet for other medical research. An ultimate aim for reporting
        would be the type of compendium discussed by Ruschhaupt and colleagues—“an interactive
        document that bundles primary data, statistical processing methods, figures, and derived
        data together with the textual documentation and conclusions.” One such compendium is
        illustrated in a paper by Robert Gentleman (Stat Appl Genet Mol Biol 4: article 2). 
        PLoS Medicine is keen to work with authors towards making such reporting
        possible. But although the time might have gone when the two-dimensional journal article
        could suffice for complex papers, clinicians should nonetheless apply the same critical
        assessment that they would for any other clinical tool. If a result is too good to be true,
        it probably is.
      
    
  

  
    
      
        Introduction
        The World Trade Center and anthrax terrorist attacks in 2001, as well as the recent West
        Nile virus and SARS outbreaks, have motivated many public health departments to develop
        early disease outbreak detection systems using non-diagnostic information, often derived
        from electronic data collected for other purposes (“syndromic surveillance”)
        [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]. These include systems that monitor the number
        of emergency department visits, primary care visits, ambulance dispatches, nurse hot line
        calls, pharmaceutical sales, and West Nile–related dead bird reports. The establishment of
        such systems involves many challenges in data collection, analytical methods, signal
        interpretation, and response. Important analytical challenges include dealing with the
        unknown time, place, and size of an outbreak, detecting an outbreak as early as possible,
        adjusting for natural temporal and geographical variation, and dealing with the lack of
        suitable population-at-risk data.
        Most analytical methods in use for the early detection of disease outbreaks are purely
        temporal in nature [18,19,20,21,22]. These methods are useful for detecting outbreaks that
        simultaneously affect all parts of the geographical region under surveillance, but may be
        late at detecting outbreaks that start locally. While purely temporal methods can be used
        in parallel for overlapping areas of different sizes in order to cover all possible
        outbreaks, that approach leads to a severe problem of multiple testing, generating many
        more false signals than the nominal statistical significance level would indicate.
        First studied by Naus [23], the scan statistic is an elegant way to solve problems of
        multiple testing when there are closely overlapping spatial areas and/or time intervals
        being evaluated. Temporal, spatial, and space–time scan statistics [24,25,26,27] are now
        commonly used for disease cluster detection and evaluation, for a wide variety of diseases
        including cancer [28,29], Creutzfeldt-Jakob disease [30], granulocytic ehrlichiosis [31],
        sclerosis [32], and diabetes [33]. The basic idea is that there is a scanning window that
        moves across space and/or time. For each location and size of the window, the number of
        observed and expected cases is counted. Among these, the most “unusual” excess of observed
        cases is noted. The statistical significance of this cluster is then evaluated taking into
        account the multiple testing stemming from the many potential cluster locations and sizes
        evaluated.
        To date, all scan statistics require either a uniform population at risk, a control
        group, or other data that provide information about the geographical and temporal
        distribution of the underlying population at risk. Census population numbers are useful as
        a denominator for cancer, birth defects, and other registry data, where the expected number
        of cases can be accurately estimated based on the underlying population. They are less
        relevant for surveillance data such as emergency department visits and pharmacy sales,
        since the catchment area for each hospital/pharmacy is undefined. Even if it were
        available, the catchment area population would not be a good denominator since there can be
        significant natural geographical variation in health-care utilization data, due to
        disparities in disease prevalence, access to health care, and consumer behavior [34]. One
        option when evaluating data that are affected by utilization behavior is to use total
        volume as the denominator. For example, one may use total emergency department visits as a
        denominator when evaluating diarrhea visits [7], or similarly, all pharmacy sales as the
        denominator when evaluating diarrhea medication sales [4]. This may or may not work
        depending on the nature of the data. For example, changes in total drug sales due to sales
        promotions or the allergy season could hide a true signal or create a false signal for the
        drug category of interest.
        In this paper we present a prospective space–time permutation scan statistic that does
        not require population-at-risk data, and which can be used for the early detection of
        disease outbreaks when only the number of cases is available. The method can be used
        prospectively to regularly scan a geographical region for outbreaks of any location and any
        size. For each location and size, it looks at potential one-day as well as multi-day
        outbreaks, in order to quickly detect a rapidly rising outbreak and still have power to
        detect a slowly emerging outbreak by combining information from multiple days.
        The space–time permutation scan statistic was gradually developed as part of the New
        York City Department of Health and Mental Hygiene (DOHMH) surveillance initiatives, in
        parallel with the adaptation of population-at-risk-based scan statistics for dead bird
        reports (for West Nile virus) [13], emergency department visits [7], ambulance dispatch
        calls [6], pharmacy sales [4], and student absentee records [3]. In this methodological
        paper, the space–time permutation scan statistic is presented and illustrated using
        emergency department visits for diarrhea, respiratory, and fever/flu-like illnesses.
      
      
        Methods
        
          New York City Emergency Department Syndromic Surveillance System
          The New York City Emergency Department syndromic surveillance system is described in
          detail elsewhere [7]. In brief, participating hospitals transmit electronic files to the
          DOHMH seven days per week. Files contain data for all emergency department patient visits
          on the previous day, including the time of visit, patient age, gender, home zip code, and
          chief complaint—a free-text field that captures the patient's own description of their
          illness. As of November 2002, 38 of New York City's 66 emergency departments were
          participating in the system, covering an estimated 75% of emergency department visits in
          the city.
          Data are verified for completeness and accuracy, concatenated into a single dataset,
          and appended to a master archive using SAS [35]. To categorize visits into “syndromes”
          (e.g., “diarrhea syndrome”), a computer algorithm searches the free-text chief complaint
          field for character strings indicating symptoms consistent with that syndrome.
          The goal of data analysis, which is carried out seven days per week, is to detect
          unusual increases in key syndrome categories. To run the space–time permutation scan
          statistic we have written a SAS program that generates the necessary case and parameter
          files, invokes the SaTScan software [36], and reads the results back into SAS for
          reporting and display.
          Two sets of analyses are performed, one based on assigning each individual to the
          coordinates of their residential zip code and the other based on their hospital address.
          With 183 zip codes versus 38 hospitals, the former utilizes more detailed geographical
          information, while the latter may be able to pick up outbreaks not only related to place
          of residence but also to place of work or other outside activities (if people go to the
          nearest hospital when they feel sick). Residential zip code is not recorded by the
          hospital for about 3% of patients, and for the analyses described here, these individuals
          are only included in the hospital-based analyses.
          The performance of the prospective space–time permutation scan statistic was evaluated
          using both hospital and residential analyses. We used historical diarrhea data to mimic a
          prospective surveillance system with daily analyses from 15 November 2001 to 14 November
          2002. For each of these days, the analysis only used data prior to and including the day
          in question, ignoring all data from subsequent days. This corresponds to the actual data
          available at the DOHMH 8–12 h after the end of that day, when that analysis would have
          been conducted if the system has been in place at that time. We also present one week of
          daily prospective analyses conducted in November 2003, where the daily analysis was run
          about 12 h after the conclusion of each day, as part of the regular syndromic
          surveillance activities at the DOHMH.
        
        
          The Space–Time Permutation Scan Statistic
          As with the Poisson- and Bernoulli-based prospective space–time scan statistics [27],
          the space–time permutation scan statistic utilizes thousands or millions of overlapping
          cylinders to define the scanning window, each being a possible candidate for an outbreak.
          The circular base represents the geographical area of the potential outbreak. A typical
          approach is to first iterate over a finite number geographical grid points and then
          gradually increase the circle radius from zero to some maximum value defined by the user,
          iterating over the zip codes in the order in which they enter the circle. In this way,
          both small and large circles are considered, all of which overlap with many other
          circles. The height of the cylinder represents the number of days, with the requirement
          that the last day is always included together with a variable number of preceding days,
          up to some maximum defined by the user. For example, we may consider all cylinders with a
          height of 1, 2, 3, 4, 5, 6, or 7 d. For each center and radius of the circular cylinder
          base, the method iterates over all possible temporal cylinder lengths. This means that we
          will evaluate cylinders that are geographically large and temporally short, forming a
          flat disk, those that are geographically small and temporally long, forming a pole, and
          every other combination in between.
          What is new with the space–time permutation scan statistic is the probability model.
          Since we do not have population-at-risk data, the expected must be calculated using only
          the cases. Suppose we have daily case counts for zip-code areas, where 
          c
          zd is the observed number of cases in zip-code area 
          z during day 
          d . The total number of observed cases (
          C ) is
          
          For each zip code and day, we calculate the expected number of cases μ
          
            zd
          conditioning on the observed marginals:
          
          In words, this is the proportion of all cases that occurred in zip-code area 
          z times the total number of cases during day 
          d . The expected number of cases μ
          
            A
          in a particular cylinder 
          A is the summation of these expectations over all the zip-code-days
          within that cylinder:
          
          The underlying assumption when calculating these expected numbers is that the
          probability of a case being in zip-code area 
          z, given that it was observed on day 
          d, is the same for all days 
          d .
          Let 
          c
          A be the observed number of cases in the cylinder. Conditioned on
          the marginals, and when there is no space–time interaction, 
          c
          A is distributed according to the hypergeometric distribution
          with mean μ
          
            A
          and probability function
          
          When both Σ
          
            z
          
          εA
          c
          zd and Σ
          
            d
          
          εA
          c
          zd are small compared to 
          C, c
          A is approximately Poisson distributed with mean μ
          
            A
          [37]. Based on this approximation, we use the Poisson generalized likelihood ratio
          (GLR) as a measure of the evidence that cylinder 
          A contains an outbreak:
          
          In words, this is the observed divided by the expected to the power of the observed
          inside the cylinder, multiplied by the observed divided by the expected to the power of
          the observed outside the cylinder. Among the many cylinders evaluated, the one with the
          maximum GLR constitutes the space–time cluster of cases that is least likely to be a
          chance occurrence and, hence, is the primary candidate for a true outbreak. One reason
          for using the Poisson approximation is that it is much easier to work with this
          distribution than the hypergeometric when adjusting for space by day-of-week interaction
          (see below), as the sum of Poisson distributions is still a Poisson distribution.
          Since we are evaluating a huge number of outbreak locations, sizes, and time lengths,
          there is serious multiple testing that we need to adjust for. Since we do not have
          population-at-risk data, this cannot be done in any of the usual ways for scan
          statistics. Instead, it is done by creating a large number of random permutations of the
          spatial and temporal attributes of each case in the dataset. That is, we shuffle the
          dates/times and assign them to the original set of case locations, ensuring that both the
          spatial and temporal marginals are unchanged. After that, the most likely cluster is
          calculated for each simulated dataset in exactly the same way as for the real data.
          Statistical significance is evaluated using Monte Carlo hypothesis testing [38]. If, for
          example, the maximum GLR is calculated from 999 simulated datasets, and the maximum GLR
          for the real data is higher than the 50th highest, then that cluster is statistically
          significant at the 0.05 level. In general terms, the 
          p -value is 
          p = 
          R/ (
          S + 1) where 
          R is the rank of the maximum GLR from the real dataset and 
          S is the number of simulated datasets [38]. In addition to 
          p -values, we also report null occurrence rates [8], such as once every
          45 d or once every 23 mo. The null occurrence rate is the expected time between seeing an
          outbreak signal with an equal or higher GLR assuming that the null hypothesis is true.
          For daily analyses, it is defined as once every 1
          /p d. For example, under the null hypothesis we would at the 0.05 level
          on average expect one false alarm every 20 d for each syndrome under surveillance.
          Because of the Monte Carlo hypothesis testing, the method is computer intensive. To
          facilitate the use of the methods by local, state, and federal health departments, the
          space–time permutation scan statistic has been implemented as a feature in the free and
          public domain SaTScan software [36].
        
        
          Implementation for New York City Syndromic Surveillance
          Depending on the application, the method may be used with different parameter
          settings. For the syndromic surveillance analyses we set the upper limit on the
          geographical size of the outbreak to be a circle with a 5-km radius, and the maximum
          temporal length to be 7 d. This means that we are evaluating outbreaks with a circle
          radius size anywhere between 0 km (one zip code only) and 5 km, and a time length
          (cylinder height) of 1 to 7 d. The latter restriction is a reflection of the belief that
          the main purpose of this syndromic surveillance system is early disease outbreak
          detection, and if the outbreak has existed for over 1 wk, it is more likely to be picked
          up by reporting of specific disease diagnoses by clinicians or laboratories.
          Another practical choice is the total number of days to include in the analysis. One
          option is to include all previous days for which data are available. We chose instead to
          analyze the last 30 d of data, adding one day and removing another for each daily
          analysis. We believe this time frame provides enough baseline beyond the 1- to 7-d
          scanning window to establish the usual pattern of visits while avoiding inclusion of data
          that may no longer be relevant to the current period.
          To reduce the computational load, we limited the centers of the circular cylinder
          bases to be a collection of 446 zip-code area centroids and hospital locations in New
          York City and adjacent areas. This ensures, among other things, that each zip-code area
          may constitute an outbreak on its own.
          The last parameter that we need to set is the number of Monte Carlo replications used
          for each analysis. For the daily prospective analyses we chose 999, which meant that the
          smallest 
          p -value we could get was 0.001, so that the smallest null occurrence
          rate possible for a signal was once every 2.7 y. In our system, signals of that strength
          clearly merit investigation. For the historical evaluation, in order to obtain more
          precise null occurrence rates, we set the number of replications to 9,999.
        
        
          Adjusting for Space by Day-of-Week Interaction
          The space–time permutation scan statistic automatically adjusts for any purely spatial
          and purely temporal variation. For many syndromic surveillance data sources, there is
          also natural space by day-of-week interaction in the data that is not due to a disease
          outbreak but to consumer behavior, store hours, etc. For example, if a particular
          pharmacy has an exceptionally high number of sales on Sundays because neighboring
          pharmacies are closed, we might get a signal for this pharmacy every Sunday unless we
          adjust for this space by day-of-week interaction. This can be done through a stratified
          random permutation procedure.
          The first step is to stratify the data by day of week: Monday, Tuesday,…, Sunday. The
          space–time permutation randomization step is then done separately for each day of the
          week. For each zip code and day combination, the expected is then calculated using only
          data from that day of the week. For each cylinder, both the observed and expected number
          of cases is summed over all day-of-week strata, zip code, and day combinations within
          that cylinder. The same technique can be used to adjust for other types of space–time
          interaction. The underlying assumption when calculating these expected numbers is now
          that the probability of a case being in zip-code area 
          z, given that it was observed on a Monday, is the same for all Mondays,
          etc.
          All our analyses were adjusted for space by day-of-week interaction.
        
        
          Missing Data
          Daily disease surveillance systems require rapid transmission of data, and it may not
          be possible to get complete data from each provider every single day. When we first tried
          the new method in New York City, a number of highly significant outbreak signals were
          generated that were artifacts of previously unrecognized missing or incomplete data from
          one or more hospitals. This is a good reflection on the method, since it should be able
          to detect abnormalities in the data no matter what their cause, but it also illustrates
          the importance of accounting for missing data in order to create an early detection
          system that is useful on a practical level.
          Depending on the exact nature of the missing data, there are different ways to handle
          it. We used a combination of three different approaches. (1) If a hospital had missing
          data for all of the past 7 d (all possible days within the cylinder), we completely
          removed that hospital from the analysis, including all previous 23 d. (2) If a hospital
          had no missing data during the last 7 d, but one or more missing days during the previous
          23 baseline days, then we completely removed the baseline days with some missing data,
          for all of the hospitals. (3) If a hospital had missing data for at least one but not all
          of the last 7 d, then we removed those missing days together with all previous days for
          the same hospital and the same day of week. That is, if Monday was missing during the
          last week, then we removed all Mondays for that hospital. This removal introduces
          artificial space by day-of-week interaction, so this approach only works if it is
          implemented in conjunction with the stratified adjustment for space by day-of-week
          interaction.
          For some analyses, more than one of these approaches were used simultaneously. Note
          that, since the missing data depend on the hospital, the solution is to remove specific
          hospitals and days rather than zip codes and days, even when we are doing the
          zip-code-based residential analyses. If there are many hospitals with missing data, then
          the second approach could potentially remove all or almost all of the baseline days. To
          avoid this, one could sometimes go further back in time and add the same number of
          earlier days to compensate. Another option is to impute into the cells with missing data
          a Poisson random number of cases generated under the null hypothesis. Given the
          completeness of our data, neither of these methods were employed (94% of analyses were
          conducted with four or fewer baseline days removed).
        
      
      
        Results
        
          Evaluation Using Historical Data: Diarrhea Surveillance
          We first tested the new method by mimicking daily prospective analyses of hospital
          emergency department data from 15 Nov 2001 to 14 Nov 2002, looking at diarrhea visits.
          Signals with 
          p ≤ 0
          . 0027 are listed in Table 1 and depicted on the map in Figure 1. That
          is, we only list those signals with a null occurrence rate of once every year or less
          often.
          For the residential zip-code analyses, there were two such signals. For the hospital
          analyses, there were six, two of which occurred in the same place on consecutive days. It
          is worth noting that at the false alarm rate chosen, none of the residential signals
          correspond to any of the hospital signals.
          For the residential analysis, the strongest signal was on 9 February 2002, covering 17
          zip-code areas in southern Bronx and northern Manhattan. This signal had 63 cases
          observed over 2 d when 34.7 were expected (relative risk = 1.82). With a null occurrence
          rate of once every 5.5 y, a spike in cases of this magnitude is unlikely to be due to
          random variation. The signal immediately preceded a sharp increase in citywide diarrheal
          visits from 10 February to 20 March (Figure 2). In both the localized 9 February cluster
          and the citywide outbreak, the increase was most notable among children less than 5 y of
          age. The weaker 26 February hospital signal and the 7 March residential signal that were
          centered in northern Manhattan occurred at the peak of this citywide outbreak. Laboratory
          investigation of the citywide increase in diarrheal activity indicated the rotavirus as
          the most likely causative agent.
          The two hospital signals on 1 November and 2 November 2002, were at the same three
          hospitals in southern Bronx and northern Manhattan, with null occurrence rates of 1.6 and
          3.4 y, respectively. These signals immediately preceded another sharp increase in
          citywide diarrheal activity, this time among individuals of all ages (Figure 2). This
          citywide outbreak lasted approximately 6 wk and coincided with a number of institutional
          outbreaks in nursing homes and on cruise ships. Laboratory investigation of several of
          these outbreaks revealed the norovirus as the most likely causative agent. A similar
          citywide outbreak of norovirus in 2001 began shortly before the 21 November 2001 hospital
          signal in northern Bronx, which had a null occurrence rate of once every 3.4 y.
          For the hospital analyses, the strongest signal was a 1-d cluster at a single hospital
          in Queens on 11 January 2002, with ten diarrhea cases when only 2.3 were expected, which
          one would only expect to happen once every 3.9 y. Being very local in both time and
          space, it is different from the previously described signals preceding citywide
          outbreaks. While examination of individual-level data revealed a predominance of infants
          under the age of two, this cluster could not be associated with any known outbreak, and
          retrospective investigation was not feasible.
          As shown in Table 1, at the 
          p = 0.0027 threshold there were six and two signals for the hospital
          and residential analyses, respectively, compared to one expected in each. Figure 3 shows
          the number of days on which the 
          p -value of the most likely cluster was within a given range. Had the
          null hypothesis been true on all 365 d analyzed, the 
          p -values would have been uniformly distributed between zero and one.
          The fact that in our data there were more days with low rather than high 
          p -values is an indication that there may be additional true
          “outbreaks” that are indistinguishable from random noise. These could be very small
          disease outbreaks, for example, due to spoiled food eaten by only a few people, or they
          could be artifacts caused by, for example, changes in the hours of operation at an
          emergency department or coding differences between the emergency department triage
          nurses.
        
        
          Daily Prospective Surveillance
          Since 1 November 2003, the space–time permutation scan statistic has been used daily
          in parallel with the population-at-risk-based space–time scan statistics [7] as part of
          the DOHMH Emergency Department surveillance system. For respiratory symptoms, fever/flu,
          and diarrhea, the results for the last week of November are listed in Tables 2 and 3. For
          diarrhea or respiratory symptoms there were no strong signals warranting an
          epidemiological investigation, and all had null occurrence rates of more often than once
          every month. This reflects a very typical week.
          For fever/flu there was a strong 7-d hospital signal in southern Bronx and northern
          Manhattan on 28 November with a null occurrence rate of once every 2.7 y. On each of the
          following 2 d, there were again strong hospital signals in the same general area as well
          as residential zip-code signals of lesser magnitude. These signals started 12 d into a
          gradual citywide increase in fever/flu that continued to grow through the end of
          December, driven by an unusually early influenza season in New York City.
        
      
      
        Discussion
        In this paper we have presented a new method for prospective infectious disease outbreak
        surveillance that uses only case data, handles missing data, and makes minimal assumptions
        about the spatiotemporal characteristics of an outbreak. When using historical emergency
        department chief complaint data to mimic a prospective surveillance system with daily
        analyses, we detected four highly unusual clusters of diarrhea cases, three of which
        heralded citywide gastrointestinal outbreaks due to rotavirus and norovirus. Three of four
        weaker signals also occurred immediately preceding or concurrent with these citywide
        outbreaks. If we assume that all of these clusters were associated with the citywide
        disease outbreaks, then the method generated at most two false alarms at a signal threshold
        where we would have expected one by chance alone.
        For disease outbreak detection, the public-health community has historically relied on
        the watchful eyes of physicians and other health-care workers. However, the increasing
        availability of timely electronic surveillance data, both reportable diagnoses and
        pre-diagnostic syndromic indicators, raises the possibility of earlier outbreak detection
        and intervention if suitable analytic methods are found. While it is still unclear whether
        systematic health surveillance using syndromic or reportable disease data will be able to
        quickly detect a bioterrorism attack [39,40], the methods described here can also be
        applied to early detection of outbreaks of other, more common infectious diseases.
        There are other alternative ways to calculate expected counts from a series of case
        data. One naive approach is to use the observed count 7 d ago in a zip-code area as the
        expected count for that same area today, and then apply the regular Poisson-based
        space–time scan statistic. When applied to the New York City diarrhea data described above,
        such an approach generated at least one “statistically significant” outbreak signal on each
        of the 365 d evaluated. The basic problem with this is that there is random variation in
        the observed counts that are used to calculate the expected, which is not accounted for in
        the Poisson-based scan statistic. If we based the expected on the average of multiple prior
        weeks of data, we would get less variability in the expected counts and fewer false
        signals, but the problem would still persist, and as the number of weeks increase beyond a
        few months other problems may gradually arise due to, for example, seasonal trends or
        population size changes.
        Computing time depends on the size of the dataset and the analysis parameters chosen.
        With 999 replications, the hospital analyses with 38 data locations take 7 s to run on a
        2.5-MHz Pentium 4 computer, while the residential analyses using 183 zip-code area
        locations take 11 s. The same numbers for 9,999 replications are 27 and 57 s,
        respectively.
        There are a number of limitations with the proposed method. The method is highly
        sensitive to missing or incomplete data. Our first implementation of the method resulted in
        a number of false alarms, and highlights the need for systematic data quality checks and
        the analytic adjustments described above. When excellent population-at-risk data are
        available, we expect the Poisson-based space–time scan statistic that utilizes this extra
        information to perform better than the space–time permutation scan statistic. If, however,
        the population-at-risk data are of poor quality or nonexistent, which is often the case,
        then the space–time permutation scan statistic should be used.
        Since the space–time permutation scan statistic adjusts for purely temporal clusters, it
        can only detect citywide outbreaks if they start locally, but not if they occur more or
        less simultaneously in the whole city. Hence, it does not replace purely temporal
        surveillance methods, but rather complements them.
        Finally, it is important to note that the geographical boundary of the detected outbreak
        is not necessarily the same as the boundary of the true outbreak. Since we used circles as
        the base for the scanning cylinder, all detected outbreaks are approximately circular.
        Other shapes of the scanning window are also available [36], but it has been shown that
        circular scan statistics are also able to detect noncircular outbreak areas [41]. The less
        geographically compact the outbreak is, though, the less power (sensitivity) there is to
        detect it. For example, using circles we cannot expect to pick up an outbreak that is very
        long and narrow such as a one-block area on each side of Broadway, stretching from southern
        to northern Manhattan.
        The emergency department data used in this study also have some limitations. In addition
        to the citywide outbreaks, there were several institutional gastrointestinal outbreaks
        reported to DOHMH during the historical 1-y period but not detected in emergency department
        data using the space–time permutation scan statistic. One reported outbreak involved school
        children that went to the emergency department of a nonparticipating hospital. Other
        outbreaks went undetected because medical care was not sought in emergency departments.
        Most people with diarrhea do not go to the hospital emergency department. Rather, they call
        or go to their primary care physician, they visit the pharmacy to buy over-the-counter
        medication, or they may have symptoms that are so mild that they do not seek medical care.
        Further studies are needed to evaluate the strengths and weaknesses of different data
        sources.
        The geographic units of analysis used were residential zip code and hospital location.
        It may be hard to detect outbreaks that affect only a small part of a single zip code,
        especially if the background rate of the syndrome is fairly high. Where available, the
        exact coordinates of a patient's residence can be used to avoid problems introduced when
        aggregating data. Furthermore, some outbreaks may not be clustered by place of residence,
        as in the case of an exposure occurring at the place of work or in a subway. Using the
        location of the hospital rather than residence may provide higher power to detect
        workplace-related outbreaks, but the only way to fully address this issue may be to conduct
        workplace surveillance.
        In spite of these limitations, we have presented a new method for the early detection of
        disease outbreaks and illustrated its practical use. The primary advantages of the method
        are that it is easy to use, it only requires case data, it automatically adjusts for
        naturally occurring purely spatial and purely temporal variation, it allows adjustment for
        space by day-of-week interaction, and it is capable of handling missing data.
        While the method was developed and applied in the context of syndromic surveillance, it
        may also be used for the early detection of diagnosed disease outbreaks, or for detecting
        changes in the pattern of chronic diseases, when population census information is
        unavailable, unreliable, or not available at the fine geographical resolution needed. The
        ability to perform disease surveillance without population-at-risk data is especially
        important in developing countries, where these data may be hard to obtain. The space–time
        permutation scan statistic could also be used for similar early detection problems in other
        fields, such as criminology, ecology, engineering, social sciences, and veterinary
        sciences.
      
    
  

  
    
      
        Preparing for Death
        How does one prepare for death? Those who have created a public persona must add to any
        spiritual ponderings about eternity the mundane chore of organizing their literary archives
        to protect any of life's secrets that seem worth the effort. That task involves choosing
        what diaries, letters, drafts, and laundry lists to donate to a university or to leave in a
        closet for legions of biographical ragpickers to quote, misquote, or variously interpret in
        as yet unimaginable contexts—or to burn.
        Many well-known figures contemplating their posthumous selves have been foiled in
        exercising control over their literary remains. Purposefully confounding future
        biographers, Sigmund Freud burned his early papers and admonished his wife Martha to
        destroy their love letters. Instead, she bequeathed us this charming insight into the
        youthful exuberance of the patriarch of psychoanalysis, written in 1884: “Woe to you, my
        Princess, when I come. I will kiss you quite red and feed you till you are plump. And if
        you are forward, you shall see who is stronger, a gentle little girl who doesn't eat enough
        or a big wild man who has cocaine in his body” [1].
        Anaïs Nin, whose voluminous diaries recorded her daily life in exquisite, compulsively
        recorded detail, had better luck in choreographing her literary afterlife. While alive, she
        published volumes of carefully edited literary diaries. When someone at a seminar remarked
        to her that her life seemed more, well, racy than those diaries revealed, she smiled
        mysteriously and said that after the death of all concerned, “unexpurgated” editions would
        be published. Several decades later, companion volumes to the literary diaries revealed
        passionate incest with her father, Joachim Nin, an affair with her analyst, Otto Rank, and
        successfully bigamous marriages in New York and California.
        When André Gide revealed that Oscar Wilde had had sexual relations with a young Arab boy
        in Egypt, Wilde's friend Robert Sherard lamented: “Heavens! The task of shooing hyenas away
        from the graves of the illustrious dead.” Sherard meant Wilde's literary grave—but what
        about actual graves? What about history's corpus delicti?
      
      
        The Line between Scientist and Grave Robber
        How many giants and tyrants unlucky enough to have left body parts or ashes behind when
        they shuffled off the mortal coil could have imagined what scientists and medical
        practitioners of the future would do with their physical remains? Here, the line between
        the scientist and the grave robber blurs, as corpses are exhumed and cremation urns raided
        to provide organic remnants for any number of curious purposes.
        Ethical debates about the appropriate care and maintenance of biological relics often
        begin at the autopsy table. Having removed Albert Einstein's brain, pathologist Thomas
        Harvey chopped it into 240 pieces and stored it in a cookie jar in his basement, often
        shipping slabs (mailed in mayonnaise jars) to brain researchers eager to count glia and
        neurons. Forty years later, Harvey lugged what remained of the brain cross-country to
        deliver it to Evelyn Einstein, a woman rumored to be the physicist's daughter from an
        affair with a New York dancer. Dr. Charles Boyd had tried to prove this paternity with his
        brain-chunk, but Einstein's DNA proved “too denatured to decipher.”
        Harvey's volunteer driver, Michael Paterniti, described getting his hands in the cookie
        jar: “I actually feel as if I might puke. The pieces are sealed in celloidin—the pinkish,
        liver-colored blobs of brain rimmed by gold wax. I pick some out of the plastic container
        and hand a few to Evelyn. They feel squishy, weigh about the same as very light beach
        stones. We hold them up like jewelers, marveling at how they seem less like a brain
        than—what?—some kind of snack food, some kind of energy chunk for genius triathletes”
        [2].
        Pilferers cannot resist snipping body parts. While Einstein was being autopsied, his
        ophthalmologist, Dr. Henry Abrams, dropped by and filched Einstein's brown eyes as a
        keepsake, storing them in a jar in a Philadelphia bank vault. There were rumors that singer
        Michael Jackson, a collector of body parts, offered Abrams several million dollars for the
        eyes.
        
          Does confidentiality extend beyond the grave?
        
        Beethoven's ears were hacked out and soon went missing. René Descartes's middle finger
        was stolen. (His head was also separated from his body for shipping—a philosopher's
        in-joke, since Descartes introduced the mind/ body split into Western philosophy.)
        Napoleon's reputed penis went on a picaresque odyssey of its own, being displayed at the
        Museum of French Art in New York, auctioned, and finally ending up in the possession of a
        urologist—or so the story goes. Josef Haydn's head was stolen by phrenologists at his
        burial.
        In 2004, Dr. Anunciada Colon presided over the opening of a golden trunk from the 16th
        century, containing ashes and bone fragments presumed to belong to her ancestor Christopher
        Columbus, an event chronicled by a television crew. Officials at the Seville Cathedral
        allowed researchers at the University of Granada to borrow the bones for a DNA study. Being
        unsuccessful at extracting DNA from pulverized fragments, Professor José A. Lorente loaded
        the bones in a shoulder bag and flew them to Dallas, Texas, where more sophisticated DNA
        tests (developed for the victims of the terrorist attack of 9/11) provided a
        disappointingly short and impure sequence of mitochondrial DNA. Remaining ashes and shards
        were inelegantly deposited on a metal storage shelf in a lab, in a Styrofoam picnic basket
        labeled “Colon” in black marker, awaiting better tests [3].
        Vladimir Ilyich Lenin remains the most visible deceased person. His body, or what
        remains of it since his brain and other organs were removed, has been viewed by the
        millions who have passed by his open casket in a mausoleum on Moscow's Red Square. A
        waterproof suit under his uniform holds in the embalming fluid. His hands and head are
        bathed frequently. His microtomed (31,000 sections) and dyed brain resides down the street
        from his body at the Moscow Brain Institute, joining the brains of his countrymen Stalin
        and Tchaikovsky. Many Russians who find Lenin's public resting place a macabre
        embarrassment think his soul will only rest (and theirs with it) once he goes underground.
        But who can decree his burial?
        When I was four, my mother found me exhuming a goldfish we had ceremoniously buried in
        the garden in a little fish coffin a few days before. How different, I wonder now, was my
        childish curiosity and wonderment at the mysterious process happening to my
        no-longer-swimming fish below the earth from that of grown-up exhumers? Consider Gira
        Fornaciari, who unearthed 49 members of the Medici family to confirm various causes of
        death, or the committee that had Beethoven and Schubert dug up to transfer them to more
        secure zinc coffins (borrowing both heads for a bit more measuring, and swiping Schubert's
        luxuriant, larvae-laden hair while they were at it). Archaeologists have braved curses and
        biohazards to retrieve mummies from pyramids. Doctors from Japan, however, were not allowed
        to take DNA from King Tut's mummy to sort out his genealogy; the Egyptian government's
        supreme council of antiquities, after first agreeing, reversed the decision. A non-invasive
        x-ray of the mummy suggests a murder plot: King Tut may have been done in by a blow to the
        back of the skull.
      
      
        Guidelines for Bioethical Research
        When a committee was convened to decide whether specimens of Lincoln's blood and bones
        should be tested for DNA to discover whether he suffered from Marfan syndrome, ethicists
        voted yes but scientists vetoed the plan, claiming that the precious material should not be
        destroyed in case future tests would prove more effective [4,5]. But what if they were even
        asking the wrong question? Lincoln once told his biographer and friend William Herndon that
        he had been infected with syphilis by a prostitute in Beardstown around 1835 [6]. What if a
        future test could prove that Lincoln had spoken the truth? Imagine, if you will, a press
        release from the Armed Forces Institute of Pathology revealing that hot potato about the
        most beloved of American presidents.
        The Lincoln testing question spurred bioethicist Lori Andrews and her colleagues at the
        Chicago Historical Society to join with the Illinois Institute of Technology to review
        existing ethical issues of biohistorical research. Their conclusion, after studying
        professional codes from 23 other organizations: none contained guidelines for conducting
        biohistorical research and analysis [7]. They recommend genetic testing for “historically
        significant” questions. But who is to define that loaded phrase?
        The newly dead are warm, soft, and somehow still human; by contrast, aged corpses and
        skeletons rising from the cold ground are the stuff of horror films, vampires and ghouls.
        While fascinating, they also unnerve. Medical examiners in fiction (Kay Scarpetta) and
        television (Dr. Quincy, Jordan Cavanaugh) capture wide audiences with their gruesome and
        graphic dissection of putrefied, maggot-ridden corpses, all in the service of solving some
        medical mystery.
      
      
        Respect for the Dead
        Does confidentiality extend beyond the grave? Should doctors publish articles in medical
        journals about diagnoses that were confidential when the patient was alive? Physicians have
        often raced to put pen to paper and reveal the signs and symptoms of their more illustrious
        deceased patients. According to Anne Sexton's biographer Diane Wood Middlebrook, who used
        tapes of hundreds of hours of therapy sessions given to her by Sexton's therapist Dr.
        Martin Orne, the dead have no rights [8]. Although Dr. Orne insisted that Sexton had given
        him permission to do what he thought appropriate with the tapes, his colleagues howled that
        he had made a travesty of doctor-patient confidentiality, Sexton's wishes be damned.
        The long-dead are latecomers to the game of lobbying for rights. Who owns their bones?
        Who is to choose the right test, the right time, the appropriate question to ask? Who gets
        to decide whether they should be sliced, diced, dyed, pulverized, displayed, x-rayed,
        photographed, and subjected to the esoteric tests developed for forensic laboratories to
        reveal secrets they carefully took to their graves or urns? An interdisciplinary committee?
        The law? The government? Should such decisions be made by bioethicists, scientists, medical
        examiners, lawyers, archaeologists, descendants of the deceased? Where does simple respect
        for the dead play into this issue?
        The answers change over time and from place to place. The quagmire of ethical, legal,
        moral, and even aesthetic questions that surround the use (and misuse) of leftover body
        parts can only become more complex and contentious, not less.
        A word of warning, then, to the famous not-yet-deceased: consider the disposition of
        your physical remains as carefully as you consider the packaging of your archive.
        Swear your doctor to posthumous secrecy.
        Be cremated.
        And have your ashes scattered to the wind.
      
    
  

  
    
      
        
        Epidemics of overt toxicity following widespread environmental contamination from
        commercial toxins heralded the discovery of children's enhanced vulnerability to lead,
        methyl mercury, polychlorinated biphenyls (PCBs), and tobacco [1,2,3,4,5] (Box 1). Over the
        past three decades, researchers have found that remarkably low-level exposures to these
        toxins are linked with less overt symptoms of toxicity—intellectual impairments, behavioral
        problems, spontaneous abortions, or preterm births [6,7,8,9,10,11,12,13,14,
        15,16,17,18,19,20,21,22,23,24,25,26,27, 28,29,30,31,32,33,34,35,36,37,38,39,40]. Moreover,
        there is emerging evidence that decrements in intellectual abilities and low birth weight
        linked with lead or tobacco are, for a given increment of exposure, greater at lower levels
        than those found at higher levels [10,41,42,43].
        The consequences of exposure to many other chemicals or mixtures of chemicals, such as
        insecticides—chemicals oftentimes specifically designed to be toxic—are largely unknown
        [33,34,35,44]. Many of these chemicals or their metabolites are routinely found in the
        blood and body fluids of pregnant women and children [45].
      
      
        Children's Vulnerability to Environmental Toxins
        The developing fetus and young child is particularly vulnerable to certain environmental
        toxins [46,47,48,49,50]. Critical neurodevelopmental processes occur in the human central
        nervous system during fetal development and in the first three years of life. These
        processes include cortical functional differentiation, synaptogenesis, myelination, and
        programmed apoptosis [46].
        Children's exposure to environmental toxins is insidious. Environmental toxins covertly
        enter a child's body transplacentally during fetal development or by direct ingestion of
        house dust, soil, and breastmilk and other dietary sources during early childhood
        [51,52,53,54,55,56]. Our ability to directly measure the actual levels of environmental
        chemicals in human tissues and body fluids using biologic markers (biomarkers) enables
        scientists to more effectively link exposures to environmental toxins with disability or
        disease [57].
        Despite our increased knowledge of the toxicity of environmental chemicals, testing for
        developmental neurotoxicity (DNT) and reproductive toxicity is rarely done. DNT testing
        uses animal experiments to provide information on the potential functional and morphologic
        toxicity to the fetal nervous system that results from the mother's exposure to toxins
        during pregnancy and lactation. Paradoxically, DNT testing of a chemical is seldom
        requested, and then typically requested only if there is pre-existing evidence that it is
        neurotoxic.
      
      
        The Prevalence of Diseases and Disabilities Linked to Environmental Toxins
        Based on parental reports, one in six United States children has one or more
        developmental disabilities, from a subtle learning disability to overt behavioral or
        emotional disorders [58]. Exposures to environmental toxins have been linked with higher
        rates of mental retardation, intellectual impairment, and behavioral problems, such as
        conduct disorder and attention deficit hyperactivity disorder [16,17,18,
        19,20,21,22,23,24,25,26,27,30,31,36,37, 38,39,40,41,42,43,59,60,61].
        One in ten US babies is born preterm and about 5% have low birth weight [62,63]. Preterm
        birth, defined as birth at less than 37 weeks of gestation, is a major determinant of
        infant mortality and morbidity throughout childhood [62,63,64]. Exposures to environmental
        toxins such as lead, tobacco smoke, and DDT have been linked with an increased risk for
        spontaneous abortion, low birth weight, or preterm birth [6,9,10,13,14,15,28,32,65,66]. The
        rate of occurrence for many of these diseases or disabilities has been rising, as has
        treatment for attention deficit hyperactivity disorder and depression in children
        [62,63,67,68,69,70].
        Multiple risk factors, including both genetic and environmental influences, interact in
        complex and often unknown ways to cause disease and disability in children. But efforts can
        be undertaken to prevent or reduce environmental exposures linked to disease without full
        elucidation of the underlying mechanism [71]. Thus, conducting some sort of test to
        identify pesticides and industrial chemicals that could cause reproductive or
        neurobehavioral toxicity before the chemical reaches widespread use is essential to protect
        pregnant women and children.
      
      
        Origin and Evolution of DNT Tests
        The process for testing potential developmental neurotoxins in laboratory animals
        evolved out of a series of tragic epidemics. Widespread use of the drug thalidomide during
        the 1950s led to an epidemic of phocomelia, an absence or deformity of limbs and other
        congenital defects in children exposed in utero to the drug [72]. Subsequently, in 1965,
        the Food and Drug Administration (FDA) developed the Teratology Guidelines. Because
        thalidomide induced gross defects in rabbits but not in rats, these guidelines called for
        toxicity tests in two species. Moreover, these guidelines focused on gross abnormalities;
        they did not require testing for behavioral or DNT.
        Following the outbreak of methyl mercury poisoning in Minamata Bay (Box 1), Japan and
        the United Kingdom added behavioral (DNT) guidelines to their teratology requirements in
        1974 and 1975, respectively [73]. In 1978, the Collaborative Behavioral Teratology Study
        (CBTS) was conceived to standardize and evaluate methods for DNT testing in the US [74].
        The final report was issued in 1985, and shortly thereafter, Dr. Donald Kennedy, who was
        then Commissioner of the FDA, supported the adoption of the CBTS recommendations. But the
        FDA failed to implement these recommendations after Kennedy's departure.
        
          Children's exposure to environmental toxins is insidious
        
        In 1990, the US Environmental Protection Agency (EPA) identified nine developmental
        neurobehavioral teratogens for both humans and animals (lead, PCBs, methyl mercury,
        cocaine, alcohol, phenytoin, heroin, methadone, and ionizing radiation) and developed rules
        for DNT testing in laboratory animals [49,50]. By 1991, the Developmental Neurotoxicity
        Test Guidelines (OPPTS 870.6300) had been established for use when submitting chemical data
        to the EPA [49]. In 1993, the National Research Council recommended that DNT data be
        included in the EPA's evaluations of pesticides, which include classes of chemicals
        specifically designed to be toxic [44].
      
      
        The Precarious US Framework for Protecting Children
        Despite numerous attempts to upgrade the regulatory system, such as the CBTS, the
        framework to protect children from environmental toxins is precarious. Under current
        regulations, manufacturers of commercial chemicals (excluding pesticides) are not required
        to supply any toxicity data before selling their products. Nor are pesticide manufacturers
        obligated to supply basic premarket toxicity and exposure data necessary to ensure that
        children will be protected from exposure and potential harm from use of those pesticides.
        Indeed, the vast majority of chemicals have not been tested for DNT. The most basic
        toxicity tests in animals are lacking for 75% of the 3,000 highest production volume
        chemicals—chemicals for which annual production exceeds 1 million pounds per year
        [49,75,76,77]. The US EPA has entered into an agreement with the American Chemistry
        Council, the chemical manufacturer's trade association, to provide basic toxicity screening
        tests for the high-production-volume chemicals by 2005
        (http://www.epa.gov/chemrtk/volchall.htm), but this is voluntary.
        For new pesticides intended for use on food crops—one of the areas in which regulations
        are most stringent—regulations require only that DNT testing be evaluated for substances
        already known or suspected of being toxins. Further, neurotoxicity testing need be
        conducted only in adult animals. The EPA acknowledges that over 140 registered pesticides
        are neurotoxic (i.e., specifically designed to act against pests by interfering with
        neurotransmitters or other processes shared by mammals and insects), but the EPA has
        received DNT testing using validated protocols for only nine pesticides [49,75,76,77].
        There is no general requirement that pesticides or other chemicals be tested for
        potential DNT prior to their registration and use [49]. For pesticides—which undergo more
        premarket testing than other chemicals—the EPA has relied on a tiered system of toxicity
        testing. The assumption underlying this system is that positive findings on earlier, more
        basic tests of neurotoxicity in adult animals will “trigger” the EPA to request more
        extensive testing by manufacturers, including tests in immature animals. Unfortunately,
        this tiered process has failed to result in appropriate DNT testing. In 1998, an internal
        EPA Toxicology Working Group concluded that these triggers may not be sufficient to
        identify all chemicals that have the potential to produce DNT [75]. Moreover, this tiered
        system discourages industry from conducting testing in immature animals because the
        findings could necessitate further costly testing and hinder a chemical from reaching the
        market.
      
      
        The European Framework: “REACH”
        In 2001, the European Commission affirmed that the European Union's legislative
        framework did not provide adequate information about the adverse effects of chemicals on
        human health, and that when hazards were identified the regulatory agencies were slow to
        assess risks and to introduce measures to reduce those risks [78]. Indeed, chemical
        manufacturers are not required to “prove” that a chemical is safe before marketing it. The
        European Commission proposed a new regulatory framework for chemicals, REACH (Registration,
        Evaluation, and Authorization of Chemicals) [78,79] (Figure 1).
        Under REACH, chemical manufacturers would have to assume a much greater burden for
        showing the lack of harm from use of their products. Specifically, REACH would require both
        European and non-European manufacturers doing business in Europe to submit more extensive
        toxicity data for about 30,000 chemicals on the market, including reproductive and DNT data
        for those chemicals produced in highest quantity. Chemicals found to be hazardous would be
        subject to an authorization procedure to show that they can be used safely or that there
        are no safer alternatives. This registration process would not guarantee that chemicals are
        safe, but it is a step in the right direction.
        The American Chemistry Council has objections to REACH, stating that “the proposed
        regulation is burdensome, costly, and impractical”
        (http://www.accnewsmedia.com/site/page.asp?TRACKID=&VID=1&CID=359&DID=1256).
        The pharmaceutical industry used similar objections to ward off regulations before the
        thalidomide epidemic ushered in requirements for pharmaceutical agents to undergo extensive
        premarket testing in clinical trials [80].
      
      
        Limitations of Existing Animal Tests for DNT
        The US EPA has been slower than the EU to adapt to the overwhelming evidence that
        low-level exposure to environmental toxins can be harmful. The EPA continues to rely
        heavily on data from animal (toxicity) testing conducted on only a single animal species
        and in adult animals. Furthermore, EPA guidelines for a general developmental toxicity
        screening test typically examine only crude toxicological endpoints such as death, body
        weight, or organ dysfunction. In contrast, the DNT includes tests of locomotor activity,
        acoustic startle, learning, and memory. But, as currently designed, the existing tests may
        miss important effects such as mood changes, impulsive behaviors, and attentional problems
        that in humans have been shown to result from exposures to environmental toxins
        [24,27,30,37,40]. While these effects might seem subtle, they can seriously interfere with
        a child's social and emotional well-being. It is also uncertain whether tests conducted
        under current EPA guidelines will detect subtle deficits in key human skills such as
        reading.
        There are other problems with relying principally on adult animals to signal the
        potential for DNT in humans. The structure and development of the cerebral cortex of
        animals commonly used in these studies differs markedly from that of humans. A chemical's
        effects on one type of animal may differ from its effects on other animals and on humans.
        In the case of thalidomide, high-dose fetal exposure had adverse morphologic effects on
        rabbits, but not rats; functional effects have only recently been described [81].
        Although there is some concordance of human and animal data for the adverse effects of
        lead, mercury, and PCBs, intake limits for these compounds established exclusively on the
        basis of rodent studies have not been sufficiently protective of human health compared with
        epidemiologic studies [47]. Indeed, there is compelling evidence from epidemiologic studies
        of widespread contaminants such as lead, tobacco, and PCBs that human studies are essential
        to ensure that children are not harmed by low levels of exposure
        [11,12,13,14,15,16,17,18,19,20,21,22, 23,24,25,26,27,28,29,30,31,32,33,34,35,
        36,37,38,39,40].
        From a scientific standpoint, data from epidemiologic studies represent the “gold
        standard” for detecting subtle effects of environmental toxins on humans. But
        epidemiological studies are expensive to mount, difficult to execute, and take years to
        complete. Using observational studies to disentangle the adverse consequences of a single
        toxin from other environmental influences and to promulgate regulations is a difficult and
        painfully slow process. There is also a financial disincentive for chemical registrants to
        voluntarily fund such studies because a positive epidemiological study could lead to
        stricter regulations. More importantly, if society continues to rely on epidemiologic
        studies to evaluate the toxicity of chemicals only after they are marketed, many children
        will first be harmed.
      
      
        Steps to Protect Children from Environmental Toxins
        Children must be better protected from both new and existing chemicals that are known or
        possible toxins [49]. To protect children from existing toxins, such as lead, mercury, and
        tobacco, the US EPA and FDA need more authority and resources to regulate and reduce
        emissions and exposures. Under our current system, efforts to enhance regulations to
        protect children from confirmed toxins are costly and protracted. Indeed, countless
        communities across the globe suffer from widespread environmental contamination. If there
        is any lesson from our experience with environmental toxins, it is that we need to identify
        environmental chemicals that are toxic before they are marketed or widely disseminated.
        For new commercial chemicals, toxicity testing in animals should be required before they
        are marketed. For all new chemicals, including pesticides, extensive premarket testing
        should be required in multiple animal species of both sexes and at different developmental
        stages. These tests should be designed to have adequate statistical power to detect subtle
        differences within the ranges of exposure that occur in human populations. If implemented,
        these testing requirements would represent a dramatic departure from existing regulations,
        while providing a powerful incentive for industry to develop less toxic chemicals.
        Toxicity testing in animals is essential but insufficient to protect pregnant women and
        children. For one thing, uncertainties about the safety of a chemical for humans will
        persist even after toxicity testing in animals is successfully completed. One additional
        safeguard that deserves further debate is whether prevalent environmental chemicals to
        which children could be exposed should undergo more extensive testing in human trials
        before they are marketed. If done, these trials should examine exposure, uptake (using
        biomarkers), and adverse effects among children or other populations only when the product
        is used as intended. For example, once animal toxicity testing of a residential pesticide
        is complete (including DNT and reproductive toxicity testing), a pesticide could undergo
        further testing in the home environment. Using an experimental group and a control group,
        researchers would compare levels of pesticides found in settled dust, on children's hands,
        and in their blood, urine, or hair. Children would be followed, when indicated, to ensure
        that an excess of neurobehavioral problems or other relevant outcomes did not develop among
        those whose homes were assigned to receive the pesticide application.
        If such trials were undertaken, families would need to be fully informed about the
        purpose, potential benefits, and risks of participating. The trials should be conducted by
        the federal government—or other independent entities that do not have any ties to the
        chemical industry—and funded by an industry fee or tax. Community representatives would
        need to be involved in the review and approval of such trials, and ethical standards would
        need to be established regarding, for example, the role of data safety and monitoring
        boards. Many families would undoubtedly find it objectionable and would choose not to
        participate. Indeed, some products might never undergo testing if they failed to offer
        meaningful benefits to families, in which case the product would either be taken off the
        market or never reach the market.
        This type of trial sounds extreme, but it is quite rational when compared to the
        existing approach of disseminating a potential toxin into children's environments without
        any human data about exposure, uptake, or toxicity. Furthermore, under our existing system,
        families are neither informed nor given an option to decline involvement in what ultimately
        are experiments exposing millions of pregnant women and children to potential toxins. Thus,
        we need to thoughtfully deliberate about whether these types of trials can be done in an
        ethical fashion. We also need to have further debate about whether it is ethical to
        continue to disseminate chemicals of unknown toxicity into children's environments or to
        allow children to continually be exposed to prevalent toxins, like lead, despite
        considerable evidence that they are toxic [82]. Too often, it is left up to a few
        investigators or community leaders to discover and quantify the adverse effects of toxins,
        and advocate efforts to reduce children's exposure.
      
      
        Conclusion
        In contrast with the EU's proposed REACH program, which would require industry to
        conduct more tests or analyses to demonstrate that high-production chemicals will not cause
        harm to fetuses or children, the Bush administration has argued—in unison with the American
        Chemistry Council—that such regulations would harm industry [83,84]. It is time to
        acknowledge that the existing requirements for toxicity testing and regulations are
        inadequate to safeguard pregnant women and children. Until a formal regulatory system is
        developed to effectively screen and identify new and existing chemicals that are toxic to
        pregnant women and children, we are left to await the next epidemic to warn us about an
        environmental disaster. Unfortunately, by then we will have once again fouled our nest
        [85].
      
    
  

  
    
      
        
        In Stockton, California, a city of 269,000 people nestled in California's largest
        agricultural valley, residents are reported to speak 100 different languages. Acculturation
        is difficult in the best of circumstances, but what happens when those people with limited
        or no proficiency in English have a medical problem? Many United States hospitals are
        required to provide some manner of interpreter services for people with limited English
        proficiency—but do those services also bridge the cultural divide?
        Meeting the challenge of providing health care for a multicultural population is now a
        major movement that is affecting health care in developed countries, principally the US but
        also in European countries and Australia. Although the bulk of studies and commentaries on
        the subject began to appear in the 1990s, the literature dates back much further, to
        articles written in the 1960s and 1970s by medical anthropologists, sociologists, nurses,
        mental health professionals, and others.
      
      
        Wake-Up Calls
        In the US, the first major alert on this problem came in 1985, when the 
        Report of the Secretary's Task Force on Black and Minority Health was
        issued [1]. (The “Secretary” was the head of the Department of Health and Human Services
        (DHHS).) The report painted a bleak picture of the quality of health care afforded to
        African-Americans and other racial and ethnic minorities.
        A decade later, reports from the US Institute of Medicine began to appear. Three of the
        ten reports, which spanned a ten-year period, dealt with the need to greatly diversify the
        health professions work force—still a somewhat unachieved goal. The most recent, considered
        a new wakeup call, was the 2003 report 
        Unequal Treatment: Confronting Racial and Ethnic Disparities in Health
        Care [2]. It minced few words in describing the problems faced by racial and ethnic
        minorities who sought health care: “The conditions in which many clinical encounters take
        place—characterized by high time pressure, cognitive complexity, and pressures for
        cost-containment—may enhance the likelihood that these processes will result in care poorly
        matched to minority patients' needs. Minorities may experience a range of other barriers to
        accessing care, even when insured at the same level as whites, including barriers of
        language, geography and cultural familiarity” (Figure 1).
        Soon afterward, another US government arm, the Agency for Healthcare Research and
        Quality of the DHHS, issued two other reports: the 
        National Healthcare Disparities Report [3] and the 
        National Healthcare Quality Report [4], with annual updates promised. The
        reports focused on seven clinical conditions, including cancer, diabetes, and mental
        health, and discussed the quality of care and differences in access to such care for
        special population groups, including minorities and the disabled.
        All of these reports make it clear that health care professionals and health systems
        need to change. In recent years, in order to improve their lives economically or avoid war
        and/or famine, many people have migrated from less to more developed areas of the world,
        changing the demographics of the US and a number of other societies. Evidence that they and
        nonmigrant minorities experience inequities in attaining quality health care is abundant
        [5].
        Studies also indicate that although genetics is involved in some health-related
        differences between racial and ethnic groups, such as in the incidence of certain diseases
        and responses to pharmaceuticals, it is probably not a major factor in explaining health
        disparities [6].
      
      
        The Era of Action
        A primary result of these reports on health disparities? A truly dizzying array of
        offices, centers, programs, and initiatives within the main DHHS as well as in some of its
        major branches such as the National Institutes of Health and the Centers for Disease
        Control and Prevention, all designed to improve health care for racial and ethnic
        minorities in one way or another. Some of these programs also fund grants to outside
        organizations, public and private, and coordinate with state offices of minority
        health.
        And there are more activities devoted to reducing health disparities: (1)
        university-level institutes, offices, and programs, such as those at the UMDNJ-Robert Wood
        Johnson Medical School and Georgetown University, (2) private foundations, such as the
        California Endowment, (3) agencies and programs within the various states, such as the very
        active Ohio Commission on Minority Health, and (4) combinations of groups, such as
        DiversityRx (www.diversityrx.org), an informational organization sponsored by the National
        Conference of State Legislatures, Resources for Cross-Cultural Health Care, and the Henry
        J. Kaiser Family Foundation.
        All these efforts might suggest that there are no problems left to be solved, but this
        is hardly the case. Providing quality health care to those who differ from a country's
        majority population in terms of language and culture (and often race) is a mammoth task
        that does not yield to easy or quick fixes, but rather to consistent and determined efforts
        at improvement.
      
      
        Cultural Competence
        The most common term used in this effort is “cultural competence,” essentially defined
        as a respectful knowledge of and attitude toward people from different cultures that
        enables health professionals who work with people from another culture to develop and use
        standard policies and practices that will increase the quality and outcome of their health
        care.
        With cultural competence as the centerpiece, social and behavioral scientists have
        started consulting companies to (1) train health care professionals working in private and
        public health care settings (hospitals, community clinics, managed health care plans) in
        cultural competence, and (2) propose as well as study the effects of such changes in these
        settings. Some hospitals and managed health care plans have developed their own programs;
        examples that stand out are the M.D. Anderson Hospital in Texas and Kaiser Permanente
        health plans.
        In 2000, the M.D. Anderson Hospital established an Office of Institutional Diversity,
        which emphasizes the use of employees with a variety of backgrounds and experiences to
        examine cancer and its impact on all kinds of people. Educational forums, employee network
        groups, and the use of evidence-based hypotheses to design and implement pilot
        interventions are all part of the effort to improve care of culturally diverse
        patients.
        Kaiser Permanente's Institute for Culturally Competent Care selects and coordinates
        Kaiser Permanente's several Centers of Excellence, which each serve specific populations.
        For example, a West Los Angeles center focuses on the diagnosis, treatment, and management
        of conditions prevalent among African-Americans, such as sickle cell disease and prostate
        cancer. The National Diversity Department emphasizes a diverse workforce and has published
        a number of providers' handbooks on culturally competent care for specific racial or
        cultural patient groups, such as Latino patients.
      
      
        Not to Be Left Out
        Pharmaceutical companies have also discovered multicultural medicine. Many that offer
        continuing medical education courses to help publicize their new drugs now also offer
        courses on diseases more prevalent in certain racial and ethnic groups than others (such as
        diabetes in the Hispanic/ Latino population). These courses include information on how to
        treat such groups with the company's drugs.
        Interestingly, in 2004 a clinical trial proved the effectiveness of the first drug
        specifically designed for the treatment of congestive heart failure in African-Americans
        [7]. The drug, a combination of fixed doses of isosorbide and hydralazine, may now be
        nearing the market. Despite the fact that the Association of Black Cardiologists was a
        cosponsor of the trial, the trial drew criticism on the basis that it allowed race to
        interfere with treatment decisions [6].
      
      
        A Global Issue
        The increased diversity of European populations, with the expected stress on entrenched
        health care systems and on the migrants themselves, has led to Migrant-Friendly Hospitals
        (http://www.mfh-eu.net), a “European initiative to promote health and health literacy of
        migrants and ethnic minorities” begun in October 2002.
        With funding from the European Commission and the Austrian Federal Ministry for
        Education, Science and Culture, a network of 12 pilot hospitals from European Union member
        states has been implementing and evaluating the effectiveness of three health care models
        for migrants and minorities. The models are: the improvement of interpreting in clinical
        communication, the creation and distribution of migrant-friendly information and training
        in mother and child care, and staff training in cultural competence. Results of the pilot
        experiences were reported at a final conference in December 2004 and will form the basis of
        European recommendations on migrant-friendliness as a quality criterion for hospital
        development and on the role of hospitals in promoting health and health literacy for
        migrants and ethnic minorities.
        One of the 12 pilot hospitals mentioned above is the Bradford Hospitals NHS Trust, long
        active among a number of other hospitals and health projects in the UK that strive to
        improve services for racial and ethnic minorities in their areas.
        Australia also has a multicultural society, and The Centre for Culture and Health of the
        University of New South Wales in Sydney has an active program aimed at increasing cultural
        competency, both among medical students at the University and in the country's medical
        community at large (http://cch.med.unsw.edu.au/). The Centre offers graduate certificates
        and diplomas in public health (culture and health), as well as a Masters in Public Health
        with a concentration in multicultural health, and a postgraduate research degree. It
        emphasizes the establishment of partnerships with Area Health Services around New South
        Wales, grassroots organizations, and governmental organizations. A number of research
        projects also are underway. There are, for example, intervention strategies designed to
        reduce risk for cardiovascular disease in various cultural groups, such as the Arabic and
        Farsi-speaking communities, and studies of cancer among Chinese families in Australia.
      
      
        Conclusion
        People's basic medical needs do not vary greatly; they can be accommodated with
        appropriate understanding, awareness, and education. In the end, medicine and health care
        can only be enhanced and informed by the broadening of cultural awareness.
      
    
  

  
    
      
        
        For disease outbreak detection, the public-health community has historically relied on
        the watchful eyes of doctors and other health-care workers, who have reported individual
        cases or clusters of cases of particular diseases to health-care and other authorities. The
        increased availability of electronic health-care data, however, raises the possibility of
        more automated and earlier outbreak detection and subsequent intervention. Besides
        diagnoses of known diseases, pre-diagnostic syndromic indicators—such as the primary
        complaints of patients coming to the emergency room or calling a nurse hotline—are being
        collected in electronic formats and could be analyzed if suitable methods existed. Martin
        Kulldorff and colleagues have been developing such methods, and now report a new and very
        flexible approach for prospective infectious disease outbreak surveillance.
        Their method, which they call the “space–time permutation scan statistic,” is an
        extension of a method called scan statistic. All previously developed scan statistics
        require either (i) a uniform population at risk (with the same number of expected disease
        cases in every square kilometer), (ii) a control group (such as emergency visits not due to
        the disease of interest), or (iii) other data that provide information about the
        geographical and temporal distribution of the underlying population at risk, such as census
        numbers. The new method, because of a different probability model, can be used for the
        early detection of disease outbreaks when only the number of cases is available. It also
        corrects for missing data and makes minimal assumptions about the spatiotemporal
        characteristics of an outbreak. To make it widely accessible, the method has been
        implemented as a feature of the freely available SaTScan software.
        In their article, Kulldorff and colleagues illustrate the utility of the new method by
        applying it to data collected from hospital emergency departments in New York City. The
        researchers analyzed diarrhea records from 2002, and did both a “residential analysis”
        (based on the home address of the patients) and a “hospital analysis” (based on hospital
        locations). The former has more detailed geographical information, the latter maybe be
        better able to detect outbreaks not primarily related to place of residence but, for
        example, school or workplace. They found four highly unusual clusters of diarrhea cases,
        three of which heralded citywide gastrointestinal outbreaks due to rotavirus and
        norovirus.
        Since November 2003, the space–time permutation scan statistic has been used daily to
        analyze emergency department data in New York City in parallel with other methods, and it
        seems to perform well. As the authors discuss, as any other surveillance method, theirs has
        limitations. Because it adjusts for purely temporal clusters, the method can only detect
        outbreaks if they start locally (not simultaneously across the entire surveillance area).
        The less geographically compact an outbreak is, the less power there is to detect it. And
        some outbreaks, for example, those caused by exposure to an infectious agent in the subway,
        will be hard to cluster by place of residence or choice of emergency department.
        In the present study, Kulldorff and colleagues have applied their method to infectious
        disease surveillance in a metropolitan area in the United States. As they state, however,
        “the ability to perform disease surveillance without population-at-risk data is especially
        important in developing countries, where these data may be hard to obtain.”
      
    
  

  
    
      
        Introduction
        In 1962, Norman Stoll, the distinguished Rockefeller Institute scientist who helped to
        establish human parasitology research in North America, described the unique health impact
        of hookworm as follows [1]: 
        
          As it was when I first saw it, so it is now, one of the most evil of infections. Not
          with dramatic pathology as are filariasis, or schistosomiasis, but with damage silent and
          insidious. Now that malaria is being pushed back hookworm remains the great infection of
          mankind. In my view it outranks all other worm infections of man combined…in its
          production, frequently unrealized, of human misery, debility, and inefficiency in the
          tropics.
        
        Like many other global disease experts who witnessed dramatic reductions in malaria
        prevalence as a result of DDT spraying during the late 1950s [2], Stoll did not anticipate
        malaria's imminent re-emergence in India. However, he articulated with eloquence the
        magnitude of the disease burden resulting from hookworm infection. He further offered the
        silent and insidious character of hookworm as a partial explanation for its neglect by the
        global medical community.
        This neglect subsequently intensified during the 1970s, 1980s, and 1990s with the
        omission of hookworm from the list of diseases covered by the World Health Organization's
        Special Programme for Research and Training in Tropical
        
          Hookworm has proven to be extremely difficult to eliminate or eradicate in areas of
          poverty and poor sanitation.
        
        Diseases, as well as from other global health initiatives. Over the last ten years,
        however, there has been increasing recognition of the global health importance of hookworm.
        Today, new international efforts to control the morbidity of hookworm and other
        soil-transmitted helminth infections are in progress (www.who.int/wormcontrol).
      
      
        Etiology and Global Distribution
        Human hookworm infection is caused by blood-feeding nematode parasites of the genus 
        Ancylostoma and the species 
        Necator americanus . Worldwide, 
        N. americanus is the predominant etiology of human hookworm
        infection, whereas 
        A. duodenale occurs in more scattered focal environments [3].
        These two hookworms, together with the roundworm, 
        Ascaris lumbricoides , and the whipworm, 
        Trichuris trichiura , are often referred to collectively as
        soil-transmitted helminths (STHs).
        No international surveillance mechanisms are in place to determine the prevalence and
        global distribution of hookworm infection. However, based on an extensive search of the
        literature since 1990, the worldwide number of cases of hookworm was recently estimated to
        be 740 million people [4]. The highest prevalence of hookworm occurs in sub-Saharan Africa
        and eastern Asia (Figure 1). High transmission (defined below) also occurs in other areas
        of rural poverty in the tropics, including southern China [5], the Indian subcontinent [6],
        and the Americas [7]. In all regions, there is a striking relationship between hookworm
        prevalence and low socioeconomic status (Figure 2) [4]. Hookworm's neglected status partly
        reflects its concentration among the world's poorest 2.7 billion people who live on less
        than $2 a day.
      
      
        Clinical Features, Epidemiology, and Disease Burden
        Hookworm infection is acquired by invasion of the infective larval stages through the
        skin (
        A. duodenale larvae are also orally infective). Following host
        entry, the larvae undergo a journey through the vasculature, then the lungs and other
        tissues, before they enter the gastrointestinal tract and molt twice to become
        one-centimeter-long adult male and female worms [3]. The worms mate and the female
        hookworms produce up to 30,000 eggs per day, which exit the host's body in the feces
        (Figure 3).
        Because hookworms do not replicate in humans, the morbidity of hookworm is highest among
        patients that harbor large numbers of adult parasites. Estimates of the intensity of
        hookworm infection are typically obtained by using quantitative fecal egg counts as a
        surrogate marker for worm burden. The World Health Organization defines moderate-intensity
        infections as those with 2,000–3,999 eggs per gram of feces, and heavy-intensity infections
        as those with 4,000 or more eggs per gram (p. 26 in [8]). Compared to other STH infections
        and schistosomiasis, hookworm infection exhibits a unique age-intensity profile—whereas the
        intensities for the former peak in childhood and adolescence, hookworm intensity usually
        either steadily rises in intensity with age or plateaus in adulthood [3,9]. The biological
        basis for this observation is unknown [10].
        Adult hookworms cause morbidity in the host by producing intestinal hemorrhage [3]. The
        adult hookworms then ingest the blood, rupture the erythrocytes, and degrade the hemoglobin
        [11]. Therefore, the disease attributed to hookworm is silent blood loss leading to iron
        deficiency anemia and protein malnutrition. There is a correlation between parasite
        intensity and host intestinal blood loss [12]; in children, women of reproductive age, and
        other populations with low iron stores, there is often a correlation between parasite
        intensity and reductions in host hemoglobin [3,12,13,14,15,16]. In children, chronic
        heavy-intensity infections are associated with growth retardation, as well as intellectual
        and cognitive impairments; in pregnant women, they are associated with adverse
        maternal–fetal outcomes [3,12,13,14,15,16].
        When measured in disability-adjusted life years, the global disease burden from hookworm
        exceeds all other major tropical infectious diseases with the exception of malaria,
        leishmaniasis, and lymphatic filariasis (pp. 192–193 in [17]). In addition, hookworm has
        been associated with impaired learning, increased absences from school, and decreased
        future economic productivity [18]. Therefore, like other neglected diseases, chronic
        infection with hookworm promotes long-term disability and increases the likelihood that an
        afflicted population will remain mired in poverty.
      
      
        Hookworm Control Strategies
        Because of its high transmission potential, hookworm has proven to be extremely
        difficult to eliminate or eradicate in areas of poverty and poor sanitation [19]. Indeed,
        in the absence of comprehensive economic development, the impact of sanitation, footwear,
        and health education has been minimal [19]. Control efforts have therefore shifted to
        reducing morbidity through mass treatment (also known as “deworming”) of affected
        populations with anthelminthic drugs [19].
        Although benzimidazoles (BZAs) are the most commonly used agents for treating STH
        infections, levamisole and pyrantel may also be used in some circumstances. Periodic and
        repeated deworming with BZAs and praziquantel, complemented by basic sanitation and
        adequate safe water, is considered the most cost-effective means to control the morbidity
        caused by STH and schistosome infections [19,20,21,22]. Efforts led by the World Health
        Organization have focused on annual, twice-yearly, or thrice-yearly doses in schools
        because the heaviest intensities of STH infections are most commonly encountered in
        school-age children [23].
        Among the health benefits of periodic deworming of schoolchildren are improvements in
        iron and hemoglobin status, physical growth and fitness, and cognition [20,21,22,23]. In
        addition, there are important externalities, including improvements in education and
        reduced community-based transmission of ascaris and trichuris infections [23]. Accordingly,
        at the 54th World Health Assembly in 2001, a resolution was passed urging member states to
        attain a minimum target of regular deworming of at least 75% and up to 100% of all at-risk
        school-age children by 2010 [20,23].
      
      
        Developing a New Control Tool: The Na-ASP-2 Hookworm Vaccine
        Deworming satisfies a number of United Nations Millennium Development Goals including
        those related to poverty reduction, child health, and education. However, there are also
        several reasons to believe that the school-based deworming programs could have less of an
        impact on the control of morbidity from hookworm than from other STH and schistosome
        infections [3]. As noted above, heavy-intensity hookworm infections are common among both
        adults and children, so school-based programs would not be expected to have an impact on
        hookworm transmission in the community [24]. School-based programs are also not likely to
        affect either preschool children or pregnant women, despite evidence for the health
        benefits from BZAs in both populations [16,25]. Finally, a single dose of mebendazole (one
        of the two major BZAs) has variable efficacy against hookworm [26], and following
        treatment, hookworm reinfection to pre-treatment levels can occur within 4–12 months [27].
        This, and the observation that the efficacy of mebendazole against hookworm can diminish
        with frequent and repeated use, has prompted concerns about the possible emergence of BZA
        resistance [28].
        As a complementary strategy, the Human Hookworm Vaccine Initiative (HHVI) is developing
        a safe, efficacious, and cost-effective vaccine, the 
        Na -ASP-2 Hookworm Vaccine, that would provide an additional tool for the
        control of hookworm [29,30]. The HHVI is a non-profit partnership comprising research,
        process development, vaccine manufacturing and control, and pre-clinical and clinical
        testing units at the George Washington University, London School of Hygiene and Tropical
        Medicine, and Oswaldo Cruz Foundation (FIOCRUZ), and sponsored by the Sabin Vaccine
        Institute (www.sabin.org).
        The HHVI selected the hookworm larval antigen ASP-2 (ancylostoma secreted protein-2)
        based on studies that (1) identified the molecule as a protective antigen linked to
        earlier-generation irradiated infective larval vaccines [29], (2) determined a relationship
        between human anti-ASP-2 antibodies and reduced risk of heavy hookworm infection in
        populations living in hookworm-endemic regions of Brazil and China ([30]; J. Bethony, A.
        Loukas, M. J. Smout, S. Brooker, S. Mendez, et al., unpublished data), and (3) confirmed
        the ability of recombinant ASP-2 to partially protect laboratory animals against larval
        hookworm challenges [30,31,32].
        Process development, cGMP manufacture and control, and pre-clinical testing of 
        Na -ASP-2 from 
        N. americanus were completed in 2004 (Figure 4). Pending United
        States Food and Drug Administration approval, clinical testing of the vaccine will take
        place in 2005. The 
        Na -ASP-2 Hookworm Vaccine will be developed almost entirely in the
        non-profit sector. Ultimately, the vaccine will be indicated for the active immunization of
        susceptible individuals against moderate and heavy necator infection. Vaccination would
        reduce the number of hookworm infective larvae entering the gastrointestinal tract, thereby
        reducing the number of adult worms and the fecal egg counts in individuals exposed to the
        larvae.
      
      
        Hookworm as a Model
        Because immunization would only affect hookworm larvae and not adult hookworms already
        residing in the gastrointestinal tract of infected individuals, the first dose of the
        vaccine would be administered following deworming. Therefore, use of the vaccine could
        build on the infrastructures developed as part of school-based programs. Given that
        hookworm afflicts only the world's most impoverished, a major hurdle for the development of
        the 
        Na -ASP-2 Hookworm Vaccine is its small commercial market. Innovative
        financing mechanisms must be considered to produce this orphan biologic. Towards that end,
        the HHVI has partnered with manufacturers in hookworm-endemic middle-income countries that
        would commit to industrial scale-up of the 
        Na -ASP-2 Hookworm Vaccine pending proof-of-principle for its efficacy.
        This approach might help to inform the development of business models for the production
        and distribution of orphan biologics for other neglected diseases.
      
    
  

  
    
      
        Health Benefits of Physical Activity
        Regular physical activity has been recognized to confer health benefits since antiquity
        [1]. However, for most of humankind, voluntary discretion over whether or not to exercise
        is a recent phenomenon limited to advanced industrialized societies.
        A large body of epidemiological literature consistently documents greater longevity in
        persons who are physically active on a near-daily basis, and reveals inverse relationships
        between levels of daily exercise and incidence of major chronic disorders such as obesity
        [2], hypertension [3], diabetes [4], ischemic heart disease, and all causes of mortality
        [5,6,7,8,9,10,11,12]. From a public health perspective, there is little question that even
        modest increases in daily activities such as walking or stair climbing would have important
        positive consequences in reducing the burden of illness.
        However, knowledge of the likely health benefits accruing to the physically active so
        far has not been a sufficient stimulus to promote sustained changes in behavior for most of
        the American population. If education and public policies are insufficient to promote
        behavioral changes to increase physical activity among most people, can advances in
        biotechnology confer such benefits to individuals unable or unwilling to perform the
        necessary physical effort?
      
      
        Translating Knowledge of Exercise Biology to Novel Therapeutics
        Greater knowledge of how cells and tissues are modified in response to recurring bouts
        of exercise provides a basis for more precise recommendations as to the mode, intensity,
        and amount of exercise required to produce specific health benefits (e.g., treatment of
        dyslipidemia [13], control of body weight [14], or prevention of diabetes [15]). In
        addition, an understanding of the molecular signaling events that drive the beneficial
        effects of exercise on human physiology could foster the development of novel drugs,
        devices, or biological agents designed to substitute for exercise.
        Many individuals who otherwise would develop diabetes or cardiovascular disease would
        benefit if advances in exercise biology revealed novel measures to promote the favorable
        effects on insulin sensitivity, lipoprotein metabolism, and blood pressure that are known
        to accrue through regular physical activity.
      
      
        Physiological Properties of Skeletal Muscle
        What do we know about basic muscle and exercise biology? The cells that constitute our
        skeletal muscles are called myofibers—large multinucleated cells that may extend for the
        full length of individual muscles. There are different types of myofibers, which vary in
        size and with respect to metabolic and contractile capability [16] (Figure 1). Skeletal
        myofibers are innervated by motor neurons that contact each myofiber, and the intensity,
        duration, and timing of each muscle contraction are determined by the pattern of motor
        neuron firing. A pattern of occasional intense contractions separated by longer periods of
        rest is called “phasic,” while a pattern characterized by brief contractions occurring
        repeatedly over an extended period is called “tonic.” Endurance training regimens like
        running or cycling employ tonic patterns of contractile work, and it is this form of
        habitual activity that serves best to reduce risk for obesity, diabetes, hypertension, and
        heart disease.
      
      
        Dynamics of Muscle Mass
        Maintenance of normal muscle mass requires some minimal level of ongoing work activity,
        and building and maintaining muscle mass is most effectively done through phasic
        contractions. A slow but inexorable loss of muscle mass is a feature of advancing age in
        human populations [17]. Loss of muscle mass and strength is an important determinant of
        injury and disability in the elderly, but even rigorous weight training programs cannot
        completely counteract this age-related decline that becomes particularly troublesome in the
        eighth and ninth decades of life. Efforts to develop effective countermeasures to maintain
        muscle mass in the elderly constitute an active and important area of current research
        [18,19,20].
        Although the molecular signaling mechanisms that transduce the effects of phasic
        patterns of work activity to modify muscle mass are incompletely understood, recent
        evidence implicates pathways that include the signaling molecules PI3 kinase, Akt, mTOR,
        S6K, and ERK, the ubiquitin ligases MAFbx and MuRF1, and transcription factors of the FOX
        superfamily in the control of both catabolic and anabolic processes [21,22,23,24].
      
      
        Contractile and Metabolic Properties
        With respect to variations in contractile and metabolic properties, myofibers are
        classified on a spectrum between two extremes on the basis of contractile (fast versus
        slow) and metabolic (glycolytic versus oxidative) properties. At one extreme, the fastest
        glycolytic fibers have high levels of enzymes that generate ATP via glycolysis but few
        mitochondria (approximately 1% of cell volume). At the other end of the spectrum, slow
        oxidative fibers generate force with slower kinetics but are capable of long periods of
        repeated contraction without fatigue. They are rich in mitochondria (3%–10% of cell
        volume). Other myofibers, called fast oxidative, are both relatively fast and resistant to
        fatigue, and are rich in mitochondria (like the slow oxidative fibers). Muscles composed
        primarily of fast glycolytic fibers are needed for rapid movements (e.g., escape from
        predators) but fatigue when sustained periods of activity are required (e.g.,
        migration).
        Most human muscles exhibit a mosaic pattern of different fiber types (Figure 1), with a
        great deal of variation among individuals, which is influenced at least in part by patterns
        of use. When we exercise daily, or at least several times weekly, we deliver a stimulus to
        the specific muscle groups involved in these activities that is sufficient to alter
        specialized properties of myofibers within these muscles. While habitual physical activity
        promotes a great variety of physiological adaptations that alter vascular reactivity,
        cardiac function, adipocyte function, and neurophysiology, adaptive responses of skeletal
        myofibers confer at least some of the health benefits.
        Patterning of skeletal muscle fiber composition is initially determined during embryonic
        development, but can be partially or completely overturned by stimuli applied to fully
        mature adult myofibers: by hormonal influences (e.g., thyroid hormone), but most
        importantly by different patterns of motor nerve activity and contractile work. Myofibers
        that experience phasic patterns of contractile work—brief bursts of activity interspersed
        within long periods of inactivity—will assume the fast glycolytic phenotype. Myofibers
        subjected to tonic patterns of work activity—sustained periods of repetitive contraction on
        a habitual basis—will take on fast oxidative or slow oxidative properties. Under
        experimental conditions in laboratory animals, it is possible to transform muscles
        completely from one myofiber phenotype to another in a reversible manner, solely by
        altering the pattern of neural stimulation. We know that having a high proportion of
        oxidative muscle fibers conveys health benefits, and the possibility to control fiber
        composition through therapeutic intervention is promising.
      
      
        Molecular Signaling Pathways
        At a cellular and molecular level, how does a fast glycolytic myofiber sense a tonic
        pattern of contractile activity and transduce that information to transform itself into a
        cell with fast oxidative or slow oxidative properties? We know that such signals must be
        transduced to the nucleus, activating certain genes and suppressing others, for myofiber
        plasticity to occur. We know the identities of some of the nuclear transcription factors
        that carry these signals, and of other proteins that regulate the function of these
        transcription factors (Figure 2).
        Quite a variety of intracellular messengers have been proposed to provide the proximate
        signals in exercising muscles to stimulate activity-dependent gene regulation. This
        discussion will focus on a signaling cascade mediated by calcineurin, a calcium-regulated
        protein phosphatase that signals to the nucleus via transcription factors of the nuclear
        factor of activated T cells (NFAT) family. Upon receipt of the appropriate calcium signal,
        calcineurin is activated and removes phosphate groups from NFAT, thereby permitting
        translocation of NFAT to the nucleus. Within the nucleus, NFAT binds DNA and activates
        transcription (in concert with other transcription factors) of relevant downstream target
        genes that encode proteins necessary for fast oxidative or slow oxidative myofiber
        phenotypes.
        Calcineurin and NFAT proteins are abundant in skeletal myofibers, and several lines of
        evidence support the viewpoint that the calcineurin–NFAT pathway plays a role in mediating
        activity-dependent gene regulation in muscle
        [25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]. For example, in mice genetically engineered
        to distinguish the inactive (cytoplasmic) from active (nuclear) forms of NFAT by means of a
        sensor, it is evident that NFAT is inactive in resting muscles, but activated by tonic
        patterns of muscle contraction (running or electrical stimulation of the motor nerve) [40].
        Using other genetic manipulations in mice to produce in muscle a form of calcineurin that
        remains active even in the absence of calcium signals, myofibers are converted from fast
        glycolytic to fast oxidative or slow oxidative forms [41]. And in muscles of mice
        genetically engineered to lack calcineurin, fiber type switching is impaired [42].
      
      
        Cellular Memory
        Muscle contractions are initiated under the influence of the motor nerve by release of
        calcium from the sarcoplasmic reticulum, which triggers actin–myosin crossbridge cycling
        (Figure 3). Calcium released via ryanodine receptors is completely sufficient to activate
        muscle contractions, and the effects are immediate (within milliseconds). It is also
        sufficient to initiate calcineurin–NFAT signaling to the nucleus, but cannot by itself
        sustain the signal in a manner necessary to promote myofiber remodeling [40]. Changes in
        gene expression evoked by neuromuscular activity are not immediate but require that the
        stimulus be sustained for an extended period (minutes to hours). Moreover, tonic
        stimulation of the motor nerve must be repeated daily, or nearly so, over several weeks for
        the changes in myofiber properties to become fully manifest. We have characterized this
        requirement for repetition of the activity stimulus over days as a form of “cellular
        memory.” The effects of the tenth or 20th day of exercise are not the same as the effects
        of the first day. The myofiber somehow “remembers” not only the pattern of activity it has
        experienced today, but what has gone on over the preceding days or weeks, such that the
        changes in abundance of proteins that control contractile function and metabolism accrue
        over time.
        To explain this cellular memory, we propose that, as the bursts of contractile activity
        are sustained over time (through a tonic pattern of neural stimulation), a second source of
        calcium is mobilized from outside of the cell and enters via a class of calcium channels
        that are called “store-operated” or “non-voltage-dependent.” This second source of calcium
        is not required for muscle contractions, but is required to sustain calcium-dependent
        signaling to the nucleus. Phasic patterns of contractile activity do not promote calcium
        entry via store-operated channels. Tonic patterns of activity, in contrast, would not only
        promote the mobilization of extracellular calcium but also increase the number of
        store-operated calcium channels with each bout of exercise. Myofibers would thereby grow
        progressively more responsive to tonic activity. Consistent with this model, we know that
        daily running increases the expression of a putative store-operated calcium channel called
        TRPC3. Moreover, increasing the abundance of TRPC3 in cultured myotubes prolongs the period
        in which intracellular calcium is elevated following a depolarizing stimulus, sustains the
        transcription factor NFAT within the nucleus, and augments expression of NFAT-dependent
        target genes [40].
        A great deal of additional research remains to be done before we have a comprehensive
        understanding of how habitual physical activity promotes changes in gene expression in
        skeletal muscles, and in turn improves fitness and reduces risk for diabetes, hypertension,
        dyslipidemia, and coronary artery disease. However, studies of the relationships between
        the proteins of calcium metabolism and calcium-regulated signaling pathways—as described
        here in a simplified manner with respect to TRPC3, calcineurin, and NFAT proteins—are
        illustrative of progress in this field. Other notable findings point to additional
        signaling proteins (CAMK, p38MAPK, and AMPK) and transcription factors (PGC-1, MEF2, ATF2,
        PPARs) active in pathways that intersect with calcineurin–NFAT signaling
        [31,43,44,45,46,47,48] (see Figure 2). It is encouraging that some of these proteins are
        attractive targets for drug discovery.
      
      
        Summary and Conclusions
        Long the province of physiologists who have contributed valuable insights in past
        decades, exercise science more recently has attracted the attention of molecular
        biologists, who have recognized the biological interest and medical importance of this
        field. Biotechnology and pharmaceutical companies also are beginning to take interest.
        This review has focused on adaptive responses of skeletal muscle to changing patterns of
        physical activity, and on the role of the calcium–calcineurin–NFAT signaling cascade in
        controlling gene expression in skeletal myofibers. Further advances in our understanding of
        signaling mechanisms that govern activity-dependent gene regulation in skeletal muscle
        could lead to drugs, gene therapy, or devices that can, at least in part, substitute for
        daily exercise. Although it is unlikely that such technologies would fully recapitulate
        exercise-induced adaptations that affect other tissues of the body, beneficial effects on
        work performance and whole-body metabolism have been demonstrated using gene transfer
        techniques to alter skeletal muscles in animal models. If it proves possible to drive
        similar effects in skeletal muscles in humans, the interventions capable of providing such
        effects would almost certainly find broad clinical application.
      
    
  

  
    
      
        
        Mental health is perhaps the most neglected area of health policy and programming.
        According to the 2001 World Health Report, “some 450 million people suffer from a mental or
        behavioral disorder, yet only a small minority of them receive even the most basic
        treatment” [1]. More than 40% of countries have no mental health policy and over 30% have
        no mental health program. Over 90% of countries have no mental health policy that includes
        children and adolescents [1]. According to the World Health Organization (WHO), mental and
        behavioral disorders are estimated to account for 12% of the global burden of disease, yet
        the mental health budgets of the majority of countries constitute less than 1% of their
        total health expenditures [1]. The relationship between disease burden and disease spending
        is clearly disproportionate.
        Those few who do receive services often fare just as badly. Mental Disability Rights
        International (MDRI; Washington, D.C., United States), a human rights group dedicated to
        the promotion of rights of the mentally disabled, has documented how, in many countries,
        severely mentally disabled individuals become targets of stigma, discrimination, and other
        human rights abuses. Routinely, children and adults with mental disabilities are
        arbitrarily detained in psychiatric facilities, social care homes, orphanages and other
        closed institutions. Out of public view, they are subject to the most extreme forms of
        inhuman and degrading treatment experienced by any population (Figure 1). In Kosovo, MDRI
        learned that women were raped in psychiatric facilities in plain view of local staff and
        international humanitarian relief workers [2]. In Hungary and Paraguay, MDRI found people
        locked in cages [3,4]. In Turkey, Peru, and Bulgaria, MDRI investigators learned of a
        practice called “unmodified ECT”—the use of without any form of anaesthesia or muscle
        relaxants—a practice that is both painful and dangerous [5,6].
      
      
        Defining a Human Rights Approach to Mental Health Policy
        The starting point for the development of a human-rights based policy on mental health
        is that mentally ill individuals are full human beings who are entitled to rights. Although
        seemingly obvious, in practice MDRI has found that the implications of these premises
        challenge predominant biomedical approaches to mental illness, as well as health services
        paradigms [7]. In a rights framework, “mental health needs” are not analyzed (as they are
        in many studies) in terms of the application of given diagnostic criteria in isolation from
        the social context that leads to use of the mental health sector, and mentally disabled
        persons are treated as more than patients who need services [7,8]. They have rights to
        exercise agency in their own lives and to participate as members of their communities and
        societies, and these rights trump other concerns such as general attitudes toward risk
        containment in society [9].
        Thus, a human rights framework calls for changes that go beyond quality of care to
        include both legal and services reforms. Further, a human rights approach demands that we
        develop policies and take actions to end discrimination in the overall society that has a
        direct effect on the health and well-being of the mentally disabled.
      
      
        Legal Reform and Accountability
        Suspicion of mental illness cannot mean untrammeled discretion to disregard due process
        concerns in detention. Whether or not ideological factors are at play, civil commitment
        laws must provide for minimum substantive and procedural protections that protect mentally
        ill individuals' fundamental agency [10]. This is often not the case. For example, MDRI
        found that civil commitment laws in Uruguay and Mexico allow commitment upon medical
        certification of “mental illness,” which MDRI found, in many cases, to be questionable.
        There are no requirements that a patient be dangerous or in need of psychiatric treatment.
        These laws do not require a right to counsel or a periodic review of commitment, as
        international law requires [11,12]. Thus, people who are found “mentally ill” can be
        deprived of their liberty indefinitely in these—and many other—countries.
        Once mental health is construed in terms of human rights, all states are required, at a
        very minimum, to establish a normative framework consistent with international law [13].
        Such a normative framework provides for procedural protections; it also provides for human
        rights oversight and remedies in the event of abuses. Mentally disabled persons have the
        same right to redress for violations of their fundamental rights as other people do
        [13,14,15]. Recourse may imply judicial remedies but in some cases a human rights ombudsman
        can be equally effective [16]. As mentally disabled individuals are often not in a position
        to avail themselves of remedies, proactive monitoring and enforcement is also necessary. In
        short, instituting legal reform, accountability procedures, and effective mechanisms to
        provide human rights oversight becomes a cornerstone of a human rights-based approach to
        mental health policy.
      
      
        Services Reform: Community Integration and Participation
        Treating mentally ill people as full human beings implies that they have rights to
        participate in their communities and societies. This, in turn, calls for community
        integration and service system reform instead of programs that merely rebuild segregated
        institutions. The WHO also recognizes that it is important to provide treatment in the
        community, but its reasoning is largely utilitarian. The WHO argues that “community care
        has a better effect than institutional treatment on the outcome and quality of life of
        individuals with chronic mental disorders. Shifting patients from mental hospitals to care
        in the community is also cost-effective” [1]. From a human rights perspective, people are
        entitled to live in and receive care in the community not because it is more efficient, but
        because all human beings develop their identities within social contexts, and have rights
        to work and study, as well as be with family and friends.
        
          People with mental disabilities are often denied the right to work outside the home,
          to marry or have children
        
        A rights-based approach calls not only for the location of care in the community, but
        also for the transfer of planning and decision-making power to the individuals and
        communities that the health system is supposed to serve. In this case, consumers and family
        members must be integrally involved in the policy-making and programming decisions
        [13,17,18,19].
        MDRI has repeatedly found that funds are misdirected toward rebuilding psychiatric
        institutions and orphanages. Further, international institutions often undermine
        rights-based approaches to policy. MDRI has documented how European governments,
        development banks and international humanitarian relief organizations fund projects to
        build new psychiatric institutions and orphanages throughout the Americas and Eastern
        Europe, rather than focusing on community care [9].
      
      
        Non-Discrimination: Within and beyond the Health Sector
        Nondiscrimination is the most fundamental tenet in human rights. Under international
        law, discrimination need not be intentional nor de jure (in law) to constitute a violation
        of various relevant treaties, but merely needs to have the “effect of nullifying or
        impairing the equal enjoyment or exercise” of rights (paragraph 11 of [20]).
        MDRI has found that discrimination against people with mental disabilities is pervasive,
        and takes many forms. In Peru, for example, the public health insurance scheme does not
        cover mental disorders [5]. However, it is critical to recognize that discrimination
        outside the health sector also affects well-being. MDRI has seen that children with
        intellectual disabilities are denied equal access to education in Peru. They are placed in
        programs that in effect warehouse them and assume that they are unable to learn [5]. As
        adults, people with mental disabilities are often denied the right to work outside the
        home, to marry or have children, or to take part in the religious and social activities
        that define people as adult members of society [21].
        A human rights approach to mental health policy demands that special attention be placed
        on remedying such inequities—both within and beyond the health sector—which affect the
        physical, mental, and social well-being of persons with mental disabilities [22]. Such an
        approach depends upon multisectoral strategies including education, housing, and work, and
        establish that people with mental disabilities are full citizens.
      
      
        Conclusions and Reflections on First Steps
        MDRI works through a human rights framework that links the improvement of mental health
        services with broader questions of social justice and nondiscrimination relating to the
        full spectrum of rights set out in international instruments [13,14]. As an immediate first
        step, all states—regardless of resources—can develop national mental health policies and
        plans of action with measurable targets, which provide for open public discussion [13,23].
        Devising a national policy is a precondition to creating rights-based programs that address
        the multivalent problems faced by persons with mental disability [1]. Stakeholder
        participation in the process affirms that mentally disabled people are rights-worthy
        [13,17].
        Both international agencies and professional associations can play critical roles in
        providing technical assistance to countries to develop rights-based national mental health
        policies [1,4]. Bilateral and multilateral donors should encourage such rights-based
        policies through their funding prerogatives.
      
    
  

  
    
      
        Introduction
        Somatic gain-of-function mutations in exons encoding the epidermal growth factor
        receptor (EGFR) tyrosine kinase domain are found in about 10% of non-small cell lung
        cancers (NSCLCs) from the United States [1,2,3], with higher percentages observed in east
        Asia [2,4,5,6]. Some 90% of NSCLC-associated mutations occur as either multi-nucleotide
        in-frame deletions in exon 19, involving elimination of four amino acids, Leu-Arg-Glu-Ala,
        or as a single nucleotide substitution at nucleotide 2573 (T→G) in exon 21, resulting in
        substitution of arginine for leucine at position 858 (L858R). Both of these mutations are
        associated with sensitivity to the small-molecule kinase inhibitors gefitinib or erlotinib
        [1,2,3]. Unfortunately, nearly all patients who experience marked improvement on these
        drugs eventually develop progression of disease. While 
        KRAS mutations have been associated with some cases of primary resistance
        to gefitinib or erlotinib [7], mechanisms underlying “acquired” or “secondary” resistance
        are unknown.
        Acquired resistance to kinase-targeted anticancer therapy has been most extensively
        studied with imatinib, an inhibitor of the aberrant BCR-ABL kinase, in chronic myelogenous
        leukemia (CML). Mutations in the ABL kinase domain are found in 50%–90% of patients with
        secondary resistance to the drug (reviewed in [8]). Such mutations, which cluster in four
        distinct regions of the ABL kinase domain (the ATP binding loop, T315, M351, and the
        activation loop), interfere with binding of imatinib to ABL [9,10,11]. Crystallographic
        studies of various ABL mutants predict that most should remain sensitive to inhibitors that
        bind ABL with less stringent structural requirements. Using this insight, new
        small-molecule inhibitors have been identified that retain activity against the majority of
        imatinib-resistant BCR-ABL mutants [12,13].
        Although imatinib inhibits different kinases in various diseases (BCR-ABL in CML, KIT or
        PDGFR-alpha in gastrointestinal stromal tumors [GISTs], and PDGFR-alpha in
        hypereosinophilic syndrome [HES]) (reviewed in [14]), some tumors that become refractory to
        treatment with imatinib appear to have analogous secondary mutations in the kinase-coding
        domain of the genes encoding these three enzymes. For example, in CML, a commonly found
        mutation is a C→T single nucleotide change that replaces threonine with isoleucine at
        position 315 (T315I) in the ABL kinase domain [9,10,11]. In GIST and HES, respectively, the
        analogous T670I mutation in KIT and T674I mutation in PDGFR-alpha have been associated with
        acquired resistance to this drug [15,16].
        To determine whether lung cancers that acquire clinical resistance to either gefitinib
        or erlotinib display additional mutations in the EGFR kinase domain, we have examined the
        status of 
        EGFR exons 18 to 24 in tumors from five patients who initially responded
        but subsequently progressed while on these drugs. These exons were also assessed in tumor
        cells from a sixth patient whose disease rapidly recurred while on gefitinib therapy after
        complete gross tumor resection. Because of the association of 
        KRAS mutations with primary resistance to gefitinib and erlotinib [7], we
        also examined the status of 
        KRAS in tumor cells from these six patients. In an effort to explain the
        selective advantage of cells with a newly identified “resistance” mutation in 
        EGFR— a T790M amino acid substitution—we further characterized the drug
        sensitivity of putatively resistant EGFR mutants versus wild-type or drug-sensitive EGFR
        mutants, using both a NSCLC cell line fortuitously found to contain the T790M mutation and
        lysates from cells transiently transfected with wild-type and mutant 
        EGFR cDNAs.
      
      
        Methods
        
          Tissue Procurement
          Tumor specimens, including paraffin blocks, fine needle biopsies, and pleural
          effusions, were obtained through protocols approved by the Institutional Review Board of
          Memorial Sloan-Kettering Cancer Center (protocol 92–055 [7] and protocol 04–103 [Protocol
          S1]). All patients provided informed consent.
        
        
          Mutational Analyses of EGFR and KRAS in Lung Tumors
          Genomic DNA was extracted from tumor specimens, and primers for 
          EGFR (exons 18–24) and 
          KRAS2 (exon 2) analyses were as published [3,7]. All sequencing
          reactions were performed in both forward and reverse directions, and all mutations were
          confirmed at least twice from independent PCR isolates.
          A specific exon 20 mutation (T790M) was also detected by length analysis of
          fluorescently labeled (FAM) PCR products on a capillary electrophoresis device (ABI 3100
          Avant, Applied Biosystems, Foster City, California, United States), based on a new NlaIII
          restriction site created by the T790M mutation (2369 C→T), using the following primers:
          EGFR Ex20F, 5′-FAM- CTCCCTCCAGGAAGCCTACGTGAT-3′ and EGFR Ex20R 5′-
          TTTGCGATCTGCACACACCA-3′. Using serially mixed dilutions of DNA from NSCLC cell lines
          (H1975, L858R- and T790M-positive; H-2030, 
          EGFR wild-type) for calibration, this assay detects the presence of the
          T790M mutation when H1975 DNA comprises 3% or more of the total DNA tested, compared to a
          sensitivity of 6% for direct sequencing (data not shown).
        
        
          RT-PCR
          The following primers were used to generate 
          EGFR cDNA fragments spanning exon 20: EGFR 2095F 5′-
          CCCAACCAAGCTCTCTTGAG-3′ and EGFR 2943R 5′- ATGACAAGGTAGCGCTGGGGG-3′. PCR products were
          ligated into plasmids using the TOPO TA-cloning kit (Invitrogen, Carlsbad, California,
          United States), as per manufacturer's instructions. Minipreps of DNA from individual
          clones were sequenced using the T7 priming site of the cloning vector.
        
        
          Functional Analyses of Mutant EGFRs
          Two numbering systems are used for EGFR. The first denotes the initiating methionine
          in the signal sequence as amino acid −24. The second, used here, denotes the methionine
          as amino acid +1. Commercial suppliers of antibodies, such as the Y1068-specific
          anti-phospho-EGFR, use the first nomenclature. To be consistent, we consider Y1068 as
          Y1092. Likewise, the T790M mutation reported here has also been called T766M. Mutations
          were introduced into full-length wild-type and mutant 
          EGFR cDNAs using a QuikChange Site-Directed Mutagenesis Kit
          (Stratagene, La Jolla, California, United States) and cloned into expression vectors as
          described [3]. The following primers were used to generate the deletion (del)
          L747–E749;A750P mutant: forward 5′- TAAAATTCCCGTCGCTATCAAGGAGCCAACATCTCCGAAAGCCAACAAGG-3′
          and reverse 5′- CCTTGTTGGCTTTCGGAGATGTTGGCTCCTTGATAGCGACGGGAATTTTA-3′. The following
          primers were used to introduce the T790M mutation: forward 5′- AGCTCATCATGCAGCTCAT-3′ and
          reverse 5′- ATGAGCTGCATGATGAGCT-3′. The L858R mutant cDNA was generated previously [3].
          All mutant clones were fully re-sequenced bidirectionally to ensure that no additional
          mutations were introduced. Various EGFRs were transiently expressed in 293T human
          embryonic kidney cells as published [3]. Cells were treated with different concentrations
          of gefitinib or erlotinib.
        
        
          Immunoblotting
          See Methods and supplementary methods in [3] for details on cell lysis,
          immunoblotting, and antibody reagents. At least three independent experiments were
          performed for all analyses.
        
        
          Cell Culture
          The NSCLC cell lines H1650, H1975, H2030, H2347, H2444, H358, and H1734 were purchased
          from American Type Culture Collection (Manassas, Virginia, United States). H3255 was a
          gift of B. Johnson and P. Janne. Cells were grown in complete growth medium (RPMI-1640;
          American Type Culture Collection catalog no. 30–2001) supplemented with 10% fetal calf
          serum, 10 units/ml penicillin, and 10 μg/ml streptomycin) at 37 °C and 5% CO
          2 . For viability studies, cells were seeded in complete growth medium
          in black 96-well clear bottom ViewPlates (PerkinElmer, Wellesley, Massachusetts, United
          States) at a density of 5,000 (H1975 and H2030) or 7,500 cells per well (H3255).
          Following overnight incubation, cells were grown for 24 h in the supplemented RPMI-1640
          medium with 0.1% serum. Cells (in supplemented RPMI-1640 medium containing 0.1% serum)
          were then incubated for 48 h in the continued presence of gefitinib or erlotinib.
        
        
          Viability Assay
          Cell viability was assayed using Calcein AM (acetoxymethyl ester of Calcein, Molecular
          Probes, Eugene, Oregon, United States). Following incubation with gefitinib or erlotinib,
          monolayers were washed twice with PBS (containing calcium and magnesium) and incubated
          with 7.5 μmol Calcein AM in supplemented RPMI-1640 (no serum) for 30 min. Labeling medium
          was removed, and cells were washed three times with PBS. Calcein fluorescence (Ex, 485
          nm; Em, 535 nM) was detected immediately using a Victor V multi-label plate reader
          (PerkinElmer). Three independent experiments were performed for each cell line; each
          experiment included four to eight replicates per condition.
        
      
      
        Results
        
          Case Reports
          We identified secondary 
          EGFR mutations in three of six individuals whose disease progressed on
          either gefitinib or erlotinib (Table 1). Brief case histories of these three patients are
          presented below.
          
            Patient 1
            This 63-y-old female “never smoker” (smoked less than 100 cigarettes in her
            lifetime) initially presented with bilateral diffuse chest opacities and a right-sided
            pleural effusion. Transbronchial biopsy revealed adenocarcinoma. Disease progressed on
            two cycles of systemic chemotherapy, after which gefitinib, 250 mg daily, was started.
            Comparison of chest radiographs obtained prior to starting gefitinib (Figure S1A, left
            panel) and 2 wk later (Figure S1A, middle panel) showed dramatic improvement. Nine
            months later, a chest radiograph revealed progression of disease (Figure S1A, right
            panel). Subsequently, the patient underwent a computed tomography (CT)–guided biopsy of
            an area in the right lung base (Figure 1A, left panel). Despite continued treatment
            with gefitinib, either with chemotherapy or at 500 mg daily, the pleural effusion
            recurred, 12 mo after initiating gefitinib (Figure 1A, right panel). Pleural fluid was
            obtained for molecular studies. In total, this patient had three tumor specimens
            available for analysis: the original lung tumor biopsy, a biopsy of the progressing
            lung lesion, and pleural fluid. However, re-review of the original transbronchial
            biopsy showed that it had scant tumor cells (Table 1).
          
          
            Patient 2.
            This 55-y-old woman with a nine pack-year history of smoking underwent two surgical
            resections within 2 y (right lower and left upper lobectomies) for bronchioloalveolar
            carcinoma with focal invasion. Two years later, her disease recurred with bilateral
            pulmonary nodules and further progressed on systemic chemotherapy. Thereafter, the
            patient began erlotinib, 150 mg daily. A baseline CT scan of the chest demonstrated
            innumerable bilateral nodules (Figure S1B, left panel), which were markedly reduced in
            number and size 4 mo after treatment (Figure S1B, middle panel). After 14 mo of
            therapy, the patient's dose of erlotinib was decreased to 100 mg daily owing to
            fatigue. At 23 mo of treatment with erlotinib, a CT scan demonstrated an enlarging
            sclerotic lesion in the thoracic spine. The patient underwent CT-guided biopsy of this
            lesion (Figure 1B, left panel), and the erlotinib dose was increased to 150 mg daily.
            After 25 mo of treatment, she progressed within the lung (Figure S1B, right panel).
            Erlotinib was discontinued, and a fluoroscopically guided core needle biopsy was
            performed at a site of progressive disease in the lung (Figure 1B, right panel). In
            total, this patient had three tumor specimens available for analysis: the original
            resected lung tumor, the biopsy of the enlarging spinal lesion, and the biopsy of the
            progressing lung lesion (Table 1).
          
          
            Patient 3
            This 55-y-old female “never smoker” was treated for nearly 4.5 y with weekly
            paclitaxel and trastuzumab [17] for adenocarcinoma with bronchioloalveolar carcinoma
            features involving her left lower lobe, pleura, and mediastinal lymph nodes. Treatment
            was discontinued owing to fatigue. Subsequently, the patient underwent surgical
            resection. Because of metastatic involvement of multiple mediastinal lymph nodes and
            clinical features known at that time to be predictive of response to gefitinib (female,
            never smoker, bronchioloalveolar variant histology), she was placed on “adjuvant”
            gefitinib 1 mo later (Figure S1C, left panel). This drug was discontinued after 3 mo
            when she developed a new left-sided malignant pleural effusion (Figure S1C, middle
            panel). Despite drainage and systemic chemotherapy, the pleural effusion recurred 4 mo
            later (Figure S1C, right panel), at which time pleural fluid was collected for
            analysis. In total, this patient had two clinical specimens available for analysis:
            tumor from the surgical resection and pleural fluid (Table 1).
          
        
        
          Patients' Tumors Contain EGFR Tyrosine Kinase Domain Mutations Associated with
          Sensitivity to EGFR Tyrosine Kinase Inhibitors
          We screened all available tumor samples from these three patients for previously
          described drug-sensitive 
          EGFR mutations, by direct DNA sequencing of exons 19 and 21 [3]. Tumor
          samples from patient 1 showed a T→G change at nucleotide 2573, resulting in the exon 21
          L858R amino acid substitution commonly observed in drug-responsive tumors. This mutation
          was present in the biopsy material from the progressing lung lesion (Figure S2A, upper
          panels) and from cells from the pleural effusion (Figure S2A, lower panels), both of
          which on cytopathologic examination consisted of a majority of tumor cells (Table 1).
          Interestingly, comparisons of the tracings suggest that an increase in copy number of the
          mutant allele may have occurred. Specifically, while the ratio of wild-type (nucleotide
          T) to mutant (nucleotide G) peaks at position 2573 was approximately 1:1 or 1:2 in the
          lung biopsy specimen (Figure S2A, upper panels), sequencing of DNA from the pleural fluid
          cells demonstrated a dominant mutant G peak (Figure S2A, lower panels). Consistent with
          this, a single nucleotide polymorphism (SNP) noted at nucleotide 2361 (A or G)
          demonstrated a corresponding change in the ratios of A:G, with a 1:1 ratio in the
          transbronchial biopsy, and a nearly 5:1 ratio in the pleural fluid (Figure 2A). Notably,
          we did not detect the 2573 T→G mutation in the original transbronchial biopsy specimen
          (Table 1; data not shown). As stated above, this latter specimen contained scant tumor
          cells, most likely fewer than needed for detection of an 
          EGFR mutation by direct sequencing (see [7]).
          All three specimens from patient 2, including the original lung tumor and the two
          metastatic samples from bone and lung, showed an exon 19 deletion involving elimination
          of 11 nucleotides (2238–2248) and insertion of two nucleotides, G and C (Figure S2B, all
          panels; Table 1). These nucleotide changes delete amino acids L747–E749 and change amino
          acid 750 from alanine to proline (A750P). A del L747–E749;A750P mutation was previously
          reported with different nucleotide changes [2]. In all samples from patient 2, the
          wild-type sequence predominated at a ratio of about 3:1 over the mutant sequence.
          Both of the available tumor samples from patient 3 contained a deletion of 15
          nucleotides (2236–2250) in exon 19 (Table 1; data not shown), resulting in elimination of
          five amino acids (del E746–A750). This specific deletion has been previously reported
          [3]. The ratio of mutant to wild-type peaks was approximately 1:1 in both specimens (data
          not shown).
          Collectively, these results demonstrate that tumors from all three patients contain 
          EGFR mutations associated with sensitivity to the tyrosine kinase
          inhibitors gefitinib and erlotinib. In addition, these data show that within individual
          patients, metastatic or recurrent lesions to the spine, lung, and pleural fluid contain
          the same mutations. These latter observations support the idea that relapsing and
          metastatic tumor cells within individuals are derived from original progenitor
          clones.
        
        
          A Secondary Missense Mutation in the EGFR Kinase Domain Detected in Lesions That
          Progressed while on Treatment with Either Gefitinib or Erlotinib
          To determine whether additional mutations in the 
          EGFR kinase domain were associated with progression of disease in these
          patients, we performed direct sequencing of all of the exons (18 through 24) encoding the
          EGFR catalytic region in the available tumor specimens.
          Analysis of patient 1's pre-gefitinib specimen, which contained scant tumor cells
          (Table 1; see above), not surprisingly showed only wild-type 
          EGFR sequence (Table 1; data not shown). However, careful analysis of
          the exon 20 sequence chromatograms in both forward and reverse directions from this
          patient's lung biopsy specimen obtained after disease progression on gefitinib
          demonstrated an additional small peak at nucleotide 2369, suggesting a C→T mutation
          (Figure 2A, upper panels; Table 1). This nucleotide change leads to substitution of
          methionine for threonine at position 790 (T790M). The 2369 C→T mutant peak was even more
          prominent in cells from the patient's pleural fluid, which were obtained after further
          disease progression on gefitinib (Figure 2A, lower panels; Table 1). The increase in the
          ratio of mutant to wild-type peaks obtained from analyses of the lung specimen and
          pleural fluid paralleled the increase in the ratio of the mutant G peak (leading to the
          L858R mutation) to the wild-type T peak at nucleotide 2573 (see above; Figure S2A), as
          well as the increase in the ratio of the A:G SNP at position 2361 (Figure 2A).
          Collectively, these findings imply that the exon 20 T790M mutation was present on the
          same allele as the exon 21 L858R mutation, and that a subclone of cells harboring these
          mutations emerged during drug treatment.
          In patient 2, the tumor-rich sample obtained prior to treatment with erlotinib did not
          contain any additional mutations in the exons encoding the 
          EGFR tyrosine kinase domain (Figure 2B, upper panels; Table 1). By
          contrast, her progressing bone and lung lesions contained an additional small peak at
          nucleotide 2369, suggesting the existence of a subclone of tumor cells with the same C→T
          mutation observed in patient 1 (Figure 2B, middle and lower panels; Table 1). The
          relative sizes of the 2369 T mutant peaks seen in these latter two samples appeared to
          correlate with the relative size of the corresponding peaks of the exon 19 deletion
          (Figure S2B). Interestingly, the SNP at nucleotide 2361 (A or G) was detected in
          specimens from patient 2 before but not after treatment with erlotinib, suggesting that
          one 
          EGFR allele underwent amplification or deletion during the course of
          treatment (Figure S2B).
          Patient 3 showed results analogous to those of patient 2. A tumor-rich pre-treatment
          specimen did not demonstrate 
          EGFR mutations other than the del E746–A750 exon 19 deletion;
          specifically, in exon 20, no secondary changes were detected (Figure 2C, upper panels;
          Table 1). However, analysis of DNA from cells in the pleural effusion that developed
          after treatment with gefitinib showed the C→T mutation at nucleotide 2369 in exon 20
          (Figure 2C, lower panels; Table 1), corresponding to the T790M mutation described above.
          There was no dramatic change between the two samples in the ratio of the A:G SNP at
          position 2361. The mutant 2369 T peak was small, possibly because gefitinib had been
          discontinued in this patient for 4 mo at the time pleural fluid tumor cells were
          collected; thus, there was no selective advantage conferred upon cells bearing the T790M
          mutation.
          To determine whether the 2369 C→T mutation was a previously overlooked 
          EGFR mutation found in NSCLCs, we re-reviewed exon 20 sequence tracings
          derived from analysis of 96 fresh-frozen resected tumors [3] and 59 paraffin-embedded
          tumors [7], all of which were removed from patients prior to treatment with an EGFR
          tyrosine kinase inhibitor. We did not detect any evidence of the T790M mutation in these
          155 tumors (data not shown; see Discussion). Collectively, our results suggest that the
          T790M mutation is associated with lesions that progress while on gefitinib or erlotinib.
          Moreover, at least in patients 1 and 2, the subclones of tumor cells bearing this
          mutation probably emerged between the time of initial treatment with a tyrosine kinase
          inhibitor and the appearance of drug resistance.
          In three additional patients (case histories not described here) with lung
          adenocarcinomas who improved but subsequently progressed on therapy with either gefitinib
          or erlotinib, we examined DNA from tumor specimens obtained during disease progression.
          In all three patients, we found 
          EGFR mutations associated with drug sensitivity (all exon 19
          deletions). However, we did not find any additional mutations in exons 18 to 24 of 
          EGFR, including the C→T change at position 2369 (data not shown). These
          results imply that alternative mechanisms of acquired drug resistance exist.
        
        
          Patients' Progressive Tumors Lack KRAS Mutations
          Mutations in exon 2 of 
          KRAS2 occur in about one-fourth of NSCLCs. Such mutations rarely, if
          ever, accompany 
          EGFR mutations and are associated with primary resistance to gefitinib
          or erlotinib [7]. To evaluate the possibility that secondary 
          KRAS mutations confer acquired resistance to these drugs, we performed
          mutational profiling of 
          KRAS2 exon 2 from tumor specimens from patients 1 to 3, as well as the
          three additional patients lacking evidence of the T790M mutation. None of the specimens
          contained any changes in 
          KRAS (Table 1; data not shown), indicating that 
          KRAS mutations were not responsible for drug resistance and tumor
          progression in these six patients.
        
        
          An Established NSCLC Cell Line Also Contains Both T790M and L858R Mutations
          We profiled the 
          EGFR tyrosine kinase domain (exons 18 to 24) and 
          KRAS exon 2 in eight established NSCLC lines (Table 2). Surprisingly,
          one cell line—H1975—contained the same C→T mutation at position 2369 (T790M) as described
          above (Figure 2D, lower panel). This cell line had previously been shown by others to
          contain a 2573 T→G mutation in exon 21 (L858R) [18], which we confirmed (Figure 2D, upper
          panel); in addition, H1975 was reported to be more sensitive to gefitinib inhibition than
          other lung cancer cell lines bearing wild-type 
          EGFR [18]. Only exons 19 and 21 were apparently examined in this
          published study.
          In our own analysis of H1975 (exons 18 to 24), the mutant 2369 T peak resulting in the
          T790M amino acid substitution was dominant, suggesting an increase in copy number of the
          mutant allele in comparison to the wild-type allele. The ratio of mutant to wild-type
          peaks was similar to that of the mutant 2573 G (corresponding to the L858R amino acid
          substitution) to wild-type T peaks (Figure 2D, all panels), implying that the T790M and
          L858R mutations were in the same amplified allele. To further investigate this
          possibility, we performed RT-PCR to generate cDNAs that spanned exon 20 of 
          EGFR and included sequences from exon 19 and 21. PCR products were then
          cloned, and individual colonies were analyzed for 
          EGFR mutations. Sequencing chromatograms of DNA from four of four
          clones showed both the 2369 C→T and 2573 T→G mutations, confirming that both mutations
          were in the same allele (data not shown).
          Other NSCLC cell lines carried either 
          EGFR or 
          KRAS mutations, but none had both (Table 2). As reported, H3255
          contained an L858R mutation [19] and H1650 contained an exon 19 deletion [18]. No other
          cell lines analyzed contained additional mutations in the exons encoding the EGFR
          tyrosine kinase domain.
        
        
          A Novel PCR Restriction Fragment Length Polymorphism Assay Independently Confirms
          the Absence or Presence of the T790M Mutation
          As stated above, the mutant peaks suggestive of a T790M mutation in exon 20 were small
          in some sequence chromatograms. To eliminate the possibility that these peaks were due to
          background “noise,” we sought to confirm the presence of the 2369 C→T mutation in
          specific samples, by developing an independent test, based on a fluorescence detection
          assay that takes advantage of a PCR restriction fragment length polymorphism (PCR-RFLP)
          generated by the specific missense mutation. After PCR amplification with
          exon-20-specific primers spanning nucleotide 2369, wild-type sequence contains specific
          NlaIII sites, which upon digestion yield a 106-bp product (see Methods; Figure 3A).
          Presence of the mutant 2369 T nucleotide creates a new NlaIII restriction digest site,
          yielding a slightly shorter product (97 bp), readily detected by fluorescent capillary
          electrophoresis. This test is about 2 -fold more sensitive than direct sequencing (see
          Methods; data not shown).
          We first used DNA from the H1975 cell line (which contains both T790M and L858R
          mutations) to confirm the specificity of the PCR-RFLP assay. As expected, analysis of
          these cells produced both the 97- and 106-bp fragments. By contrast, analysis of DNA from
          H2030 (which contains wild-type 
          EGFR; Table 2) showed only the 106-bp fragment (Figure 3A). These data
          show that this test can readily indicate the absence or presence of the mutant allele in
          DNA samples. However, this test was only semi-quantitative, as the ratio of the mutant
          97-bp product versus the wild-type 106-bp product varied in independent experiments from
          approximately 1:1 to 2:1.
          We next used this PCR-RFLP assay to assess various patient samples for the presence of
          the specific 2369 C→T mutation corresponding to the T790M amino acid substitution. DNA
          from the progressing bone and lung lesions in patient 1 produced both the 97- and 106-bp
          fragments, but DNA from the original lung tumor did not (Figure 3B). The ratio of mutant
          to wild-type products was higher in the cells from the pleural fluid, consistent with the
          higher peaks seen on the chromatograms from direct sequencing of exon 20 (see Figure 2A).
          Likewise, DNA from progressive lesions from patients 2 and 3 yielded both 97- and 106-bp
          fragments in the PCR-RFLP assay (Figure 3B), whereas the pre-treatment specimens did not
          produce the 97-bp product. Collectively, these data from an independent assay confirm
          that the T790M mutation was present in progressing lesions from all three patients. We
          were also unable to detect the T790M mutation in any specimens from the three additional
          patients with acquired resistance that failed to demonstrate secondary mutations in 
          EGFR exons 18 to 24 by direct sequencing (data not shown).
        
        
          Biochemical Properties of EGFR Mutants
          To determine how the T790M mutation would affect EGFR proteins already containing
          mutations associated with sensitivity to EGFR tyrosine kinase inhibitors, we introduced
          the specific mutation into 
          EGFR cDNAs that encoded the exon 21 and 19 mutations found in patients
          1 and 2, respectively. Corresponding proteins ([i] L858R and L858R plus T790M, [ii] del
          L747–E749;A750P and del L747–E749;A750P plus T790M, and [iii] wild-type EGFR and
          wild-type EGFR plus T790M) were then produced by transient transfection with expression
          vectors in 293T cells, which have very low levels of endogenous EGFR [3]. Various lysates
          from cells that were serum-starved and pre-treated with gefitinib or erlotinib were
          analyzed by immunoblotting. Amounts of total EGFR (t-EGFR) were determined using an
          anti-EGFR monoclonal antibody, and actin served as an indicator of relative levels of
          protein per sample. To assess the drug sensitivity of the various EGFR kinases in
          surrogate assays, we used a Y1092-phosphate-specific antibody (i.e., phospho-EGFR
          [p-EGFR]) to measure the levels of “autophosphorylated” Tyr-1092 on EGFR in relation to
          levels of t-EGFR protein. We also assessed the global pattern and levels of induced
          tyrosine phosphorylation of cell proteins by using a generalized anti-phosphotyrosine
          reagent (RC-20).
          Gefitinib inhibited the activity of wild-type and L858R EGFRs progressively with
          increasing concentrations of drug, as demonstrated by a reduction of
          tyrosine-phosphorylated proteins (Figure 4A) and a decrease in p-EGFR:t-EGFR ratios
          (Figure 4B). By contrast, wild-type and mutant EGFRs containing the T790M mutation did
          not display a significant change in either phosphotyrosine induction or p-EGFR:t-EGFR
          ratios (Figure 4A and 4B). Similar results were obtained using erlotinib against
          wild-type and del E747–L747;A750P EGFRs in comparison to the corresponding mutants
          containing the T790M mutation (Figure 4C). These results suggest that the T790M mutation
          may impair the ability of gefitinib or erlotinib to inhibit EGFR tyrosine kinase
          activity, even in EGFR mutants (i.e., L858R or an exon 19 deletion) that are clinically
          associated with drug sensitivity.
        
        
          Resistance of a NSCLC Cell Line Harboring Both T790M and L858R Mutations to
          Gefitinib or Erlotinib
          To further explore the functional consequences of the T790M mutation, we determined
          the sensitivity of various NSCLC cells lines grown in the presence of either gefitinib or
          erlotinib, using an assay based upon Calcein AM. Uptake and retention of this fluorogenic
          esterase substrate by vehicle- versus drug-treated live cells allows for a comparison of
          relative cell viability among cell lines [20]. The H3255 cell line, which harbors the
          L858R mutation and no other 
          EGFR TK domain mutations (Table 2), was sensitive to treatment with
          gefitinib, with an IC
          50 of about 0.01 μmol (Figure 5). By contrast, the H1975 cell line,
          which contains both L858R and T790M mutations (Table 2), was approximately 100-fold less
          sensitive to drug, with an IC
          50 of about 1 μmol (Figure 5). In fact, the sensitivity of H1975 cells
          was more similar to that of H2030, which contains wild-type 
          EGFR (exons 18 to 24) and mutant 
          KRAS (Figure 5). Very similar results were obtained with erlotinib
          (Figure S3).
        
      
      
        Discussion
        Specific mutations in the tyrosine kinase domain of 
        EGFR are associated with sensitivity to either gefitinib or erlotinib,
        but mechanisms of acquired resistance have not yet been reported. Based upon analogous
        studies in other diseases with another kinase inhibitor, imatinib, a single amino acid
        substitution from threonine to methionine at position 790 in the wild-type EGFR kinase
        domain was predicted to lead to drug resistance, even before the association of exon 19 and
        21 mutations of 
        EGFR with drug responsiveness in NSCLC was reported. The T790M mutation
        was shown in vitro in the context of wild-type EGFR to confer resistance to gefitinib [21]
        and a related quinazoline inhibitor, PD153035 [22].
        We show here, through molecular analysis of tumor material from three patients and one
        NSCLC cell line, as well as additional biochemical studies, that acquired clinical drug
        resistance to gefitinib or erlotinib is indeed associated with the T790M mutation.
        Importantly, we find that the T790M mutation confers drug resistance not just to wild-type
        EGFR but also to mutant EGFRs associated with clinical responsiveness to EGFR tyrosine
        kinase inhibitors [1,2,3]. Our results further demonstrate that an analogous mechanism of
        acquired resistance exists for imatinib and EGFR tyrosine kinase inhibitors (Table 3),
        despite the fact that the various agents target different kinases in distinct diseases.
        In tumors from patients not treated with either gefitinib or erlotinib, the 2369 C→T
        mutation (T790M) appears to be extremely rare. We have not identified this mutation in 155
        tumors (see above), and among nearly 1,300 lung cancers in which analysis of 
        EGFR exons 18 to 21 has been performed [1,2,3,4,5,6], only one tumor
        (which also harbored an L858R mutation) was reported to contain the T790M mutation. Whether
        the patient from which this tumor was resected had received gefitinib or erlotinib is
        unclear, and the report did not note an association with acquired resistance to either drug
        [5].
        How tumor cells bearing the T790M mutation emerge within gefitinib- or erlotinib-treated
        patients is a matter of investigation. Subclones bearing this mutation could arise de novo
        during treatment
        . However, based upon analogous studies in CML, it is also possible that
        NSCLC subclones bearing this secondary mutation pre-exist within the primary tumor clone in
        individual patients, albeit at low frequency [23]. In either scenario, treatment with
        gefitinib or erlotinib subsequently allows these resistant subclones to become apparent,
        because most cells bearing sensitivity-conferring mutations die, while cells with the T790M
        mutation persist.
        From analysis of the crystal structure of the EGFR kinase domain bound to erlotinib, it
        is has been shown that the wild-type threonine residue at position 790 is located in the
        hydrophobic ATP-binding pocket of the catalytic region, where it forms a critical hydrogen
        bond with the drug [24]. The related compound, gefitinib, is predicted to interact with
        this threonine residue as well. Substitution of the threonine at position 790 by a larger
        residue like methionine would probably result in steric clash with the aromatic moieties on
        these two drugs [25]. By contrast, ATP would likely not depend on the accessibility of the
        same hydrophobic cavity and is therefore probably not affected by the incorporation of a
        bulky methionine side chain [25]. Consistent with this, the T790M mutation has been shown
        not to abrogate the catalytic activity of wild-type EGFR [22].
        The T790M mutation could also affect the kinase activity or alter the substrate
        specificity of mutant EGFRs, such that a proliferative advantage would be conferred upon
        cells bearing the mutation. Consistent with this, the H1975 NSCLC cell line reported here
        to contain both T790M and L858R did not to our knowledge undergo any prior treatment with
        gefitinib or erlotinib; the doubly mutated cells must have become dominant over time
        through multiple passages in vitro. This scenario could explain the seemingly contradictory
        report by others who found the H1975 cell line to be highly sensitive to gefitinib [18];
        our H1975 cells could represent a subclone that emerged over time. Analysis of earlier
        passages of H1975 cells for the T790M mutation would be informative in this regard.
        Recently, new small-molecule inhibitors have been identified that retain activity
        against the majority of imatinib-resistant BCR-ABL mutants. The new drugs bind to ABL in an
        “open” conformation, as opposed to imatinib, which binds ABL in a “closed” conformation
        [12,13]. Analogously, it may be possible to find EGFR tyrosine kinase inhibitors that bind
        to the EGFR kinase domain in different ways than gefitinib and erlotinib. For example, the
        crystal structure of another EGFR inhibitor, lapatinib (GW572016), was recently solved
        bound to EGFR [26]. This study revealed that the quinazoline rings of erlotinib and
        lapatinib interact differently with the EGFR kinase domain, suggesting that while the T790M
        mutation may affect inhibition by erlotinib and gefitinib, it may not affect inhibition of
        EGFR by compounds similar to lapatinib. To our knowledge, no NSCLC patient who initially
        responded to but then progressed on either gefitinib or erlotinib has yet been treated with
        lapatinib.
        In some of the patient specimens analyzed, the actual sequencing peaks demonstrating the
        T790M mutation were smaller than originally anticipated. These results differ from those of
        acquired resistance mutation in CML [10], GIST [15,27], and HES [16]. However, in contrast
        to all of these diseases, in which tumor cells are readily accessible, lung-cancer-related
        tumors are more difficult to access, as illustrated by the limited manner in which we were
        able to obtain tumor cells from various sites of disease (see Figure 1). Moreover,
        re-biopsy of patients with lung cancer is not routinely performed. The use of position
        emission tomography scans to identify the most metabolically active lesions for biopsy
        could possibly circumvent this factor in the future, as long as such lesions are
        resectable. Additionally, as more molecularly tailored treatment options become available
        for lung cancer, re-biopsy of progressive sites of disease should become a standard
        procedure, especially for patients on clinical trials of targeted agents.
        Since tumor specimens from three additional patients with acquired resistance to EGFR
        tyrosine kinase inhibitors did not demonstrate the T790M mutation, this specific lesion
        does not account for all mechanisms of acquired resistance to gefitinib or erlotinib. Given
        the paradigm established with imatinib, other drug-resistance mutations in 
        EGFR, either within or outside the tyrosine kinase domain, are likely to
        exist. It is also possible that 
        EGFR amplification itself plays a role in acquired resistance, since
        imatinib-resistant clones have been shown to lack resistance mutations but contain
        amplified copies of 
        BCR-ABL [11,28]. Nonetheless, studies presented here provide a basis for
        the rational development of “second generation” kinase inhibitors for use in NSCLC.
      
      
        Supporting Information
        
          Accession Numbers
          The LocusLink (http://www.ncbi.nlm.nih.gov/LocusLink/) accession number for the 
          KRAS2 sequence discussed in this paper is 3845; the GenBank
          (http://www.ncbi.nlm.nih.gov/Genbank/) accession number for the 
          KRAS2 sequence discussed in this paper is NT_009714.16. Reference 
          EGFR sequence was obtained from LocusLink accession number 1956 and
          GenBank accession number NT_033968.
        
      
    
  

  
    
      
        
        Acquired resistance to chemotherapy is a major obstacle to successful cancer treatment.
        Understanding the mechanisms by which tumors become resistant to a particular agent is key
        to identifying new drugs or combination regimens.
        Kinases are signaling molecules that control many aspects of cell behavior, including
        cell proliferation, i.e., whether and how fast cells divide. Abnormally active kinases
        promoting tumor growth are found in many cancers and are a focus of rational cancer drug
        design. One target for kinase inhibitors is the epidermal growth factor receptor (EGFR).
        Two EGFR inhibitors, gefitinib and erlotinib, showed therapeutic benefits in a subset of
        patients with non-small cell lung cancer. Recent work has helped us understand why some
        patients respond and some don't: responsive tumors usually harbor activating mutations in
        the EGFR gene, which somehow make the tumors sensitive to treatment. Nearly all patients
        whose tumors initially respond to EGFR inhibitors, however, eventually become resistant to
        the drugs and progress despite continued therapy.
        William Pao and colleagues examined tumors from six patients with non-small cell lung
        cancer who initially responded to gefitinib or erlotinib but subsequently relapsed. Tumors
        from all six patients carried activating mutations in the EGFR gene. In addition, in three
        out of the six cases, the resistant tumor cells carried an identical second mutation in the
        EGFR gene. Whereas the activating mutation was present in tumor cells before treatment with
        erlotinib or gefitinib, the second mutation was not found in pre-treatment biopsies from
        these patients, nor in over 150 lung cancer samples from patients who had not been treated
        with either drug. Additional cell culture studies supported the notion that the secondary
        mutation causes resistance to gefitinib or erlotinib. It is clear, though, that this is
        only one mechanism of resistance, because in the three other cases resistance occurred in
        the absence of the second mutation. What caused the resistance in those tumors is not
        known.
        All kinases share some common features, and a resistance mutation very similar to the
        one identified here has also been found in other kinase genes from tumors with acquired
        resistance to imatinib, another kinase inhibitor. As Gary Gilliland and colleagues point
        out in an accompanying Perspective (DOI: 10.1371/journal.pmed.0020075), the initial
        identification three years ago of resistance mutations against imatinib led to the rapid
        development of alternative kinase inhibitors that work even against tumors with the
        resistance mutation. Similarly, the results by Pao and colleagues should help researchers
        develop second generation drugs for lung cancer.
      
    
  

  
    
      
        Kinase Inhibition for Treatment of Cancer
        Uncontrolled proliferation of tumor cells is a hallmark of cancer. In many types of
        cancer, mutations in genes that activate cellular signal transduction pathways contribute
        to enhanced proliferation and survival of cancer cells. One well-characterized example is
        mutation in tyrosine kinases, enzymes that regulate the growth and survival of cells.
        Tyrosine kinase activity is tightly regulated in normal cells, but is dysregulated due to
        mutation in some cancers, including lung cancer, resulting in enhanced proliferation and
        survival of cancer cells. The tyrosine kinases are attractive candidates for molecularly
        targeted therapy in cancer, because cancers become dependent on growth signals from the
        mutant tyrosine kinases. Tyrosine kinases require ATP for their enzymic activity, and thus
        small molecules that mimic ATP can bind to mutant kinases and inactivate them.
        The paradigm for tyrosine kinase inhibition as treatment for cancer using small-molecule
        inhibitors was first established in the context of chronic myelogenous leukemia (CML)
        associated with the 
        BCR-ABL gene rearrangement [1]. Imatinib (Gleevec), a
        2-phenylaminopyrimidine, is a competitive inhibitor of ATP binding to the ABL kinase,
        thereby inhibiting the constitutively activated BCR-ABL tyrosine kinase. Imatinib induces
        complete remission in most patients with CML in stable phase [1], and also has activity in
        CML that has progressed to blast crisis [2].
        Imatinib is also a potent inhibitor of the ARG, KIT, PDGFRA, and PDGFRB tyrosine
        kinases. As a consequence, there have been additional dividends from the United States
        Federal Drug Administration approval of imatinib for treatment of BCR-ABL-positive CML. For
        example, imatinib is effective in treatment of chronic myelomonocytic leukemia with gene
        rearrangements that constitutively activate 
        PDGFRB [3], of hypereosinophilic syndrome with activating mutations in 
        PDGFRA [4], and of gastrointestinal stromal cell tumors associated with
        activating mutations in 
        KIT [5] (all reviewed in [6]).
        More recently, this paradigm has been extended to treatment of non-small cell lung
        cancer (NSCLC). Several mutations have been identified in the context of 
        epidermal growth factor receptor (EGFR) in patients with NSCLC that are
        associated with clinical response to the small-molecule EGFR inhibitors gefitinib (Iressa)
        or erlotinib (Tarceva) [7,8,9], including in-frame deletions such as del L747–E749;A750P in
        exon 19, or L858R in exon 21. Although responses are often dramatic, most responding
        patients ultimately develop clinical resistance and relapse of disease [7,8,9]. The basis
        for resistance had not been known, in part owing to the difficulty in obtaining tissue from
        re-biopsy at time of relapse.
      
      
        Resistance to Small-Molecule Tyrosine Kinase Inhibitors
        As might have been anticipated in treatment of cancer with any single agent, resistance
        to small-molecule tyrosine kinase inhibitors has emerged as a significant clinical problem.
        This was first appreciated in patients with CML treated with imatinib whose tumors
        developed resistance, and has been most extensively studied in that context. Although there
        are many potential mechanisms for development of clinical resistance, most cases of
        imatinib-resistant CML are due to point mutations in the 
        BCR-ABL kinase domain itself, including T315I [10,11]. Similar mutations
        in the homologous residues of the kinase domains of PDGFRA (T674I) and KIT (T670I) account
        for imatinib resistance in some patients with hypereosinophilic syndrome and
        gastrointestinal stromal cell tumors, respectively [4,12]. These findings suggest
        strategies to overcome resistance that include the use of alternative small-molecule
        inhibitors. Indeed, about three years after the recognition of imatinib resistance
        mutations in BCR-ABL-positive CML, new drugs are now in clinical trials that are potent
        inhibitors of imatinib-resistant BCR-ABL mutants [13,14].
      
      
        A Basis for Resistance to Small-Molecule EGFR Inhibitors in NSCLC
        In an elegant new study in 
        PLoS Medicine , Pao and colleagues have identified acquired mutations in
        patients with NSCLC that appear to explain clinical resistance to gefitinib or erlotinib
        [15]. The mechanism of resistance in three patients was acquisition of a T790M substitution
        in EGFR that was not present at time of diagnosis, but was detected with progression of
        disease after initial response to gefitinib or erlotinib. T790M in the context of either
        transiently expressed wild-type EGFR or the mutant alleles del L474–E749;A750P or L858R
        impairs inhibition by gefitinib or erlotinib as assessed by autophosphorylation.
        Furthermore, the NSCLC cell line H1975 harbors both the L858R and T790M mutations, and is
        resistant to inhibition by gefitinib or erlotinib, unlike cell lines that express the L858R
        allele alone. In the H1975 cell line, it was possible to obtain adequate quantities of RNA
        to confirm that the L858R and T790M mutations are present on the same allele, as would be
        predicted if T790M confers resistance to inhibition of the L858R allele.
        Structural models of EGFR provide structural insights into these biological data. A
        ribbon structure of erlotinib bound to the EGFR kinase domain (Figure 1) shows the
        threonine residue at position 790 in green and the positions of the exon 19 and L858R
        gain-of-function mutations. Substitution of methionine for threonine at position 790 would
        be predicted to result in steric hindrance of erlotinib binding to EGFR (Figure 2).
        These observations provide convincing evidence that, at least in some patients with
        NSCLC, resistance to gefitinib or erlotinib can be attributed to acquisition of a T790M
        mutation in the context of EGFR. However, three additional patients with clinical
        resistance to gefitinib or erlotinib did not have the T790M mutation, nor did they have
        mutant 
        KRAS alleles that have previously been shown by these same authors to
        confer resistance to these inhibitors [9]. Thus, mechanisms of resistance are
        heterogeneous.
      
      
        Next Steps, and Lessons Learned
        It will be important to identify alternative small-molecule inhibitors for the T790M
        resistance mutation. Structural data suggest that one compound, lapatinib, may subserve
        this purpose [16], but it has not been tested for biological activity in this context. New
        chemical screens and/or rational drug design to identify alternative inhibitors is
        warranted. In addition, only half of this small cohort of patients with NSCLC with clinical
        resistance to gefitinib or erlotinib had the T790M substitution. Efforts to identify
        alternative mechanisms for resistance may be guided by experience with imatinib resistance
        in the context of BCR-ABL, and should include full-length sequencing of EGFR to identify
        other resistance mutations, and analysis for evidence of gene amplification, as well as
        investigation of other well-characterized mechanisms of drug resistance such as drug efflux
        or increased drug metabolism.
        Pao and colleagues' superb study also highlights several important points that may guide
        development of kinase-targeted therapies in the future. It is clear that, to the extent
        that small-molecule kinase inhibitors are effective as single agents in treatment of
        cancer, resistance will develop. Furthermore, based on previous experience, some of these
        patients are likely to harbor acquired point mutations in the target kinase that confer
        resistance. Resistance mutations identified via in vitro screens have shown a high degree
        of correlation with those that develop in vivo, as shown in screens for imatinib-resistant
        BCR-ABL mutants [11] and PKC412-resistant FLT3 mutants [17], as well as the T790M
        resistance mutation to gefitinib in the context of EGFR [18]. Thus, in vitro screens for
        mutations that confer resistance to kinase inhibitors are warranted, followed by efforts to
        identify drugs that overcome resistance. This proactive approach should shorten the time
        frame for new drug development.
        These findings also emphasize the critical need for re-biopsy of patients with cancer
        treated with molecularly targeted therapies at time of relapse. Tissue acquisition is more
        challenging in solid tumors than for hematopoietic malignancies, and may entail risk.
        Nonetheless, it is clear that data derived from such analyses will be essential to inform
        approaches to improving therapy for NSCLC and other solid tumors.
      
    
  

  
    
      
        
        Gatz's statement, “At least half of the explanation for individual differences in
        susceptibility to Alzheimer disease is genetic” [1], is, in my opinion, incorrect. As the
        one who led the team debating Ashford and Mortimer, whose 2002 article [2] supports this
        statement, at the 2001 conference on Alzheimer disease (AD) in Cincinnati (“Challenging
        Views of Alzheimer's Disease”) [3], I think that the evidence that dietary and lifestyle
        factors explain the majority of the individual risk for AD in the US is very strong. My
        original paper in 1997 [4] found that total dietary fat and energy intake were the most
        important dietary risk factors, while fish and cereal intake were the most important risk
        reduction factors. These findings have been generally confirmed by Drs. Luchsinger and
        Morris and others. The reason I did my study was that the Honolulu Heart Study reported
        that Japanese American men in Hawaii had 2.5 times the risk of AD of native Japanese.
        African-Americans have about four times the risk of AD of native Nigerians. If genetics
        were the primary risk factor, those living in the US would have a risk of developing AD
        very similar to that of individuals living in their ancestral home. The reason this is not
        the case is that the American diet provides too much food, which is a particular problem
        for those genetically predisposed to AD.
      
    
  

  
    
      
        
        We appreciate the note from Drs. Koudinov and Berezov [1]. In our opinion, no model has
        yet been presented that plausibly accounts for all the data on statins, cholesterol,
        amyloid-ß protein (Aß), and Alzheimer disease. In our paper [2], we present evidence that
        the isoprenoid pathway contributes to statin-activated shedding of the APP ectodomain in
        cultured cells. We do not yet know which (if any) other “cholesterol-related” Alzheimer
        phenomena are also attributable to modulation of isoprenoids, Rho, or ROCK.
        Previously, conventional wisdom held that Aß load and hypercholesterolemia were directly
        related, based on observations that high-fat diet aggravated amyloid pathology in
        plaque-forming mice [3,4,5]. More recently, however, the formulation that statins act
        simply via cholesterol-lowering fails to account for several observations that cannot
        immediately be reconciled, either with the original “dogma” or with each other.
        First, Fagan et al. [6] questioned the role of cholesterol as the final common pathway
        in Aß load specification, since, in their experiments, low cholesterol per se apparently
        had no impact on brain Aß load in plaque-forming transgenic mice. Then, equally puzzling
        pharmacological data emerged. Atorvastatin was shown to lower brain amyloid load and Aß
        levels, but brain cholesterol levels were unaffected by the drug [7]. In an apparent
        complete contradiction with the original observations, now, some investigators have been
        able to devise circumstances under which there is an inverse relationship between
        cholesterol and Aß, with low neuronal cholesterol increasing Aß generation [8], and vice
        versa [9]. These newer observations are unexpected and extremely puzzling, and no
        comprehensive explanation has yet emerged.
        For those readers seeking an update on this challenging area, we would direct your
        attention to the Alzheimer Research Forum Web page
        (http://www.alzforum.org/new/detailprint.asp?id=1135), where you will find an excellent
        review of the literature as well as a series of evaluations of how our data fit into
        existing scenarios and models regarding cholesterol, statins, cerebral amyloidosis, and the
        cognitive failure of Alzheimer disease.
      
    
  

  
    
      
        
        Grant [1] describes as incorrect the statement that at least half of the explanation for
        individual differences in risk for Alzheimer disease is genetic. He suggests instead that
        dietary and lifestyle factors explain the majority of individual susceptibility to
        Alzheimer disease.
        The basis for asserting a 50% or greater role for genetics in Alzheimer disease risk
        comes from family studies and from twin studies. In family studies, first-degree relatives
        of individuals with Alzheimer disease are at more than double the risk of Alzheimer disease
        compared to those with no affected relatives [2,3]. In twin studies, across different
        Scandinavian twin registries, estimates of heritability of Alzheimer disease range from 55%
        to over 70% [4].
        Genetic risk undoubtedly represents the cumulative influence of many genes, including
        apolipoprotein E (APOE) and other genes not yet identified. In particular, it appears that
        the magnitude of the genetic component of Alzheimer disease risk is similar across ethnic
        communities, but that different genetic factors may contribute differently to that risk in
        white, Latino, and African American families [5].
        Further, there are interactions between genetic and environmental risks, for example,
        between the APOE e4 allele and high cholesterol [6] or head injury [7].
        Clearly Alzheimer disease is the outcome of multiple genetic and multiple environmental
        influences, operating additively and interactively. If genetic effects account for half of
        individual differences in liability, then environmental influences also account for half of
        the variation in susceptibility. From a public health viewpoint, it is vital to identify
        those influences that are modifiable. Controlling blood pressure and avoiding head trauma
        are examples. However, it is also important to appreciate that individuals bring
        differences in genetic risk to the table.
      
    
  

  
    
      
        
        P
        LoS Medicine was launched at a time of unprecedented concern about the
        influence of hidden competing interests on the medical literature. As a new journal, we had
        the opportunity to help create “the fully transparent world that is desirable” [1]. What
        are we doing to promote transparency, and is our strategy going far enough?
        We ask all authors and reviewers to declare any competing interests—financial, personal,
        and professional [2]—and authors' declarations are included with all published articles. We
        reject articles when we believe that authors' competing interests have compromised their
        work; for research articles, this means compromised in either the conduct of a study or its
        interpretation. When we are concerned that reviewers' competing interests may prevent them
        from giving an unbiased assessment, we find alternative reviewers. And, as recommended by
        the International Committee of Medical Journal Editors [3], we decline to publish studies
        when the sponsor controls the decision on publication.
        Our main strategy for managing competing interests is disclosure. Financial
        relationships between industry, researchers, and academic institutions are widespread [4],
        and disclosing these competing interests is a crucial step in helping to protect the public
        and the reputation of authors and of 
        PLoS Medicine . Disclosure also matters because there is increasing
        evidence that authors' competing interests have a strong influence on their conclusions.
        For example, review articles looking at the scientific evidence on the health effects of
        passive smoking have reached different conclusions; a study of these review articles found
        that the only factor associated with the conclusion that passive smoking was harmless was
        whether an author was affiliated with the tobacco industry [5]. A study of 159 randomized
        clinical trials found a significant association between authors' financial competing
        interests and their favorable conclusions about an experimental intervention [6].
        We cannot rely entirely on peer review to detect bias in the conclusions of an article,
        because although peer review may help to uncover some types of bias, such as bias in the
        study design, it cannot detect other types, such as bias in the study's conduct. Because a
        competing interests statement accompanies every article published in 
        PLoS Medicine , readers can take these interests into account when they
        assess a paper themselves. The reality is that readers are wary of competing interests. In
        one study, readers were randomly sent the same paper with or without
        
          Financial relationships between industry, researchers, and academic institutions are
          widespread.
        
        the authors having disclosed a financial competing interest [7]. Readers scored the
        paper with the competing interest significantly lower on all five measures: interest,
        importance, relevance, validity, and believability of the study.
        Our policy of asking authors to disclose their competing interests is not, of course,
        foolproof. The Center for Science in the Public Interest recently examined 163 articles in
        four scientific journals and identified at least 13 articles for which authors did not
        disclose relevant conflicts of interest that should have been disclosed according to the
        journals' policies [8]. Bero and colleagues compared the statement of competing interests
        of the authors of a 2003 
        BMJ paper on the health effects of secondhand smoke with internal tobacco
        industry documents describing financial ties between the industry and the authors [9].
        Although the authors met the 
        BMJ 's requirements for financial disclosure, the disclosure did not
        provide readers with a full picture of the industry's long-standing involvement with the
        authors.
        Should we as journal editors be investigating authors' ties for ourselves? We do not
        have the resources to do so, and agree with Bero and colleagues that “an elaborate policing
        operation is not feasible or necessarily desirable” [9]. But in this particular case, say
        the authors, a quick search of the tobacco documents that are freely available online
        (e.g., at http://www.legacy.library.ucsf.edu or http://www.bat.library.ucsf.edu) would have
        been revealing. Maybe we are heading towards an era when such searches become more
        common—perhaps initially by randomly selecting papers for investigation.
        At last year's Council of Science Editors retreat on competing interests, editors came
        up with a list of questions to think about when formulating a journal's competing interests
        policy (Table S1). Some are straightforward. Should editors declare their own competing
        interests? We think so (and have declared ours at
        http://medicine.plosjournals.org/perlserv/?request=get-static&name=editors_interests).
        Others are more complex. When editors discover that a published author failed to declare a
        significant competing interest, what should they do? Should they impose sanctions on the
        author? Should they publish a correction or even retract the paper?
        To help us answer such questions, and to advise us on individual cases for which we are
        concerned about competing interests or broader ethical questions, we have appointed an
        external advisory group. The group (Table S2) has expertise in clinical medicine, medical
        editing, research, health policy, law, and bioethics, and includes a lay member. In
        addition, an internal committee at PLoS meets monthly to consider competing interests
        across the organization. We are taking this issue seriously, because we recognize that
        journals are seen as the gatekeepers of published research. We welcome your feedback on how
        we are doing at protecting the probity of our content.
      
      
        Supporting Information
      
    
  

  
    
      
        
        What is the level of medical evidence that should be used to inform medical practice? At
        the bottom of the hierarchy of evidence are anecdotes, expert opinion, case reports, and
        case series, and at the top is the systematic review of published (and sometimes
        unpublished) evidence. By necessity, systematic reviews come many years after hypotheses
        are first raised, and in the interim recommendations for practice may sway back and forth.
        One example of this is the debate over the role of uric acid in heart disease, which has
        been going on for more than 50 years. It started with a paper published in 1951 in the 
        Annals of Internal Medicine that found higher serum uric acid
        concentrations in patients with coronary heart disease (CHD) compared with controls. Since
        then, measurement of serum uric acid has been suggested as a predictor of CHD. But many of
        the studies on serum uric acid are epidemiologic studies—somewhere in the middle of the
        hierarchy of evidence—and have come to different conclusions about how useful measurement
        of uric acid is.
        Dissecting out the role of uric acid is further complicated by three things: high levels
        of uric acid are associated with hypertension and being overweight (other risk factors for
        CHD); levels of uric acid can be altered by drugs such as diuretics that people with CHD
        often take; and finally, alteration of renal function can affect uric acid levels. Another
        problem is the type of studies that have been used to address the question of uric acid's
        role in CHD. Retrospective studies may be unable to control adequately for other risk
        factors—hence prospective, ideally population-based, studies would be the best to answer
        the question of whether there really is an association between high uric acid and CHD.
        In this month's 
        PLoS Medicine , John Danesh and colleagues from the University of
        Cambridge, along with investigators from the Icelandic Heart Association, report the single
        largest prospective study addressing the role of uric acid in heart disease. Further, their
        systematic review combines their findings with those of 15 previously published prospective
        studies of serum uric acid—9,458 cases of CHD and 155,084 controls in all.
        The paper answers the question of the role of uric acid in prediction of CHD clearly:
        the risk ratio for prediction of disease was 1.13 (1.07–1.20), but it was only 1.02
        (0.91–1.14) in the eight studies that had the most complete adjustment for possible
        confounders. What this paper does not do is directly address the question of whether or not
        serum uric acid is involved in causing CHD through intermediates; however, it does suggest
        that serum uric acid levels are unlikely to be a major determinant of CHD.
        Where does such a result leave patients? Well, it is likely that improving diet, losing
        weight, and controlling blood pressure may all contribute to reducing both one's risk of
        CHD and one's serum levels of uric acid. The role of uric acid in CHD is now likely to be
        of interest only to those studying basic science; for now, the clinical question seems
        closed.
      
    
  

  
    
      
        
        What holds cells together or connects them with the extracellular matrix—and what
        happens when these interactions break down—is one of the keys to determining how tumors
        metastasize. One group of compounds—integrins—are a central part of these interactions. Not
        only do integrins play a part in cell–cell and cell–matrix adhesion, but they also are
        involved in signal transduction (the method by which a cell relays information from
        receptor binding to cellular response) and in triggering cell death by linking to other
        molecules. One such member of this receptor family is the avß3 integrin, which is expressed
        on both the tumor cells and the new vasculature of various tumors, including melanomas.
        avß3 integrin has a role in cell migration and extravasation, which occurs during
        metastasis, and also in angiogenesis—the development of new blood vessels that are
        essential for the growth of tumors. These blood vessels are the target for one class of
        anti-cancer drugs—angiogenesis inhibitors. Molecules that bind to avß3 integrin have also
        been used to target therapeutic compounds to tumors: compounds that antagonize this
        integrin can lead to apoptosis (programmed cell death) of cells that express it.
        Haubner and colleagues, the authors of a paper in this month's 
        PLoS Medicine , have previously developed a fluorine-labeled peptide, [
        18 F]Galacto-RGD, that has a high affinity for avß3 integrin. [
        18 F]Galacto-RGD has many of the features essential for a tracer: it is
        specifically accumulated by tumors that express avß3 integrin, it is efficiently eliminated
        by the kidneys, and it is stable in vitro and in vivo.
        In the research paper in 
        PLoS Medicine , Haubner and colleagues take the development of the
        compound further towards clinical application. First, in a mouse with human melanoma they
        used highly sensitive positron emission tomography (PET) scanning to show not only that the
        level of uptake of integrin was specific for the tumor, but also that the uptake was in
        direct proportion to the amount of avß3 expressed, thus potentially allowing quantification
        of receptor expression; however, larger tumors showed a poorer correlation, possibly
        because of the presence of necrotic areas that do not express the integrin.
        In humans, this picture was a little less clear; in a small study of patients with
        tumors including melanoma, the authors found a good deal of difference between patients in
        the uptake of the marker by tumor cells and the corresponding tumor vasculature. However,
        there was good correlation between the tracer uptake and conventional staining for the
        integrin by immunohistochemistry—again suggesting that the marker is truly reflecting the
        in vivo level of the integrin.
        What do these results mean for clinical applications? As well as identifying tumors that
        express this marker, this approach might also offer a noninvasive way to assess the degree
        of new vessel formation in tumors. The approach could provide important information for
        planning and monitoring anti-angiogenic therapies targeting this integrin and could reveal
        the involvement and role of this integrin in metastatic and angiogenic processes in various
        diseases.
      
    
  

  
    
      
        
        The prevalence of asthma and allergy has risen in all industrialized countries during
        recent decades, and there is much debate about exposure to pets in early life and later
        development of asthma and allergy. Some studies have suggested that keeping pets actually
        protects against later allergy—i.e., that early exposure may somehow modify an individual's
        immune system to tolerate specific antigens. What might be the mechanism for such
        protection against allergy? One theory of how allergies arise is that an imbalance in T
        helper cell subtypes tips the body's immune response towards overreacting to a particular
        antigen. There is some evidence that early exposure to high natural levels of cat allergens
        can prevent such an inappropriate immune response. Other researchers have suggested that
        normally immune responses are kept under control by another group of T cells—regulatory T
        cells. The two mechanisms may be linked, since exposure to high levels of cat allergens may
        induce regulatory T cells.
        Various attempts to modify aberrant immune responses to specific allergens, such as
        those to cat dander, have been made. Investigators have treated patients with related
        molecules, either peptides derived from the allergen itself, or much smaller peptides
        produced synthetically. Although therapy with peptides seems to reduce allergic responses,
        the mechanism of the response to treatment has not been clear, in particular, exactly which
        cells, cell surface markers, and cytokines are involved in modifying the immune
        response.
        In a paper in this month's 
        PLoS Medicine , Mark Larché and colleagues have attempted to dissect out
        this pathway in a group of individuals with asthma and allergy to cats. They treated the
        individuals with short synthetic peptides derived from the sequence of the major cat
        allergen, 
        Felis domesticus allergen 1, and then measured the clinical and
        immunological response to allergen. They found that treatment with the peptides led to the
        induction of a population of T cells that were capable of suppressing the proliferation of
        allergen-reactive T cells in vitro. Peptide treatment also resulted in increased levels of
        a molecule called CD5 on the surface of blood T cells—CD5 has recently been associated with
        suppressing T cell sensitivity to stimulation. Finally, the authors found that the degree
        of suppression was not related to the amount of peptide given to the patients.
        Where does this finding leave patients who might wonder about exposure to cats and the
        development of allergy? The simple answer is that we do not know exactly how exposure to
        antigen triggers either an immune reaction or tolerance. Once triggered, an immune reaction
        to a cat may be hard—though not impossible—to reverse, but how or why a specific individual
        becomes sensitized is as yet far from clear.
      
    
  

  
    
      
        Michael Makover's Viewpoint: We Should Use High-Sensitivity Carotid Ultrasound to
        Detect Very Early Atherosclerosis and Treat Aggressively
        Atherosclerotic vascular disease is the greatest cause of death and disability in
        developed countries. The best way to prevent these outcomes is to detect disease at the
        earliest possible stage and to attack it with pinpoint-targeted aggressive treatment.
        B-mode ultrasonography of the carotid artery to measure the intima-media thickness (IMT)
        (Figure 1) is the most effective way to do that.
        It is time to end the mindset that thinks of atherosclerosis as an inevitable function
        of aging. It is not: it is a disease, whether it causes an acute event or gradual decline.
        As with any disease, the earlier and more intensively we attack it—with medications and
        lifestyle changes—the more successful we will be in containing and reversing it.
        Ultrasonography of the carotid to detect plaque and increased intimal wall thickness is the
        best, safest, and easiest early detector we have.
        Patients at risk for atherosclerotic cardiovascular disease are identifiable only up to
        a point using traditional methods, including the Framingham risk score [1], the European
        SCORE (Systemic Coronary Risk Evaluation) [2], and the C-reactive protein level [3]. These
        are particularly effective for people in the highest risk groups, but they have serious
        flaws. They do not take family history of premature coronary artery disease into account;
        they do not benefit from new assays that can differentiate lipoproteins by particle size
        and number, both of which are important factors in atherogenicity; and they do not include
        the quality of the patient's diet, abdominal fat content, racial and ethnic genetic
        differences, confounding medical conditions, certain gender differences, and other factors.
        Thus, many patients deemed “low risk” for acute events within a period of time might still
        develop disease.
        All of the current risk models were developed within the obsolete gestalt of life spans
        of perhaps 60 to 70 years, whereas people are now increasingly living productively into
        their 80s and 90s. Risk tables are based only on cardiac events over a relatively short
        time period of ten years. Progress in medical science is now on the steepest slope of
        exponential growth. It is reasonable to expect that those who maintain healthy circulation
        will experience good quality life beyond the 100-year mark in the surprisingly near future,
        and perhaps to 120 years not long after. Furthermore, risk escalates with age. Thus, an
        initial low risk profile becomes far more significant when considered in the context of
        long lives. Equally important, gradual impairment of blood flow that nourishes tissues can
        lead to peripheral vascular disease, increased susceptibility to infection, cognitive
        decline, frailty, and other changes that impair life quality and longevity.
        While risk factors help individualize treatment, they do not address the most critical
        issue: identifying atherosclerosis at the very earliest stage to try to stop and reverse
        the process 
        before any damage has occurred. Risk factors identify those who 
        might develop disease. The “smoking gun” is to identify disease when it
        actually first develops. Atherosclerosis begins as a lipid and inflammatory cell deposition
        in the intimal subendothelial space, which expands outward, with only a slight inward
        intrusion on the lumen. This does not affect blood flow unless the lumen-side plaque cap
        ruptures, and a clot forms and expands into the blood stream. Most clots are limited by
        thrombolytic mechanisms, but significant blockage usually results as the clot is scarred
        over. Some clots expand all the way across to the other side of the artery, completely
        occluding it, causing a heart attack or stroke [4].
        Thus, if we could discover plaque at the very earliest stage, we could aggressively
        attack it with our full armamentarium of lifestyle modification, addressing all the factors
        noted above, and medications—statins, ACE inhibitors, aspirin, niacin, ezetimibe, and
        others—to stabilize and then reverse the process before a rupture occurs. The REVERSAL
        trial, among many others, has shown that atherosclerosis is reversible with aggressive
        treatment [5]. Carotid ultrasound is an easy, safe, noninvasive method for detecting early,
        focal plaques and early thickening of the inner lining of the artery [6]. There is ample
        evidence that this is an effective and reproducible detector, and a predictor of
        progression and symptomatic disease [7]. In contrast, angiography is invasive and is
        insensitive to early changes in the artery wall thickness. Magnetic resonance imaging is
        expensive, cumbersome, and still experimental. Calcium scoring by computed tomography is
        less sensitive and less reproducible and suggests plaque presence only indirectly [8].
        Angiography and calcium scoring entail considerable X ray exposure.
        Using high-sensitivity carotid ultrasound as a primary screening tool would add modest
        cost, but that would be more than offset by sharply reducing the enormous worldwide toll of
        atherosclerosis. Screening tests should be easy, affordable, widely available, and
        predictive. Carotid ultrasound meets all these criteria and should be used to screen
        everyone early on who is at any increased risk by measure of the expanded risk factors
        noted above.
      
      
        Makover's Response to Ebrahim's Viewpoint
        While the “population” approach is obviously very important, there is no reason not to
        use the “high risk” approach as well. “High risk” is the wrong term—
        all arteries matter over a 120-year life span. We must attack
        atherosclerosis at the very onset, not wait until it is “high risk.” All previous
        approaches were designed to detect those at allegedly higher risk by various criteria. This
        misses the point. Once plaque exists, risk exists, especially when viewed over a full
        lifetime, not the current ten-year standard. And although the aggressive approach I
        outlined in my viewpoint would obviously have a cost, the savings (a reduction in the
        financial toll of cardiovascular disease) would be greater than the cost.
        Ultrasonography of the carotid artery intimal wall is effective and highly predictive
        according to the great preponderance of studies [8]. Ebrahim cites two studies to support
        his argument that measuring carotid IMT offers no additional predictive power over
        conventional screening tools [18,19], but the first of these has serious flaws (such as a
        significant dropout rate that the authors assumed was random, a short follow-up period, and
        the inclusion only of people of an advanced age) [18], and the other concluded that carotid
        IMT measurement “substantially improved prediction of future coronary heart disease”
        [19].
        Millions of people are at risk now and cannot wait for endless studies to prove over and
        over what we already know. It is not “naïve” for well people to want to remain well and to
        maintain their arteries to enable healthy older lives. Nor should we disparage as “worried”
        the very human desire to remain that way. Population-wide planning is important, but
        doctors take care of each patient as an individual. Carotid ultrasonography is by far the
        best way to design treatment for every patient exactly as needed. I believe that there is
        already sufficient evidence and sensible rationale to meld already well-proven technologies
        and approaches into a comprehensive, aggressive attack on atherosclerosis.
      
    
  

  
    
      
        A World of Contrasts
        Medical students today enter a profession defined by stark contrasts. In developed
        countries, public health improvements have nearly doubled life expectancy in the last
        century, and sophisticated technology and innovative research hold the promise of longer
        and higher quality life. At the same time, life-threatening infectious diseases such as
        tuberculosis and malaria continue to affect billions of people, and a rapidly escalating
        HIV/AIDS pandemic now kills more than 8,300 people each day [1]. New drugs are being
        developed at a record pace, but only about 1% of drugs that reach the market are aimed at
        treating neglected diseases, such as kala-azar, Chagas' disease, and sleeping sickness,
        even though these account for over 10% of the global disease burden [2]. Both within and
        between countries, the gap between wealthy and poor has continued to grow, leading to
        widening disparities in health outcomes.
        Despite these contrasts, physicians-in-training around the globe—in rich and poor
        countries—have a common goal: we seek the skills and knowledge to improve the health of
        individuals and populations as we devote ourselves to a career of service. The
        transformative quest to become a physician imbues the learner with insights from within and
        outside the profession of medicine. As medical students, we hold ourselves accountable to
        our patients, and we critically evaluate the state of our profession as we see it with new
        eyes.
        This issue of 
        PLoS Medicine heralds the start of the Student Forum section, a space
        where medical students from across the world can exchange ideas about the critical issues
        affecting health and health care from their unique perspective. It is appropriate, then, in
        this inaugural Student Forum, that we consider some of the key issues of information
        exchange in the context of global disease and medical education. How do we access
        information? How is that information shaped and influenced? How can we shape the content of
        the debate?
      
      
        Open Access to the Medical Literature: Critical for Our Future
        As physicians-in-training, we appreciate that the medicine we will practice will differ
        from that practiced by our predecessors. Mastery of access to information now trumps
        mastery of facts, and we enter an era of medical practice that will develop synergistically
        with biomedical research. Our generation of physicians will face the daunting task of
        translating an exploding body of biomedical and clinical research into medical practice.
        This task increasingly relies not only on the ability to access all the available medical
        literature, but on the ability to mine and synthesize it. PLoS represents one of a number
        of possible approaches toward ensuring free and open access to the world's literature [3],
        and students should be at the forefront of this movement.
        As the democratization of access brings developed and developing countries together, the
        very content of the research published should follow suit. Too often the thoughts and
        opinions of those in the rich world have dominated research agendas and the content of
        medical journals. For example, less than 8% of articles published in the six leading
        tropical medicine journals in 2000–2002 were generated exclusively by scientists from
        developing countries [4]. Bringing the entire world of learners into a global conversation
        should be accompanied by a shift in editorial priorities to ensure that medical journals
        inform practice in a way that offers the greatest benefits to those suffering the greatest
        burdens.
      
      
        The Influence of the Pharmaceutical Industry
        Journal editors and publishers pride themselves on publishing high-quality research
        subjected to rigorous peer review. Indeed, this model has contributed to significant
        advances in the standards for clinical investigation and rightly drives much of clinical
        practice. However, without the least bit of acknowledged irony, medical journals have begun
        publishing articles about how pharmaceutical industry advertising and marketing drives
        clinical practice in nonrational ways while at the same time publishing the very
        advertisements they critique.
        The American Medical Student Association (AMSA), which has entered into a formal
        partnership with 
        PLoS Medicine , accepts no pharmaceutical industry funding and encourages
        its members to eschew the industry largess through its “PharmFree” initiative
        (www.amsa.org/prof/pharmfree.cfm). Similar initiatives are afoot in medical student
        organizations around the world as we consider appropriate relationships with industry.
        Surely it is time that journals examine the content of their pages and stop serving as
        vehicles through which physician practice is influenced by industry. 
        PLoS Medicine is the first major international general medical journal to
        refuse to advertise medical drugs or devices [5]. We hope that other major journals will
        follow suit.
      
      
        The Need for Student Ideals and Ideas
        Throughout the world, students represent the leading edge of social change. Indeed Paulo
        Freire, one of the most influential educational thinkers of the late 20th century,
        describes education as the “practice of freedom,” as learners continuously work to
        integrate action and reflection in a spiral of deepening connections. This process of
        integration is rooted in a deep-seated longing for the possible, informed by the lived
        experience of the painful. Nowhere is this more true than in medicine, where the suffering
        of our patients underpins our scientific inquiry, our clinical development, and our
        personal growth as healers.
        As we students learn medicine, we begin to ask hard questions about the proximal—and the
        distal—causes of our patients' suffering. The reductionistic logic of medical education
        offers only false comfort, and only to those who content themselves with pathways and
        receptors. Learners want to understand what led to the derangement of the pathway, of the
        receptor. Genes? Environmental factors? Economics? In asking, we bring a sense of
        possibility, of hope, to the process. What 
        could be? What tools do we need to address these broader causes? And we
        hope that thinking about these questions from a student's view may offer our senior
        colleagues a chance to reflect on their own work as physicians and their own place in our
        world.
        We hope this space emerges as a vibrant source of student energy and passion, grounded
        in evidence, accountable to the patients who will entrust their care to us, and reflecting
        the hope we have for the future of medicine.
      
      
        Toward the Future
        As we come together from three continents to inaugurate the Student Forum in 
        PLoS Medicine , we are committed to ensuring that this section of the
        journal remains a source of vigorous debate and thoughtful analysis. The section will
        initially be published quarterly, and will contain essays of up to 1,000 words that address
        any health-related issue from a medical student perspective (see Box 1).
        The 
        PLoS Medicine medical student advisory group, which currently has
        representatives from the American Medical Student Association, the Indian Medical Students'
        Organization, and the European Medical Students' Association, will be involved in selecting
        and editing essays for publication.
        The Student Forum will be freely available for all to read and critique, free from
        inappropriate financial interests, and committed to the drive toward justice that student
        ideals embody. We look forward to helping shape this forum in the best traditions of free
        exchange, and we look forward to collaboration with our senior colleagues to hold us to
        these standards.
      
    
  

  
    
      
        
        One of the most fundamental human rights is the assumption that each person matters, and
        everyone deserves to be treated with dignity—this is the tenet from which all other human
        rights flow. Another is that those who are most vulnerable deserve special protection.
        However, in many developing countries, vast numbers of children are born but never counted,
        and their health and welfare throughout their lives remains unknown. And because
        single-mean measures of population health mask inequalities among the best-off and
        worst-off, the health of vulnerable populations is not effectively documented and
        acknowledged. Health information systems can play an important role in supporting these
        rights by documenting and tracking health and health inequities, and by creating a platform
        for action and accountability.
        A human rights approach to health information systems also supports effective health
        development. To effectively improve population health, governments and communities need
        access to socioeconomically disaggregated population health data. Because the relationship
        between such information and human rights has received little attention, the two areas of
        health information systems and human rights have done little to support each other.
      
      
        The Health Metrics Network
        The Health Metrics Network (HMN) is a global collaboration focused on strengthening
        country health information systems to generate sound data for decision-making at country
        and global levels (see http://www.who.int/healthmetrics/en/). Its interim secretariat is
        based at the World Health Organisation. The development group of HMN recently considered
        strategies for strengthening health information systems within countries. The Equity
        Working Group of HMN made recommendations that outlined the content of equity-sensitive
        information systems, identified opportunities for minimizing collection burdens, and
        suggested strategies to foster an equity-oriented decision-making culture. Although the
        recommendations were implicitly focused on human rights and on improving opportunity for
        the worst-off populations, making that framework explicit helps us to acknowledge and
        clarify the values on which decisions for developing information systems are made. The
        framework is based on several principles rooted in human rights and their implied actions
        (see Sidebar).
      
      
        Every Individual Matters
        Despite the acknowledged importance of counting everyone, only 57 of the 192 WHO member
        states, almost all of which are developed countries, have vital registration systems that
        report on births and deaths for at least 90% of the population. Consequently, a primary
        recommendation from the Equity Working Group is that health information systems should
        support the most basic acknowledgment of human rights—one's existence—by counting births
        and deaths in every country through a vital registration system (Figure 1).
      
      
        Opportunities and the Means to Health
        A person's human rights are recognized to include “a standard of living adequate for the
        health and well-being of himself and of his family, including food, clothing, housing, and
        medical care and necessary social services, and the right to security in the event of…lack
        of livelihood in circumstances beyond his control” [1]. In other words, the means necessary
        to achieve health are part and parcel of a right to health. However, many country studies
        have repeatedly shown that there are large gaps between the opportunities of advantaged and
        disadvantaged sub-populations. For example, in South Africa, average household expenditure
        for whites was five times the rate for blacks in 1995, and female children in Bangladesh
        were less likely to be brought to clinic than their male counterparts [2]. Examples
        demonstrating inequalities between various economic, ethnic, gender-based, and other social
        groups are replicated time and time again in all countries [2,3].
        Of 192 countries, 39 have a health information system sufficient to support basic
        analysis of socioeconomic inequalities and health. Such a health information system would
        include the vital registration system coupled with a major household survey. Ninety
        countries have only a census, an old household survey, or no data at all, whereas the
        remaining countries fall into a “middling” category (a census and a recent household
        survey, or two household surveys) (L. Bambas, P. Braveman, N. Dachs, I. Delgado, E.
        Gakidou, et al., unpublished data).
        The two issues of health inequalities and human rights overlap in a number of ways that
        enrich a perspective on health as a human right. A human rights framework can inform a
        minimally acceptable level of data collection in health information systems—that is, a core
        set of equity indicators—as well as the conditions surrounding the release and use of that
        data. In contrast to individual measures of health inequalities, populations as units of
        analysis are especially useful for examining achievement of human rights (as well as
        identifying health inequities) since they indicate patterns of differential opportunity in
        society among various social strata. A human rights-oriented indicator of health
        inequalities, then, would be composed of two measures: (1) A health measure, including
        health status, health care, and other determinants of health or the social/ economic
        consequences of ill health; and (2) A measure of social position/advantage (also called an
        equity stratifier or social stratifier) that defines strata in a hierarchy, e.g., income or
        economic assets, education, sex, or ethnic group.
        Health differences across social strata in comparison with the most advantaged group or
        an absolute standard suggest inequities in health [4,5]. This interpretation of health
        inequities directly coincides with a human rights framework since it focuses on a broad
        concept of health as well as on means to and opportunities for health. Critical issues of
        methodology for using these indicators also need attention, including identification of
        appropriate measures of social position and how to meaningfully compare indicators across
        countries.
        Consequently, the Equity Working Group recommended that health information systems
        should include the information necessary to create equity indicators, and that research
        into methodologies should be given due attention. On a practical level, equity strata and
        health indicators can be integrated into a number of existing data sources, including
        censuses, vital registries, household surveys, small areas data, and administrative data
        sources. Such empirical information not only clarifies distributions of health and
        achievement of rights but can also identify barriers to health and provide insights on
        multi-sectoral approaches in planning and interventions to support the most vulnerable
        populations.
      
      
        Accountability and Autonomy
        When equity-sensitive information is collected, access to information is often
        restricted to the government and is rarely disaggregated to show differences between
        socioeconomic groups. But a human rights approach to health information implies not only
        particular content but also mechanisms to promote the effective use of information,
        including the public release of data in a useful form.
        Confidentiality and privacy issues arise in relation to information disaggregated by
        equity stratifiers, especially in the context of the public release of such data. There are
        strong arguments for the public release of health equity information, including the fact
        that such information is a determinant of health, and that civil society can play a vital
        role in improving health opportunities, both directly and by influencing governmental
        priorities. The public's general lack of knowledge regarding patterns of health
        inequalities and their causes within societies further supports the need for the public
        release of such information.
        If information is disaggregated for very small populations, such as within a village,
        particular individuals or households may be identifiable and feel their privacy rights were
        impinged upon. Therefore, the public release of equity-oriented information on health
        should be explicitly planned for in the development of health information systems.
        Principles to guide release of disaggregated data should be followed, and communities
        should have a voice in the decision to release highly disaggregated information when
        privacy rights might be compromised. The HMN Equity Working Group suggests development of
        international standards for collection and sharing of disaggregated data and its use. Given
        the potential conflict between the two interests, we should continue to investigate
        possible technologic and/or policy solutions.
      
      
        Mutual Responsibility
        Another mechanism for promoting the effective use of information is to support cultures
        of equity-oriented decision-making. In addition to the public release of information,
        strategies should include supporting research on pathways of health inequities and
        interventions; building capacity for analyzing information and developing interventions;
        encouraging demand for equity-sensitive data in government and the public; and supporting
        broad participation in the promotion of health equity.
        Standards for improvements and target dates for achieving a minimally acceptable
        information system should take into account differences in the resources and needs of those
        implementing changes. The Equity Working Group recommended that target dates be developed
        with countries to define and integrate core equity indicators into routine information
        sources. All countries should be able to achieve, within the next 5–15 years, at least the
        middle-level information system, which could be considered a minimum standard. However, the
        rationale for this standard is predicated on the assumption of significant financial and
        technical support being given to the effort, and on the least well-off countries receiving
        the most support (L. Bambas, P. Braveman, N. Dachs, I. Delgado, E. Gakidou, et al.,
        unpublished data).
      
      
        Conclusion
        These recommendations provide a strategy for strengthening decision- and policy-making
        by providing a stronger empirical base for human rights considerations. This
        equity-oriented empirical base could strengthen health rights, not only through health
        sector decision-making, but also through decision-making in sectors related to
        determinants.
        The initial development stage of the HMN has now ended, and hopefully the refinement and
        implementation stages of the effort will begin within the next year. Regardless of the
        existence of a centralized effort, countries would greatly promote health rights by
        integrating equity issues into health information systems, releasing that information
        publicly, and supporting participation and decision-making attentive to the concerns of
        health equity and human rights.
      
    
  

  
    
      
        Introduction
        Advances in transplant-based therapy for type 1 diabetes mellitus [1] and a dearth of
        cadaveric pancreatic islets of Langerhans have focused interest on developing renewable
        sources of transplant-ready islet-replacement tissues. Pancreatic islets derive from
        embryonic endoderm, but display features of neurons, including a shared set of
        cell-autonomous developmental regulators [2]. Islets are the principal source of insulin in
        humans, but in some invertebrate species, such as 
        Drosophila, brain neurons are the main source of circulating insulin
        [3,4]. The similarities between islet cells and neurons are further underscored by the
        demonstration of insulin gene transcription in the vertebrate brain [5], although it
        remains unclear whether these vertebrate neurons produce or secrete insulin protein [6].
        Moreover, recent studies also suggest that in some settings, mouse pancreatic epithelial
        cells can engender neuron-like cells [7,8]. These and other findings [2] suggest that
        common, ancient developmental programs may govern the differentiation of islet cells and
        neurons. Methods promoting neural differentiation by embryonic stem (ES) cells have been
        adapted [9,10,11,12,13] to derive insulin-producing cells (IPCs), but it remains unclear
        whether such IPCs were derived from neural progenitors, or whether a neural-based strategy
        can generate transplantable human IPCs.
        Neural cells derive from multipotent progenitor cells, and recently, clonal human neural
        progenitor cells that remained multipotent and karyotypically stable through at least 15
        passages were purified by flow-cytometry-based methods [14]. The availability of human
        neural progenitor cells allowed us to investigate whether inductive signals involved in
        normal pathways of islet development could direct these neural progenitors to develop into
        glucose-responsive IPCs. Here we describe experimental strategies for developing IPCs
        directly from human neural stem cells.
      
      
        Methods
        All studies were performed in accordance with appropriate human participants' approval
        according to Stanford University Institutional Review Board guidelines.
        
          Cell Lines and Culture Conditions
          The human neurosphere (NS) cell lines (line numbers 1651, 1664, and 1673) were
          established following enrichment for AC133
          + cells, as previously described [14], although none of the cell lines
          was expanded from a single cell, and they are therefore not clonal. The different cell
          lines were from distinct donors and karyotypically normal. They gave comparable results,
          but line 1664 produced less insulin by stage 4 in our protocol than the other cell lines.
          Following cell thaw, NSs were expanded in suspension culture, and in each case were used
          between passages 5 and 7. Undifferentiated human NS cells (stage 1) were cultured and
          expanded in medium containing X-VIVO15 (Cambrex, Walkersville, Maryland, United States),
          N2 supplement (Invitrogen, Carlsbad, California, United States), 2 μg/ml heparin, 10
          ng/ml leukemia inhibitory factor (Chemicon, Temecula, California, United States), 20
          ng/ml epidermal growth factor (R&D Systems, Minneapolis, Minnesota, United States),
          and 20 ng/ml fibroblast growth factor-2 (R&D Systems), and they were frozen after
          enzymatic dissociation. At stage 2, we thawed and cultured 10
          7 cells in medium made from a 1:1 mixture of glucose-free DMEM
          (Invitrogen) and F-12 medium (Invitrogen) containing 0.2% bovine serum albumin (Sigma,
          St. Louis, Missouri, United States), N2 supplement, 2 μg/ml heparin, 10 ng/ml leukemia
          inhibitory factor, 20 ng/ml epidermal growth factor, and 20 ng/ml fibroblast growth
          factor-2. The final concentration of glucose in stage 2 was 5 mM. Cultures were fed
          weekly for up to 2 wk. Cells formed NSs during stages 1 and 2. After 2 wk, we transferred
          40–80 NSs containing a total of approximately 2–4 × 10
          5 cells to single wells in plates coated with 0.005% poly-L-ornithine
          (Sigma) and 3 μg/ml fibronectin (Invitrogen). These were cultured for 2 wk (stage 3) in a
          1:1 mixture of low-glucose DMEM and F-12 media containing 100 mg/l apo-transferrin
          (Sigma), 288 mg/l glucose, 73 mg/l 
          L -glutamine, 1.69 g/l sodium bicarbonate, 25 μg/ml bovine insulin
          (Sigma), 20 nM progesterone, 100 μM putrescine, 30 nM sodium selenite, and
          penicillin/streptomycin. Stage 3 medium was supplemented with 2 μM all-
          trans retinoic acid (RA) (Sigma) and had a final glucose concentration
          of either 5 or 17 mM. At stage 4, cells were cultured in N2 medium [10] supplemented with
          10 mM nicotinamide (Sigma) and 10 nM insulin-like growth factor-1 (R&D Systems) for
          up to 6 d. Stage 4 medium included 17.3 mM glucose. During stage 3 and 4, medium was
          changed every other day. To examine the effect of Sonic hedgehog (Shh) (R&D Systems),
          we added 300 nM mouse Shh in PBS every other day during stage 4. To quantify neurite
          outgrowths per cluster, 20 Shh-treated or control stage 4 IPC clusters were photographed
          under light microscopy, and digitized images were used to score neurites. Data are
          presented as the average ± the standard error of the mean. Two-tailed 
          t tests were conducted to determine statistical significance.
        
        
          RT-PCR
          Total RNA was prepared by using TRIZOL (Invitrogen) and RQ1 RNase-free DNase (Promega,
          Madison, Wisconsin, United States). For cDNA synthesis, random sequence primers were used
          to prime reverse transcription reactions and synthesis was carried out by Thermoscript RT
          (Invitrogen). A total of 35 cycles of PCR were performed using Platinum 
          Taq High Fidelity DNA Polymerase (Invitrogen). GAPDH expression was
          used to normalize input template cDNA to analyze relative gene expression. Primer
          sequences and gene accession numbers are listed in Table 1. To confirm their identity,
          DNA products from PCR reactions were excised, cloned, and sequenced.
        
        
          Immunohistochemistry and Morphometry
          Cell clusters were fixed in 4% paraformaldehyde, embedded in HistoGel (Richard-Allan
          Scientific, Kalamazoo, Michigan, United States), and then embedded in paraffin. We
          performed immunohistochemistry on 6-μm tissue sections. We used primary antibodies at the
          following dilutions: guinea pig anti-insulin, 1:200 (Linco Research, St. Charles,
          Missouri, United States), mouse anti-β-tubulin III, 1:500 (Sigma); rabbit anti-C-peptide,
          1:100 (Linco Research); mouse anti-Nestin, 1:200 (Chemicon); mouse anti-Ki67, 1:100
          (Novocastra, Newcastle, United Kingdom); rabbit anti–cleaved caspase-3, 1:200 (Cell
          Signaling, Beverly, Massachusetts, United States); rabbit anti-glucagon, 1:200 (Dako,
          Carpinteria, California, United States); rabbit anti-Glut-2, 1:200 (ADI, San Antonio,
          Texas, United States); mouse anti-proinsulin, 1:500 (O. Madsen, Hagedorn, Gentofte,
          Denmark); mouse anti-glucokinase, 1:200 (C. Newgard, Duke University, Durham, North
          Carolina, United States); mouse anti-GFAP, 1:1,000 (Dako); mouse anti-MAP2, 1:500
          (Sigma); and rabbit anti-Olig2, 1:1,000 (H. Takebayashi, National Institute for
          Physiological Sciences, Okazaki, Japan). Confocal immunofluorescence microscopy with an
          optical slice thickness of 0.6 μm was performed on a Bio-Rad (Hercules, California,
          United States) MRC1000.
          Cell counting and point-counting morphometry were performed using standard
          morphometric techniques [15]. To obtain representative results, all quantification of
          immunostaining was performed by counting numbers of positive-stained cells and dividing
          by the area of total tissues using a standard 10 × 10 microscope grid. For quantification
          of cells expressing Nestin, Ki67, β-tubulin III, activated caspase-3, insulin, or
          C-peptide, NS-derived cell clusters were fixed, and sectioned to generate 7-μm-thick
          tissue sections. Appropriately stained cells were counted in a minimum of ten random
          microscopic fields obtained from at least ten cell clusters per condition.
        
        
          In Situ Hybridization
          We used human insulin cDNA (320 bp) cloned in plasmid pCR4-TOPO (Invitrogen) as a
          template for in vitro transcription to produce riboprobes with a digoxigenin-RNA labeling
          kit (Ambion, Austin, Texas, United States), performed hybridization with IPC cluster
          sections as described [16], and used DIG Nucleic Acid Detection Kit (Roche, Indianapolis,
          Indiana, United States) for development according to the manufacturer's instructions.
        
        
          Insulin C-Peptide Quantification and In Vitro Insulin Secretion Assay
          At each stage, 300 NS or IPC clusters were handpicked, washed with PBS, and
          homogenized; then intracellular C-peptide content was measured with a C-peptide ELISA kit
          (American Laboratory Products Company, Windham, New Hampshire, United States). Stage 4
          IPC clusters were cultured in glucose-free RPMI (Invitrogen) supplemented with 2.8 mM
          glucose, 20 mM HEPES, and 10% newborn calf serum (Invitrogen) for at least 2–3 h. At the
          end of these washes, no insulin was detectable in the wash supernatant using an ELISA kit
          (American Laboratory Products Company). Fifty IPC clusters were handpicked, transferred
          to a 24-well plate, and incubated for 2 h at 37 °C in the presence of 2.8 mM glucose, 25
          mM glucose, or 25 mM sucrose. Supernatants were then harvested for enzyme-linked
          immunosorption assay (ELISA)–based quantification of released insulin.
        
        
          IPC Transplantation and Physiologic Tests
          All animal studies were performed in accordance with Stanford University Animal Care
          and Use Guidelines. Under general anesthesia, 9- to 10-wk-old male NOD 
          scid mice (purchased from Jackson Laboratories, Bar Harbor, Maine,
          United States) were engrafted with 1,000 handpicked IPC clusters in the right and left
          subcapsular renal space (500 IPC clusters each) or received a sham transplant of saline
          solution. Transplantation of more IPC clusters in a renal graft site was not feasible
          because of the size of the clusters. Two weeks after IPC cluster transplantation, we
          subjected mice to an overnight fast. Then we measured serum human C-peptide before and 30
          min after intraperitoneal injection with 3 g of glucose per kilogram of body weight using
          a human C-peptide ELISA kit. When stage 4 IPC clusters were transplanted, tumors were not
          observed 4 wk after engraftment, the maximum period of observation (
          n = 5). All data represent the average (from the indicated number of
          samples) ± standard error of the mean. Two-tailed 
          t tests were conducted to determine statistical significance.
        
      
      
        Results
        
          Glucose Restriction Initiates IPC Development
          Culture media with high glucose concentrations have been useful for isolating and
          maintaining progenitor cell populations from the central and peripheral nervous systems
          [14,17] and for permitting differentiation by neurogenic cells [18]. We routinely
          expanded NS cultures in medium containing 37 mM glucose, a level 5- to 6-fold higher than
          physiologic glucose concentration. Isolated human neural progenitors spontaneously
          aggregate in these conditions to produce a cell cluster called a NS, and 85%–90% of NS
          cells produce the intermediary filament Nestin (Figures 1 and 2), a putative marker of
          neural stem and progenitor cells [19]. Prolonged exposure to high glucose levels can also
          severely reduce insulin expression in pancreatic β-cells [20]. Thus, we postulated that
          glucose reduction might blunt neurogenic programs by NSs and promote alternate fates,
          including development of IPCs. Therefore, we switched NS culture conditions from high
          glucose (37 mM, “stage 1”) to low glucose (5 mM, “stage 2”; see Figure 1A) to initiate
          development.
          To assess developmental changes resulting from glucose reduction and later culture
          modifications, we examined NS cell composition, and expression of gene products known to
          regulate or define in vivo fates of embryonic neural or islet cells. A β-tubulin III
          isoform produced in differentiating neurons was not detected in stage 1 cells, whereas
          expression of Ki67, an S-phase-associated nuclear antigen, was expressed in the majority
          of stage 1 cells (see Figure 2). Genes specifying transcription factors essential for in
          vivo differentiation of lineage-restricted neural progenitors and their differentiated
          progeny [21,22,23], including 
          En1, Hb9, Isl1, Hoxc6, NRSF/REST, and 
          Nkx6.1 , were expressed at low or undetectable levels in stage 1 (see
          Figure 1B–1C), consistent with the reported multipotency of proliferating NSs grown in
          high glucose [14].
          After 2 wk at stage 2, NS-derived cells remained in clusters (see Figure 1A), but
          Nestin and Ki67 were expressed in fewer than 35% of cells (see Figure 2), indicating
          possible differentiation of cells toward lineage-restricted fates. Consistent with this
          view, we detected increased expression of 
          Nkx6.1, NRSF/REST, and β-tubulin III (see Figures 1B and 2). We also
          detected increased expression of 
          Neurogenin3 (ngn3) (see Figure 1C), a transcription factor expressed in
          differentiating neurons, and required for development of pancreatic islet progenitor
          cells [24,25,26]. 
          Nkx6.1 and 
          ngn3 expression were never observed in NS cultures maintained in 37 mM
          glucose (see Figure 1B; data not shown). These data suggest that glucose restriction
          initiates developmental programs known to promote differentiation of mammalian neural and
          neuroendocrine cells. As shown below, insulin expression in NS-derived cells required
          culture in low-glucose medium at stage 2, but we did not consistently detect insulin
          + cells by immunohistochemistry until stage 4.
        
        
          RA Promotes IPC Development
          RA induces development of primitive endodermal cells from a subset of embryonal
          carcinoma cell lines, and is an endogenous signal that directs development of posterior
          organs like the pancreas from embryonic endoderm [27,28]. In vivo exposure to RA is
          sufficient to induce ectopic development of insulin-expressing tissue in the anterior
          foregut [27]. Moreover, stage 2 NS-derived cells expressed RA receptors including RAR-α
          and RAR-γ (see Figure 1C), indicating competence for RA signals. To investigate whether
          RA could stimulate development of NS-derived cells toward an IPC fate, we measured
          expression of 
          Pdx1, Cdx1, and 
          FoxA3, transcription factors that regulate gastrointestinal organ
          development [29,30]. 
          Pdx1, Cdx1, and 
          FoxA3 expression was induced in “stage 3,” when cell clusters were
          allowed to adhere to pre-coated wells at a density of 40–80 NSs/cm
          2 and cultured in low-glucose medium with 2 μM RA for 2 wk (see Figure
          1A and 1C). We then tested other concentrations of RA and other plating densities. NS
          cells exposed to 100 nM RA at 20, 40, or 80 NSs/cm
          2 and NS cells exposed to 2 μM RA at a plating density of 20 NSs/cm
          2 did not express these markers or insulin at a later stage (“stage
          4”; Figure 3A; data not shown). Thus, higher doses of RA induced markers of posterior
          fate in NS-derived cells, a dosage response like that seen during RA patterning of
          posterior gut development in vivo. Moreover, this response was sensitive to plating
          density, revealing that additional cell-non-autonomous factors may regulate this
          response. Mesoderm develops in close association with gastrointestinal endoderm both in
          vivo and in vitro [31,32]. Thus, detection of gut markers like 
          Pdx1 and 
          FoxA3 in NS-derived cell cultures raised the possibility that
          mesodermal cells might co-develop. However, we did not detect expression of known markers
          of mesoderm formation, including 
          brachyury, flk-1, myosin light chain-2, and 
          β-globin, at any stage in our cultures (see Figure 1B). By contrast,
          mesoderm development invariably accompanies endoderm development in embryoid bodies
          derived from human ES cells or embryonic germ cells [31,32,33]. In stage 3 cultures,
          Nestin and Ki67 expression was nearly extinguished (see Figure 2), while expression of 
          En1, ngn3, and 
          Hb9 increased (see Figure 1B and 1C). While their numbers increased
          slightly during stage 3, β-tubulin III
          + cells composed fewer than 10% of cells in NS-derived clusters (see
          Figure 2), suggesting that the majority of differentiated cells had acquired non-neural
          fates. Consistent with this interpretation, we also did not detect expression of
          homeodomain proteins Hoxc6 or Hoxb9 (see Figure 1B; data not shown), which are induced by
          RA in spinal cord neuronal precursor cells [21].
          RA can activate signaling through the Hedgehog (Hh) pathway, and recent studies show
          that Hh signals control development of embryonic pancreatic islets in vivo [16,34]. To
          elucidate the mechanisms underlying RA-induced expression of markers like 
          FoxA3 and 
          Pdx1, we examined Hh signaling during IPC development. We detected
          expression of the Hedgehog receptors 
          Patched (Ptc) and 
          Smoothened (Smo) at all stages (Figure 3B), consistent with the
          possibility that NS-derived cells are competent for Hh signaling. However, we did not
          detect NS expression of genes encoding Hh ligands like 
          Shh (Figure 3B), similar to previous studies [19]. Increased 
          Ptc transcription is a known consequence of Shh signaling, but we did
          not detect changes of 
          Ptc expression during stages 1–4, consistent with the absence of
          detectable expression of Hh ligands. To test directly whether the absence of Hh signaling
          was required for expression of endodermal or islet markers, including 
          FoxA3, Pdx1, and insulin, we added Shh protein to stage 4 cultures,
          which do not express 
          Shh or genes encoding other Hh ligands like 
          Desert hedgehog or 
          Indian hedgehog (Figure 3C). At 300 nM, a dose used to induce
          neuronal development in ES cell cultures [21], Shh produced a significant increase in
          neurite outgrowth in NS-derived cells (Figure 3D and 3E; see Methods), and resulted in
          increased 
          Ptc expression (Figure 3F). Moreover, Shh treatment during stage 4
          eliminated expression of 
          Pdx1, FoxA3, and insulin (Figure 3F). These data are reminiscent of
          prior studies demonstrating that excess endodermal Shh signaling disrupts pancreas
          development in vivo [16,34]. Thus, in contrast to the response of ES cells [21], RA
          treatment of NS-derived cells does not stimulate endogenous Hh signaling. The lack of Hh
          signals in NS-derived cells blunts neural differentiation and permits differentiation
          toward endocrine-like cell fates.
        
        
          Insulin Expression by IPCs
          
          ngn3, Pdx1, and 
          Hb9 are essential factors for mammalian β-cell development [22], and
          their expression in stage 3 suggested the potential for deriving IPCs. Insulin expression
          has not, to our knowledge, been reported in NS-derived cells [19,35]. Moderate
          hyperglycemia is a potent stimulus for β-cell differentiation and expansion in vivo and
          in utero [20], so we tested whether exposure of stage 3 cells to elevated glucose levels
          could induce insulin expression. Semi-quantitative RT-PCR measures of insulin mRNA
          expression showed that exposure of cultures grown in 5 mM glucose at stage 3 to 17 mM
          glucose at stage 4 induced insulin expression (Figure 3A). Growth of cells at stage 2 in
          5 mM glucose followed by exposure in stages 3 and 4 to 17 mM glucose also resulted in
          insulin expression at stage 4, but at lower levels. By contrast, no insulin was expressed
          by cultures maintained in 5 mM glucose or 17 mM glucose throughout stages 2–4 (Figure
          3A). Thus, reduction of glucose in stage 2 growth medium followed by a later increase in
          glucose at stage 3 or 4 was essential for development of IPCs. Insulin-like growth
          factor-1 may promote insulin secretion and prevent apoptosis of β-cells in vivo [36], and
          addition of this factor at stage 4 reduced apoptosis approximately 10-fold (see Figure
          1A; data not shown). Nicotinamide, which can stimulate in vivo pancreatic endocrine cell
          differentiation [37], did not affect cell survival, but did increase insulin C-peptide
          expression approximately 1.5- to 2-fold (data not shown). Thus, simultaneous exposure to
          high glucose levels, insulin-like growth factor-1, and nicotinamide optimized insulin
          expression at stage 4. Comparable results were obtained with three independently derived
          NS lines.
          Coinciding with the onset of high insulin expression levels in mouse islets, 
          ngn3 expression is virtually extinguished [25], while expression of 
          Pdx1, FoxA3, Isl1, and 
          Nkx6.1 in islet β-cells is maintained or increased [22]. Similarly, we
          noted that IPC maturation from stage 3 to 4 was accompanied by increased insulin
          expression, reduced 
          ngn3 expression, and increased or maintained levels of 
          Isl1, FoxA3, and 
          Pdx1 expression (see Figure 1B and 1C). Thus, tissues derived from
          neural progenitor cells express insulin, 
          Pdx1, and 
          FoxA3, markers typically co-expressed in foregut-derived tissues.
          Expression of these markers suggests differentiation of some neural progenitor cells
          toward an endoderm-like fate. We were unable to detect expression of Pdx1 and FoxA3 using
          immunohistochemical methods in stage 4 IPCs, (data not shown). We also did not detect
          expression of Nkx6.1 or islet amyloid polypeptide, markers of mature pancreatic β-cells
          (see Figure 1C; data not shown). Thus, the sequence of gene expression accompanying
          formation of IPCs from NSs was similar but not identical to that described for
          differentiating pancreatic β-cells, and further molecular studies are required to
          determine the degree of endoderm-like differentiation of cells in IPCs.
          We detected expression of insulin protein by immunohistochemistry in 90% of stage 4
          cell clusters (Figure 4). An average of 26% of cells in stage 4 clusters were insulin
          + , and appeared healthy, with abundant cytoplasm and a well-defined
          nucleus delineated by the nuclear stain 7AAD (Figure 4A). Of the cells composing stage 4
          clusters, 45% were β-tubulin III
          + , none of which contained insulin (Figure 5A–5C). Insulin
          + cells showed little or no co-expression of neural stem cell markers
          like Nestin (see Figure 1), or other markers of neural lineages known to develop from
          human neural progenitors, like MAP2 (differentiated neurons; Figure 5D–5F), Olig2
          (multipotent precursors, bipotent glial precursors, and oligodendrocytes; Figure 5J–5L;
          see [38]), or myelin basic protein (data not shown). Occasional rare insulin
          + cells expressed detectable GFAP (a marker of astrocytes; Figure
          5G–5I). Thus, while we cannot completely rule out the possibility that some neural cell
          types expressed insulin, these data suggest that nearly all insulin
          + cells produced in stage 4 cultures were non-neuronal.
          Nuclei in insulin
          + cells were not stained by an antibody to Ki67 (see Figures 2 and
          4B), suggesting that insulin
          + cells at stage 4 are post-mitotic, like mature pancreatic β-cells.
          Immunostaining with specific antibodies revealed proinsulin and C-peptide, an internal
          portion of the proinsulin translation product, in the cytoplasm of all insulin
          + cells (see Figure 4D–4G and 4O), supporting our conclusion that
          insulin protein was produced in stage 4 IPCs. In situ hybridization with antisense
          riboprobes specific for human insulin mRNA labeled 25%–30% of stage 4 cells (see Figure
          4H–4J), providing further evidence of insulin production. We observed only rare insulin
          + cells stained by antibodies specific for activated caspase-3
          (<0.3%; see Figure 4K) or TUNEL assay (data not shown). In stage 4 cell clusters we
          did not consistently detect expression of glucagon (see Figure 4L), pancreatic
          polypeptide, or somatostatin by immunohistochemistry. The absence of glucagon gene
          expression in our IPC cultures was further confirmed by RT-PCR (data not shown). Thus,
          like in pancreatic islets, insulin was the principal hormone produced by IPCs. However,
          the composition of IPCs was distinct from that of pancreatic islets, and further studies
          are required to determine the basis for this difference.
          To quantify insulin expression in IPCs, we used a human insulin C-peptide-specific
          ELISA (see Figure 4P). By ELISA, we did not detect C-peptide in stage 3 or stage 4
          culture media, which contained supplementary bovine insulin. Thus, use of a C-peptide
          assay did not detect medium-derived bovine insulin in measures of IPC insulin content
          [39]. C-peptide levels at stages 1–3 were low or undetectable (see Figure 4P), consistent
          with the lack of immunostainable insulin (see Figure 2) in cells at these earlier stages.
          Similarly, in cultures maintained in 17 mM glucose during stages 1–4 (“HHHH”), C-peptide
          levels were not detectable (see Figure 4P). In cell clusters exposed to 5 mM glucose in
          stages 2–3 then switched to 17 mM glucose at stage 4 (“HLLH”), insulin C-peptide levels
          were 0.36 fmol per cluster, and the average number of C-peptide
          + cells per cluster was 200. Thus, we estimate 0.0018 fmol C-peptide
          per NS-derived IPC. A single β-cell contains approximately 0.6 fmol C-peptide and insulin
          [40], so we calculate that insulin C-peptide content in one stage 4 IPC is approximately
          0.3% of the level in isolated human β-cells. Collectively, these experiments confirm that
          IPCs transcribe and translate insulin, and rule out that insulin measured in IPCs is
          derived from medium, as shown in another system [39].
        
        
          IPCs Are Glucose Responsive
          Glucokinase and glucose transporters like Glut-2 are essential regulators of glucose
          responses in pancreatic β-cells, and expression of these regulators in stage 4 insulin
          + cells (see Figures 1C, 4M, and 4N) suggested that IPCs could sense
          and respond to glucose. To test whether IPCs respond appropriately to glucose
          stimulation, we measured insulin release in static batch in vitro assays. We found that
          IPC insulin release increased markedly following a step increase of glucose from 2.8 to
          25 mM (see Figure 4Q). To examine whether IPCs are similarly responsive to glucose
          stimulation in vivo, we performed a series of IPC grafting experiments in
          immunocompromised recipient mice. Two weeks following engraftment (1,000 IPC-containing
          clusters/mouse), recipient mice and sham-transplanted controls were fasted overnight, and
          human C-peptide levels measured. Human C-peptide was undetectable in mouse sera prior to
          glucose challenge, or following glucose challenge of sham-transplanted controls (see
          Figure 4R). In contrast, 30 min after intraperitoneal glucose challenge of IPC recipients
          we detected 6.1 ± 1.2 pmol/l human C-peptide (
          p < 0.001), approximately 0.5%–1% of serum C-peptide levels observed
          following engraftment of 2,000 human islet equivalents in a prior study [41]. Thus, IPCs
          respond to glucose challenge in vivo by releasing insulin C-peptide. Analysis of IPC
          graft sites 4 wk after transplantation revealed nests of transplanted cells without
          obvious tumor formation. Histologic analysis and immunohistochemical detection revealed
          that C-peptide-expressing cells persisted in the graft site (see Figure 4S and 4T). Thus,
          engrafted IPCs remained differentiated and survived up to a month following
          transplantation.
        
      
      
        Discussion
        There is widespread interest in developing tissue replacement strategies for treatment
        of human disorders like diabetes mellitus and Parkinson disease. While much attention has
        been focused on the promise of ES cells for tissue replacement, recent work suggests that
        neural stem cells, like ES cells, may have an unusually broad differentiation potential.
        For example, Gage and colleagues demonstrated the conversion of mouse neural stem cells to
        the endothelial lineage, indicating that plasticity is a bona fide property of cultured
        neural stem cells [42]. These results were also unexpected because endothelial cells and
        neuronal cells normally derive, respectively, from mesoderm and ectoderm, distinct
        embryonic germ layers. Here we show that human neural stem cells have a similarly broad
        differentiation potential, and that specific in vitro culture conditions can divert neural
        stem-derived cells from neural lineages toward a fate with endocrine and endodermal
        characteristics.
        Our study shows that application of endogenous signals governing pancreas development to
        human neural progenitor cells can generate glucose-responsive IPCs. Systematic variation of
        the identity, concentration, and sequence of these signals led to discovery of methods
        culminating in IPC formation. However, there are limitations to our findings that should be
        noted. First, our molecular analysis of differentiating IPCs showed that they remain
        distinct from mature pancreatic islet β-cells; in other words, the insulin
        + cells derived by our methods are not mature β-cells. For example,
        reverse transcriptase polymerase chain reaction confirmed that IPCs were enriched for a
        combination of gene products that approximate those expressed in developing pancreatic
        islet cells. However, the temporal sequence of expression of some of these products, like
        glucokinase, Glut-2, and Pdx1 does not precisely recapitulate that observed in the
        embryonic pancreas (reviewed in [22]), and transcription of other genes typically expressed
        in β-cells, like 
        Nkx6.1, was not detected in later stage IPCs (see Figure 1). Thus, the
        insulin
        + cells derived in this study are not bona fide β-cells. Further
        experimentation may elucidate the basis for these differences, revealing the extent of
        neural progenitor cell development toward an endocrine cell fate. Recent studies suggest
        that neural cells can be derived from adult pancreatic epithelium [7,8], adding to a
        growing body of data demonstrating numerous similarities in neural and pancreatic endocrine
        development (reviewed in [43]). In light of these similarities, we speculate that methods
        leading to the production of β-cell factors like Nkx6.1 in stage 4 clusters may enhance the
        β-cell-like qualities of neural-progenitor-derived insulin
        + cells, and thereby permit differentiation of tissues that more closely
        resemble endoderm-derived islet cells. Second, our immunohistochemical analysis shows that
        stage 4 insulin-expressing cells do not express markers like Nestin, β-tubulin III, MAP2,
        or Olig2, suggesting that these insulin
        + cells are non-neuronal, but we cannot formally rule out the alternate
        possibility that our methods produced neural cells capable of secreting insulin. Third, we
        have not demonstrated fully that the stimulus–secretion coupling apparatus in IPCs is
        similar to that in pancreatic islets. While insulin release by IPCs produced here appears
        to be glucose-sensitive, both in vitro and in vivo, future work should elaborate whether
        IPC insulin is stored in dense-core secretory vesicles, and whether secretogogues or
        secretion potentiators other than glucose (like amino acids or sulfonylureas) stimulate
        insulin release by IPCs, like in pancreatic islets. Lastly, we have not ameliorated glucose
        regulation in diabetic animal models with the human IPCs described here. Serum levels of
        insulin C-peptide expression achieved following IPC transplantation in glucose-challenged
        mice were less than 1% of normal (see Figure 4). Thus, we did not attempt IPC
        transplantations in overtly diabetic animals, since prior studies suggest that insulin
        production at 10% of normal (or greater) may be required to improve glucose regulation in
        diabetic patients and animal models [10,41]. Additional studies to test the impact of
        transplanted IPCs in animal models of diabetes therefore await production of IPCs capable
        of insulin secretion at levels higher than achieved here.
        Nevertheless, compared to other methods for IPC development from human stem cells
        [13,44], our methods produced insulin at the highest levels yet achieved from an
        expandable, human stem-cell-derived tissue. Multiple experimental approaches were taken to
        demonstrate that these IPCs transcribe, translate, and secrete insulin, and to rule out the
        possibility that insulin measured in IPCs derived from the culture media. In vitro studies
        demonstrate that IPCs release insulin in a glucose-responsive manner, like islets.
        Moreover, IPCs transplanted into mice remained differentiated and released circulating
        human insulin in a glucose-dependent manner. Thus, for the first time, we demonstrate
        moderately efficient production of glucose-responsive IPCs from an expandable population of
        human stem cells. Current islet transplantation methods require an estimated 5 × 10
        8 to 10
        9 β-cells per recipient [45]. From 10
        6 cells at stage 2, we produced an average of 200–400 clusters with
        approximately 2,000 cells per cluster; approximately 25% of these cells are C-peptide
        + and release 0.5%–1% of C-peptide secreted by β-cells. Based strictly
        on these yields, we would need to expand IPC production approximately 10
        5 -fold to meet the need for one transplantation. Undifferentiated NSs
        can be readily expanded through at least a dozen passages [14], suggesting sufficient cell
        numbers could be generated to “scale up” this protocol for transplant-based therapies.
        We did not exhaust all possible factor combinations in our study: we speculate that
        further method refinements may improve the efficiency of NS conversion into IPCs, as well
        as IPC insulin synthesis and stimulus–secretion coupling, the hallmark functions of mature
        β-cells. If so, then human neural stem cells may serve as a valuable model for elucidating
        the mechanisms and factors that regulate neuroendocrine cell differentiation. For instance,
        addition of glucagon-like peptide-1, TGF-β ligands, or other factors that potentiate β-cell
        maturation, growth, and insulin secretion [46,47,48] may improve the methods described
        here. Because our method is based solely on extracellular factor modulation, in the absence
        of genetic manipulations, it could serve as the basis for developing replacement islets
        from a wide range of human stem cells, including neural stem cells and ES cells.
      
      
        Supporting Information
        
          Accession Numbers
          The National Center for Biotechnology Information (www.ncbi.nlm.nih.gov/) accession
          numbers for the genes and gene products discussed in this paper are 
          brachyury (AF012130), 
          Cdx1 (NM_000209), 
          Desert hedgehog (NM_021044), 
          En1 (NM_001426), 
          flk-1 (AF035121), 
          FoxA3 (NM_004497), GAPDH (NM_002046), GFAP (NM_002055), glucokinase
          (M90299), 
          Hb9 (NM_005515), 
          Hoxb9 (NM_024017), 
          Hoxc6 (NM_004503), human insulin cDNA (J00265), 
          Indian hedgehog (NM_002181), 
          Isl1 (NM_002202), MAP2 (U01828), myelin basic protein (M13577), 
          myosin light chain-2 (X57542), Nestin (NM_006617), 
          ngn3 (AF234829), 
          Nkx6.1 (NM_006168), 
          NRSF/REST (U13879), Olig2 (NM_005806), 
          Pdx1 (NM_000209), 
          Ptc (U59464), RAR-α (NM_000964), RAR-β (BC060794), RAR-γ (NM_000966), 
          Shh (L38518), 
          Smo (AH007453), 
          β-globin (NM_000518), and β-tubulin III (BC000748).
        
      
    
  

  
    
      
        
        Most people believe that as societies advance economically they have higher levels of
        cardiovascular disease (CVD) and other noncommunicable disease (NCD) risks. However, a more
        detailed analysis of how parameters of economic development are associated with health
        outcomes as well as NCD risk factors is needed to inform local and global health policies.
        Such an analysis might dispel prejudices about the “diseases of affluence” and stimulate
        policy approaches and research that appropriately target emerging risk groups across the
        globe, regardless of socioeconomic status.
      
      
        From Intuition to Data
        In a study in this month's 
        PLoS Medicine , Ezzati and colleagues have taken a close look at
        population data available in 80 or so countries on body mass index (BMI), cholesterol, and
        hypertension [1]. They looked at how these CVD risk indicators are predicted by three broad
        economic parameters: national income, average share of household expenditure spent on food,
        and proportion of the population living in urban areas. This cross-sectional analysis drew
        on published studies, reports from ministries of health and the World Health Organization,
        data from household surveys, demographic data, and centrally available economic indicator
        data to model the relationship between the selected CVD risk factors and
        economic/demographic status.
        The overall results suggest that average BMI and cholesterol increase with national
        income and then flatten out at higher incomes (or even decline), except in the United
        States, the home of the Big Mac and of leading practitioners of the sedentary urban
        lifestyle (in the US, BMI and cholesterol levels do not flatten out with higher income).
        Not surprisingly, there is an inverse relationship between BMI and proportion of household
        income needed for buying food in most countries.
        Urbanization is associated with higher average income, and Ezzatti and colleagues found
        that urbanization is associated with higher BMI and cholesterol. As urbanization progresses
        and food availability equalizes among both urban and rural populations, there is less of an
        association between increasing income and increasing BMI and cholesterol. However, some
        persistently agricultural economies with large populations (such as Nigeria and Indonesia)
        tend to retain the inequities that influence diets, leading to protein-calorie deficiencies
        among those with lower income and increased BMI among those with higher income. Systolic
        blood pressure did not have as robust a relationship as BMI or cholesterol to urbanization
        or national income.
      
      
        Looking Ahead
        Despite the limitations of multinational data and the broad brush approach used to
        interpret these data, there are some important lessons that emerge from this study
        regarding the population distribution of multiple CVD risk factors. These risk factors are
        systematically finding their way to low- and middle-income countries and the vulnerable
        populations therein that still suffer from childhood illness and high communicable disease
        burdens.
        This shift is already having an effect on the epidemiology of CVD and other NCDs,
        particularly in middle-income countries. And we can predict that as low-income countries
        achieve economic growth, the disease burden of NCDs will be waiting for them as well.
        Effective policies can prevent some of this impact, if action is taken now; Ezzati and
        colleagues' work provides a basis for planning such interventions.
      
      
        Planning for the Future
        To target these interventions effectively, better data are needed on the prevalence of
        CVD risk factors in low- and middle-income populations, and on the association of these
        risk factors with NCD health outcomes. CVD risk factor surveillance should be incorporated
        into national program planning and into best practices for NCD control supported by the
        World Health Organization and other health development agencies. Multinational agreements,
        such as the recently activated Framework Convention on Tobacco Control
        (www.who.int/tobacco/en), can create effective international cooperative efforts to stem
        the tide of global tobacco use. Global tobacco control should be a health and foreign
        policy concern even for those countries that have not ratified the treaty.
        As populations assume more of an urban lifestyle, they should not be limited in their
        choices for healthy foods, suffer from lack of safe water, or lose opportunities for
        physical activity. These problems can be reduced through good urban planning, better food
        policies, improved environmental engineering, and better attention to healthy lifestyle
        practices in our growing cities. Screening for hypertension, hypercholesterolemia, and
        nicotine addiction need to become a part of good clinical practices in low- and
        middle-income countries. Of course, screening for these risks should then also be
        accompanied by better availability of low-priced secondary prevention therapies such as
        generic versions of anti-hypertensives, statins, and nicotine replacement therapies.
      
      
        Getting the Balance Right
        This is not to say that the big infectious disease killers and child health problems
        should be ignored. Rather, we need to learn from the history of socioeconomic development
        that it is not simply affluence that permits the increased impact of CVD and other NCDs; it
        is the risk factors for these diseases that spread across socioeconomic boundaries, causing
        the same illnesses regardless of the socioeconomic status of the population.
        Increased attention should be paid to these diseases not just in the developed world,
        but also in the developing world, where the unfinished agendas on communicable disease and
        childhood illness have drawn the most attention. Addressing CVD risk factors could best be
        accomplished through improved international cooperation, better understanding of the risks
        of globalization, and development of appropriate research and technologies that apply to
        low- and middle-income populations.
      
    
  

  
    
      
        
        Hypertension is common in affluent societies and a major risk factor for heart disease.
        In Canada, hypertension is the leading primary diagnosis for patient visits to physicians'
        offices. Beyond recommending lifestyle changes such as losing weight, quitting smoking, and
        lowering salt and alcohol intake, prescription drugs are indicated in many patients. As a
        consequence, antihypertensive drugs are the leading category of prescription drugs in
        Canada, accounting for 20% of prescription drug sales.
        Several classes of drugs are available for treatment, including diuretics, ACE
        inhibitors, and calcium channel blockers. First-line treatment with thiazide diuretics—the
        oldest and by far the cheapest drug class—has been shown in randomized trials to reduce
        serious cardiovascular morbidity and mortality with benefits at least as great as
        first-line treatment with other drug classes.
        Steve Morgan and colleagues set out to examine whether prescribing practices were in
        accordance with this evidence. They analyzed administrative claims data from a public drug
        plan for seniors (residents of age 65 and older) to determine trends in first-line
        hypertension drug use. During the period from 1993 to 2000, over 82,000 seniors were
        identified as new users of hypertension drugs. Less than a third of these patients received
        thiazides (alone or as part of a combination regimen) as a first-line treatment.
        The share of new patients receiving a thiazide increased over the study period, but did
        not exceed 45% at any point. Women were more likely than men, and older patients were more
        likely than younger ones, to receive thiazides. Comorbidities also influenced prescribing
        practices: patients without concurrent diagnoses were more likely to receive thiazides.
        While for some comorbidities (such as previous acute myocardial infarction) evidence
        suggested that there were good reasons to prescribe drugs other than thiazides, no such
        evidence existed for many of the other conditions that nevertheless were associated with
        lower prescription of thiazides.
        Changes in drug availability and existing evidence during the period studied make it
        difficult to calculate the exact extent to which thiazides were under-prescribed. However,
        the study shows that many patients received drugs that had previously been found to be no
        better at treating hypertension than much cheaper alternatives. Drug prices changed over
        the study period as well, but even comparing the lowest price for any of the alternatives
        to thiazides, $0.34 per day, with the constant cost of less than $ 0.01 per day for a
        thiazide makes it clear that a lot of money was wasted.
        On a more positive note, prescription of thiazides as a first-line therapy rose over the
        study period—from 25% to 42% in patients without comorbidities. Most of the increase
        occurred shortly after a specific local education campaign. This suggests that repeated
        targeting of prescribing physicians—many of whom receive regular marketing material from
        pharmaceutical companies and subscribe to the general view that newer drugs are better—with
        current evidence-based information should be considered.
        One of the most influential studies comparing antihypertensive drugs, the ALLHAT study,
        also supported the use of thiazides as first-line drugs. ALLHAT was published in 2003, and
        its results widely publicized. According to Steve Morgan, “Anecdotal evidence suggests that
        ALLHAT has had an influence on prescription practices, but I am not aware of a large-scale
        analysis yet.”
      
    
  

  
    
      
        
        There are 300 million cases of malaria each year worldwide, causing one million deaths.
        Around 90% of these deaths occur in Africa, mostly in young children. One of the greatest
        challenges facing Africa in the fight against malaria is drug resistance; resistance to
        chloroquine (CQ), the cheapest and most widely used antimalarial, is common throughout
        Africa, and resistance to sulfadoxine-pyrimethamine (SP), the first-developed and least
        expensive alternative to CQ, is also increasing in eastern and southern Africa. These
        trends have forced many countries to change their treatment policies and use more expensive
        drugs, including drug combinations that will hopefully slow the development of resistance.
        One avenue of research is to identify combinations that minimize gametocyte emergence in
        treated cases and prevent selective transmission of parasites resistant to any of the
        partner drugs.
        In this month's 
        PLoS Medicine Colin Sutherland and colleagues tested two leading
        combination therapies in children with uncomplicated malaria. One regimen was an
        artemisinin-based combination consisting of artemether and lumefantrine (co-artemether,
        trade names CoArtem and Riamet). The other was a combination of CQ and SP—currently under
        consideration in several African countries, largely due to its low cost. In this
        randomized, controlled trial, 497 children with acute uncomplicated falciparum malaria were
        given either a combination of CQ and SP or six doses of co-artemether (91 received CQ/SP
        and 406 received co-artemether), and their blood was tested for infectivity to mosquitoes
        seven days after treatment. During follow up at seven, 14, and 28 days the team found that
        children treated with co-artemether were significantly less likely to carry gametocytes in
        their blood than children treated with CQ and SP—7.9% compared with 48.8%.
        Altogether, the six-dose regimen of co-artemether was highly effective at reducing the
        prevalence and duration of gametocyte carriage. The numbers of gametocytes and the
        infectiousness to mosquitoes at day 7 were also reduced compared to a combination of CQ and
        SP, said the authors. Other studies have already shown the potential of co-artemether
        combination therapy to both cure malaria and reduce gametocyte carriage, acknowledged the
        authors. However, this study is the first to demonstrate the treatment's potential to
        markedly reduce the infectiousness of patients to mosquitoes, and has done so in a
        sub-Saharan African setting with highly seasonal transmission and where asymptomatic
        infections are common.
        Do the results mean co-artemether should be introduced as a first-line treatment for
        malaria in Africa? The authors are hesitant and suggest there might be compliance issues
        with the six-dose regimen. The requirement of oily food for adequate absorption might also
        lead to inadequate drug levels in the blood of many treated individuals.
        The authors suggest that co-artemether as a first-line treatment is not likely to reduce
        overall transmission of 
        Plasmodium falciparum within the community but rather would
        reduce selective transmission of resistant parasites in treated patients. Hence,
        co-artemether could have a public health benefit by reducing the impact of drug
        resistance.
      
    
  

  
    
      
        
        Before insulin was discovered and purified, doctors could only watch as their patients
        slowly died of type 1 diabetes mellitus. In January 1922, the prognosis was changed
        dramatically when a teenager with diabetes in a Toronto hospital became the first recorded
        recipient of an injection of insulin. Although this preparation was far from perfect, as
        Frederick Banting said in his Nobel lecture of 1925 (Nobel laureates did not have to wait
        so long as they do now for recognition), “There was a marked reduction in blood sugar and
        the urine was rendered sugar free.” Suddenly diabetes was a potentially curable disease.
        But insulin's very success brought trouble, as demand far exceeded the supply.
        Now recombinant insulin is available in plentiful supply, but the control of type 1
        diabetes remains far from perfect, and researchers have looked towards an ideal of
        transplanting insulin-producing cells instead. The best protocol for this, also pioneered
        in Canada, the Edmonton protocol, uses ß-cells isolated from human cadavers and has shown
        some remarkable results with around 84% of patients remaining insulin-free after one year
        and 89% of patients still producing insulin after three years. However, the isolation of
        ß-cells is laborious and limited by donor availability.
        The next logical step, then, is to look for renewable sources of insulin-producing
        cells. The emerging science of human stem cell research makes this step possible, and in a
        paper in this month's 
        PLoS Medicine Seung Kim and colleagues from Stanford University suggest
        that researchers should look beyond just pancreatic precursors and embryonic stem cells to
        other cell-type precursors for ideas about ß-cell replacement.
        The rationale for the approach in this study comes from observations that although the
        pancreatic islet cells are the principal source of insulin in humans, in some invertebrate
        species, such as 
        Drosophila , most circulating insulin is produced by brain
        neurons. Intriguingly, the gene encoding insulin is also transcribed by some vertebrate
        neurons—although it is not clear whether they then produce or secrete insulin protein.
        Kim and colleagues took human neural progenitor cells derived from brain, and exposed
        them to a series of signals that are known to drive pancreatic islet development. They were
        able to produce clusters of insulin-producing cells that were responsive to glucose in
        vitro. After transplantation into immunocompromised mice, circulating human insulin and
        C-peptide derived from the proinsulin precursor were detected when the mice were given
        glucose.
        Of course, results in mice do not mean that such treatments would automatically work in
        humans, and before any such therapies become available there are many hurdles to overcome.
        Some of the most important include the long-term stability and safety of the cells
        (although the cells remained differentiated in the mice and did not form tumors, such a
        risk would need to be very thoroughly investigated because of the chronic nature of
        diabetes), and how to scale up such a process to produce the much larger numbers of cells
        needed for human treatment. Nonetheless, work like this study from Kim's group might point
        to where the future of diabetes treatment lies.
      
    
  

  
    
      
        
        Compared with malaria, dengue fever has a rather lower profile in the public mind,
        although to those who have had it, it leaves a great impression. The name dengue fever is
        derived from the Swahiliwords 
        Ki denga pepo (“it is a sudden overtaking by an evil spirit”), which
        gives an idea of the rapid onset of the disease. The dengue virus is carried by the
        mosquito 
        Aedes aegypti , and the disease often occurs as epidemics.
        Although the classic illness is a fairly benign acute febrile syndrome, it may be very
        painful—hence the English nickname, breakbone fever. The virus can also cause a much more
        serious illness known as dengue hemorrhagic fever, which can progress to dengue shock
        syndrome. There are four main serotypes of the dengue RNA virus; dengue hemorrhagic fever
        is more likely to occur during dengue infection in people with preexisting active or
        passive (e.g., maternally acquired) immunity who are exposed to a different dengue virus
        serotype. In contrast to classic dengue, the hemorrhagic fever and shock syndromes are
        mostly diseases of children and, if untreated, have a mortality of around 50%.
        Around two-fifths of the world's population are now at risk of the disease (one estimate
        is that 80 million people are infected each year). The number at risk will increase as
        population growth, urbanization, international travel, and climate change influence
        transmission of the disease. Understanding how all these factors interact is important in
        planning for disease outbreaks. However, the incidence of dengue is not easily predictable,
        varying with season, and also between years. For example, although dengue is most prevalent
        in the wet season, dengue epidemics have also been associated with drought in some
        countries. El Niño is the best known climatic event affecting climate between years, and
        some research already suggests that there is a relationship between the timing of dengue
        epidemics and El Niño in the Pacific Islands and in other countries.
        Previous research has uncovered traveling waves of dengue in Thailand, but the cause of
        these has been obscure. In a paper in this month's 
        PLoS Medicine Bernard Cazelles and colleagues looked at the details of
        the relationship between dengue incidence and El Niño in Thailand. Their results, based on
        complex mathematical analysis, do not provide easy answers for those who might want to plan
        for dengue outbreaks, though they do go some way to helping to understand the complex
        interplay between the various factors. In essence, the researchers found that there was a
        significant association between El Niño oscillations, climate variables, and dengue
        hemorrhagic fever incidence with a 2- to 3-year repeat, for both Bangkok and the rest of
        Thailand. However this association was significant only for the years 1986–1992, and
        outside these years factors other than climate were probably responsible for triggering the
        disease outbreaks.
      
    
  

  
    
      
        
        As the HIV-1 epidemic continues to grow, mutations in the virus that confer drug
        resistance are becoming increasingly important in the clinical management of patients
        worldwide. Of all the different virus subtypes (A, B, C, D, F, G, H, J, and K) and a
        rapidly increasing number of established and emerging recombinant viruses, it is subtype B
        that predominates in Western Europe, the United States, and the rest of the industrialized
        world. Antiretroviral drugs were developed by studying subtype B, and most data on the
        genetic mechanisms of HIV drug resistance are also from subtype B. However, worldwide,
        subtype B is in the minority (~10% of the infected population). In Africa, for example,
        where there is broad viral diversity, there is a greater spread of subtypes, with subtype C
        being the most common, representing over half of all infections. Although it seems that
        current drugs—developed against subtype B virus—are active against non-subtype-B virus, one
        critical issue is whether viruses from some subtypes or particular regions are more likely
        than others to develop resistance against certain drugs. Another crucial issue is to
        identify the mutations that confer drug resistance in non-B subtypes. Answering these
        questions might determine whether initial treatment strategies should be different for
        people with non-subtype-B viruses, and also could help decide how patients with
        non-subtype-B virus who fail antiretroviral therapy should be managed.
        In a paper in this month's 
        PLoS Medicine , Rami Kantor and colleagues from a worldwide collaboration
        have looked at the mutations found in 3,686 people with non-subtype-B HIV-1 infections
        compared with those in 4,769 people with subtype B infections. They wanted to answer two
        questions: first, whether the mutations that cause drug resistance in subtype B viruses
        also develop in non-subtype-B viruses exposed to antiretroviral drugs, and second, whether
        novel mutations (i.e., not previously seen in subtype B virus) develop in non-subtype-B
        viruses when they fail to respond to antiretroviral drugs. What they found was that all of
        the 55 drug-resistance mutations that have been known to occur in subtype B also occurred
        in at least one non-subtype-B isolate, and most of these mutations were also statistically
        associated with antiretroviral treatment in at least one non-B subtype. Conversely, of the
        67 mutations associated with antiretroviral therapy in at least one non-B subtype, 61 were
        also associated with antiretroviral therapy in subtype B isolates.
        So it appears that few novel mutations are arising in non-subtype-B viruses exposed to
        the current antiretroviral drugs and that the present focus on subtype B mutations for
        global surveillance and genotypic assessments of drug resistance is a reasonable approach.
        However, the authors emphasize that differences in the types and patterns of
        drug-resistance mutations are likely to differ between the subtypes, and that larger
        numbers of samples and further analyses are needed to exclude the possibility of new and/or
        rare subtype-specific mutations.
      
    
  

  
    
      
        
        Medicine might not be the oldest profession, but the practice and the teaching of it
        certainly stretch far back into antiquity. In the developed world, medical schools are now
        thriving enterprises whose main aim remains to turn out sufficient numbers of well-trained
        medical practitioners. But alongside this primary purpose, medical schools have become
        centers of excellence for research also, mostly basic scientific research. Certainly the
        medical schools of the University of Cambridge and University of California at San
        Francisco (UCSF)—the two schools closest to the PLoS editorial offices—have large research
        programs.
        Getting the balance right between understanding the research that will drive future
        medical discoveries, and providing training in clinical practice is obviously crucial in
        training tomorrow's doctors. However, follow-up studies of those who graduate suggest that
        medical schools may not be producing doctors who are happy with the profession, or who fit
        all the needs of medical services today. In the United Kingdom, two striking observations
        came out of recent surveys. One is the difficulty in recruiting doctors who want to become
        general practitioners, the primary care doctors who form the backbone of the UK health
        service. Whereas in the 1970s and 1980s 40%–50% of the qualifiers intended to enter general
        practice, in the 1990s the proportion was only 20%–26%. (BMJ 326: 194–195). But even more
        disturbing is the substantial proportion of doctors in the same surveys who indicated that
        they regularly consider leaving medicine altogether. Contrast these results with those of a
        survey from Uganda, where the survival of physicians was the most urgent concern. From a
        cohort of 77 doctors who graduated from the University of Makere, Uganda, in 1983, 22 had
        died over the following 20 years (BMJ 329: 600–601). The most important cause of death was
        presumed to be AIDS. But all of the surviving doctors were in some form of medical
        work.
        What these figures show perhaps more than anything is the difficulties of generalizing
        about what are the top priorities for medical students across the globe. To provide a place
        for all students to debate the issues that matter to them, 
        PLoS Medicine is launching the Student Forum, a new quarterly section
        written by medical students. An international team of student advisers has been elected
        that will help shape the content of this new section. The Student Forum recognizes the
        vital role that medical students have in guiding the future of the medical profession.
        The first Student Forum essay brings together students from three continents: Brian
        Palmer of the American Medical Student Association, Amanda Wong of the European Medical
        Students' Association, and Mohit Singla of the Indian Medical Students' Organization. They
        lay out some of their concerns, such as the need for medical education to distance itself
        from the pharmaceutical industry and the importance of students worldwide having unfettered
        access to the medical literature. The students also ask questions about their training,
        commenting that the “reductionistic logic of medical education offers only false comfort,
        and only to those who content themselves with pathways and receptors.”
        Perhaps surprisingly, this opinion is not far away from that of an established academic
        clinician and editorial board member of 
        PLoS Medicine , Jonathan Rees. In another essay in this issue, Rees
        similarly questions the overemphasis on basic research in medical schools, posing the
        question, “why are our institutions not fit for the purpose of improving patients'
        health?”
        Not an easy question to answer, but one that might also be relevant to the question of
        what a medical journal, especially one that wants to engage students, should publish.
        Should we follow Rees's advice on what the composition of a medical school curriculum
        should be and have no more than 20% of our articles devoted to basic scientific research as
        applied to medicine? In this issue the balance of our research papers comes close to this
        suggestion, with one paper on the differentiation of insulin-producing cells from human
        neural progenitors, and other papers throughout the journal on topics as diverse as climate
        and dengue fever, treatment of malaria, HIV genotypes, prescription practices, counterfeit
        drugs, Buruli ulcer, and health and human rights. But, perhaps like the medical schools, we
        are not paying enough attention to other topics Rees points out as important—operations
        research, decision-making, informatics, and economics.
        In his lightning trip through medicine, 
        Blood and Guts: A Short History of Medicine (W. W. Norton, 2003), the
        late medical historian Roy Porter wrote that “the biomedical model can be myopic, searching
        ever more microscopically for disease but often omitting the wider picture of populations,
        environments and health.” In his description of the differing opinions on the role in 19th
        century medical schools of the new practice of laboratory medicine, it becomes clear that
        the debate over what should be taught and what matters to medical students might not be
        new, but it is clearly as important as ever.
      
    
  

  
    
      
        
        The case report by Nautiyal et al. [1] is an instructive reminder that the first episode
        of an acute painful Horner Syndrome should prompt imaging of the ipsilateral internal
        carotid artery, since carotid dissection (as well as other conditions, such as high-grade
        stenosis) needs to be ruled out. Unfortunately, the authors perpetuate the extremely common
        misconception that enophthalmos accompanies ptosis and miosis in human Horner Syndrome. It
        is only an illusion of enophthalmos caused by the ptosis. This is evident in the left eye
        of their patient in Figure 1 of the case report.
        Actual measurement with exophthalmometry clearly demonstrates the lack of enophthalmos.
        As stated by Loewenfeld ([2], p. 1139), “Animals such as cats, rats, or dogs have
        enophthalmos on the side of the sympathetic lesion. But in man, the enophthalmos is only
        apparent. The small palpebral fissure makes the eye look sunken in on the affected side,
        but the position of the globe in the orbit remains virtually unchanged. This has been found
        by all workers who have measured the supposed enophthalmos objectively.” Loewenfeld cites
        four supportive references.
        Thompson and Miller ([3], p. 964) provide four additional references that the
        enophthalmos “is apparent rather than real.”
      
    
  

  
    
      
        Introduction
        Coronary heart disease (CHD) remains the leading cause of morbidity and mortality in the
        United States and is associated with substantial economic cost [1]. Hyperlipidemia
        represents an important modifiable risk factor in the development and progression of CHD.
        Estimates indicate that nearly 100 million American adults have total blood cholesterol
        levels of greater than 5.17 mmol/l (200 mg/dl) with 40% having levels greater than 6.21
        mmol/l (240 mg/dl) [2]. Identification and treatment of patients with hyperlipidemia play
        an essential role in the primary and secondary prevention of CHD.
        Currently, evidence-based practice guidelines focus on low-density lipoprotein
        cholesterol (LDL-C) as the primary target for risk reduction therapy and recommend that the
        intensity and target goals of LDL-C-lowering therapy should be adjusted to individual
        absolute risk for CHD [3]. Absolute CHD risk is categorized as low, moderate, or high based
        on the presence or absence of CHD, CHD-equivalent conditions, and major risk factors other
        than LDL-C. While therapeutic lifestyle changes are integral to general risk reduction,
        drug treatment proves necessary for selected patients whose absolute risk is high and/or
        whose LDL-C is inadequately controlled with lifestyle modifications alone. Among existing
        drug therapies, 3-hydroxy-3-methyl-glutaryl coenzyme A reductase inhibitors, more commonly
        known as statins, provide a generally well-tolerated and effective option for lowering
        LDL-C levels and decreasing the likelihood of subsequent CHD events [3,4].
        Despite the compelling evidence of statins' therapeutic benefits, the literature abounds
        with documentation of wide treatment gaps in clinical practice [5–10]. Available research,
        however, offers only a limited understanding of how statin therapy varies by CHD risk,
        particularly for statin-eligible patients in the moderate-risk group. Also, national data
        are limited regarding recent changes in statin use.
        Using serial cross-sectional data from 1992 through 2002, we tracked trends in statin
        use in the United States during ambulatory visits categorized by CHD risk, with or without
        a diagnosis of hyperlipidemia. In addition, we analyzed the independent associations of
        patient and physician characteristics with statin use for insights as to how to target
        interventions to improve statin use.
      
      
        Methods
        
          Data Sources
          Annual data from1992 through 2002 were obtained from the National Ambulatory Medical
          Care Survey (NAMCS) and the outpatient department component of the National Hospital
          Ambulatory Medical Care Survey (NHAMCS). The National Center for Health Statistics
          provides complete descriptions of both surveys and yearly data at
          http://www.cdc.gov/nchs/about/major/ahcd/ahcd1.htm. These surveys, particularly NAMCS,
          have been validated against other data sources [11,12], and have also been utilized in
          past research of cholesterol management [13].
          In brief, NAMCS captures health-care services provided by office-based physicians,
          while NHAMCS assesses services offered at hospital outpatient departments. Both surveys
          utilize multistage probability sampling procedures, enabling the generation of nationally
          representative estimates. Between 1992 and 2002, annual participation rates among
          physicians selected for NAMCS averaged 70%, while the participation rate in NHAMCS by
          selected hospitals with outpatient departments was 90%. In our study, we combined NAMCS
          and NHAMCS data to obtain a wider range of outpatient settings and a broader
          socioeconomic spectrum of patients seeking ambulatory care.
          Standard encounter forms were completed for a systematic random sample of patient
          visits during randomly assigned reporting periods. Item nonresponse rates were mostly 5%
          or less in both surveys for all years. Yearly encounter forms varied slightly between
          NAMCS and NHAMCS and were revised every two years. Our analysis focused on domains of
          data that were consistently collected in both NAMCS and NHAMCS for the time period
          1992–2002, including patient demographic and geography characteristics, reasons for visit
          (up to three), diagnoses (up to three), new and continuing medications (up to five in
          1992–1994 and six in 1995–2002), and lifestyle counseling services provided or ordered at
          the visit.
        
        
          Participants
          
            CHD risk categorization
            We estimated CHD risk for adults aged 20 y and older based on risk factor counting.
            CHD risk was mutually exclusively categorized as low (0–1 risk factors), moderate (2+
            risk factors), or high (CHD, other atherosclerotic diseases, or diabetes). The
            moderate-risk group included visits by patients without CHD or equivalent but with at
            least two of the following risk factors: age (for men, >45 y; for women, >55 y),
            cigarette smoking, or a physician-reported diagnosis of hypertension. Unfortunately,
            the other two major CHD risk factors—high-density lipoprotein cholesterol levels and
            family history of premature CHD—were not captured in either data source. Also, neither
            data source provided actual cholesterol measurements. Disease conditions were
            identified by International Classification of Disease (ICD-9) codes, as well as by the
            appropriate reason-for-visit codes that are specific to NAMCS and NHAMCS. For instance,
            we identified patients as having hyperlipidemia if their encounter forms contained an
            International Classification of Disease code within 272.0–272.4. For the sake of this
            study, patients whose encounter forms did not indicate the presence of a condition were
            assumed to not have that condition.
          
          
            Patient visit characteristics.
            Nonclincal characteristics included patient age, gender, race/ethnicity, medical
            insurance, visit status, United States census region, metropolitan area status,
            physician specialty, and practice setting. Medical insurance was classified as
            private/commercial, public (i.e., Medicare and Medicaid), or other (e.g., workers'
            compensation or self-pay). Visit status distinguished first-time visits from return
            visits to a practice. Physician specialty was available only from NAMCS, which
            contributed more than 90% of the total visits for each of the study years. We
            categorized physician specialties as cardiology, internal medicine, general and family
            practice, or other.
          
          
            Measures
            Of primary interest were the rate of statin use relative to CHD risk and the
            relationship of statin use to patient visit characteristics. The rate of statin use was
            calculated as the proportion of patient visits where a statin was reported (i.e.,
            atorvastatin, lovastatin, pravastatin, simvastatin, or fluvastatin). Before its removal
            from the market in 2001, cerivastatin was used scarcely (<2% among visits by
            patients with hyperlipidemia) and therefore is not reported in this study. Measuring
            the rate of statin use by CHD risk category provided a relative indicator of
            appropriate prescribing patterns, that is, the prevalence of statin use should be
            highest among high-risk patients, for whom secondary prevention is a priority.
            Variations of statin use by patient visit characteristics, if detected, would reflect a
            lack of equity in processes of care in that uniform practices are expected unless
            evidence-based guidelines recommend otherwise.
          
          
            Analyses
            Statistical analyses were performed using SAS for Windows software (SAS Institute,
            Cary, North Carolina, United States) and SAS-callable SUDAAN software (RTI, Research
            Triangle Park, North Carolina, United States) to account for sampling weights and the
            complex survey design. The unit of analysis is the patient visit. We report national
            annual means of the rate of statin use by CHD risk category and corresponding 99%
            confidence intervals for the years 1992 through 2002. χ
            2 tests examined the association of statin use with individual
            patient visit characteristics for combined 1995–2002 NAMCS and NHAMCS data. The
            independent effect of each patient visit characteristic on statin use after controlling
            for all other characteristics was assessed with multivariate logistic regression.
          
        
      
      
        Results
        In 2002, visits by patients at moderate or high risk involved higher proportions of
        older patients (mean age 65 y) than low-risk patient visits (mean age 51 y), and
        consequently were more likely to be covered by public insurance, particularly Medicaid
        (Table 1). Moderate- and high-risk patient visits also were made up of more men and return
        patients. In addition, a greater percentage of high-risk patient visits (11%) were seen by
        cardiologists than patient visits at low and moderate risk (2% and 4%, respectively).
        Internists and general and family practitioners played a dominant role in the care of
        moderate- and high-risk patients, accounting for 69% of visits by moderate-risk patients
        and 58% of visits by high-risk patients. Distributions by race/ethnicity, geographic
        region, residence area, and practice setting did not differ by CHD risk. Overall, the
        majority of patient visits were return visits to office-based physicians made by
        non-Hispanic whites and residents living within metropolitan statistic areas. Patient
        visits were distributed similarly across the four geographic regions, with a slightly
        higher proportion from the southern region.
        Throughout the study period, statins were primarily used among patients whose visit
        involved reported hyperlipidemia, representing 97% of all statin use in 1992 and 91% in
        2002. Statin use increased nearly 5-fold from 9% (99% confidence interval: 7%–12%) of all
        visits with reported hyperlipidemia in 1992 to 49% (42%–55%) in 2000, but then declined to
        36% (31%–42%) in 2002 (Figure 1). Of note, however, the annual rate of increase in
        frequency of patient visits with reported hyperlipidemia was 34% in 2001 and 21% in 2002,
        while it averaged only 12% through 2000. The dominance of statins as lipid-lowering agents
        grew markedly from 47% of all lipid-lowering medications in 1992 to 87% in 2002 (Figure 1).
        Among available statins, lovastatin remained the therapeutic choice through 1996, after
        which it was surpassed by other statins, particularly simvastatin and then atorvastatin
        (Figure 2). Atorvastatin constituted 51% (46%–56%) and simvastatin 32% (27%–36%) of all
        statin use in 2002.
        As expected, high CHD risk patient visits resulted in greater statin use, and the
        divergence in statin use among the three risk categories has grown in recent years.
        Absolute increases in the rate of statin use were greatest for high-risk patient visits
        with or without reported hyperlipidemia—a 15 percentage-point increase from 4% of all
        visits in 1992 to 19% in 2002—followed by a nine percentage-point increase (2% to 11%) for
        moderate-risk patient visits and a 2.5 percentage-point increase (0.3% to 2.8%) for
        low-risk patient visits (Figure 3). Statin use in the moderate-risk group peaked at 14%
        (10%–17%) in 1999. Similarly, the rate of statin use in the high-risk group declined
        slightly from 2001 to 2002.
        Among patient visits with reported hyperlipidemia, statins were used in 14% (8%–19%) of
        high-risk visits and 9% (5%–14%) of moderate-risk visits in 1992. The high-risk group's
        statin use rate rose to 60% (49%–71%) in 2000 and was 50% (40%–61%) in 2002. Likewise, the
        rate in the moderate-risk group climbed to 56% (42%–70%) in 1999 and stabilized at 44%
        (32%–57%) in 2002. In addition, lifestyle counseling (i.e., regarding diet, exercise, or
        smoking cessation) occurred in only 43% (32%–53%) of new and general medical examination
        visits in 2002 for patients who had moderate CHD risk and were diagnosed with
        hyperlipidemia. Improvements over time in counseling rates were minimal.
        The increase in statin use with CHD risk and with the year of study persisted after
        controlling for physician-reported hyperlipidemia, number of medications, and nonclinical
        patient visit characteristics (Table 2). Moderate- to high-risk patient visits had a 1.2-
        to 2.5-fold greater likelihood of taking a statin relative to visits by patients at low
        risk. Statin use was approximately three times as likely in 2001 and 2002 as in 1995 and
        1996. Additionally, lower statin use was independently associated with younger patient age,
        female gender, African American background (versus non-Hispanic white), non-cardiologist
        care, and fewer total reported medications.
      
      
        Discussion
        Despite significant increases from 1992 to 2002 in use of statins associated with
        hyperlipidemic patient visits, the magnitude of increases is smaller than expected and the
        rate of use remains suboptimal according to the best available evidence. The underuse of
        statins is most prominent among visits by patients at high or moderate risk of CHD who do
        not have a physician-noted diagnosis of hyperlipidemia but may nonetheless be eligible for
        lipid-lowering drug therapy. Previous research reports that physicians are more likely to
        diagnose hyperlipidemia if laboratory reports show abnormal lipid levels [14]. However, the
        normal ranges of lipid levels on many laboratory reports do not take into account
        individual patients' absolute risk.
        When evaluating statin use across different CHD risk categories, the observed trends
        raise several issues. Both the rate of statin use and the absolute increases in the rate
        over time were positively associated with the level of CHD risk, which appropriately
        conforms to the notion of risk stratification. The associations persisted after adjusting
        for potentially confounding factors such as a hyperlipidemia diagnosis and nonclinical
        patient visit characteristics. Even so, in 2002, one year after the publication of Adult
        Treatment Panel III [3], statins were reportedly used in only 19% of patient visits with
        established CHD or its equivalents, and the average rate was no higher than 50% among
        high-risk visits where a diagnosis of hyperlipidemia also was noted. These data suggest a
        dramatic treatment gap. Another analysis based on national data estimated that 72% of
        Americans with existing CHD would benefit from drug therapy to achieve the target LDL-C
        goal of 2.59 mmol/l (100 mg/dl) or less, assuming a 10% LDL-C reduction with diet [15].
        However, only 11% of those eligible individuals received lipid-lowering drug therapy,
        suggesting a gap of 89% [15]. These obvious treatment gaps are disconcerting, especially in
        light of the recent Adult Treatment Panel III update[16] that supports more intensive
        lipid-lowering drug therapy for patients at high and moderately high risk for a heart
        attack. Barriers to adequate treatment of high-risk patients may stem from the patient
        (e.g., lack of drug adherence, concern about adverse effects, inadequate knowledge of their
        hyperlipidemia, and drug cost), the physician (e.g., lack of guideline awareness, failure
        to measure lipid levels, and overestimation of actual treatment), and the health-care
        system (e.g., lack of monitoring and follow-up and emphasis on acute medical problems)
        [10]. If the current practice continues, the observed treatment gaps are expected to
        persist or even widen.
        While statins deliver the greatest benefits when used for secondary prevention, evidence
        continues to accumulate that suggests an important role of statins in the primary
        prevention of cardiovascular events, particularly for patients at increased risk [6,17].
        Our data show an increase in statin use from 2% of moderate-risk patient visits in 1992 to
        14% in 1999, but without continued growth subsequently. Optimal proportions could not be
        determined because of the lack of detailed clinical data. Nonetheless, National Health and
        Nutrition Examination Survey III data showed that 60% of 38.5 million adult Americans
        without CHD who had two or more risk factors had an LDL-C level above the recommended 3.36
        mmol/l (130 mg/dl) and that 45% would remain eligible for drug therapy even after a 10%
        decrease in LDL-C with diet [15]. In addition, Fedder and colleagues found a doubling
        effect in the number eligible for primary prevention drug therapy by switching to
        Framingham risk scoring [18]. Other researchers have reported that the proportions of
        treatment-eligible primary prevention patients who received no drug therapy reached as high
        as 97% [5,19]. In our study, statin use was reported in only 44% of moderate-risk patient
        visits for which a diagnosis of hyperlipidemia was noted, which is surprisingly low given
        that the entire group would be expected to benefit from statin therapy. We also concur with
        other researchers who have discussed the role that inadequate lifestyle counseling plays in
        the existing cholesterol treatment gaps [10,13]. Our data show that lifestyle counseling
        occurred during fewer than 50% of new and general medical examination visits by
        moderate-risk patients, even though these types of visits arguably represent better
        opportunities for counseling services than return, illness-focused visits.
        It is intriguing to note that earlier increases in statin use were not sustained in 2001
        and 2002. Studies using alternative data sources are needed to corroborate this
        observation, and detailed market research is necessary for understanding the underlying
        causes of this unexpected decline in use. We speculate that the observed trends may be
        partially explained by discordant rates of increase in the diagnosis of hyperlipidemia
        versus the prescribing of statins. Also, NAMCS and NHAMCS data released after 2002 will
        help determine whether the noted declines are due to random fluctuations in data
        reporting.
        Wide gaps between evidence-based lipid-lowering therapy and physician practice were
        reported in many other western countries as well. For instance, a survey conducted in nine
        European countries found that only 32% of patients with confirmed CHD received
        lipid-lowering medications [20]. Likewise, in a population-based study from the
        Netherlands, merely 16% of individuals eligible for lipid-lowering drugs were actually
        treated [7].
        In spite of being clearly underused, statins increasingly dominate lipid-lowering drug
        therapy, accounting for 92% of all lipid-lowering medications used in 2002, which confirms
        the trends seen in United States retail pharmacy dispensing data [2]. Also, in concert with
        other researchers [21], we observed a shift in the leading statin prescribed over time,
        from lovastatin to simvastatin and then to atorvastatin, corresponding to their market
        entry. Atorvastatin accounted for over half of all statin use in 2002. Even though most
        statins share similar tolerability, some evidence shows that atorvastatin has greater
        dose-specific potency for lowering LDL-C and total cholesterol [22].
        Additionally, our data add support to available literature documenting inequities in use
        of statins for patients with different social and clinical characteristics [23–26]. Of
        particular note are the lower rates of statin use in at-risk younger patients, females,
        African-Americans, and patients cared for by non-cardiologists. These findings may be
        useful for guiding targeted interventions that aim to bring physician practice into
        agreement with published guidelines for cardiovascular risk reduction.
        Our findings must be interpreted in the context of data limitations. Although both NAMCS
        and NHAMCS are designed to produce nationally representative estimates, these estimates are
        not linked to individuals but to patient visits. As a result, reported statin use may
        overestimate the actual administration because patients prescribed drug therapy likely make
        more visits because of greater disease severity and/or the need of frequent follow-ups.
        Also, we are missing people with risk factors who have not been seen by a physician or
        whose risk factors failed to be recorded. On the other hand, underestimation is also
        possible, for example, because of physicians' lack of awareness or incomplete reporting of
        patient medication uses. However, the failure to inquire or report an important agent such
        as a statin may be a clinical oversight in itself and contribute to therapeutic gaps. The
        degree of inaccuracy in our estimates is perhaps small, however, as suggested by the
        comparability of the current results to previous reports.
        Lack of detailed clinical data prohibits accurate risk assessment based on Framingham
        risk scoring. While the risk factor counting algorithm that we used may simulate practical
        risk estimation by many physicians, it precludes the assessment of appropriateness of
        statin use in relation to the latest lipid-lowering guidelines. This creates difficulty
        interpreting the rate of statin use observed for the moderate-risk group. In particular,
        adequate information is not available to differentiate varying levels of absolute risk
        among the moderate-risk group. We likely misclassified some patients as moderate risk when
        they may have actually been high risk despite the absence of CHD or CHD equivalents. On the
        other hand, indications for statins might be marginal for some young patients with modestly
        elevated risk factors. A final caveat is that neither NAMCS nor NHAMCS captures patient
        compliance or outcomes, although these are perhaps separate issues from physician adherence
        to evidence-based medicine.
        Despite the acknowledged limitations, NAMCS and NHAMCS cover a longer consecutive time
        span and provide more complete information about disease-specific physician activities than
        many other national data bases, e.g., the Medical Expenditures Panel Survey, the National
        Health Assessment Nutrition Examination Survey, and the National Health Interview
        Survey.
        In conclusion, persistent gaps in statin therapy suggest a continued need for improved
        CHD risk stratification of all patients, and treatment with statins when indicated.
        Information technology and broader national policy around quality measurement and reporting
        are just two potential strategies that could be used to improve current practice.
        Patient-centered interventions should strengthen patient education and improve patient
        access to different treatment options. Interventions should be targeted to at-risk patients
        whose drug regimens need to be reassessed and to physicians, particularly
        non-cardiologists, whose practices need be improved. Guidelines for cardiovascular risk
        reduction treatment and determination of the specific patients who can benefit from statin
        therapy will continue to evolve. Indications for use in primary CHD prevention are likely
        to expand for statins. Given the observed practice shortfalls, drug therapy in
        moderate-risk patients remains an important priority for improvement.
      
    
  

  
    
      
        DESCRIPTION of CASE
        An 18-year-old Caucasian male with type 1 diabetes presented to the emergency department
        complaining of severe left knee pain and swelling after sustaining a knee injury that
        occurred during a high school football match. Joint effusions were visible and palpable
        above the left knee, and there was significant loss of smooth motion of the knee, passively
        performed. Plain X rays showed no signs of fractures. The patient had had type 1 diabetes
        for six years, and his insulin regimen consisted of insulin glargine, 35 units at 8:00
        p.m., and insulin lispro, 23 units at 8:00 a.m. and 16 units at 8:00 p.m. The patient had
        no apparent complications related to type 1 diabetes.
        On examination he was alert, his pulse was 76 bpm regular, and his blood pressure was
        118/66 mm Hg. Recently, the patient had had frequent episodes of both hyperglycemia and
        hypoglycemia. However, he had never developed diabetic ketoacidosis (DKA). His recent HbA1c
        was 9.5%, demonstrating inadequate glycemic control.
        The patient was referred to an orthopedic surgeon, and arthroscopy was scheduled a few
        days later. A complex tear of the medial meniscus extending to the articular surfaces was
        diagnosed. Partial meniscectomy was recommended. (This procedure usually takes about one
        hour—nonetheless, the preoperative preparation for general anesthesia and the postoperative
        recovery may add several hours to this time.)
        
          When Would You Have This Patient Report to the Hospital? The Day before Surgery or
          the Morning of Surgery?
          This patient should be hospitalized no later than the evening before surgery, given
          his history of frequent episodes of hypo- and hyperglycemia and his poor glycemic
          control. This should allow for final optimization of glucose control before surgery.
          Ideally, frequent contact with the patient and adjustment of the insulin regimen prior to
          surgery should lead to an excellent glycemic control. This 18-year-old patient on
          glargine might benefit from additional doses of lispro given before lunch and dinner
          (rather than at 8 p.m., unless this is before dinner).
          A patient with poorly controlled diabetes should not undergo elective surgery until
          glycemic levels are reasonably controlled. For example, bringing the patient into the
          hospital with blood glucose levels of 450 mg/dl will likely result in a canceled surgery.
          If a patient is scheduled for day of admission surgery, one way to avoid having to send
          the patient out without being able to do the procedure is to perform a finger stick blood
          sugar on arrival so one can cancel before admission.
          Patients undergoing minor surgical procedures (e.g., arthroscopy) may be brought to
          the hospital on the morning of surgery. As the patient is usually NPO (i.e., has been
          given orders to fast before the surgery), a lower dose of the intermediate- or
          long-acting insulin is administered, and the regular insulin is withheld (Box 1). Insulin
          therapy should never be withheld in a patient with type 1 diabetes as this can result in
          DKA.
        
        
          How Should His Insulin Be Managed before Surgery?
          The degree of metabolic control should be carefully evaluated before surgery; the goal
          is to improve the patient's blood glucose readings on an outpatient basis before
          undergoing surgery. If hyperglycemia has been present for a prolonged period of time
          before surgery, this could result in dehydration, which is commonly associated with
          electrolytic abnormalities such as sodium and potassium loss and possibly intravascular
          volume depletion. Prolonged hyperglycemia will delay healing and increase the risk of
          ischemia. A number of observations have indicated that hyperglycemia impairs collagen
          formation and causes a decrease in the tensile strength of surgical wounds [1,2]. Simply
          avoiding hyperglycemia can prevent these consequences. The reduction of glucose levels to
          below 200 mg/dl has been shown to improve granulocyte adherence and granulocytosis, both
          key components of the innate immunity and the defense against bacterial infections.
          Studies in both humans and animals suggest that high glucose levels might exacerbate
          ischemic brain damage [3].
          Admission to the hospital is recommended for all patients with type 1 diabetes, and a
          stabilization period of 12–16 hours is also recommended for urgent procedures if severe
          hyperglycemia is present [4,5]. However, the widespread use of home blood glucose
          monitoring makes the improvement of glycemic control possible prior to admission.
          Traditionally, long-acting (e.g. ultralente) insulin is discontinued 2–3 days before
          surgery, and the patient is stabilized on a regimen of intermediate-acting (neutral
          protamine hagedorn [NPH] or lente) and short-acting (regular or humalog) insulin twice a
          day, or regular insulin before meals and intermediate-acting insulin at bedtime. However,
          if the glycemic control is good and the patient is being treated with glargine, it is
          acceptable to continue the regimen until the day of surgery [3]. Alternatively, 1/2–2/3
          of the usual insulin regimen is given on the day of the procedure [3]. On the morning of
          surgery patients with type 1 diabetes should receive the insulin regimen that is used
          intraoperatively (Box 2) [6].
          Evaluation of metabolic homeostasis, lipid profile, and kidney and myocardial function
          must be completed before surgery. The presence of diabetic autonomic neuropathy should
          also be assessed prior to surgery because this condition predisposes to perioperative
          hypotension. In such patients meticulous monitoring of blood pressure and volume status
          is essential during the perioperative period [7].
          The common agreement is that 4–8 hours before surgery, the patient should be kept NPO,
          subcutaneous (SC) insulin should be discontinued, and an intravenous (IV) infusion line
          should be inserted. It should be emphasized that if the elective surgical procedure can
          be scheduled for early morning hours and the procedure lasts at least four hours, this
          limits the NPO to about 4–6 hours and allows administration of usual or 1/2–2/3 usual
          insulin and use of a glucose infusion to replace breakfast and supplemental insulin
          either subcutaneously or intravenously. If both glucose and insulin are infused, it
          should be pointed out that these must be two separately controlled infusions, so glucose
          and insulin infusion can be varied independently.
        
        
          How Should His Insulin Be Managed during Surgery?
          Major surgery and general anesthesia can cause severe metabolic abnormalities in
          patients with type 1 diabetes. Given the limitations of SC insulin therapy, such as
          unpredictable absorption and variable plasma insulin levels, constant infusion of insulin
          is recommended [3]. Anesthesia induces complex neuroendocrine stress responses and
          activates the sympathetic nervous system. The abnormal release of growth hormone,
          cortisol, and epinephrine leads to impaired insulin secretion, and causes insulin
          resistance and hyperglycemia due to increased glycogenolysis, gluconeogenesis, and
          decreased glucose disposal. Many other conditions may cause severe insulin resistance in
          a patient with diabetes (Box 3).
          For patients scheduled for elective surgery, IV insulin and glucose infusion are
          usually started several hours preoperatively, and glucose levels should be maintained
          between 100 and 125 mg/dl. Slightly higher targets (100–150 mg/dl) have been recommended
          by some diabetologists to minimize the risk of hypoglycemia. The maintenance of glucose
          levels below 200 mg/dl has been shown to prevent bacterial infections and ischemic brain
          damage [3,8]. Suggested guidelines for management of patients with type 1 diabetes by use
          of insulin are outlined in Box 2. Since IV regular insulin has a short half-life (ten
          minutes), hypoglycemia is of little concern, as the infusion can be decreased and the IV
          glucose rate increased. The infusion rate can be adjusted by a floor nurse before surgery
          and by the anesthesiologist intraoperatively. An insulin infusion algorithm is
          constructed to allow easy titration of the insulin dose, and IV glucose must also be
          infused to obtain glucose levels within target (Box 2). This type of algorithm is
          effective in the majority of patients; however, it is based on the “average” patient and
          might require individualization. If the patient has a coexisting condition associated
          with increased insulin requirement, IV insulin doses need to be increased. Glycemic
          levels must be monitored at hourly intervals to keep glucose levels between 100 and 125
          mg/dl.
          Patients with type 1 diabetes who are treated with continuous SC insulin infusion by
          an insulin pump should be easily converted to IV regular insulin infusion just before
          surgery. Continuous SC insulin infusion is an acceptable regimen for surgical procedures
          requiring local anesthesia.
        
        
          How Should Fluids and Electrolytes Be Managed in This Patient?
          An adult without diabetes requires a minimum of 100 to 125 grams (400 to 500 calories)
          of glucose per day to prevent protein catabolism and the development of ketosis. Hence,
          this patient with types 1 diabetes should be treated with 5–10 grams of glucose per hour
          (1.2 to 2.4 mg/kg/min in a 70-kilogram subject) to provide sufficient basal energy
          requirement and prevent hypoglycemia during surgery. The dextrose concentration of the IV
          solution (Box 2) is adjusted based on the expected length of surgery. Thus, in this case
          5% of dextrose in water (D5W) can be administered intravenously via infusion pump. For
          longer surgical procedures (intraabdominal or intrathoracic surgery), 10% of dextrose
          should be used to avoid excessive fluid administration. A 20% or 50% dextrose solution
          can be infused through a central venous catheter if fluid restriction is critical. If
          additional fluids are required, for instance, to replace unexpected intraoperative blood
          losses, non-glucose-containing solutions should be administered.
          As a general rule, normal serum potassium levels do not necessarily imply that the
          total body potassium content is normal, as only 2% of total body potassium stores are
          extracellular [9]. In patients with diabetes, the metabolic homeostasis can rapidly be
          altered and many factors may influence serum potassium levels and total body potassium
          stores. These factors are (a) insulin, which increases potassium uptake by cells; (b)
          acidemia, which causes hyperkalemia as a result of the exchange of intracellular
          potassium for hydrogen ions; and (c) hyperosmolarity, which causes a rearrangement of
          potassium and fluid from intracellular to extracellular compartments. In patients with
          diabetes who have normal renal function and normal serum potassium concentration, 10 to
          20 mEq of potassium should be added to each liter of dextrose-containing fluid. A higher
          dose is required in patients with hypokalemia. If serum potassium levels are greater than
          5.5 mEq/l, potassium therapy should be withheld from the IV fluids and potassium serum
          levels should be monitored closely.
        
        
          How Would the Management of This Patient Be Different for Emergency Rather Than
          Elective Surgery?
          It has been estimated that as many as 5% of all patients with diabetes require surgery
          at some point during their lives [3]. Many of these patients undergo emergency surgery as
          a result of lower extremity infections requiring incision and drainage or even lower limb
          amputations. More than 50% of lower limb amputations in the United States occur among
          people with diabetes. The majority of patients with diabetes admitted for emergency
          procedures have a poor glyco-metabolic control and some of them may have coexisting DKA.
          The first step in management is to assess glycemic, electrolyte, acid-base, and volume
          status. An IV saline infusion should be started while waiting for laboratory tests to
          correct possible volume loss. Insulin infusion should be started at an appropriate rate
          (Box 2), and frequently insulin requirement might increase during emergency surgery. It
          is crucial that volume losses and electrolyte abnormalities are corrected prior to
          surgery.
          If DKA is diagnosed, immediate treatment is indicated, and, if possible, surgery
          should be delayed until the glyco-metabolic control is corrected and stabilized. If
          emergency surgery procedures cannot be delayed, DKA can be treated concurrently with
          surgery.
        
        
          How Should the Postoperative Glyco-Metabolic Management Be Handled in This
          Patient?
          Both IV insulin and glucose (D5W, 0.45% normal saline) infusion should be continued
          until the glycol-metabolic control is stable and until 1–2 hours after the patient is
          able to resume oral feeding without difficulty. If postoperative nausea and vomiting are
          present, IV insulin and glucose infusion should not be discontinued. Furthermore, in
          patients with type 1 diabetes ketonuria could be an early sign of impending DKA, which
          could be triggered by starvation. Capillary glucose should be monitored every 1–2 hours
          at the bedside, and the variable insulin and glucose infusion should be adjusted to
          maintain blood glucose levels between 100 and 150 mg/dl (Box 4). Serum electrolytes
          should be measured immediately after surgery. Hypo- and hyperkalemia are fairly common in
          the postoperative period and should be corrected without delay. The presence of a widened
          anion gap suggests the possibility of DKA or lactic acidosis that might be caused by
          systemic infections or hypoperfusion.
          If the patient is stable and can tolerate oral feeding, the regular home dose of
          insulin may be administered 20–30 minutes before the meal and the insulin infusion
          stopped 15–20 minutes after the meal. Of note, insulin infusion should be discontinued
          only after the SC insulin regimen is started to avoid any gaps in plasma insulin levels
          that may lead to a loss of metabolic control.
          Resuming a sliding scale SC insulin treatment postoperatively has been advocated to
          improve metabolic control in patients with type 1 diabetes. However, there are several
          drawbacks with this approach. First of all, this approach is based on a retrospective
          treatment for hyperglycemia, which reflects the degree of insulin sensitivity and glucose
          disposal rate in the preoperative period. The use of a SC insulin treatment
          postoperatively tends to create fluctuations in blood glucose levels that could be
          difficult to control. Moreover, there is a risk of exposing the patient to the risk of
          hypoglycemia if excessive doses of insulin are administered. More importantly, the use of
          a sliding scale treatment might predispose to DKA in insulin-deficient patients before
          the development of hyperglycemic levels.
        
        
          What Are the General Principles for Surgery in a Patient with Type 2
          Diabetes?
          Patients with type 2 diabetes require good blood glucose control prior to undergoing
          surgery. Although these patients seldom develop DKA, the same adverse effects of poor
          glycemic control may develop. For procedures that require general anesthesia, specific
          treatment, other than hourly glycemic monitoring, is not required for patients with type
          2 diabetes whose diabetes is well controlled with diet (i.e., fasting blood glucose less
          than 125 mg/dl). However, insulin should still be administered as described in Box 2, as
          hyperglycemic responses may still occur intraoperatively in these patients. Targeted
          glycemic levels are identical to those for patients with type 1 diabetes. For elective
          procedures requiring general anesthesia, insulin infusion is usually required to control
          hyperglycemic levels that occur during surgery. Frequent intraoperative glucose
          monitoring is imperative in this setting to avoid complications resulting from a poor
          glycemic control. Patients with type 2 diabetes who require insulin and who are scheduled
          for surgery should be managed similarly to patients with type 1 diabetes.
          As a general rule, SC insulin should not be used in patients with type 2 diabetes
          requiring surgery and general anesthesia, since insulin absorption from the SC tissue
          could be quite variable, particularly in obese individuals.
          With regard to patients with type 2 diabetes who require general anesthesia and whose
          diabetes is well controlled with sulfonylureas, there is some degree of disagreement.
          Many diabetologists recommend withholding the sulfonylurea the morning of surgery for
          procedures requiring general or local anesthesia and using continuous insulin
          infusion.
          Any patient treated with metformin should discontinue this medication at least 48
          hours before surgery. The drug should be completely cleared after discontinuing metformin
          48 hours preceding surgery. This is a prophylactic measure in an effort to lower the risk
          of lactic acidosis that could be secondary to complications of surgical procedures such
          as hypotension, myocardial infarction, or septic shock.
          Treatment decisions for patients with type 2 diabetes receiving local anesthesia and
          undergoing minor surgical procedures are similar to those described above for patients
          with type 1 diabetes.
        
      
      
        DISCUSSION
        Patients with type 1 diabetes undergoing elective or emergency surgical procedures have
        a higher degree of morbidity and mortality than those without diabetes, as a result of
        impaired glyco-metabolic homeostasis and electrolyte balance. The magnitude of the
        catabolic responses is not only related to the severity of surgical and postsurgical
        complications but also to the effects of inadequate perioperative management on metabolic
        control. Health-care providers should be very familiar with the perioperative management of
        type 1 diabetes; with individualized insulin and glucose variable infusions, young patients
        affected by type 1 diabetes can undergo surgery with a minimal risk.
        The goal of glycemic management in these situations is to maintain normal glucose
        homeostasis and normal metabolism. As insulin resistance and gluconeogenesis increase
        during surgery-related stress and anesthesia, additional insulin will be needed to prevent
        excessive hepatic glucose release. An important issue is maintaining a physiological fluid
        and electrolyte balance.
        Perioperative hyperglycemia increases the risk of infections, delayed wound healing, and
        ischemia. Achieving preprandial glycemic levels between 70 and 150 mg/dl before surgery in
        the preoperative period and maintaining plasma glucose levels between 100 and 125 mg/dl
        during surgery and between 100 and 150 mg/dl after surgery using simple and safe algorithms
        (Boxes 1, 2, and 4) can significantly decrease the risk of these complications.
      
    
  

  
    
      
        
        The development of early warning systems (EWSs) for epidemics of infectious diseases
        based on recurrent statistical patterns in other kinds of information, particularly data on
        climate, is an active area of research [1,2]. Judging from the estimated burden of diseases
        for which EWSs might be developed, such systems, if effective, would contribute greatly to
        human welfare and could potentially save many lives [2]. According to a recent report [2],
        EWSs have two principal aims: (i) to identify whether an epidemic will occur and (ii) to
        predict the number of cases that will result from it. For directly transmitted diseases,
        this second aim may be unattainable at the desired levels of precision, regardless of the
        quality of information.
        As an example, in a recent report on the relationship between climate and outbreaks of
        meningococcal meningitis, the authors found that the timing of epidemics is highly
        predictable from information on the dynamics of a seasonal weather pattern, the Harmattan
        winds, but that the final epidemic size is not [1]. This finding is not surprising. The
        characteristics of disease outbreaks, particularly outbreaks of emerging diseases to which
        human populations are highly susceptible, prevent highly precise forecasts.
        The reason that precise estimates of the final epidemic size cannot be obtained can be
        understood intuitively. Consider the following description of a typical outbreak.
        Characteristically, an outbreak begins with a small number of initially infectious
        individuals. Subsequent infectious contacts are mediated by a wide range of social
        interactions—contacts within and among households and communities—so that even individuals
        that are virtually identical can differ considerably in the number of secondary infections
        they cause. This is a micro-scale cause of variation compared with macro-scale,
        population-level sources of variation. The important implication for EWSs is that in such
        situations, especially where the basic reproductive ratio of infections (R0) is initially
        very high but is rapidly reduced (perhaps by public-health interventions), small deviations
        in the realized number of infectious contacts are amplified, resulting in relatively large
        variation in the final size of the outbreak. Because this variation reflects differences in
        individual behavior and not macroscopic characteristics of epidemic spread, it is unlikely
        that climate or other data contain any information about this source of variation (though
        such data do contain information about macroscopic variation).
        A more formal explanation of this phenomenon can be formulated based on a simple model
        of an epidemic in which an EWS captures all macroscopic causes of variation in the final
        epidemic size but no microscopic causes. Obviously, an EWS cannot realistically be expected
        to capture even all the macroscopic information. Thus, this limit to precision is a
        fundamental limit and should be interpreted as a theoretical upper bound on forecast
        precision. The simplest case considers a disease with only two macroscopic epidemiological
        characteristics, an infection rate and a removal rate, which may change over time as in the
        case of meningococcal meningitis. In particular, we assume that there is no immunity in the
        population and that infection and removal are independent in time. This model of disease
        dynamics belongs to a class of stochastic processes known as nonhomogenous birth–death
        processes, which, conveniently, turn out to be reasonably tractable. More than 50 years
        ago, Kendall [3] showed how models for the mean and the variance in the final epidemic size
        are affected by these parameters. The variance can be interpreted as a measure of the
        precision with which the final epidemic size can be predicted. Kendall's results can be
        broken down to show that this quantity is equal to the sum of the average final epidemic
        size and another quantity (x) minus one. For most realistic epidemiological parameters,
        this other quantity, which is related to the covariance between final epidemic size and the
        size of the infected population, will be much greater than one. In these cases, the
        variance in the final epidemic size will be much greater than the average final epidemic
        size itself.
        This fundamental limit to the precision of forecasts does not imply that EWSs cannot be
        used effectively to plan a response to outbreaks. Rather, it suggests what expectations of
        EWSs are reasonable. Further, since the precision with which forecasts of the final
        epidemic size can be obtained will depend on many disease-specific properties and maybe
        other factors, too, case studies of the potential effectiveness of EWSs for different
        diseases are needed. These studies should exploit recent advances in modeling birth–death
        processes [4] to gain further understanding of the differences among diseases and of the
        causes of geographic variation in the intensity of epidemics. Finally, notwithstanding
        limits to precision, the benefits to be obtained from estimates of the average final
        epidemic size and the timing of epidemics alone may warrant considerable investment in
        EWSs.
      
    
  

  
    
      
        
        A link between high levels of homocysteine, a sulfur-containing amino acid, and heart
        disease was first suggested in the 1960s, when it became clear that patients with inborn
        errors of homocysteine metabolism were prone to develop severe cardiovascular disease in
        their teens and twenties. Treatment with homocysteine-lowering substances such as folate,
        vitamin B12, and betaine reduces the incidence of heart attacks and strokes in these
        patients.
        This led to the hypothesis that mildly elevated levels of homocysteine might contribute
        to vascular disease. Subsequently, several studies have found higher mean homocysteine
        levels in patients with coronary, peripheral, and cerebral vascular disease, particularly
        in those with vascular disease not readily explained by conventional risk factors such as
        high low-density lipoprotein (LDL) cholesterol, diabetes, or smoking. Several studies then
        sought to determine whether elevated homocysteine levels were a cause or effect of
        cardiovascular disease, and evidence for a causal relationship is accumulating. However,
        whether reduction in homocysteine levels translates into a reduction in heart disease is
        still an open question.
        “We are keenly awaiting the results from several ongoing trials. In the meantime, our
        group is trying to determine the risks and benefits associated with different
        homocysteine-lowering nutrients,” said Margreet Olthof. She and her colleagues at the
        Wageningen Centre for Food Science analyzed four independent, placebo-controlled,
        randomized intervention studies that examined the effects of betaine, folic acid, and
        phosphatidylcholine on plasma homocysteine concentrations in healthy volunteers. They
        combined blood lipid data from the individual studies and compared changes in blood lipid
        concentrations between individuals taking homocysteine-lowering nutrients and those taking
        placebo.
        They found that that betaine supplementation, while effective at lowering homocysteine,
        also increased LDL cholesterol and triacylglycerol.
        This raises the possibility that any potential benefits for cardiovascular health would
        be undermined by the adverse effects on blood lipids, and make betaine less suitable as a
        homocysteine-lowering agent in healthy individuals. The data on phosphatidylcholine were
        inconclusive, but supplementation of folic acid—the most common way to lower homocysteine
        levels—does not seem to affect blood lipids. The researchers conclude that folic acid
        “therefore remains the preferred treatment for lowering of blood homocysteine
        concentrations” in healthy individuals.
      
    
  

  
    
      
        
        Schizophrenia is a devastating mental illness and a major contributor to the global
        burden of disease. In their quest to understand schizophrenia epidemiology, John McGrath
        and colleagues have previously undertaken a systematic review of schizophrenia
        incidence—that is, the number of new cases diagnosed each year in a specified population
        (see 
        BMC Medicine 2: e13). They now report results from a second systematic
        review that examines published studies on the prevalence of the disease—i.e., on the number
        of people who are suffering from the disease at a given time or within a specified time
        interval. (Incidence studies can suggest risk factors that may underlie variations in the
        disease. Prevalence studies are central to health systems planning.)
        Analyzing a total of 1,721 estimates from 188 studies and covering 46 countries, they
        calculated the following median prevalence estimates: 4.6 per 1,000 for point prevalence
        (defined as prevalence during any interval of less than a month), 3.3 for period prevalence
        (defined as prevalence during a period from 1 to 12 months), 4.0 for lifetime prevalence
        (the proportion of individuals in the population who have ever manifested the disease and
        who are alive on a given day), and 7.2 for lifetime morbid risk (which attempts to include
        the entire lifetime of a birth cohort, both past and future, and includes those deceased at
        the time of the survey).
        These numbers are consistent with key policy documents about point prevalence, but
        suggest that the 0.5%–1% estimate for lifetime prevalence given in many textbooks is an
        overestimate. This estimate, the authors suggest, “is another example where the research
        community needs to review their belief systems in the face of data.” Another often quoted
        statistic, namely that “schizophrenia affects about one in a hundred” most sensibly refers
        to lifetime morbid risk data. Here as well, the systematic analysis suggests that the
        reality is somewhat lower, and the authors suggest that “if we wish to provide the general
        public with a measure of the likelihood that individuals will develop schizophrenia during
        their lifetime, then a more accurate statement would be that about seven to eight
        individuals per 1,000 will be affected.”
        The authors were surprised to find no difference in prevalence between males and
        females, because their incidence review had found a male/female risk ratio of 1.4. On the
        other hand, the incidence study had revealed a higher incidence among migrant groups than
        among native-born individuals, and this was true for prevalence estimates as well. Compared
        to economically developed nations, the prevalence of schizophrenia is lower in developing
        nations, which is consistent with the literature showing that the course (i.e., prognosis)
        of schizophrenia is better in developing nations.
        Systematic reviews are secondary research, where the object of scrutiny is not the
        prevalence of schizophrenia per se but the literature on the topic, and the estimates in
        this review have to be treated accordingly. Regardless of exact numbers, however, the
        authors conclude that “many people with schizophrenia have persisting symptoms, despite the
        best mix of interventions we can offer.” It has been estimated that current interventions
        can at most reduce 25% of disease burden, thus the authors conclude that “this is a
        powerful argument for investing in applied and basic research.”
      
    
  

  
    
      
        
        Cardiovascular diseases (CVDs) are responsible for more than 16 million deaths
        worldwide, about 30% of total global deaths. Many of these deaths could be prevented by
        tackling major risk factors such as overweight and obesity as a result of unhealthy diet
        and physical inactivity, and smoking. Traditionally, CVDs have been considered a “Western”
        disease or a “disease of affluence” and not a pressing public health concern for low-income
        populations. However, within upper-middle-income and high-income countries, CVDs and their
        associated risk factors are increasingly concentrated among the lowest socioeconomic
        groups, and globally, 80% of all CVD deaths are in low-income and middle-income
        countries.
        In this month's 
        PLoS Medicine , Majid Ezzati and colleagues conclude that a large
        proportion of the world's population living in low-income and middle-income countries
        should indeed be the focus of attention for CVD risk factors. This attention is needed
        because the aging populations of the currently low-income and middle-income countries are
        expected to be those among whom major cardiovascular risk factors will increasingly be
        concentrated.
        The patterns of risks in relation to one another and to economic variables such as
        income are not fully established at the population level but need to be understood if
        better long-term policies and interventions are to be deployed. Ezzati and colleagues
        examined when interventions should be started by looking at the relationship between
        nutritional cardiovascular risk factors—overweight and obesity, and elevated blood pressure
        and cholesterol—and three economic indicators, using data for more than 100 countries.
        Their analysis uncovered economic–epidemiological patterns more complex than the “Western”
        or “affluence” labels would suggest. They found that body mass index (BMI) and cholesterol
        increased rapidly in relation to national income, then flattened, and eventually declined.
        BMI increased most rapidly until an income of about I$5,000 (international dollars) and
        peaked at about I$12,500 for women and I$17,000 for men. Cholesterol showed a similar
        pattern, but with some delay. The authors also found an inverse relationship between
        BMI/cholesterol and the share of household expenditure put towards food, and a positive
        relationship with proportion of population in urban centers, which may be due to changes in
        patterns of diet and physical activity with city life. For blood pressure and cholesterol,
        possible contributors to the decline at higher levels of income include dietary changes and
        use of pharmacological interventions.
        As more interventions for blood pressure and cholesterol are adopted in high-income
        societies, the three risk factors will become a feature of low-income and middle-income
        nations, the authors say. Demographic and technological changes are increasingly modifying
        the income patterns of cardiovascular risk factors and shifting their burden to the
        developing world; as a result, low-income and middle-income countries will simultaneously
        face the burden of infectious disease and cardiovascular risk factors. Unless better
        interventions are pursued, we will face a world in which all major diseases are the
        diseases of the poor, the authors warn. (See also the Perspective by Thomas Novotny [DOI:
        10.1371/journal.pmed.0020104].)
      
    
  

  
    
      
        
        Coronary heart disease (CHD) is the leading cause of morbidity and mortality in
        developed countries, and identifying and treating patients with high cholesterol has an
        essential role in the prevention of CHD. Therapeutic lifestyle changes are important for
        general reduction of risk overall, but patients who are more likely to develop CHD, or who
        have high cholesterol, should be treated with statins. Statins inhibit HMG-CoA reductase—a
        key enzyme in the cholesterol synthesis pathway.
        Although we know that not all patients who may benefit from statins receive treatment,
        there is limited information on how patients are treated according to their estimated risk
        of developing CHD. Patients may be classified as at low, medium, or high risk of developing
        CHD according to the presence of CHD, some other medical conditions (for example,
        diabetes), and major risk factors including cholesterol level, smoking, lifestyle, and
        family history. Jun Ma and colleagues analyzed data from the National Ambulatory Medical
        Care Survey and the outpatient department component of the National Hospital Ambulatory
        Medical Care Survey to identify changes in treatment from 1993 to 2002, and to identify
        current clinical practice. These surveys have been validated against other data sources,
        and used in past research on cholesterol management.
        The researchers found that between 1993 and 2002 the use of statins increased nearly
        5-fold, from 9% to 49%, in ambulatory visits by patients with high cholesterol, but then
        declined to 36% in 2002. Overall, the use of statins was three times more likely in 2001
        and 2002 than in 1995 and 1996. Patients at high risk of CHD were more likely to use
        statins than other patients. However, among patients whose visit had a reported high
        cholesterol, only 50% of patient visits at high risk of CHD and 44% of those at moderate
        risk were prescribed statins in 2002, well below the recommendations of current guidelines.
        Less than half the patient visits that arguably represent optimal opportunities for
        counseling services received counseling about how they might change their lifestyle. The
        study also showed inequities in use of statins for patients with different social and
        clinical characteristics, with lower usage in younger patients, females, African-Americans,
        and patients cared for by doctors who are not cardiologists.
        As the authors declare, the study was funded by a maker of one of the statins, but the
        information acquired is of general interest. Persistent gaps in statin therapy suggest a
        need for improved identification of patients who may develop CHD, and treatment with
        statins when indicated, the authors say. A particular focus should be patients who are at
        risk of developing CHD. Education should be aimed at improving the practice of
        physicians—above all, those who are not heart specialists—so that they adhere to
        evidence-based medicine and published guidelines for cardiovascular risk reduction. In an
        accompanying Perspective (DOI: 10.1371/journal.pmed.0020131), Fiona Turnbull from the
        George Institute for International Health says that physicians need to move away from
        making treatment decisions based on single risk factors and instead use an approach based
        on absolute risk. “An understanding of the concept of ‘absolute risk’—the probability of a
        patient developing a cardiovascular event over a specified time period—is crucial,” she
        says.
      
    
  

  
    
      
        
        There are at least 300 million acute cases of malaria each year globally, resulting in
        more than a million deaths. Ninety percent of deaths due to malaria occur in Africa south
        of the Sahara, and most occur in young children. Of the four types of human malaria—
        Plasmodium vivax , 
        P. malariae , 
        P. ovale , and 
        P. falciparum —
        P. falciparum malaria is most common in Africa, and it accounts
        for most of the extremely high mortality south of the Sahara.
        Malaria parasites are developing unacceptable levels of drug resistance, and many
        insecticides are no longer useful against mosquitoes transmitting the disease. Vaccine
        research has produced few hopeful candidates, and although millions of dollars are poured
        into research, an effective vaccine is years away.
        One recurring theme in malaria vaccine research has been the high frequency of the gene
        for sickle cell hemoglobin (HbS) in malaria endemic regions, which is believed to be due to
        a heterozygote (HbAS) advantage against fatal malaria. The mechanism behind the high degree
        of resistance conferred by HbAS in severe and complicated malaria is still unknown, but
        recent observations have suggested the mechanism might involve an immune component.
        In this month's 
        PLoS Medicine , Thomas Williams and colleagues reason that the best way
        to test whether malaria protection by HbAS has a significant immune component is to see
        whether protection varies with age. They studied the age-specific malaria pattern in 1,054
        children and adults living in Kilifi District on the coast of Kenya. They argued that if
        the malaria protection provided by HbAS were innate, it should be independent of malaria
        exposure and remain constant with age. However, if immune mechanisms were involved,
        protection should increase with age until children become functionally immune, when
        additional immunological advantage should be lost.
        They found that overall HbAS was nearly 40% protective against mild clinical malaria.
        Protection varied with age, increasing from 20% to 60% during the first ten years of life,
        and thereafter returning to 30% in children more than ten years old.
        The authors admit that this observation could be due to any factor that affects malaria
        risk and varies with age but state that accelerated immune acquisition seems the most
        likely explanation. They suggest several mechanisms for how HbAS could accelerate immune
        acquisition; for example, immunity could be mediated by accelerated acquisition of
        antibodies to altered host antigens expressed on the parasite-infected red cell
        surface.
        In discussing their findings the authors point out that their study focused on mild
        malaria. For accelerated malaria-specific immunity to be relevant to HbAS selection it
        would have to operate within a period of maximum risk for severe and fatal malaria. They
        note that in a recent study, conducted by another group in western Kenya, protection
        against severe malaria by HbAS was only seen in children 2–16 months old, but that in that
        study, no analysis was presented that addressed the effect of age within that range.
        The authors conclude that the relevance of their current observations on mild clinical
        malaria to protection against severe and fatal malaria are unknown, and that further work
        must be done to better understand the role of HbAS in protection against malaria.
      
    
  

  
    
      
        
        Recently 
        PLoS Medicine published our paper entitled “Designing Equitable
        Antiretroviral Allocation Strategies in Resource-Constrained Countries” [1]. We were
        disappointed to find that the editorial perspective written by the World Health
        Organization (WHO) ethicists regarding our paper [2] was based upon a substantial
        misunderstanding of our novel quantitative analyses and our important results. Hence, they
        misunderstood the significance of the health-policy implications of our results. Thus, we
        wish to correct the record.
        Firstly, Capron and Reis [2] misunderstood our quantitative analyses. They stated that
        “Wilson and Blower developed a mathematical model that could inform policy-makers'
        decisions regarding the optimal distribution of treatment sites to ensure equal access by
        all individuals infected with HIV.” However, our model does not determine the optimal
        distribution of treatment sites. As we clearly state in our paper [1] (and is also stated
        in the synopsis [3]), we developed a model that policy makers can use to make decisions
        regarding how to achieve the optimal allocation of scarce antiretrovirals among the
        available health-care facilities (HCFs) if the objective is to ensure treatment equity. We
        also calculated how the optimal allocation of antiretrovirals would vary if the number of
        HCFs utilized increased and/or the size of the catchment area that each HCF services
        increased [1]. Thus, we took the treatment sites (i.e., HCFs) as given, and we used their
        specific spatial location in South Africa as inputs to our model in order to determine
        optimal antiretroviral allocation strategies under a variety of conditions.
        Secondly, Capron and Reis [2] misunderstood our important results. They stated that
        “applying this tool to the South-African province of KwaZulu–Natal, Wilson and Blower were
        able to confirm mathematically the intuitive assumption that using a maximum number of
        centers, at the least possible distance from most affected populations, would lead to the
        greatest fairness in the geographical distribution of ART [antiretroviral therapy].” We
        agree that if these had been our results, they would have been trivial and obvious.
        However, Capron and Reis [2] did not discuss our actual results: we determined how to
        decide how many drugs to allocate to each of the available HCFs in order to achieve an
        optimal allocation if the objective is to ensure treatment equity. This is a very complex
        problem and the antiretroviral allocation strategies that we calculated (by using our
        model) to be optimal are very complex (see Figure 3 in our paper, which graphically shows
        the proportion of drugs that should be allocated to each of the available HCFs).
        Furthermore, we also determined what catchment area each HCF should service; specifically,
        we calculated that each HCF should serve (if the objective is to achieve treatment equity)
        a catchment area of 40–60 km. Thus, our results demonstrate (to our knowledge for the first
        time) that patients infected with HIV will have to travel extremely large distances (i.e.,
        40-–60 km) in order to receive antiretrovirals, if the objective is to achieve treatment
        equity in South Africa. We stress that currently it is unknown what the actual size of the
        catchment area is around HCFs in South Africa. Catchment areas may in fact be very small.
        Thus, we suggested [1] that a primary goal should be to obtain empirical data of the
        distances that patients in South Africa are willing (or able) to travel in order to receive
        antiretrovirals. We have been the first to provide a quantitative assessment of the
        necessary size of the catchment area, and our results have identified that there is an
        urgent need to collect these critical data for quantifying the size of the catchment areas
        around HCFs. We have determined that the size of the catchment area will be a critical
        component in the ability to achieve treatment equity in South Africa. We also compared the
        optimal antiretroviral allocation strategies that we calculated with the current plan of
        the South African government for allocating antiretrovirals [4], and we determined that the
        current antiretroviral allocation strategies in South Africa will not achieve treatment
        equity. Taken together, our quantitative results are novel and controversial, providing
        important quantitative insights into a complex public-health problem.
        We applaud the ambitious “3 by 5” WHO target for the antiretroviral rollout. However,
        the WHO has not yet devised a quantitative policy for determining how to allocate
        antiretrovirals in situations where the demand for drugs greatly exceeds the supply [5].
        Health-policy officials in each country will have to make these important and difficult
        decisions, and they will all make different decisions based upon what objectives they wish
        to optimize and prioritize. There are a multitude of factors to consider (these factors are
        well described in the recent 
        Institute of Medicine report [6]). We stress that the alternative to a
        quantitative rational approach for allocating scarce resources is an ad hoc approach, which
        is how the scarce supply of antiretrovirals is currently being distributed in many
        resource-constrained countries. Our operations research modeling approach is based upon
        spatial heterogeneity in the distribution of HCFs in South Africa and the spatial
        heterogeneity of the HIV-infected population. The most important “real world” result is
        that we show that what the South African government is currently doing is inequitable. We
        show them how to achieve equity, if they wish to do so. We hope that our novel approach for
        deciding how to allocate antiretrovirals will be of use to the WHO and also to the relevant
        authorities in the many resource-constrained countries who will soon have to make very
        difficult decisions as to who lives and who dies. Our analysis is to our knowledge the
        first analysis to show how a rational and scientific solution can be reached for deciding
        how to allocate a limited amount of antiretrovirals, if the goal is to achieve treatment
        equity. Clearly, other goals must be taken into consideration (and our model can be
        modified to include these other goals); however, we hope that treatment equity will be a
        very high priority during the antiretroviral rollout that is just beginning.
      
    
  

  
    
      
        
        Turner and Tramèr provide a cogent argument in favor of the ethical use of placebo
        controls despite “proven effective treatment” [1]. However, they are wide of the mark
        citing the APPROVe trial in support of their position. Because there is no established
        treatment to prevent adenomatous polyps, few commentators would have any objections to the
        use of placebo controls in this study. Nevertheless, they are right to suggest that it
        would have been desirable to have included a placebo control in the VIGOR study to provide
        a more rigorous assessment of safety. Whether, all things considered, a placebo control
        would have been ethical in this study of treatment for rheumatoid arthritis is
        debatable.
        Another issue not discussed in this 
        PLoS Medicine Debate is the value of placebo controls in early “proof of
        concept” efficacy trials, despite the existence of established treatment. The efficiency of
        seeking a rigorous efficacy signal before moving on to larger-scale trials (and exposing as
        few subjects as possible to drugs that might not work or turn out to be toxic) is a valid
        ethical reason for using placebo controls, provided subjects are not exposed to undue risks
        of harm from withholding established treatment [2].
      
    
  

  
    
      
        
        PLoS was founded in October 2000 not as a publisher, but as a grassroots movement of
        researchers who believed that the results of the global research enterprise should be a
        freely available public resource. So why did PLoS become a publisher, and what are we doing
        in this role to serve the world's varying biomedical researchers and clinicians?
        At its inception, the first action by PLoS was to encourage scientific and medical
        journal publishers to make the archival research literature freely available. An open
        letter, signed by almost 34,000 scientists from 180 countries, urged publishers to deposit
        copies of their research articles in a full text public repository, such as PubMed Central,
        within six months of publication. Sadly, the vast majority of publishers declined to
        deposit their works—a depressing situation that continues to this day.
        We concluded that the only way forward was to publish our own journals. These would
        provide an alternative, open-access venue for important discoveries in science and medicine
        and would serve as a model for showing that open-access publication is viable.
        The first phase of our life as a publisher involved launching our two flagship journals—
        PLoS Biology in October 2003 and 
        PLoS Medicine in October 2004. These two journals provide an open-access
        alternative to the best subscription journals in the life sciences and medicine,
        respectively.
        These first journals have helped to put open access on the map. In the 18 months since
        we started as a publisher not only have many researchers embraced these fledgling journals
        as a top tier “home” for their work, but also there has been growing international support
        for open-access initiatives.
        For example, 55 institutions worldwide have so far signed the Berlin Declaration on Open
        Access to Knowledge in the Science and Humanities. Beginning this month, researchers who
        are funded by the United States National Institutes of Health are being asked by the agency
        to deposit a copy of their accepted research papers into PubMed Central. And in the United
        Kingdom, the Wellcome Trust is making it a requirement of its grant conditions that
        Wellcome Trust–funded researchers deposit an electronic version of their manuscripts in a
        UK portal of PubMed Central within six months of publication.
        PLoS is now entering its second phase as a publisher, in which we launch the first three
        PLoS community journals. The case for launching these journals was compelling—we wanted to
        serve three research communities that had few open-access alternatives to the subscription
        journals in their field. As a result, we are launching 
        PLoS Computational Biology (www.ploscompbiol.org), a collaboration
        between PLoS and the International Society for Computational Biology, scheduled to start
        publishing in June 2005; 
        PLoS Genetics (www.plosgenetics.org), scheduled for July 2005; and 
        PLoS Pathogens (www.plospathogens.org), scheduled for September 2005.
        With the arrival of the community journals, we are providing a greater range of
        open-access venues for researchers who wish to ensure that anyone can read, use, and build
        on their work. Unlike 
        PLoS Biology and 
        PLoS Medicine , which are run by PLoS editorial staff, each community
        journal is run by the community itself—that is, by an academic editor-in-chief and
        editorial board, with production support from PLoS staff. The community-led nature of the
        new journals, coupled with a business model in which publication costs are borne largely by
        publication charges, provides an example for other journals that wish to transition to open
        access.
        When submitting their work to PLoS, how do researchers and clinicians distinguish
        between 
        PLoS Medicine and the PLoS community journals? 
        PLoS Medicine remains committed to publishing the best medical research
        that is relevant to a broad international community of clinicians and researchers. The
        community journals are aimed at a more specialized audience. 
        PLoS Pathogens , for example, will publish basic scientific research that
        “significantly advances the understanding of pathogens and how they interact with their
        host organisms.” But we also think that 
        PLoS Medicine readers will find much of interest in the community
        journals, and vice versa, and we will cross-link between articles in the different
        journals.
        The different PLoS journals will be editorially independent and submissions will remain
        confidential to each journal. But if an author would like a manuscript that is not thought
        to be appropriate by one journal to be passed on to another, along with the reviewers'
        reports and their identities, we are happy to cooperate, subject to the permission of the
        reviewers. This can help to speed up the review process.
        As we roll out the new community journals, we are already planning the third phase of
        our life as a publisher—the creation of an online repository for all technically sound
        research reports in both biology and medicine, including clinical trial reports. It is an
        ambitious plan, but one that we believe will provide authors with more choices and in the
        end, an open-access venue for the widest range of research.
      
    
  

  
    
      
        Introduction
        In addition to the role of smoking in cancer initiation and promotion, cigarette smoking
        accelerates atherogenic cardiovascular disease in both a dose- and a duration-dependent
        manner through several concurrent pathways. Smoking incites an immunologic response to
        vascular injury, described as oxidative stress leading to lipid peroxidation, endothelial
        cell dysfunction, and foam cell proliferation in the tunica media [1,2]. Smoking also
        enhances platelet aggregation, impairs lipoprotein metabolism, depresses high-density
        lipoprotein (HDL) cholesterol, and reduces distensibility of vessel walls [3,4].
        Cigarette smoking is associated with increased levels of inflammatory markers. During
        the acute phase of inflammatory states, there are quantifiable increases in C-reactive
        protein, white blood cell count, and fibrinogen, and decreases in serum albumin [5–8].
        Acute inflammatory markers have been shown to be both prognostic and predictive of future
        cardiovascular events in several populations. For example, C-reactive protein has been
        discussed extensively as a marker of cardiovascular disease risk [9–16].
        C-reactive protein in particular is also increasingly being implicated in the
        pathogenesis of atherosclerosis [17–19]. C-reactive protein is a pentaxin, highly conserved
        across species [17,20,21] and stimulated synergistically by both IL-6 and IL-1β [22–25].
        Previously believed to be synthesized by the liver, recent evidence suggests that
        C-reactive protein is also produced at the site of atherosclerosis by smooth muscle cells
        [17]. In the vessel wall, it induces expression of adhesion molecules on endothelial cells
        and increases monocyte chemotactic protein-1, which attracts monocytes and T cells into the
        vessel wall [26–28]. Additionally, C-reactive protein reduces endothelial nitric oxide
        synthase, upregulates the proatherosclerotic NF-κB pathway, enhances low-density
        lipoprotein (LDL) uptake by macrophages, which become foam cells, and facilitates T-cell-
        and complement-mediated destruction and apoptosis of the endothelium [2,26,29–31]. Thus,
        C-reactive protein is increasingly described not only as a marker of the inflammatory
        response, but also as a mediator in the pathogenesis of atherosclerotic cardiovascular
        disease [2,27]. Other inflammatory markers may also have causal properties, but are not as
        well understood. The inflammatory response therefore not only indicates atherosclerotic
        potential, but may accelerate atherosclerosis.
        Several studies have described a prevalent inflammatory state in smokers. Despite the
        known impact of smoking on cardiovascular disease progression, few studies have examined
        the impact of smoking cessation on levels of inflammatory markers or on cardiovascular risk
        reduction [6,32]. The level to which the inflammatory response subsides following smoking
        cessation and the rate at which the inflammatory response subsides are uncertain.
        Furthermore, it remains unclear whether traditional risk factors can adequately explain the
        decline in cardiovascular risk following smoking cessation.
        Using the Third National Health and Nutrition Examination Survey (NHANES III), a
        population-based representative sample of United States adults, we investigated the
        association between smoking and smoking cessation and levels of inflammatory markers and
        traditional cardiovascular risk factors. We examined the association between changes in the
        inflammatory markers—C-reactive protein, white blood cell count, albumin, and
        fibrinogen—and the traditional risk factors—total cholesterol, HDL cholesterol,
        triglycerides, systolic blood pressure, and diabetes—with decreased smoking intensity and
        increased time since smoking cessation. Our primary objective was to investigate changes in
        C-reactive protein in an effort to characterize the excess cardiovascular risk associated
        with smoking and any associated decline in risk with smoking cessation. We also aimed to
        characterize whether the inflammatory markers or traditional risk factors explained
        observed cardiac risk reduction following smoking cessation.
      
      
        Methods
        
          Study Population
          The NHANES III survey and data collection procedures have been described in detail
          elsewhere [33,34]. Briefly, NHANES III was a national probability survey conducted
          between 1988 and 1994. This survey used a complex, multi-stage, stratified cluster
          sampling design to obtain a representative sample of the non-institutionalized civilian
          United States population. Participation included a home examination and a visit to a
          mobile examination center in one of 89 locations. For those unable to travel to a mobile
          examination center, a special home visit was arranged.
          Of the 19,618 persons aged 18 and over included in the NHANES III population, we
          excluded 3,021 persons for whom C-reactive protein levels were missing, given that this
          was our primary marker of interest. Another 292 persons were excluded because they were
          pregnant, and 816 people were excluded because they reported cigar, pipe, snuff, or
          chewing tobacco use. Data for 15,489 persons were analyzed.
        
        
          Measures
          Self-reported race/ethnicity was categorized as non-Hispanic white, non-Hispanic
          black, Mexican-American, or other. The self-reported poverty to income ratio, a marker of
          socioeconomic status used in multiple prior studies [35,36], was classified as <1.5,
          1.5–3, or >3.
          Self-reported clinical factors were dichotomized for analysis. Participants reported
          on use of non-steroidal anti-inflammatory drugs, estrogen replacement therapy, and
          vitamin supplements. Use of any alcohol in the past 24 h was measured by dietary recall
          questionnaire. Prior physician-diagnosed angina, myocardial infarction, or stroke was
          defined as prevalent atherosclerotic cardiovascular disease (ASCVD). Prevalent
          inflammatory disease was defined as the presence of rheumatoid arthritis, asthma,
          emphysema, or chronic bronchitis. Presence of acute illness was indicated by a positive
          answer to the question: “In the past few days have you had a cough, cold, or other acute
          illness?”
          Additional clinical factors were measured by a combination of participant self-report
          and clinical exam and were also dichotomized for analysis. Hypertension was indicated by
          use of anti-hypertensive medication, an average systolic blood pressure over 140 mm Hg,
          or an average diastolic blood pressure over 90 mm Hg, over four readings. Diabetes
          mellitus was considered present if the participant reported physician diagnosis of
          diabetes mellitus (excluding diagnosis during pregnancy), was taking diabetes
          medications, had fasting plasma glucose over 7 mmol/l (126 mg/dl), or had non-fasting
          glucose levels over 11.1 mmol/l (200 mg/dl). Finally, serum cholesterol and triglyceride
          levels were measured enzymatically (Hitachi 704 analyzer, Boehringer Mannheim, Mannheim,
          Germany). LDL was calculated using the Friedewald equation and measured fasting
          triglycerides, total cholesterol, and HDL cholesterol. LDL cholesterol and triglycerides
          were included as continuous variables and were logarithmically transformed to approximate
          the normal curve.
          Smoking history was categorized based on both self-report and serum cotinine levels.
          Persons who gave a history of current smoking and/or had serum cotinine levels greater
          than 56.8 nmol/l (10 ng/ml) were considered current smokers. Current smokers were
          categorized into four roughly equal groups, based on number of cigarettes per day: 1–9,
          10–19, 20–29, and over 30 cigarettes per day. Former smokers were defined by self-report
          as having smoked over 100 cigarettes in their lifetime and as having quit smoking. Former
          smokers were categorized by years since smoking cessation. Since the zero year since
          cessation point is subject to misclassification bias, the following levels were used in
          analyses: <1, 1–3, 3–5, 5–7, 7–9, and >9 ysince cessation. Never smokers were
          defined by self-report and as having serum cotinine levels under 56.8 nmol/l (<10
          ng/ml). Passive smokers with cotinine levels over 56.8 nmol/l (10 ng/ml) were categorized
          as smokers at the lowest dose-intensity level.
          Our main outcome variable, C-reactive protein, was measured using a latex-enhanced
          Behring Nephelometer Analyzer System (Dade Behring, Deerfield, Illinois, United States).
          Because the presence or absence, rather than the absolute level, of C-reactive protein
          appears to be associated with cardiovascular risk, we compared undetectable versus
          detectable levels of this marker. This is consistent with previous research, given the
          nonlinearity of C-reactive protein interpretation [37,38]. Since the limit of detection
          for the latex-enhanced nephelometry assay was 2.1 mg/l, we defined undetectable as less
          than 2.1 mg/l. Other outcome variables were treated continuously. White blood cell count
          was determined using a fully automated Coulter S-PLUS JR hematology analyzer (Beckman
          Coulter, Fullerton, California, United States). Albumin was measured using the
          bromocresol purple method. Fibrinogen was measured using a quantitative assay of clotting
          time compared to a known standard.
        
        
          Statistical Methods
          The distributions of traditional risk factors, inflammatory markers, and clinical and
          sociodemographic factors were examined by smoking status. Chi-square and analysis of
          variance tests, as appropriate, were performed to test for statistically significant
          differences. Variables identified as potential confounders by these bivariate analyses
          were included in multivariate models, as were the cofactors believed to be clinically
          relevant.
          For multivariate analyses, our primary independent variables were smoking intensity
          and time since smoking cessation. Our primary dependent variables were the inflammatory
          markers—C-reactive protein, fibrinogen, white blood cell count, and serum albumin—and the
          traditional risk factors—systolic blood pressure, total cholesterol, triglycerides, HDL,
          LDL, diabetes, and alcohol use. Two sets of multivariate models were developed. The first
          set of analyses (minimally adjusted) were adjusted for age, sex, and race, given
          previously reported differences in the distribution of inflammatory markers in those
          groups. The second set of analyses (fully adjusted) were adjusted for all covariates and
          potential confounders: age, sex, race, poverty to income ratio, body mass index,
          prevalent cardiovascular disease, prevalent diabetes, prevalent chronic inflammatory
          condition, current acute illness, and use of alcohol, non-steroidal anti-inflammatory
          medication, aspirin, and estrogen replacement.
          For inflammatory markers and traditional risk factors that were measured continuously,
          linear regressions were used to model the relationship to smoking intensity and time
          since smoking cessation. Least square means were calculated. C-reactive protein (a
          dichotomous variable) was modeled with logistic regression, using never smokers as the
          reference category. Odds ratios, however, do not approximate risk ratios, given that the
          prevalence of elevated C-reactive protein is over 10%.
          The dose–response relationship between both smoking intensity and time since smoking
          cessation and each of the outcomes was assessed using Mantel tests for trend. Three sets
          of tests were performed, evaluating the trends: (1) among current smokers by cigarettes
          per day; (2) among former smokers by time since cessation; and (3) among smokers, former
          smokers, and non-smokers, despite the potential nonlinearity between groups. To further
          explain the relationship between smoking exposure and C-reactive protein, to increase the
          power to detect a trend, and to reduce residual confounding, C-reactive protein was
          categorized into three levels: under 2.1 mg/l (undetectable), 2.1–9.9 mg/l (mildly
          elevated), and ≥10.0 mg/l (clinically significant).
          Lastly, blocked group comparisons were done using adjusted chi-square tests.
          Inflammatory and traditional risk factors were compared across current, former, and never
          smokers to validate findings from previous studies.
          All analyses were weighted to represent the total civilian, non-institutionalized
          United States population; they are, therefore, population prevalence estimates. Analyses
          were conducted in SAS (version 8.02; SAS Institute, Cary, North Carolina, United States)
          and SUDAAN (version 9.0; Research Triangle Institute, Research Triangle Park, North,
          Carolina, United States), incorporating sampling weights to account for nonresponse and
          oversampling. All analyses were conducted using two-sided tests with alpha set to
          0.05.
        
      
      
        Results
        Of the 15,489 persons in our sample, 7,665 were classified as never smokers, 3,459 were
        classified as former smokers, and 4,365 were classified as current smokers. The average
        time since smoking cessation for former smokers was 13 y. The average cotinine level for
        current smokers was 1,255 nmol/l (221 ng/ml); for both former and never smokers the average
        cotinine level was <3 nmol/l (<0.5 ng/ml). Weighted descriptive statistics are given
        in Table 1.
        Bivariate analyses of smoking status showed that among the inflammatory risk factors,
        unadjusted C-reactive protein, white blood cell count, and fibrinogen were all
        significantly and positively associated with smoking status (
        p <0.01). Albumin levels were similar across smoking categories.
        Among traditional risk factors, smokers had higher total cholesterol and triglycerides, but
        were otherwise younger, had lower body mass index, and had lower blood pressure, and a
        smaller proportion were diabetic. Former smokers were most likely to have prevalent
        atherosclerotic cardiovascular disease.
        Bivariate analyses of the inflammatory markers showed that C-reactive protein was
        significantly associated with older age (
        p- trend < 0.01 by 10-y age group), female sex (
        p <0.01), and black race (
        p <0.01). Serum albumin and serum fibrinogen showed similarly
        significant associations. White cell count was associated with race (
        p <0.01), but not with age or sex. Among traditional risk factors,
        analyses showed that total cholesterol, triglycerides, HDL cholesterol, LDL cholesterol,
        and systolic blood pressure were each associated with age, sex, and race (all 
        p <0.01). Diabetes was associated with age and race only (
        p <0.01). Alcohol use was associated with age and sex only (
        p <0.01). All models, therefore, were adjusted for age, race, and
        sex.
        Table 2 displays abatement of the inflammatory response with (1) reduced intensity of
        smoking and (2) increased time since smoking cessation, adjusted for age, sex, and race.
        C-reactive protein, white blood cell count, and fibrinogen were all positively associated
        with increased smoking intensity (
        p ≤ 0.01) and were all significantly negatively associated with time
        since smoking cessation (
        p <0.05). Similarly, albumin showed a negative association with
        intensity of smoking (
        p ≤ 0.01), but no relationship after cessation. All acute phase reactant
        inflammatory markers had significant trends overall, from current to former to never
        smokers (
        p ≤ 0.01) (Figure 1).
        Differences in the traditional cardiovascular risk factors are shown in Table 3,
        adjusted for age, sex, and race. Total and HDL cholesterol, triglycerides, alcohol usage,
        and systolic blood pressure all showed a dose-dependent association with smoking intensity
        (
        p <0.05). Triglycerides and alcohol use showed time-dependent
        associations with smoking cessation (
        p ≤ 0.01). Overall trends (from current to former to never smokers) were
        present for alcohol use, triglycerides, total cholesterol, and HDL cholesterol, as shown in
        Figure 2. In models fully adjusted for all covariates (not shown), dose-dependent
        associations persisted for HDL cholesterol and triglycerides (
        p ≤ 0.01). Both total cholesterol and triglycerides showed an association
        with time since smoking cessation (
        p = 0.03 for both) and an overall trend from current to never smokers in
        fully adjusted models.
        Lastly, the inflammatory markers were examined in fully adjusted models. C-reactive
        protein continued to show abatement of the acute phase response with reduced smoking
        intensity and increased time since cessation despite adjustment for covariates (Table 4).
        White blood cell count and albumin were associated with smoking intensity but not with time
        since cessation. All positive acute phase reactants showed an associated decline from
        current to former to never smokers in fully adjusted models (
        p ≤ 0.01); albumin, likewise showed a significant increase (
        p -trend ≤ 0.01).
        Analyses using pack-years were also done (not shown); the results were unchanged using
        this measure. Additionally, data were reanalyzed excluding those individuals with prevalent
        cardiovascular disease (not shown); results were again unchanged and measures of
        association were even slightly stronger. Overall, we observed the following associations:
        (1) improvement in both inflammatory markers and traditional risk factors with decreased
        intensity of smoking, and (2) improvement in inflammatory markers with increased time since
        smoking cessation.
      
      
        Discussion
        In this cross-sectional population-based study, we were able to demonstrate dose effects
        from smoking and temporal effects from cigarette smoking cessation. The NHANES III dataset
        allowed us to utilize both self-reported smoking and serum cotinine levels to minimize
        misclassification error. It also allowed us to control for a wide range of confounders and
        assess consistency across measures by analyzing four acute phase proteins and several
        traditional risk factors. By examining both traditional risk factors and inflammatory
        markers of cardiovascular risk, this study contributes to the literature by providing a
        more complete analysis with regard to the effects of smoking and smoking cessation.
        It is known that after smokers give up smoking, their risk of mortality and future
        cardiac events declines [32,39], but there is little data quantifying the rate of risk
        reduction and when, or even whether, cardiovascular risk for former smokers reaches that of
        never smokers [12,40,41]. We found that the smoking-associated inflammatory response
        subsides within 5 y after smoking cessation. This suggests that the vascular effects are
        reversible and that cardiovascular risk subsides gradually with reduced exposure. The time
        to risk factor “correction” was longest with C-reactive protein; only one traditional risk
        factor—triglycerides—showed a similar pattern. Our findings are consistent with prior
        studies, suggesting that the full benefit of smoking cessation may be achieved gradually
        [39,42,43].
        Our estimates of the time to risk factor “correction” are shorter than those for the
        decline in mortality risk and risk for specific cardiac events reported in some prospective
        studies [42,44], but on par with those in other prospective [45], case-control [46,47], and
        cross-sectional [48] studies. Discrepancies may be due to smoking recidivism in prospective
        studies, where former smokers recommence smoking and increase their risk, such that the
        reported time since cessation is longer than the true time, or due to a lag between changes
        in cardiovascular risk factors and regression of disease. Additionally, one would expect
        morbidity risk to subside first, since mortality is a more distant endpoint. Prior studies
        also did not control for known cardiovascular risk factors, did not use serum cotinine
        levels, did not examine the associated change in inflammatory markers post-cessation, or
        grouped participants into broad categories of current, former, and never smokers,
        increasing residual confounding while failing to explore the relative effects within each
        group [9,12,49]. Nevertheless, it remains unclear whether C-reactive protein's ongoing
        decline in former smokers simply indicates a prevalent underlying burden of atherosclerotic
        plaque, or “simmering” ongoing disease processes. The direct associations of C-reactive
        protein with mortality decrement seen in these other studies—and the dose–response seen in
        our own study—suggest the latter.
        Three studies have systematically examined inflammatory markers following smoking: the
        MONICA Study (Monitoring Trends and Determinants in Cardiovascular Disease) [46], the
        Cardiovascular Health Study (CHS) [48], and the Northwick Park Heart Study [45]. The
        Cardiovascular Health Study found no association between inflammatory markers and smoking
        status itself, although C-reactive protein was strongly related to lifetime smoking
        exposure as measured by pack-years. Our results, while based on more complete modeling,
        support findings from the Northwick Park Heart Study and MONICA Study, both of which found
        that fibrinogen levels (adjusted only for age) reached normal levels within 5 y.
        Our study supports the hypothesis that cardiovascular risk falls with inflammatory
        abatement, and that inflammatory markers are better indicators of such risk reduction. In
        this analysis, triglycerides showed the strongest dose-intensity and temporal trends of the
        traditional cardiovascular risk factors. This is both consistent with prior research and
        not surprising since the strongest association between C-reactive protein and lipid
        measures is for triglycerides. Despite the colinearity, the inflammatory markers studied
        appear to have a much clearer trend and longer lasting effect after smoking cessation than
        traditional risk factors. This would suggest their greater utility in being more accurate
        markers of disease, particularly given C-reactive protein's increasingly apparent role in
        the pathogenesis of atherosclerosis. Alternative explanations include (1) the presence of
        multiple independent causal pathways leading to cardiovascular disease and (2) traditional
        risk factors correlating more closely with mortality than morbidity or underlying
        pathophysiology. The relative value of novel inflammatory markers versus traditional risk
        factors remains of much debate [50,51].
        Limitations of this study include measurement error from self-reporting, residual
        confounding from use of indicator variables, smoking recidivism, lack of newer measures
        such as interleukin-6 and high-sensitivity C-reactive protein [52], and lack of data on
        secondhand smoke. Recent studies have suggested that toxin exposures from secondhand smoke
        may impact the biomarkers used in this study [53,54]. We attempted to adjust for bias in
        self-reporting and secondhand exposure by using serum cotinine levels, but residual
        confounding may exist. It has also been suggested that circulating C-reactive protein
        levels may not reflect all relevant inflammatory effectors, owing to post-transcriptional
        regulation of C-reactive protein [22]. We chose C-reactive protein, however, for its clear
        role in the inflammatory response and for its increasing clinical relevance. Lastly, it is
        noteworthy that we are not using a linear scale, but measures of both intensity and
        duration. Time since cessation and cigarettes per day are distinctly different measures,
        and a discontinuity arises at cessation.
        Further research should explore the acute phase response in the months following smoking
        cessation [53,55–57]. Time intervals of less than a year since cessation have not been
        assessed adequately in other studies nor in this study because of sample size limitations
        of the NHANES III data. Additionally, if cardiovascular risk subsides gradually upon
        smoking cessation, do vessel wall damage and pro-aggregatory cascades all completely
        reverse course upon cessation? Future research should continue to explore the relationship
        between the inflammatory cascade and alterations in the vessel wall [7,58].
        This and other similar studies suggest that smoking cessation should play a larger role
        in public policy. Linked to poverty, decreased productivity, and premature death, tobacco
        remains the second major cause of preventable death in the world and fourth most common
        risk factor for disease worldwide [59]. While tobacco is clearly a worldwide concern,
        states and localities are left responsible for addressing tobacco use and smoking in a
        comprehensive manner. Our results suggest that policy-makers faced with escalating
        health-care costs should look to smoking cessation as an opportunity to achieve both long-
        and short
        - term health-care cost savings through cardiovascular risk reduction
        [60]. If cardiovascular benefits are, in fact, readily attainable, there exists an
        opportunity for short-term gain from smoking cessation. This may be sufficient to induce
        policy change. Larger scale action is also needed. Arguments persist in the developing
        world regarding the forced opening of tobacco markets as a result of the Global Agreements
        on Tariffs and Trade. Guidance may be forthcoming, however, from the World Health
        Organization Framework Convention on Tobacco Control, which began operation in February
        2005 as an international treaty aimed at controlling tobacco packaging, marketing, and use
        [61]. Over 60 nation-states are currently parties to the Framework Convention on Tobacco
        Control, including many European nations and Japan, although the United States has yet to
        approve this treaty. Opportunities for tobacco control remain available, and it is becoming
        increasingly clear that smoking cessation models deserve high priority both in research and
        in any preventive health care system.
      
    
  

  
    
      
        Introduction
        Embryonic stem (ES) cells are pluripotent cells derived from the inner cell mass of the
        blastocyst that can be maintained in culture for an extended period of time without losing
        differentiation potential. The successful isolation of human ES cells (hESCs) has raised
        the hope that these cells may provide a universal tissue source to treat many human
        diseases. However, directed differentiation of hESCs into specific tissue types poses a
        formidable challenge. Protocols are currently available for only a few cell types, mostly
        of neural identity [1–3], and differentiation into many of the cell types derived from the
        paraxial mesoderm has not been reported, with the exception of a recent study indicating
        osteoblastic differentiation [4]. Mesenchymal stem cells (MSCs) have been isolated from the
        adult bone marrow [5], adipose tissue [6], and dermis and other connective tissues [7].
        Harvesting MSCs from any of these sources requires invasive procedures and the availability
        of a suitable donor. The number of MSCs that can be obtained from a single donor is
        limited, and the capacity of these cells for long-term proliferation is rather poor. In
        contrast, hESCs could provide an unlimited number of specialized cells. In this study, we
        present techniques for the generation and purification of mesenchymal precursors from hESCs
        and their directed differentiation in vitro into various mesenchymal derivatives, including
        skeletal myoblasts. Our isolation method for mesenchymal precursors is the first example,
        to our knowledge, of efficiently deriving structures of the paraxial mesoderm from ES
        cells, and further highlights the potential of hESCs for basic biology and regenerative
        medicine.
      
      
        Methods
        
          Cell Culture and FACS
          Undifferentiated hESCs, H1 (WA-01, XY, passages 40–65) and H9 (WA-09, XX, passages
          35–45), were cultured on mitotically inactivated mouse embryonic fibroblasts (Specialty
          Media, Phillipsburg, New Jersey, United States) and maintained under growth conditions
          and passaging techniques described previously [3]. OP9 cells were maintained in alpha MEM
          medium containing 20% fetal bovine serum (FBS) and 2 mM L-glutamine. Mesenchymal
          differentiation was induced by plating 10 × 10
          3 to 25 × 10
          3 cells/cm
          2 on a monolayer of OP9 cells in the presence of 20% heat-inactivated
          FBS in alpha MEM medium. Flow-activated cell sorting (FACS) (CD73-PE; PharMingen, San
          Diego, California, United States) was performed on a MoFlo (Cytomation, Fort Collins,
          Colorado, United States). All human ES cell–derived mesenchymal precursor cell (hESMPC)
          lines in this study are of polyclonal origin. Primary human bone marrow–derived MSCs and
          primary human foreskin fibroblasts (both from Poietics, Cambrex, East Rutherford, New
          Jersey, United States) were grown in alpha MEM medium containing 10% FBS and 2 mM
          L-glutamine.
        
        
          Adipocytic Differentiation
          hESMPCs are grown to confluence followed by exposure to 1 mM dexamethasone, 10 μg/ml
          insulin, and 0.5 mM isobutylxanthine (all from Sigma, St. Louis, Missouri, United States)
          in alpha MEM medium containing 10% FBS for 2–4 wk. Data were confirmed in hESMPC-H1.1,
          -H1.2, -H1.3, and -H9.1 (hESMPC-H1.4 was not tested).
        
        
          Chondrocytic Differentiation
          Differentiation of hESMPCs was induced in pellet culture [5] by exposure to 10 ng/ml
          TGF-β3 (R & D Systems, Minneapolis, Minnesota, United States) and 200 μM ascorbic
          acid (Sigma) in alpha MEM medium containing 10% FBS for 3–4 wk. Data were confirmed in
          hESMPC-H1.1, -H1.3, and -H9.1 (hESMPC-H1.2 and -H1.4 were not tested).
        
        
          Osteogenic Differentiation
          hESMPCs were plated at low density (1 × 10
          3 to 2.5 × 10
          3 cells/cm
          2 ) on tissue-culture-treated dishes in the presence of 10 mM
          β-glycerol phosphate (Sigma), 0.1 μM dexamethasone, and 200 μM ascorbic acid in alpha MEM
          medium containing 10% FBS for 3–4 wk. Data were confirmed in hESMPC-H1.1, -H1.3, and
          -H9.1 (hESMPC-H1.2 and -H1.4 were not tested).
        
        
          Myogenic Differentiation
          Confluent hESMPCs were maintained for 2–3 wk in alpha MEM medium with 20%
          heat-inactivated FBS. More rapid induction was observed in the presence of medium
          conditioned for 24 h by differentiated C2C12 cells. Coculture of hESMPCs and C2C12 cells
          was carried out in alpha MEM with 3% horse serum and 1% FBS [8]. Data were confirmed in
          hESMPC-H1.3, -H1.4, and -H9.1 (hESMPC-H1.1 and -H1.2 were not tested).
        
        
          Cytochemistry
          Immunocytochemistry for all surface markers was performed on live cells. Monoclonal
          antibodies VCAM, STRO-1, ICAM-1(CD54), CD105, CD29, and MF20 were from Developmental
          Studies Hybridoma Bank (University of Iowa, Iowa City, Iowa, United States); CD73, CD44,
          and ALCAM(CD166) were from BD Biosciences Pharmingen (San Diego, California, United
          States). All other immunocytochemical analyses were performed after fixation in 4%
          paraformaldehyde and 0.15% picric acid, followed by permeabilization in 0.3% Triton X100.
          Polyclonal antibodies used were MyoD (Santa Cruz Biotechnology, Santa Cruz, California,
          United States) and nestin (gift from R. McKay); monoclonal antibodies were vimentin,
          alpha smooth muscle actin, fast-switch myosin, pan-cytokeratin (all from Sigma), and
          human nuclear antigen (Chemicon, Temecula, California, United States).
          Alkaline phosphatase reaction was performed using a commercially available kit
          (Kit-86; Sigma) and the mineral was stained with silver nitrate according to the von
          Kossa method. Fat granules were visualized by Oil Red O staining solution (Sigma). Alcian
          Blue (Sigma) was used to detect extracellular matrix proteoglycans in chondrogenic
          cultures.
        
        
          Gene-Expression Analyses
          
            RT-PCR analysis
            Total RNA was extracted by using the RNeasy kit and DNase I treatment (Qiagen,
            Valencia, California, United States). Total RNA (2 μg each) was reverse transcribed
            (SuperScript; Invitrogen, Carlsbad, California, United States). PCR conditions were
            optimized and linear amplification range was determined for each primer by varying
            annealing temperature and cycle number. PCR products were identified by size, and
            identity was confirmed by DNA sequencing. Primer sequences, cycle numbers, and
            annealing temperatures are provided in Table S1.
          
          
            Affymetrix analysis
            Total RNA (5 μg) from primary MSCs, from hESMPC-H9.1, hESMPC-H1.2, and three samples
            of undifferentiated hESCs (H1; passages 42–46), were processed by the Memorial
            Sloan-Kettering Cancer Center Genomics Core Facility and hybridized on Affymetrix
            (Santa Clara, California, United States) U133A human oligonucleotide arrays. Data were
            analyzed using MAS5.0 (Affymetrix) software. Transcripts selectively expressed in each
            of the mesenchymal cell populations (MSC, hESMPC-H9.1, and hESMPC-H1.2) were defined as
            those called “increased” by the MAS5.0 algorithm in each of three comparisons with
            independent samples of undifferentiated hESCs. A Venn diagram was generated to
            visualize overlap in gene expression. Further statistical analyses were performed as
            described below.
          
        
      
      
        Results
        Mesenchymal differentiation of hESCs (lines H1 [WA-01] and H9 [WA-09]) [9] was induced
        by plating undifferentiated hESCs on a monolayer of murine OP9 stromal cells [10], in the
        presence of 20% heat-inactivated FBS in alpha MEM medium. OP9 cells have been previously
        shown to induce blood cell differentiation from mouse ES cells [11]. After 40 d of
        coculture, cells were harvested and sorted by FACS for CD73, a surface marker expressed in
        adult MSCs [5] (Figure 1A). An average of 5% CD73+ cells was obtained from the mixed
        culture of OP9 and differentiated hESC progeny. CD73+ cells were replated in the absence of
        stromal feeders on tissue culture plates and expanded in alpha MEM medium with 20% FBS for
        7–14 d. We next established the membrane antigen profile of the resulting population of
        flat spindle-like cells. The H1- and H9-derived CD73+ cells expressed a comprehensive set
        of markers that are considered to define adult MSCs, including CD105(SH2), STRO-1, VCAM
        (CD106), CD29(integrin β1), CD44, ICAM -1(CD54), ALCAM(CD166), vimentin, and alpha smooth
        muscle actin (Figure 1B and 1C). The cells were negative for hematopoietic markers such as
        CD34, CD45, and CD14. They were also negative for neuroectodermal, epithelial, and muscle
        cell markers including nestin, pancytokeratin, and desmin (data not shown). The human
        identity of these presumed mesenchymal cells (termed hESMPC-H1.1, -H1.2, -H1.3, -H1.4, and
        -H9.1) was confirmed for all experiments by immunocytochemistry for human nuclear antigen
        to rule out the possibility of contamination with OP9 cells (Figure S1).
        To further characterize hESMPCs, we performed genome-wide expression analysis using
        oligonucleotide arrays (Affymetrix U133A). The expression profiles of hESMPC-H1.2 and
        hESMPC-H9.1 were compared with that of human primary adult MSCs. Housekeeping genes for
        each of the mesenchymal cell populations were eliminated by subtracting those transcripts
        also expressed in at least one of three independent samples of undifferentiated hESCs.
        Based on this analysis, 1,280 transcripts were selectively expressed in hESMPC-H1.2, 932
        transcripts in hESMPC-H9.1, and 1,218 transcripts in primary adult MSCs. A remarkable
        overlap of 579 transcripts shared among the three mesenchymal populations was observed
        (Figure 1D). Using the genes that were selected in the initial filter, we performed a
        statistical analysis on the expression levels to determine whether the genes were expressed
        significantly differently in the two cell types. We used a Bayesian extension to the
        standard 
        t -test [12] to assess this difference. Of the 579 genes, 412 of them
        were significantly different, at a false discovery rate cutoff of 0.05. The relative fold
        changes were also extremely large in many of the cases. We also looked at the variance of
        the expression levels within the cell types. For the MSCs, 94% had a coefficient of
        variation less than 20% for the expression (log transformed); for the ES-derived cells, 72%
        had a coefficient of variation less than 20%. Numerous known MSC markers were included in
        the list of 412 genes, such as the 
        mesenchymal stem cell protein DSC54 (13.9-fold increase, 
        p < 0.001), 
        neuropilin 1 (30.4-fold increase, 
        p < 0.001), 
        hepatocyte growth factor (48.1-fold increase, 
        p < 0.001), 
        forkhead box D1 (14.8-fold increase, 
        p < 0.001), and 
        notch homolog 2 (2.9-fold increase, 
        p < 0.001) . Table S2 lists the 
        p -values from the test, the mean and standard deviation of the
        expression levels, and the relative fold change of all 412 genes between the two types.
        Known markers of MSCs, such as mesenchymal stem cell protein DSC54, were all included
        within the 579 shared transcripts. These findings support the immunocytochemical data and
        suggest that hESMPCs and primary MSCs are highly related.
        MSCs are characterized functionally by their ability to differentiate into mesenchymal
        tissues, such as fat, cartilage, and bone. Therefore, we tested whether hESMPCs have the
        same potential (Figure 2).
        Adipocytic differentiation of hESMPCs was induced under conditions described previously
        for primary adult MSCs [5]. Appearance of cells harboring fat granules was observed after
        10–14 d in culture. After 3 wk of induction, more than 70% of the cells displayed Oil Red
        O+ fat granules, and 
        PPARγ, a marker of adipocytic differentiation, was detected by RT-PCR.
        (Figure 2A).
        Chondrocytic differentiation was achieved using the pellet culture system [5]. After 28
        d in culture, more than 50% of all cells exhibited robust staining for Alcian Blue, a
        marker specific for extracellular matrix proteoglycans. Chondrocytic differentiation was
        confirmed by the gene expression of 
        collagen II and 
        aggrecan , two components of extracellular matrix selectively expressed
        by chondrocytes, using RT-PCR (Figure 2B).
        Osteogenic differentiation was induced in the presence of β-glycerolphosphate [5].
        Osteogenesis was demonstrated by specific staining for calcium deposition in the matrix
        (von Kossa, Figure 2C; or Alizarin Red, Figure S2A) and increased expression of 
        bone-specific alkaline phosphatase and 
        bone sialoprotein at day 28 of treatment (Figures 2C and S2B). At day 28,
        Alizarin Red staining was detected in approximately 70% of all cells. Throughout these
        studies, human adult MSCs and foreskin fibroblasts were used as positive and negative
        controls, respectively.
        In addition to adipocytic, chondrocytic, and osteogenic differentiation, reports
        suggested that adult MSCs can form skeletal muscle [13]. Although generation of skeletal
        muscle cells from adult MSCs remains controversial, we tested whether hESMPCs exhibit this
        potential. Under the conditions previously described [13], hESMPC-H1.1 and -H9.1 did not
        yield significant numbers of MyoD+ cells after 15–20 d in culture. However, when confluent
        cells were maintained in culture in the presence or absence of 5-AzaC without passage for
        more than 21 d, expression of specific skeletal muscle markers such as MyoD and fast-switch
        myosin was observed (Figure 3A). More rapid myogenic differentiation was obtained in the
        presence of 24-h-conditioned medium from the murine myoblastic cell line C2C12 previously
        induced to form myotubes [14]. Direct coculture of hESMPCs with C2C12 cells led to the
        formation of hESMPC-derived myotubes, as visualized by expression of human nuclear antigen
        (Figure 3B), similar to those formed by host C2C12 cells. After 1 wk of coculture, myotubes
        composed of human nuclei accounted for more than 10% of the total number of human cells
        present, and each human myotube was composed of up to ten human nuclei. Human cell
        contribution to myotubes in coculture was confirmed by expression of human muscle-specific
        transcripts such as 
        MyoD, myosin heavy chain IIa , and 
        myogenin (data not shown). These data demonstrate that hESMPCs can give
        rise to mesenchymal derivatives typically obtained from primary adult MSCs, as well as to
        cells expressing markers of skeletal muscle.
        One concern for the clinical application of hESC-derived progeny in regenerative
        medicine is the risk of teratoma formation due to the presence of residual undifferentiated
        ES cells among the differentiated progeny. We did not detect markers of undifferentiated
        hESCs, such as 
        Nanog [15] or 
        Oct-4 [16], in any of the hESMPCs by RT-PCR (see Figure 2D) and
        immunocytochemistry (data not shown), suggesting the lack of any undifferentiated ES cells
        in hESMPC cultures. However, future in vivo studies are required to rule out the potential
        of these cells for teratoma formation.
      
      
        Discussion
        Previous studies have demonstrated the derivation of neural cells [1–3], hematopoietic
        [17] and endothelial lineages [18], and cardiomyocytes [19] from hESCs. This study presents
        the induction of paraxial mesoderm with the generation of multipotent mesenchymal
        precursors. We calculate that under these conditions a single undifferentiated hESC yields
        an average of one CD73+ cell at day 40 of differentiation, suggesting a balance between
        cell proliferation and cell selection. There were no obvious differences in marker and
        gene-expression profile or in differentiation behavior among the five hESMPC lines
        generated. However, some of the lines (e.g., hESMPC9.1) exhibited a tendency of spontaneous
        osteogenic differentiation after long-term propagation. Directed differentiation of hESCs
        into somatic stem-cell-like precursors represents a substantial advancement in harnessing
        the developmental potential of hESCs. The high purity, unlimited availability, and
        multipotentiality of hESMPCs will provide the basis for future therapeutic efforts using
        these cells in preclinical animal models of disease. Such in vivo studies will also be
        required to properly assess the safety profile of these cells. Furthermore, our system also
        offers a novel platform to study basic mechanisms of mesodermal induction and
        differentiation during early human development.
      
      
        Supporting Information
        
          Accession Numbers
          The Gene Expression Omnibus (GEO) (http://www.ncbi.nlm.nih.gov/geo) accession number
          for all raw microarray data used in this study is GSE2248.
          The Unigene (http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=unigene) accession
          numbers for the gene products discussed in this paper are aggrecan (Hs.2159
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=2159; bone sialoprotein
          (Hs.518726 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=518726;
          bone-specific alkaline phosphatase (Hs.75431
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=75431; collagen II
          (Hs.408182 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=408182; forkhead
          box D1 (Hs.519385 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=519385;
          hepatocyte growth factor (Hs.396530
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=396530; mesenchymal stem
          cell protein (DSC54, Hs.157461
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=157461; MyoD (Hs.520119
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=520119; myogenin (Hs.2830
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=2830; myosin heavy chain
          IIa (Hs.513941 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=513941;
          Nanog (Hs.329296 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=329296])
          [15]; neuropilin 1 (Hs.131704
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=131704; notch homolog 2
          (Hs.549056 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=549056; Oct-4
          (Hs.504658 [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=504658; and
          PPARγ (Hs.162646
          [http://www.ncbi.nlm.nih.gov/UniGene/clust.cgi?ORG=Hs&CID=162646]).
        
      
    
  

  
    
      
        Introduction
        Twins have long provided a unique opportunity to study how health is shaped from
        conception to death by biological and social factors [1â€“5]. At issue are
        the contributions, singly and combined, of genetic inheritance, in utero postzygotic events
        before and after twinning, and familial plus societal contexts, including the ways in which
        twins are treated by family members, each other, and society at large
        [1â€“7].
        To explore these issues, one major trend in twin research has focused on comparing
        health status of twins raised separately since birth or early childhood
        [1â€“7]. Far fewer studies have investigated how twins raised together,
        but who differ in their postadolescent socioeconomic position, compare on adult health
        status [2,4]. Yet such research could potentially inform current debates over the
        contribution of lifecourse socioeconomic conditions to adult health
        [8â€“11], given twins' shared genetic inheritance and early life
        socioeconomic plus biological exposures. Twins afford an important opportunity to examine
        the additional impact of adult experiences on adult health in a population matched on early
        life experiences.
        In particular, one important unresolved issue in the burgeoning literature on lifecourse
        analysis of health concerns how well early life social circumstances are measured, since
        this these data are essential for distinguishing between the influence of early life and
        adult conditions on adult health status [8,9]. At issue are the often limited data on
        childhood socioeconomic conditions [8,9], plus the possibility of systematic error, by
        adult socioeconomic position, when adults recollect their early childhood circumstances
        [12â€“14]. Limited data, recall bias, and poor measurement together hinder
        obtaining accurate effect estimates, due to confounding by unmeasured factors. This concern
        is especially salient for studies investigating the social patterning of health, precisely
        because living and working conditions influence health through myriad discrete yet
        entangled pathways [8,15].
        Further complicating analysis of the impact of childhood and adult socioeconomic
        position on health are choices regarding the socioeconomic measure(s) employed
        [16â€“18]. As discussed in several comprehensive review articles
        [16â€“18], considerable evidence exists demonstrating that different
        socioeconomic measuresâ€”e.g., education, occupation, income, wealth,
        housing tenure, etc.â€”are not simply
        â€œexchangeableâ€ with each other and instead often yield
        different estimates of the magnitude of the socioeconomic gradient and affect health by
        independent as well as correlated pathways. For example, while education has often been
        valued as a socioeconomic measure precisely because, once achieved, it is not subject to
        reverse causation (e.g., poorer health leading to lower income), it also has been shown to
        be insensitive to subsequent changes in adult socioeconomic position (e.g., income
        dynamics) that also can affect adult health status [16â€“22]. An important
        implication is that studies concerned with the joint impact of childhood and adult
        socioeconomic position on health must take into account how their choice of socioeconomic
        measures may influence their results.
        Of note, studies of adult twins, and especially monozygotic twins, can usefully address
        problems of capturing early life circumstances and assessing the contribution of childhood
        and adult conditions on adult health. This is because monozygotic twins raised together
        through childhood (a) are tightly matched on both genetic endowment and the socioeconomic
        circumstances characterizing their gestation and early life and childhood household
        resources [4â€“6], and (b) are the same biological sex, so they are likely
        to be accorded the same gender expectations and not have differential treatment or access
        to household resources because of their gender [6,15]. Thus, even without any measurement
        of childhood conditions, a comparison of adult monozygotic twins who are concordant versus
        those who are discordant on adult socioeconomic position allows ascertainment of the extent
        to which socioeconomic position after childhood affects adult health, above and beyond the
        impact of childhood socioeconomic position. Twin analyses employing diverse socioeconomic
        measures capturing circumstances earlier versus later in adult life could also potentially
        yield insight into the impact of cumulative socioeconomic position on health, with
        interpretation of results for monozygotic twins, including patterns of within-pair
        variability, aided by comparison to results to same-sex dizygotic twins.
        Thus, our objective, framed by ecosocial theory and its concern with the lifelong
        embodiment of social conditions [15,23], was to compare health status among a cohort of
        monozygotic and dizygotic women twins with shared upbringing (at least until age 14) and
        concordant versus discordant adult socioeconomic position. Outcomes included biological
        markers, anthropometric and health outcomes, and health behaviors. We employed data on both
        adult occupational class and educational level, hypothesizing that the former might capture
        relevant aspects of socioeconomic position occurring after completion of educational
        attainment.
      
      
        Methods
        
          Study Population
          The study twin pairs were members of the Kaiser Permanente Women Twins Study
          Examination II, conducted in 1989â€“1990 in Oakland, California, United
          States [24]. The Examination I cohort included 434 twin pairs recruited in
          1978â€“1979 from a twin registry established in 1974 at the Northern
          California Kaiser Permanente Medical Care Program. All participants resided in the San
          Francisco Bay Area at the time of Examination I and were born in or prior to 1960 (mean
          age, 41 y; range, 18â€“85 y). Zygosity for each pair was determined by
          analysis of 20 polymorphic loci, such that the probability of misclassification as
          monozygous was less than 0.001 [24].
          For Examination II (1989â€“1990), original cohort members were sent a
          self-administered questionnaire on their health and sociodemographic characteristics plus
          an invitation to return for a physical exam [24]. Cohort retention was high: Only 72
          women (8.3%) did not respond, of whom 36 were deceased. Among the 796 respondents, only
          87 (10.9%) did not return for a physical exam. After additionally excluding five women
          whose twin was a nonrespondent, the Examination II cohort included 352 twin pairs (58%
          monozygotic and 42% dizygotic), representing 81.1% of the original cohort. Enrollment and
          study of the twins in both cohorts was approved by the Kaiser Permanente Medical Care
          Program, Northern California Region, Institutional Review board; analyses for this
          investigation were additionally approved by the Harvard School of Public Health Human
          Subjects Committee.
        
        
          Socioeconomic Data
          Childhood and adult socioeconomic position were measured at Examination II using a
          self-administered questionnaire. We employed a modified version of Erik Olin Wright's
          occupational class schema [12,16,25â€“27], analogous to the United
          Kingdom's newly established National Statistics Socioeconomic Classification system
          (NS-SEC) [28]. Distinctions, in order of dominance, were between persons classified as
          â€œnonworking classâ€ (NWC; own a business and employ others,
          self-employed, or supervisory employees), â€œworking classâ€
          (WC; nonsupervisory employees), or not in the paid labor force
          [12,16,25â€“27].
          We defined â€œchildhood household social classâ€ as the
          occupational class position of the person identified as the head-of-household when the
          respondent was age 14; we also ascertained the proportion of twins who lived together
          until at least age 14. We measured adult household social class using a validated,
          gender-appropriate approach, equal to the most dominant class position, taking into
          account the usual individual class position of the respondent and her partner or other
          head-of-household, if any [16,25,26]. Using a gender-neutral approach to measuring
          household class is increasingly recognized as a more valid means of assessing household
          socioeconomic position than one which automatically assigns it to either (a) the
          respondent, whether a woman or man, or (b) the occupation of the adult man in the
          household (if one is present), given the rise of dual wage-earner households
          [16â€“18,27â€“32]. For example, the new UK NS-SEC
          measure explicitly rejects the prior conventional practice of â€œmales
          taking precedence over femalesâ€ when selecting the
          â€œhousehold reference personâ€ for assignment of household
          class, and instead chooses based upon â€œthe person responsible for
          owning or renting or who is otherwise responsible for the accommodation,â€
          regardless of gender [29]. We also obtained data on the educational level attained by
          each twin and their father. No data were available, however, on childhood or adult
          household income, wealth, or debt, or on the educational level of the mother.
          Because the twin pairs were matched, by definition, on socioeconomic position in utero
          through age 14, we categorized twin pairs in relation to adult socioeconomic position.
          For adult household social class, three types of pairs were possible: two concordant
          (both WC:WC or NWC:NWC) and one discordant (WC:NWC). For education, the pairs were
          defined in relation to being concordant or discordant for fewer than 4 y versus 4 y or
          more of college.
        
        
          Health Outcome Data
          The selected health outcomes were chosen because of their well-documented associations
          with socioeconomic position and because risk could plausibly be affected by both early
          life and adult circumstances [8,9,25]. Self-report data were analyzed for self-rated
          health (dichotomized as excellent/good versus fair/poor) and medication use. A validated
          interviewer-administered questionnaire was used to obtain data on physical activity (kcal
          per kg per y); this instrument assessed the typical amount of time spent in activities of
          varying intensity at home, at work, and during recreation [33].
          Data on anthropometric and biological characteristics were obtained by physical
          examination and laboratory analysis [24]. Height was recorded to the nearest 0.5 cm,
          weight was measured to the nearest 0.1 kg, and these data were used to calculate body
          mass index (BMI, in kg/m
          2 ). Participants' minimum waist girth was measured using a steel tape
          at the natural indentation or at a level midway between the iliac crests and the lower
          edge of the rib cage if no natural indentation was present; hip girth was measured at the
          level of the greatest protrusion of the buttocks. These measurements were recorded to the
          nearest 0.5 cm, and the averages of two measures (different by no more than 1 cm) were
          used to calculate the waist-to-hip ratio (WHR). After participants rested for 5 min, a
          mercury sphygmomanometer was used to take two measures each of systolic and diastolic
          blood pressure (seated, right arm); averages of these two measures were used for data
          analysis. High blood pressure was defined as systolic blood pressure 140 mm Hg or higher,
          or diastolic blood pressure 90 mm Hg or higher, or taking antihypertensive
          medication.
          Blood for lipid and lipoprotein measurement was obtained after participants had fasted
          overnight. It was collected into tubes containing ethylenediaminetetraacetic acid (EDTA).
          Total, high-density lipoprotein (HDL), and low-density lipoprotein (LDL) cholesterol were
          measured by standard methods [24]. Glucose level (mg/dl [amount Ã— 0.0555 =
          mmol/l]; measured 2 h post-load) was determined using the glucose oxidase method [24];
          analyses using data on glucose levels excluded the five twin pairs for whom one or both
          twins had values 300 mg/dl or higher.
        
        
          Data Analysis
          First, to establish the analytic cohort, we identified twin pairs for whom we could
          determine both that they had lived together until at least age 14 and their joint
          socioeconomic trajectory (
          n = 308 pairs). This analytic cohort excluded 44 twin pairs (21 missing
          data on duration lived together; one reporting separation prior to age 14; two where one
          twin said below age 14 and the other age 14 or above; plus 20 pairs for whom the joint
          data on adult household class was either missing, inconsistent, or not in the labor
          force). Second, we ascertained the retained twins' sociodemographic and health
          characteristics. Third, for continuous outcomes, we calculated (a) the mean matched
          difference for twin pairs discordant on adult socioeconomic position, setting the twin
          with the most socioeconomic resources as the baseline, so as to determine both the
          magnitude and direction of differences in the outcomes among these pairs, and (b) the
          mean matched absolute difference for twin pairs in each socioeconomic stratum, to
          ascertain the variability of outcomes among the twin pairs both discordant and concordant
          on adult socioeconomic position. Additionally, for the categorical outcomes, we
          calculated the kappa statistic and associated 95% CI [34]. We do not report data on the
          18 WC:WC twin pairs, because small numbers rendered the parameter estimates
          uninterpretable. All analyses were done in SAS [35].
        
      
      
        Results
        As shown in Tables 1 and 2, the sociodemographic and health characteristics of the full
        cohort (
        n = 352 pairs) and the analytic cohort (
        n = 308 pairs) were quite similar, with about 40% having grown up in
        working class households and 80% in households in which the father had less than a 4-y
        college education. At Examination II, 32% of the twin pairs in the analytic cohort were
        discordant for adult household occupational class, and 20% were discordant for individual
        college attainment.
        Results pertain to both the direction and magnitude of the difference in health status
        among the twin pairs, in relation to both different measures of adult socioeconomic
        position and zygosity. First, regarding the magnitude of health disparities among
        monozygotic twins discordant on adult socioeconomic position, for the analyses using data
        on adult occupational class (Tables 3 and 4), the WC twin had significantly higher systolic
        blood pressure (mean matched difference = 4.54 mm Hg; 95% CI,
        0.10â€“8.97), diastolic blood pressure (mean matched difference = 3.80 mm
        Hg; 95% CI, 0.44â€“7.17), and LDL cholesterol than her NWC twin (mean
        matched difference = 7.82 mg/dl [amount Ã— 0.0259 = mmol/l]; 95% CI,
        1.07â€“14.57). Additionally, among the monozygotic twin pairs, a greater
        proportion of twin pairs discordant on occupational class were discordant for self-reported
        health compared to twin pairs concordant on occupational class (27.5% versus 6.9%; 
        p = 0.0178). Poorer health was also more likely to be reported by the
        working class twin; among the 51 monozygotic pairs discordant on class, the proportion of
        pairs in which the WC twin reported fair or poor health while her NWC twin reported
        excellent or good health (17.6%) was almost twice that of the converse (9.3%, i.e., pairs
        in which the WC twin reported good or excellent health and the NWC twin reported fair or
        poor health).
        By contrast, corresponding analyses using data on educational level (Tables 5 and 6)
        revealed little difference in patterns of health among monozygotic twin pairs discordant on
        educational attainment. Dizgyotic twins discordant on adult socioeconomic position, whether
        categorized by occupational class or educational level, likewise did not notably differ on
        their adult health status (Tables 3â€“6).
        Second, regarding the variability in health outcomes among twin pairs in relation to
        their adult socioeconomic position, the mean matched absolute difference was similar among
        both monozygotic twins who were discordant and concordant on occupational class, and also
        was similar among dizygotic twins discordant and concordant on occupational class (Tables 3
        and 4). Within occupational class strata, however, for all the continuous outcomes other
        than diastolic blood pressure, the magnitude of variability typically was greater for the
        dizygotic than the monozygotic twin pairs (Tables 3 and 4). Similar results were obtained
        for analyses based on educational level (Tables 5 and 6), with some important exceptions.
        Specifically, for several outcomes among the monozygotic twins, especially average systolic
        and diastolic blood pressure, post-load glucose, and physical exercise, variability was
        greatest among twin pairs in which both had fewer than 4 y of college, intermediate among
        discordant pairs, and least among those where both had 4 y of college or more.
      
      
        Discussion
        Our study provides novel evidence suggesting that correlations in health outcomes among
        adult women twin pairs who lived together through childhood vary by their subsequent
        socioeconomic position, with results sensitive to choice of socioeconomic measure. Although
        small numbers limit precision of estimates, cardiovascular factors differed more among
        twins who were discordant on adult occupation class than twin pairs concordant on being
        professionals, and, within twin pairs discordant on occupational class, the working-class
        twin typically fared worse than the professional twin. These patterns were much weaker or
        not evident for analyses using data on educational attainment. Together, these results,
        combined with our prior research showing that the twins who experienced cumulative
        deprivation had the worst health [26], lend additional support to the hypothesis that
        cumulative experiences across the lifecourse, including those after adolescence and after
        completion of educational attainment, and not just early life experiences, shape adult
        health [8,9].
        Additionally, the greater magnitude of variability in outcomes among dizygotic compared
        to monozygotic twins within the same socioeconomic strata is what would be expected, given
        the tighter matching on genetic endowment among the monozygotic twins
        [3â€“5]. However, the suggestive finding of greater magnitude of
        variability, within both the monozygotic and dizygotic twins, among pairs with the least
        education compared to the most education, especially for the cardiovascular-related
        results, has not to our knowledge previously been reported. Given that low educational
        attainment is highly correlated with low socioeconomic resources during childhood
        [16â€“18], our results lend tentative support to the hypothesis that
        increased variability of physiological traits such as blood pressure may be positively
        associated with greater early-life and cumulative exposure to economic deprivation [36]. A
        related body of research suggests that chronic exposure to social stressors associated with
        socioeconomic deprivation may result in repeated activationâ€”and
        ultimately harmful dysregulationâ€”of physiological systems that respond
        to stress, thereby increasing risk of elevated blood pressure, insulin resistance, and
        visceral fat deposition and thus risk of cardiovascular disease, obesity, and diabetes
        [37â€“39].
        Study limitations include (a) the relatively small number of twin pairs (albeit similar
        to other twin studies [3,11]); (b) lack of data on detailed occupational class position
        over time and on age at obtaining a college degree, plus prior or current data on income,
        poverty, wealth, and debt; (c) lack of data on gestational age, birth weight, birth order,
        and whether the twins had shared or separate chorions and amniotic sacs [2,7,11,40]; (d)
        lack of data on differences in the twins' childhood experiences and exposures (e.g.,
        differential treatment accorded to first- versus second-born twins, and to monozygotic
        versus dizygotic twins [6]); and (e) lack of data on male twins; in addition, the small
        number of women twins who were concordant on adult working class position limits
        generalizability (but not internal validity) of results. Most studies assessing the impact
        of childhood socioeconomic position on health, however, have relied on occupational and
        sometimes educational data [8,9,26,41â€“46], reflecting difficulties in
        obtaining income data across the lifecourse [16â€“18].
        By contrast, strengths of our study include: (a) biological confirmation of zygosity;
        (b) identical gestational age; (c) identical biological sex, relevant to gender
        expectations and gendered exposures (more similar for same- versus opposite-sex twins
        [5,6]); (d) data on age until which the twins lived together; (e) use of a validated and
        gender-appropriate household occupational class measure, plus data on education; and (f)
        measurement of anthropometric and physiologic characteristics, not just self-reported
        health. Moreover, by focusing on postadolescence divergence of socioeconomic position, the
        study avoided concerns affecting comparisons of twins raised separately versus together,
        e.g., difficulties in assessing similarities versus differences of the family of origin
        versus adoptive family [4â€“6]. A recent analysis of United Kingdom twins'
        earnings in relation to educational level additionally underscores the utility of using
        twin analyses to gauge the impact of childhood and adult socioeconomic conditions, at the
        individual and the household level, on adult economic and health-related outcomes (e.g.,
        smoking) [47].
        Overall, results of this study are in accord with other research suggesting that
        cumulative exposures related to socioeconomic position, not only genetic inheritance and
        early life experiences, shape adult health
        [8â€“11,26,41â€“46,48]. As with our findings, these
        studies typically have documented the strongest joint impacts for outcomes pertaining to
        cardiovascular health [8â€“11,26,41â€“46,48]. Unlike
        prior research, however, the present study newly employed a same-gender twin design,
        affording comparatively tight matching on life circumstances through early adolescence,
        with monozygotic twins additionally matched on genetic inheritance, thereby circumventing
        important concerns raised about likely unmeasured confounders affecting results of prior
        studies dependent upon adult recall ofâ€”and limited data
        onâ€”childhood socioeconomic position. Even so, generalizability of
        results to nontwins could be hampered if twins differ systematically from nontwins on
        factors influencing associations between socioeconomic position and adult health, as
        perhaps related to maternal and zygotic characteristics relevant to risk of monozygotic or
        dizygotic twinning or to exposures contingent upon being a twin in utero (e.g.,
        down-regulation of growth) [2,4â€“7,49â€“52].
        In summary, creative use of social and biological twin data concerning both social and
        biological aspects of twinship [1â€“3,6] has the potential to inform
        current debates about the impact of lifecourse socioeconomic position on health. Suggesting
        such investigations could have public health import, prior research has estimated that a
        reduction of 2 mm Hg in the average diastolic blood pressure in the United
        Statesâ€”i.e., about half the difference we observed in the comparison of
        working class to nonworking class monozygotic twinsâ€”would translate to a
        17% decrease in hypertension, a 6% reduction in coronary heart disease, and a 15% reduction
        in risk of stroke and transient ischemic attacks [53]. Given the longstanding fascination
        with twins [1â€“3,6], if additional and larger twin studies of
        economically diverse women and men twins confirmed the relevance of cumulative and
        intergenerational lifetime socioeconomic resources for health, and were also able to
        include a wider array of socioeconomic measures (e.g., income, wealth, debt, and mother's
        education) and data on gestational age and birth weight, the evidence would likely have
        high policy salience, plus importantly enhance understanding of how embodiment of societal
        conditions shapes population patterns of health, disease, and well-being [15,23,54].
      
    
  

  
    
      
        
        In the past 20 years, advancing maternal age and greater reliance on assisted-conception
        techniques have dramatically increased the incidence of multiple births, including
        monochorionic twins (see Glossary) [1,2]. Concomitantly, there have been advances in our
        understanding of these high-risk pregnancies.
      
      
        Risks of Multiple Pregnancy
        The offspring of multiple pregnancies are at greater risk for adverse perinatal outcomes
        compared to their singleton counterparts, predominantly due to increased risks for preterm
        delivery and due to monochorionicity [3]. The link between monochorionicity and adverse
        perinatal outcomes has become increasingly strong [4]. Monochorionic pregnancies have an
        approximately 15% risk of developing the twin-to-twin transfusion syndrome (TTTS), which
        can be associated with perinatal mortality and morbidity despite treatment [5]. In
        addition, single intrauterine fetal demise (IUFD) in a monochorionic pregnancy may be
        associated with a more-than 20% risk for multicystic encephalomalacia in the surviving
        co-twin [6].
      
      
        Managing High-Risk Pregnancies
        Despite these advances in knowledge about risks, there is very little consensus about
        the management of these high-risk pregnancies. In addition, there is the temptation to be
        reassured by increasing gestational age as the potential complications of prematurity
        recede. Recent studies suggest, however, that the offspring of a multiple gestation may
        benefit from delivering prior to their expected date of delivery [7,8].
        Several studies have focused on the “prospective risk of fetal death” to help determine
        by which gestational age a multiple pregnancy should be delivered [7,8]. For twins, the
        prospective risk of fetal death appears to be equivalent to that of post-term singletons at
        approximately 37 to 38 weeks' gestation [7,8]. The prospective risk of fetal death for
        twins intersects with neonatal death at approximately 39 weeks' gestation, indicating that
        it may be reasonable to consider delivery of uncomplicated twins prior to 40 weeks'
        gestation [8]. These studies, however, did not address the impact of chorionicity on the
        decision to deliver a multiple pregnancy.
      
      
        A New Study of Monochorionic Twins
        In this issue of 
        PLoS Medicine , Barigye et al. report the prospective risk of fetal death
        in uncomplicated monochorionic diamniotic twin pregnancies derived from a cohort of
        pregnancies that were managed at a single tertiary care referral center and that delivered
        after 24 weeks' gestation [9]. Patients were excluded from the study if the pregnancy was
        complicated by TTTS, monoamnionicity, intrauterine growth restriction, growth discordance,
        structural anomalies, or twin reversed arterial perfusion sequence. Conjoined twins and
        high-order multiples were also excluded. All patients with uncomplicated monochorionic
        twins were managed according to a standard protocol. They received first-trimester nuchal
        translucency assessment and chorionicity determination, a midtrimester anatomical survey,
        and fetal echo followed by growth scans, amniotic fluid assessment, and Doppler studies
        every two weeks. The rate of fetal death was derived for two-week periods starting at 24
        weeks' gestation. The prospective risk of fetal death was calculated by determining the
        number of IUFDs that occurred within the two-week block divided by the number of continuing
        uncomplicated monochorionic twin pregnancies during that same time period.
        There were ten unexpected deaths (three double IUFDs and four single IUFDs) in a total
        of seven (4.6%) of the 151 seemingly uncomplicated monochorionic diamniotic pregnancies.
        These IUFDs occurred at a median gestational age of 34 weeks 1 day (range 28 weeks 0 days
        to 36 weeks 3 days). Between 24 and 34 weeks' gestation, the prospective risk of fetal
        death varied between 1/22 and 1/30 pregnancies. After 32 weeks' gestation, the prospective
        risk of unexpected antepartum stillbirth was 1/23. In six out of the seven pregnancies, the
        fetal death was incidental and found on routine ultrasound. (One case presented with
        decreased fetal movement.) There were no significant differences between the IUFD-affected
        pregnancies and the unaffected pregnancies with regards to antenatal indicators of
        intrauterine growth restriction and TTTS. In three of the five pregnancies (autopsy was
        declined in two cases), deaths remained unexplained after autopsy. Two cases, both double
        IUFDs, exhibited signs of TTTS. The authors concluded that despite intensive fetal
        surveillance, uncomplicated monochorionic diamniotic twin pregnancies are at risk for
        unexpected intrauterine death. The deaths occurred in the third trimester and predominantly
        after 32 weeks' gestation. As a result, the authors suggested that after 32 weeks'
        gestation, the prospective risk for fetal death in these pregnancies might be eliminated by
        elective preterm delivery.
      
      
        Implications of the Study
        Despite the limitations of the study, which Barigye et al. elucidate well (small
        numbers, retrospective nature, lack of non-monochorionic twin comparative data), this study
        highlights an important question that many practicing obstetricians are confronting
        increasingly often: 
        when is the ideal gestational age to deliver apparently uncomplicated
        monochorionic twins? The authors suggest that 32 weeks' gestation may be reasonable. At
        that gestational age, many of the risks associated with prematurity, such as
        intraventricular hemorrhage, necrotizing enterocolitis, and respiratory distress syndrome,
        have abated.
        Nonetheless, the risks of prematurity are not negligible at 32 weeks' gestation.
        Balancing the risk of iatrogenic preterm birth in an 
        apparently uncomplicated monochorionic twin pregnancy with the risk of
        double IUFD or single IUFD with the concomitant risk of multicystic encephalomalacia for
        the surviving co-twin is challenging. In our practice, we have been conducting antenatal
        surveillance more frequently than once every two weeks, and we have been using the
        nonstress test in addition to ultrasound and Doppler studies. Although in our preliminary
        experience we have not had any unexplained IUFDs, we are uncertain if the frequency of
        testing could account for the prospective risk of fetal death found in the study by Barigye
        et al. In addition, we have been offering delivery of these 
        apparently uncomplicated monochorionic twins at approximately 34 to 35
        weeks' gestation following antenatal corticosteroid administration and thorough counseling
        regarding the risks of expectant management versus elective preterm delivery. While we
        acknowledge that our practice pattern is by no means a standard-of-care requirement, we
        feel it is a reasonable approach to this dilemma until larger, prospective observational
        studies have been conducted to better elucidate the natural history of these high-risk
        pregnancies and to better answer the question of when the ideal gestational age is to
        deliver apparently uncomplicated monochorionic twins.
      
    
  

  
    
      
        
        Randomized trials and observational studies have conclusively shown a marked improvement
        in several cardiovascular risk factors when obese individuals lose weight, including
        decreases in blood pressure, and improvements in the lipid profile and glucose metabolism.
        These results are closely consistent with observational data linking weight gain to
        worsening of these cardiovascular risk factors, and to their natural sequelae, clinical
        cardiovascular events. Obesity is also strongly associated with increased risk of cancer at
        various sites. With such compelling evidence of the adverse effects of obesity and
        overweight [1–4], why has there been a continuing controversy about its association with
        overall mortality?
        In their paper in this month's 
        PLoS Medicine , Jaakio Kaprio and colleagues conclude that “deliberate
        weight loss in overweight subjects without known co-morbidities may be hazardous in the
        long term” [5]. Do the data really support such a conclusion? An examination of this paper
        illustrates several of the complexities in approaching this seemingly simple question.
      
      
        Evidence from Epidemiological Studies
        Epidemiologic studies of the relation between overweight and mortality typically must
        address three principal concerns [6]. First, in many populations, cigarette smokers tend to
        be leaner than nonsmokers. Because cigarette smoking is such a strong risk factor for
        mortality, failure to adjust for this adequately can lead to confounding, with the
        erroneous conclusion that leanness carries increased risk of death. Statistical adjustment
        for smoking is often insufficient to account for this difficulty. Smoking can lead to
        medical conditions, sometimes sub-clinical, that are associated with decreased body weight,
        such as chronic obstructive pulmonary disease. The presence of symptoms or diagnosed
        conditions may induce smokers to quit. Moreover, the intensity of smoking is related to
        both risk of death and body mass index. For these reasons, the best way to assess the
        impact of overweight on risk of mortality is simply to exclude current and past smokers.
        Kaprio and colleagues' study differentiated only current smoker or nonsmoker. Thus,
        never-smokers were included in the same category as past smokers, regardless of how much
        the past smokers had smoked or their reasons for quitting.
        A second difficulty in some epidemiologic studies is the inclusion of intermediary
        factors as co-variates. Weight loss improves hypertension and diabetes, so including these
        as co-variates would tend to attenuate the apparent benefit of weight loss. In the present
        study, the authors appropriately excluded people with diabetes, and adjustment for
        hypertension appeared to have little impact.
        The third and most difficult issue in studies of overweight and mortality is reverse
        causation, the impact of disease on body weight. This can occur either through the
        biological impact of a condition (diagnosed or preclinical) or as an inducement to attempt
        to lose weight as a means to improve health. The authors' keen recognition of this problem
        provides a significant strength to the present study. The authors appropriately excluded
        individuals with a wide range of conditions to identify an apparently healthy cohort. This
        critical step is often ignored. Sometimes, investigators also exclude deaths that occur in
        the first few years after follow-up, to reduce the impact of reverse causation. Such lagged
        analyses can be helpful, but some chronic conditions of long duration, such as depression,
        chronic lung disease, and heart failure (conditions that often may not reach the level of
        clinical diagnosis) can lead to lower body weight and higher mortality risk. Hence, that
        strategy (not employed by Kaprio and colleagues) may not fully avoid the problem.
      
      
        The Difficulties of Assessing the Effects of Intention to Lose Weight
        As a further step toward reducing the potential bias of the impact of disease on body
        weight, the authors identified individuals with intent to lose weight. Unintended weight
        loss is well known as an ominous clinical sign, usually signifying a serious illness. Just
        over a third of the overweight subjects in the Kaprio cohort had expressed intent to lose
        weight at baseline in 1975. It is interesting to note that despite this intention, as a
        group, the median weight change in the ensuing six years was a gain of 0.33 kg/m
        2 , almost identical to the weight change in the group that had not
        expressed intent to lose weight (gain of 0.31 kg/m
        2 ). Thus, this study cannot fairly be characterized as an assessment of
        intentional weight loss and its subsequent effect on mortality. Because the changes in
        weight are so similar in these two groups, it is implausible to attribute the weight loss
        in the intent-to-lose group to that intention. These findings render the results
        particularly difficult to interpret.
        Other important limitations include the very small number of endpoints—only 268 total
        deaths in the cohort. When further subdivided according to intention to lose weight, and
        categories of weight change, the numbers are far from sufficient for reliable estimates.
        For example, the main conclusions are based on the subgroup of those with intent to lose
        weight who actually lost weight; this group had only 42 deaths, of which ten were violent.
        Another related difficulty is that this cohort, though overweight, is not drastically
        obese. The median body mass index (BMI) was 26.7 kg/m
        2 , and fewer than 10% of people had a BMI greater than 30. With such a
        narrow range of BMI, coupled with the very small changes in weight during the six years of
        initial follow-up, even a very large study would not have sufficient power to detect
        plausible relative risks associated with weight change.
      
      
        What Advice Should Be Given?
        Given the well-known adverse metabolic consequences of overweight and obesity, the
        strong links with cardiovascular disease and several other serious illnesses, and the
        absence of any plausible adverse biologic consequences of maintaining normal weight (BMI 19
        to 25), it seems prudent to continue advising adults to seek to maintain a weight within
        that range. The results from Kaprio et al, although they raise questions about the effects
        of weight loss, do not cast serious doubt on that advice.
      
    
  

  
    
      
        Introduction
        More than 35 million HIV-infected people live in developing countries with significant
        resource limitations. Although 6 million people living in developing countries are in
        urgent need of antiretroviral therapy, only 700,000 currently receive effective treatment
        [1]. Global treatment efforts, including the World Health Organization's
        â€œ3 by 5â€ Initiative, aim to extend therapy to several million
        people over the next few years [2]. While the cost of antiretroviral medications has
        dropped considerably, other obstacles, including the cost, technical, and operational
        requirements of CD4 counts, viral loads, and other sophisticated diagnostic tests used to
        initiate and monitor HIV treatment, remain to be addressed.
        In particular, measurements of CD4
        + T lymphocytes are essential for staging HIV-infected patients,
        determining their need for antiretroviral medications, and monitoring the course of their
        infection [3]. The CD4 countâ€”expressed in adults as the absolute number
        of CD4 cells per microliter of blood, and in children as a percentage of total lymphocytes
        or total T lymphocytesâ€”has enormous prognostic and therapeutic
        implications, and forms the basis for most HIV treatment decisions
        [4â€“6]. In developed countries, CD4 counts are typically performed every
        three to six months for each patient using the method of flow cytometry. Flow cytometers
        use lasers to excite fluorescent antibody probes specific for CD4 and other cell surface
        markers, to distinguish one type of lymphocyte from another. Several
        factorsâ€”including the cost of a flow cytometer (which ranges from
        $30,000 to $150,000), technical and operational complexity, the need for reliable
        electricity, and the high cost of reagentsâ€”have made these instruments
        impractical and/or difficult to sustain in resource-scarce settings. The urgent need for
        affordable and technically simple CD4 diagnostics is widely recognized
        [7â€“11].
        Several efforts have been made to develop alternative, affordable CD4 counting methods
        for resource-poor settings. Single-purpose flow cytometers have been designed solely for
        counting CD4 cells, such as the Becton Dickinson FACSCount, the Partec CyFlow, and desktop
        instruments from Guava and PointCare Technologies. Although these newer versions make flow
        cytometry more affordable in some settings, reagent costs remain high, and the instruments
        remain expensive and in most cases, technically complex [7â€“13]. Low-cost
        microbead separation of CD4 cells from other blood cells, followed by standard manual cell
        counting techniques using a light microscope, offers significantly lower reagent costs than
        flow cytometry. These methods, however, are low throughput and extremely labor intensive,
        and appear to be less accurate than traditional flow cytometry; thus, they have not been
        widely adopted [13â€“18].
        Less expensive CD4 counting methods that capitalize on low-cost microfabrication,
        efficient light sources, and affordable microelectronics and digital imaging hardware have
        been conceptualized, but never realized [19,20]. One of us (JTM) has previously reported
        the development of a novel microchip-based detection system for measuring analytes such as
        acids, bases, electrolytes, and proteins in solution phase [21â€“23]. This
        electronic taste chip (ETC) system carries out chemical and immunological reactions on
        microspheres positioned in the inverted pyramidal microchamber wells of a silicon or
        plastic microchip, which is housed in a miniature flow cell. Microfluidic channels deliver
        a series of small-volume reagents and washes to the flow cell, and hence to the chip and to
        each one of the microspheres. Optical signals generated by the reactions on the
        microspheres are visualized and captured on a charge-coupled device (CCD) with the use of
        transfer optics and a digital video chip. Using the ETC system, complex immunological
        assays, such as the ones developed to quantify cardiac risk factors in serum, can be
        performed with small sample volumes, short analysis times, and markedly reduced reagent
        costs [22].
        Further development of the ETC system has shown that it could be adapted to the
        detection of bacteria, spores, and living cells [24]. We hypothesized that additional
        modifications could be made to provide accurate, low-cost CD4 counts to monitor HIV
        infection in resource-constrained settings. We show that a microchip-based system can
        perform CD4 counts from 16.5 Î¼l of whole blood rapidly, simply, and with a high
        degree of accuracy compared to flow cytometry, particularly for patients with CD4 counts
        below 500 cells/Î¼l. We suggest how this prototype system can be readily
        developed as a low-cost, portable device for use in resource-poor settings.
      
      
        Methods
        
          Flow Cell
          The ETC system was originally designed for microsphere-based assays
          [21â€“23]. The modified version of the flow cell (see Figure 1) is
          enclosed within a three-piece metal casing with a flat platform permanently affixed to a
          circular vertical support, which is in turn connected to a screw-on cap. Within the metal
          casing there are top and bottom plastic inserts made from PMMA. Fluids are introduced to
          and drained out of the flow cell through integrated stainless steel tubing within the
          inserts. The bottom PMMA insert also features a plastic screen disc that acts as a
          support for a 3-Î¼m Nuclepore polycarbonate, track-etch filter (Whatman,
          Florham Park, New Jersey, United States), which serves as a lymphocyte capture and red
          blood cell separation membrane. A gasket between the membrane and the top insert prevents
          leaks and ensures that the entire sample is delivered into the flow cell and filtered
          through the membrane. The top outlet is used with lateral flow for the removal of air
          bubbles.
        
        
          Fluid Delivery System
          In initial studies, we used a single peristaltic pump to deliver sample and washes to
          the flow cell. Subsequently, a partially automated fluid delivery system was developed.
          This functional adaptation uses two miniature OEM peristaltic pumps, each in conjunction
          with a pinch valve, and 0.031-in. (0.79-mm) silicone tubing capable of delivering flow
          rates of 46â€“920 Î¼l/min to the flow cell. Integrated
          software (LabVIEW, National Instruments, Austin, Texas, United States) directs delivery
          of whole blood samples and washes to the flow cell using the appropriate pumps and
          valves. Sample filtrate, including red blood cells, is captured in a waste reservoir.
        
        
          Optical Station and Image Capture
          The flow cell was positioned on the stage of a modified BX2 Olympus (Tokyo, Japan)
          compound microscope equipped with a 10Ã— objective lens and a high-pressure
          100 W mercury burner arc lamp as a light source. Focusing was maintained on a fixed plane
          throughout the duration of the assay. Visualization of AlexaFluor-647-stained lymphocytes
          was achieved using a Cy5 filter cube (620 nm excitation, 660 nm long-pass beam splitter
          dichroic mirror, and 700 nm emission), while AlexaFluor-488-stained lymphocytes were
          visualized with a fluoroisothiocyanate (FITC) filter cube (480 nm excitation, 505 nm
          long-pass beam splitter dichroic mirror, and 535 Â± 25 nm emission). For each
          study participant, images were obtained from each of five nonoverlapping regions of the
          lymphocyte capture membrane in the flow cell, using a 12-bit CCD digital camera (DVC,
          Austin, Texas, United States) mounted on the microscope. Each imaged region represented
          0.18 Î¼l of whole blood, so that for each assay, cells were counted from a
          total volume of 0.9 Î¼l of blood. Each region was imaged serially with both
          filter cubes. The corresponding images were stored separately as monochromatic eight-bit
          images for subsequent digital image analysis and automated cell counting.
        
        
          Image Analysis
          Images were analyzed using a custom algorithm supported by Image-Pro Plus (Media
          Cybernetics, Silver Spring, Maryland, United States) processing software. An iterative
          approach allowed for flexible analysis of data acquired under different conditions of
          illumination, focus, and sampling. For each iteration, an upper and lower value defined a
          range of green or red intensities that were then used to segment the image. Pixels whose
          intensity values fell within the defined range were reassigned values of one, while all
          others were set to zero. The process yielded a binary version of the original eight-bit
          image. A lymphocyte selection algorithm was then applied. Objects (i.e., lymphocytes)
          were defined as contiguous groups of pixels with values of one. Object selection was
          refined by a lymphocyte profile (defined by size, aspect ratio, and uniformity); objects
          not fitting the profile were not counted. The number of counted objects was recorded for
          each iteration. From one iteration to the next, the upper and lower intensity limits used
          to segment the image were both increased by a single intensity count. The final cell
          count per image was the maximum object count over 256 iterations (upper intensity limits
          1â†’255) for which the average object roundness fell below a threshold
          value. In this manner, the software algorithm determined the optimal analysis parameters
          for each image individually, greatly relaxing the stringency of image capture
          requirements. Cell counts were recorded in a spreadsheet as numbers of CD4
          + CD3
          âˆ’ , CD4
          + CD3
          + , CD4
          âˆ’ CD3
          + , CD8
          + CD3
          âˆ’ , CD8
          + CD3
          + , CD8
          âˆ’ CD3
          + , and CD4
          + CD8
          + cells, depending on the combination of antibodies used. Absolute CD4
          counts were recorded as the summed number of CD4
          + CD3
          + cells counted over five images, normalized per microliter of imaged
          blood. CD4:CD8 ratios were recoded as the ratio of CD4
          + CD3
          + cells to CD8
          + CD3
          + cells counted over five images. Relative CD4 abundance as a
          percentage of total T lymphocytes was recorded as 100 times the ratio of CD4
          + CD3
          + cells to total CD3
          + cells, with cells counted over five images.
        
        
          Lymphocyte Staining and Delivery
          Antibodies utilized in these studies were stored at 4 Â°C and centrifuged to
          remove precipitated material prior to use. This process ensured removal of fluorescent
          particulate matter that could be captured by the membrane and might interfere with
          imaging. For the initial dilution control studies, CD4 cells were purified by
          immunomagnetic separation from donor buffy coats. CD4 cells labeled with
          AlexaFluor-488-conjugated anti-CD4 antibodies (A21335, clone
          289â€“14120, Molecular Probes, Eugene, Oregon, United States) were
          introduced to the flow cell in amounts ranging from zero to 200,000 cells, and washed
          with phosphate buffered saline (PBS). For whole blood studies, 33 Î¼l of whole
          blood collected by venipuncture was incubated at ambient temperature
          (20â€“25 Â°C) with 3 Î¼l of AlexaFluor-488- and
          AlexaFluor-647-conjugated antibodies to CD4 and CD3 (A21332, clone
          289â€“13801, Molecular Probes), respectively, and allowed to react for 8
          min. Similarly, for CD8 enumeration, 33 Î¼l of whole blood with 3 Î¼l
          of AlexaFluor-488- and AlexaFluor-647-conjugated antibodies to CD8 (A21340, clone
          289â€“13804, Molecular Probes) and CD3, respectively, was allowed to
          react for 8 min at ambient temperature. Stained blood samples were brought up to 1,000
          Î¼l with PBS, half of which was introduced directly into the flow cell
          (representing 16.5 Î¼l of the original sample of blood) and then washed with 1
          ml of PBS. Because red blood cells are mechanically separated from white blood cells, red
          blood cell lysis is not necessary. Images of labeled cells captured on the membrane were
          obtained and analyzed as described above. For SEM (scanning electron microscopy), a
          fixative (2% paraformaldehyde/2.5% glutaraldehyde) was added into the flow cell and
          rinsed with PBS. The filter was removed from the flow cell, fixed for 90 s with OsO
          4 vapor, and then dehydrated with EtOH/HMDS. The same SEM protocol was
          applied to a drop of whole blood on a glass slide.
        
        
          Study Participants and Comparison to Flow Cytometry
          Blood was obtained from HIV-1-uninfected control participants and HIV-infected
          participants at the Massachusetts General Hospital in Boston, Massachusetts, United
          States, and from HIV-infected participants at the Botswanaâ€“Harvard
          AIDS Institute HIV Reference Laboratory in Gaborone, Botswana. The Botswana samples
          originated from a study of HIV-infected pregnant women attending
          maternalâ€“child health clinics in Gaborone or three nearby villages,
          Molepolole, Mochudi, and Lobatse. Six infants were also included in the study. Three
          milliliters of venous whole blood was collected from each participant (in EDTA
          anticoagulant). All samples were run on the microchip on the day of blood collection.
          Parallel samples were processed using standard four-color flow cytometry on a Becton
          Dickinson FACSCalibur, using the MultiTEST reagents and TruCOUNT beads, and analyzed
          using MultiSET software. All samples were processed by flow cytometry according to
          standard operating procedure in the HIV reference laboratory in Botswana. The majority
          were processed within 24 h of blood collection, and all were processed and analyzed
          within 72 h of blood collection. A total of 70 participants were enrolled, including 64
          adults and six infants. Three adults did not have flow cytometry results available,
          leaving 67 participants for analysis. The study was approved by the institutional review
          boards of the participating institutions. For a preliminary assessment of assay
          variability, blood from a single study participant was assayed as described above 20
          separate times over the course of a single afternoon by a single operator.
        
        
          Statistical Methods
          The accuracy of the microchip-based CD4 counting system was determined by comparing
          results directly to parallel samples processed by flow cytometry using
          Passingâ€“Bablok regression analysis and the
          Blandâ€“Altman methods comparisons approach [25,26]. For assay
          reproducibility, a coefficient of variance was calculated from 20 replicates of a single
          participant. Data were analyzed and processed using Analyse-It software (Analyse-It
          Software, Leeds, United Kingdom).
        
      
      
        Results
        In initial experiments using the original ETC system [21â€“23],
        microspheres were coated with monoclonal antibodies to the lymphocyte surface markers CD3,
        CD4, or CD8, followed by microfluidic delivery of fluorescently labeled lymphocytes from
        whole blood obtained from non-HIV-infected participants. Although lymphocytes were readily
        captured, precise quantification of cell numbers and CD4 cell counts were not possible
        using the microsphere as a surface for lymphocyte capture (data not shown). We next
        modified the flow cells with a disposable, microporous membrane filter for lymphocyte
        capture. A single polycarbonate, track-etch membrane with 3-Î¼m pores was
        immobilized and secured within the flow cell, creating a lymphocyte capture surface with a
        surface area of 80 mm
        2 . Whole blood samples were delivered to the flow cell from a sample
        reservoir tube, and the membrane within the flow cell was washed with PBS from a second
        reservoir. As in the original ETC system, cells were imaged under fluorescence optics using
        a mercury arc lamp light source and a CCD camera (Figure 1).
        To confirm that cells could be adequately captured, 33 Î¼l of unprocessed
        whole blood from non-HIV-infected participants was incubated for 8 min with
        fluorophore-conjugated anti-CD4 antibodies, and delivered by a peristaltic pump to the
        modified microfluidics chip. Red blood cells passed readily through the pores under
        appropriate fluid flow conditions. In contrast, the majority of white blood cells were
        captured onto a single imaging focal plane (Figure 2). This mechanical separation of
        autofluorescent red blood cells allows for the imaging and counting of white blood cells
        from unprocessed whole blood without additional sample processing, such as centrifugation
        or red blood cell lysis. Using the digital imaging system originally developed for
        microsphere-based capture in the ETC system, fluorescently labeled white blood cells can
        then be imaged directly on the chip and counted.
        To assess the analytical validity of the membrane-based microchip system, we first
        performed a dilution control study to evaluate the correlation between total fluorescence
        intensity and the absolute number of purified CD4 cells from non-HIV-infected participants
        (labeled with fluorophore-conjugated anti-CD4 antibody) captured in the microchamber. The
        results show a linear correlation between the number of cells in the sample and the
        intensity of light emitted from the membrane filter (
        R
        2 = 0.999) for a range of CD4 cell counts relevant to advanced HIV
        disease (0â€“200 CD4 cells/Î¼l blood) (Figure 3). This
        doseâ€“response study established proof of the concept that a modified
        microfluidic flow cell and a digital image analysis system can accurately detect and
        measure populations of whole blood lymphocytes labeled with fluorescent markers.
        We next quantified the percentages of CD3, CD4, and CD8 cells in whole blood samples
        from healthy control participants using this system. Prior to delivery to the flow cell, we
        labeled a 33-Î¼l whole blood sample with 3 Î¼l of
        fluorophore-conjugated anti-CD3 and anti-CD4 antibodies for 8 min off chip, then diluted
        the sample with 961 Î¼l of PBS, and delivered 500 Î¼l of the resulting
        sample (containing 16.5 Î¼l of blood) to the flow cell using a fluidics
        controller. Digital images from one region of the lymphocyte capture membrane were obtained
        with two different emission filters, one specific for the AlexaFluor-488-conjugated
        antibody used to stain CD4
        + T lymphocytes green (Figure 4A), and the other specific for the
        AlexaFluor-647-conjugated antibody used to stain CD3
        + T lymphocytes red (Figure 4B). Automated digital merging of the two
        images and image processing allowed the system to distinguish the CD3
        + CD4
        + T lymphocytes of interest (i.e., â€œCD4
        cellsâ€), which appear yellow, from the CD4
        + CD3
        âˆ’ monocytes (green) and the CD3
        + CD4
        âˆ’ T lymphocytes (red) (Figure 4C).
        We next developed a custom algorithm for translating these digital images into accurate
        CD4 and CD8 T cell counts using pixel analysis with the aid of a commercial image
        processing package. Automated counting of the three subsets of cells was based on object
        size, aspect ratio, and uniformity, iterated across the range of color intensity levels. As
        shown in Figure 4D, a binary mask first removes the unwanted cell types, and residual
        objects representing CD4 T cells are counted. A similar protocol was applied to a second
        aliquot of blood stained with AlexaFluor-647-conjugated CD3-specific antibody and
        AlexaFluor-488-conjugated CD8-specific antibody to visualize and count CD3
        + CD8
        + T lymphocytes.
        In order to calculate an absolute CD4 count with standard flow cytometry, one of two
        measures must be undertaken to calculate a concentration in cells per microliter. Either a
        standardized reference reagent, such as calibration beads at a known concentration, can be
        added to the assay (â€œsingle-platformâ€ flow cytometry), or an
        absolute total lymphocyte count in cells per microliter can be obtained on a hematology
        analyzer (â€œdual-platformâ€ flow cytometry). The microchip
        assay we describe here uses a direct volumetric method and functions as a single-platform
        approach. By delivering a consistent volume of blood to the flow chamber (16.5
        Î¼l of stained whole blood, diluted to a total volume of 500 Î¼l of
        PBS), and calculating the unit volume of blood per digital image (0.18 Î¼l), we
        were able to count the total number of CD4
        + CD3
        + cells in 0.9 Î¼l of blood, and determine the absolute CD4
        count per microliter.
        We next tested this rapid, whole blood microchip assay in a series of samples acquired
        in an HIV reference laboratory in Botswana. Seventy consecutive HIV-infected participants
        presenting to the HIV reference laboratory for standard CD4 counting as part of a vertical
        transmission study were enrolled, of whom 64 were adult women and six were infants.
        Parallel samples were processed by standard four-color flow cytometry on a Becton Dickinson
        FACSCalibur. The time from blood collection to complete analysis and results reporting
        using the chip-based assay was approximately 15 min per sample. Three adult participants
        did not have valid flow cytometry results available, leaving 61 adults and six infants for
        analysis.
        Representative processed data images from three participants, two adult women and one
        infant, are shown in Figure 5. Figure 5A shows a 31-y-old woman with an absolute CD4 count
        by flow cytometry of 83 cells/Î¼l. While numerous CD3
        + T cells (red) are present as well as scattered monocytes (green), her
        low CD4 count is reflected in the few double-labeled CD3
        + CD4
        + T cells (yellow) seen in the image. Similar representative data images
        from a young woman with a CD4 count of 271 cells/Î¼l by flow cytometry and a
        5-mo-old infant with a CD4 percentage of T lymphocytes of 0.39 by flow cytometry are also
        shown in Figure 5B and 5C, respectively. These images illustrate the dynamic range of the
        membrane capture and digital image analysis system, including the ability to quantify both
        absolute CD4 counts and CD4 percentages.
        We compared results from our microchip assay with results available from flow cytometry,
        the latter obtained on a FACSCalibur through standard clinical laboratory operating
        procedures. The data for adult absolute CD4 counts are plotted in the
        Blandâ€“Altman methods comparison plot shown in Figure 6. For 61 adult
        participants with CD4 counts ranging from 35 to 1,087 cells/Î¼l (mean, 372
        cells/Î¼l) by flow cytometry, results show a good correlation between absolute
        CD4 counts measured by our microchip assay and those measured by flow cytometry.
        Blandâ€“Altman methods comparison analysis shows a bias of
        âˆ’50 cells/Î¼l (95% confidence interval, âˆ’81
        to âˆ’20 cells/Î¼l), and good 95% limits of agreement (Figure 6).
        Several of the results from participants at the higher end of absolute CD4 counts fall
        outside the 95% limits. For these participants, individual lymphocytes may overlap in the
        digital images (as seen in Figure 5C), which can interfere with the accuracy of the
        lymphocyte counting algorithm. In resource-limited settings, the primary use of CD4 counts
        is as a trigger to initiate antiretroviral therapy, which typically occurs at a CD4 count
        of 200 cells/Î¼l. Higher CD4 count thresholds of 350 and 500 cells/Î¼l
        are also used to increase the intensity of monitoring. For these values, the sensitivity
        and specificity of our method are: CD4 < 250, sensitivity = 0.86, specificity = 0.81;
        CD4 < 350, sensitivity = 0.97, specificity = 0.83; and CD4 < 500, sensitivity = 0.96,
        specificity = 0.85.
        One important application of our method is in pediatric HIV monitoring. The wide range
        of normal absolute CD4 counts in infants and children requires the use of CD4:CD8 ratios or
        CD4 percentages in pediatric infection. Results for CD4:CD8 ratios and CD4 percentages of T
        lymphocytes for all 67 participants (61 adults and six infants) are shown in Figure 7.
        Agreement, bias, and correlations between the microchip method and flow cytometry are
        excellent for both CD4 percentages of T lymphocytes (Figure 7A and 7B) and CD4:CD8 ratios
        (Figure 7C and 7D). Blandâ€“Altman plots for both CD4 percentages of T
        lymphocytes and CD4:CD8 ratios show low proportional bias, with tight 95% limits of
        agreement. Correlations are excellent for both CD4 percentages of T lymphocytes (
        r = 0.98, 
        p < 0.0001) and CD4:CD8 ratios (
        r = 0.98, 
        p < 0.0001). Overall, the data show that all three approaches to
        measuring CD4 cell counts can be accurately quantified using the microchip method, and that
        both adult and pediatric CD4 results can be obtained.
        To determine assay variability, we examined 20 replicate samples of blood from a single
        participant over the course of one day, using the established basic protocol. We determined
        that the coefficient of variance was 12% (data not shown), which is similar to other
        methods of CD4 counting [27]. Although the assay described here introduced 16.5
        Î¼l of blood into the system, the actual volume of blood analyzed by digital
        image analysis is only 0.90 Î¼l. We have conducted preliminary studies that
        suggest that we can accurately measure CD4 counts from less than 5 Î¼l of blood
        obtained via fingerstick (data not shown); additional studies will be required to assess
        the correlation between CD4 counts obtained by fingerstick and by venipuncture.
      
      
        Discussion
        Our results provide proof of principle that low-cost microfluidic structures combined
        with fluorescence imaging and digital image analysis can be successfully applied to the
        measurement of CD4 cell counts, which are critical to the clinical management of HIV
        infection. The method described here can deliver both absolute CD4 counts for adult
        monitoring, and CD4 percentages or CD4:CD8 ratios for pediatric monitoring. Most
        importantly, the rapid and accurate CD4 assessments obtained with this method, together
        with its anticipated low cost relative to flow cytometry, may make this type of approach
        ideal for resource-scarce settings. As our results show, this method may be less accurate
        at the higher range of CD4 counts, where cells may be more likely to overlap in our digital
        images. While this may limit its applicability, our method is accurate at CD4 counts below
        500 cells/Î¼l, which represent the clinically relevant CD4 levels in
        resource-poor settings. In addition, both the bias in the method described here
        (âˆ’50 cells) and the accuracy at higher CD4 counts are likely to be
        improved significantly by the further development of a disposable microfluidic cartridge,
        where the volume of distribution of the sample will be much smaller, and more accurate
        volumetric control will be possible.
        Our study was designed to evaluate the accuracy of our method against the gold standard
        in a population of adults. During enrollment, a small number of pediatric samples were made
        available to us by the staff at Princess Marina Hospital in Botswana. We chose to include
        these samples in the data presented here to provide proof of principle that pediatric CD4
        percentages can also be assessed with this method. Although only six pediatric samples were
        available, limiting claims of statistical significance, we believe the issue of pediatric
        CD4 count monitoring to be of such importance that the data merited inclusion. Excluding
        the six pediatric samples does not affect the analysis.
        The results presented here were obtained with a stationary, tabletop monitoring system
        using a standard epifluorescence microscope and commercial image processing software. While
        the methods we described provide the basis for a highly portable and flexible miniaturized
        CD4 counting system, it should be emphasized that a number of additional developments are
        required to enable the widespread use of this approach in resource-limited settings. With
        additional engineering of optics, electronics, and mechanical components along with
        advancements in integrated microfluidic systems, it should be possible to develop a
        point-of-care instrument that is battery-powered, uses simple light emitting diodes (LEDs),
        and secures analyzable digital images with affordable video imaging chips. When combined
        with an embedded microprocessor and disposable assay cartridges for both adult and
        pediatric monitoring manufactured from injection-molded plastic, it should be possible to
        create a functional CD4 counting device that can be used at the point of care. Further
        trials in a larger, more diverse cohort of patients, including adult men and children, will
        be necessary to confirm the accuracy of the method, including an assessment of assay bias
        and reproducibility. Such a device is currently in commercial development, and may be
        available by early 2006. While it is too early to provide an accurate cost estimate for a
        portable instrument and disposable plastic CD4 assay, we expect the equipment cost would be
        substantially lower than for flow cytometry, and the assay cost would be similar to assays
        using existing methods (Table 1).
        Although several CD4 counting systems are now used in resource-limited settings, they
        remain suboptimal to meet the needs of HIV care and treatment scale-up. None can truly be
        used at the point of care beyond a district hospital or similar facility, and either the
        capital and operating costs remain high, or throughput is low, or both (Table 1). Pediatric
        monitoring using CD4 percentages also remains largely unavailable. The method we describe
        here addresses several of the limitations of performing diagnostic assays in
        resource-limited settings. First, sample volumes are minimal, so that tests can be
        performed on fingerstick samples of blood, circumventing the need for venipuncture, and
        minimizing both medical waste and operator exposure to biohazardous material. Second,
        reagent use is minimized in the microchip system, reducing reagent costs by as much as 90%.
        Third, labor- and equipment-intensive sample preparation is eliminated. Fourth, the
        microchip CD4 assay is extremely rapid. CD4 results in the prototype system described here
        are available in less than 15 min from the time of blood collection. In a mature
        microfluidic device with push-button operation, results should be available in less than 10
        min, and thus can be used to make real-time clinical decisions at the point of care. Fifth,
        the assay is technically simple, analogous to a portable glucometer, and ultimately will be
        useable by a health-care worker in remote settings with minimal training, extending the
        reach of CD4 assays to district hospitals and remote clinics, and reducing labor costs.
        Sixth, both adult and pediatric monitoring are possible.
        We believe that the future of low-cost diagnostics for use in the developing world lies
        in the development of new lab-on-a-chip technologies that integrate sample preparation and
        sample measurement systems into miniaturized devices with minimal power requirements.
        Preliminary cost estimates for the instrumentation here described suggest, at a minimum, a
        10-fold reduction in the cost for the associated measurement system. Further, reagent
        consumption for the microchip system can be reduced by a similar factor relative to flow
        cytometry, while sample storage and shipping costs are expected to be reduced dramatically
        by virtue of the point-of-care capabilities of this new lab-on-a-chip structure. The
        importance of microtechnologies to the realities of laboratory infrastructure worldwide has
        been recognized previously [28â€“30]. Although CD4 counting represents the
        most urgent need in HIV diagnostics for resource-poor settings, the microchip platform is
        adaptable to other important assays. Through the interface of the lymphocyte capture
        membrane described here with the previously reported microchip arrays, cellular assays like
        CD4 counts can be multiplexed with other molecular biomarker measurements (i.e., proteins
        and nucleic acids) on a single miniaturized chip. The rapid extension of the chip-based CD4
        counting method described here to HIV RNA measurements, diagnostics for opportunistic
        infections, liver enzymes, and other biochemical markers of interest in infectious disease
        is feasible.
      
    
  

  
    
      
        
        We appreciate Dr. Miller's contribution to this debate [1]. Whether one uses his phrase
        “established treatment” or “proven therapy,” we urge caution in using such a litmus test to
        decide whether the use of placebo is or is not acceptable. Such terms beg to be defined
        carefully. Should nonsteroidal anti-inflammatory drugs be considered “proven” for
        arthritis, despite their problems with assay sensitivity [2,3]? Dr. Miller states that
        there is no established treatment for adenomatous polyps. However, there is evidence from
        epidemiological studies and clinical trials supporting the use of aspirin and other
        nonsteroidal anti-inflammatory drugs for this condition [4–10]. Armed with such
        evidence—whether it qualifies aspirin as “proven therapy” is open to subjective
        interpretation—many placebo opponents, we maintain, would argue for an active-controlled
        design as a more ethical alternative to the placebo-controlled design actually used in
        APPROVe [11].
        Dr. Miller focuses on the question of defending the use of placebo in the APPROVe study.
        This focus reflects the prevailing bias, which is to choose when it is acceptable to use
        placebo rather than to choose when it is acceptable to omit placebo. This bias is evident
        in the current version of the Declaration of Helsinki, with wording such as, “Extreme care
        must be taken in making use of placebo-controlled trials.” Thus, the use of placebo is
        typically presumed “guilty until proven innocent,” while active-controlled designs are
        presumed “innocent until proven guilty.” The declaration is silent on the possibility that
        omitting placebo can lead to problems, too, as we have now witnessed with Vioxx.
        So perhaps the more important question should be whether it was defensible to exclude
        placebo in the VIGOR study. Dr. Miller acknowledges that placebo would have provided a
        better assessment of the safety signal in the VIGOR study. Indeed, because placebo was not
        used, the authors were able to plausibly conclude that the difference between the two
        groups was due to naproxen causing benefit rather than to Vioxx causing harm [12]. (The
        plausibility of this conclusion has since been questioned [13,14].) This misleading safety
        signal only delayed the withdrawal of Vioxx. Its design was superficially ethical, but
        science was not advanced, and the public health was ill-served.
        One might protest that these comments are made with the benefit of hindsight. It is true
        that the medical community at large did not become aware of this safety issue until
        September 2004, when the results of the placebo-controlled APPROVe study were made public
        and Vioxx was withdrawn. However, according to David Graham's testimony to the United
        States Senate [15] and a report on internal Merck documents [16], there was good reason for
        concern about a possible safety signal before 1999, when Vioxx was approved and recruitment
        for VIGOR began [15].
        If the VIGOR study had included a placebo arm, the truth about Vioxx could have been
        learned in February 2001 instead of September 2004 [14]. That is over 180 weeks during
        which the now infamous “two to four jumbo jetliners” were allowed to continue “dropping
        from the sky every week” [15]. Using the midpoints of Graham's range estimates, this works
        out to 94,500 excess heart attacks and strokes, including 33,000 deaths, in the US
        alone.
        The authors of the VIGOR study said, “We could not include a placebo group” [12]. Was
        the idea of including a placebo arm suggested but rejected as “unethical,” even though
        rescue medication could have been used? Or did they take the path of least resistance in
        the interest of rapid institutional review board approval and ease of patient recruiting?
        Whatever the reason, the decision to omit placebo led to ambiguity and inaction.
        In clinical trials, whether one looks at efficacy (please see our opening argument in
        this debate [17]) or safety, omitting placebo often muddies the scientific waters and
        places the public health at increased risk. Good science and good ethics cannot be divorced
        from one another. We believe these considerations should factor into discussions on the
        ethics of clinical trial design. Before we experience another Vioxx, we hope that a future
        version of the Declaration of Helsinki will add, “Extreme care must be taken when omitting
        placebo.”
      
    
  

  
    
      
        
        Shah Ebrahim says, in his answer to my statement in our debate [1], that plaque is so
        common that it makes sense to treat only those likely to have an acute event in the near
        future. Not everyone has a heart attack but everyone ages, nor can we be sure who will be
        lucky as they age and who will not. Anyone with a low-density lipoprotein level above 70
        mg/dl is at risk [2]. Narrowing of arteries must certainly contribute to health decline in
        aging. He says that most plaque is stable and does not rupture. The half-million people who
        have a heart attack and the hundreds of thousands of individuals who have a stroke in the
        United States alone each year would disagree. Waiting decades more for further studies will
        not help all those now succumbing to the disease, as long as all the components have
        already been well vetted, as they have been. Ebrahim's statement that “most [plaques] are
        stable and unlikely to rupture” is based on his own study published in 
        Stroke [3], but the paper has no data relating to how many patients found
        to have increased intima-media wall thickness (IMT) or plaque actually had heart attacks or
        strokes. It did find that intimal-wall changes were highly associated with ischemic heart
        disease.
        It is correct that risk factors can predict heart disease and stroke to some degree,
        though I have many patients for whom that approach fails while IMT detects the risk
        otherwise missed. However, risk factor analysis is not enough. Changes in smoking, diet,
        weight, blood pressure, and such are all targets for treatment, but how do doctors know
        whether they have controlled them adequately? If IMT increases, more control is required.
        If IMT stabilizes or reverses a little, then it means that control is at goal. Family
        history of premature coronary artery disease is a major but underused risk factor. The
        causative factors have not yet been elucidated by research, so we do not know what the
        goals are. However, IMT provides a highly satisfactory parameter by which to judge the
        effectiveness of treatment in familial coronary artery disease: IMT stabilization and
        reversal are good, whereas progression is bad and requires more intensive measures.
        Epidemiology and public-health planning correctly look at policies to apply to large
        populations. However, the practice of medicine is patient by patient, accomplished in the
        face-to-face doctor–patient relationship. Policy can be useful as a resource, but each
        patient should have the maximum individualized care and access that a doctor can provide.
        Patients should be able to make their own informed choices and not be dictated to by
        policies meant for masses. Shouldn't everyone with increased IMT at least be informed of
        the options available to limit it rather than waiting until acute events or advanced
        narrowing occur? It would be best to start a healthy lifestyle from birth, but fortunately
        by adulthood there is still time to make an enormous difference in practical terms if we
        take action at the stage when intimal widening is detectable with the highly sensitive
        ultrasound described in my original viewpoint [1].
      
    
  

  
    
      
        
        The excellent article by Jordan Paradise, Lori B. Andrews, and colleagues, “Ethics.
        Constructing Ethical Guidelines for Biohistory” [1], neither advocates nor argues against
        biohistorical research; instead, it points out that such investigations are currently
        taking place without guidelines—ethical, scientific, moral, or religious. The question
        remains: if such guidelines were to be established, what individuals, institutions,
        governments, medical examiners, family members, or intrepid biographers are to be given
        permission? Who is to decide what is “historically significant”? Not to mention the
        meta-question: who is to decide who is to decide? I apologize to the authors if my brief
        comments [2] implied that they took a position on this issue.
      
    
  

  
    
      
        
        I am writing in response to an essay published in the most recent issue of 
        PLoS Medicine by Deborah Hayden, entitled “Alas, Poor Yorick: Digging Up
        the Dead to Make Medical Diagnoses” [1]. As a co-author of the 
        Science piece with Lori B. Andrews that Hayden references, I am troubled
        by her comment on our article. Nowhere in that article, “Ethics. Constructing Ethical
        Guidelines for Biohistory” [2], do we suggest that genetic testing be done on deceased
        individuals for historically significant questions. In fact, we specifically highlight some
        of the ethical, legal, social, and scientific issues that such testing raises and recommend
        that guidelines be developed in order to monitor current research that is being undertaken
        in this area. The article does not advocate biohistorical research. This distinction is
        very important and one that is quite evident upon a careful reading of our article.
      
    
  

  
    
      
        
        Exposure to short periods of very loud noise can cause tinnitus—a persistent ringing or
        buzzing in the ears that cannot be blocked out. Tinnitus may affect around 10%–15% of the
        population; severe tinnitus is very debilitating (1%–2% of the population). Previous work
        has shown that tinnitus has a neurophysiological basis, but precisely which parts of the
        brain and the auditory circuits are involved is not yet understood.
        The human ear is essentially a very sensitive vibration sensor, one that is able to
        receive the minute longitudinal vibrations in air that make up sound waves. It can detect
        sounds from 20 Hertz (Hz) (very low pitch) to 20,000 Hz (very high pitch) but is
        particularly sensitive to sounds in the range of 500–5,000 Hz—the so-called speech
        frequencies. However, the ear, and in particular the cochlea, or inner ear, can be damaged
        by exposure to excess noise, leading to permanent damage to the ear, i.e., deafness.
        Some studies in both animals and humans have suggested that tinnitus and hearing loss
        may be related. These studies have found that neurons in regions of the auditory cortex
        that have been deprived of stimuli because of hearing loss change their receptive field and
        may develop enhanced spontaneous activity. Other studies, such as some involving
        neuroimaging using positron emission tomography, have suggested that parts of the brain
        involved in attention and emotional regulation might be involved in the production of
        tinnitus.
        One of the key research targets in tinnitus has been investigation of cortical activity,
        especially in animal models of tinnitus, but studies in humans have been rare. Previous
        studies have identified temporal and frontal temporal changes in individuals whose tinnitus
        is severely disabling; however, there have been no group studies comparing abnormalities of
        ongoing, spontaneous neuronal activity in people with and without tinnitus.
        In this month's 
        PLoS Medicine , Nathan Weisz and colleagues studied 17 patients with
        chronic tinnitus and hearing loss and 16 control individuals with normal hearing. Patients
        were asked to fill in a questionnaire about the impact of tinnitus on their lives and had
        their levels of tinnitus assessed.
        The team's methods differed from previous work in that the team chose to examine the
        power spectrum of neuromagnetic oscillatory activity during rest, whereas previous studies
        had focused on measuring neurophysiological responses following sounds.
        Normally in awake and healthy subjects a certain rhythm of brain activity at 8–12 Hz—the
        so-called alpha rhythm—is dominant. Finding enhanced slow-wave, or delta, activity (<4
        Hz) in awake subjects is usually a sign of a dysfunctional neuronal network, as these waves
        can be observed in various neurological and psychiatric disorders. Weisz and colleagues'
        analysis of the frequency spectrum of recorded magnetic fields revealed that the energy in
        the alpha band was strongly reduced and that of the delta band enhanced in the group with
        tinnitus compared with the individuals with normal hearing. This pattern was particularly
        pronounced in the temporal regions, and overall the effects were stronger for the alpha
        than for the delta frequency band.
        This is the first study to show these changes in delta and alpha spontaneous cortical
        activity, say the authors. But they concede it is still unclear whether the enhancement of
        delta activity compared with alpha is the abnormal activity perceived as tinnitus. However,
        the fact that regions that show slow-wave activity during slow-wave sleep are also regions
        of low alpha activity supports the idea that changes in cortical activity might be mediated
        by sensory deprivation, in this case that partial hearing loss might be involved in
        producing tinnitus.
        Tinnitus-related distress as assessed by the questionnaire was strongly associated with
        this abnormal spontaneous activity, especially in the right temporal and left frontal
        areas, thus pinpointing a possible tinnitus-related cortical network.
        A limitation of this study was that the tinnitus group also had high-frequency hearing
        loss, whereas the control group did not; the ideal control group would have been patients
        with the same sort of hearing loss but no tinnitus.
        In discussing their findings, the authors suggest that their study supports previous
        work indicating that the prefrontal cortex is a candidate region for integration of the
        sensory and emotional aspects of tinnitus. Further studies should focus on frontal areas,
        which could allow identification of interactions and modulating influences that
        higher-order psychological processes (e.g., emotions and thoughts) may have on the
        generation of tinnitus in the auditory cortex.
      
    
  

  
    
      
        
        As mothers get older and assisted conception becomes more common in developed countries,
        the incidence of multiple births—primarily of nonidentical siblings, but also of identical
        ones—has dramatically increased. Multiple pregnancies are high-risk pregnancies, with
        preterm delivery and monochorionicity (shared placenta) the major problems. Consequently,
        efforts are underway to optimize the management of these pregnancies.
        While identical (monozygotic) twins are much less common than dizygotic ones,
        monozygotic twinning events are increased after induced ovulation and in vitro
        fertilization. Monozygotic twins can be diamniotic dichorionic (two amniotic sacs, two
        placentas), monoamniotic monochorionic (one amniotic sac, one placenta), or diamniotic
        monochorionic (two amniotic sacs, one placenta). The last type accounts for approximately
        two-thirds of all monozygotic twins.
        Monochorionic twins are at higher risk because they share a common placenta; they are
        primarily at risk from circulation abnormalities like twin–twin transfusion syndrome (the
        smaller twin [donor] does not get enough blood while the larger twin [recipient] becomes
        volume overloaded) and intrauterine growth restriction. However, the majority of diamniotic
        monochorionic twin pregnancies do not develop such complications.
        Nicholas Fisk and colleagues have studied records of 151 seemingly uncomplicated
        diamniotic monochorionic pregnancies and found a surprisingly high rate of fetal death: ten
        unexpected intrauterine deaths occurred in seven of the 151 pregnancies with no prior signs
        of complications. All deaths occurred within two weeks of a normal scan, at a median
        gestational age of 34 weeks and 1 day.
        The authors conclude that “despite intensive fetal surveillance, structurally normal
        monochorionic diamniotic twin pregnancies without twin–twin transfusion syndrome and
        intrauterine growth restriction are complicated by a high rate of intrauterine death.” As
        the deaths occurred predominantly after 32 weeks' gestation, the authors suggest that the
        prospective risk for fetal death in these pregnancies might be eliminated by elective
        preterm delivery after 32 weeks.
        In an accompanying Perspective (DOI: 10.1371/journal.pmed.0020180), Jane Cleary-Goldman
        and Mary D'Alton agree that, despite the limitations of the study (its retrospective
        nature, small numbers, and lack of dichorionic controls), it highlights a critical question
        for obstetricians, namely, “when is the ideal gestational age to deliver apparently
        uncomplicated monochorionic twins?” As Cleary-Goldman and D'Alton discuss, at 32 weeks of
        gestation many of the risks associated with prematurity have abated, but the remaining ones
        are not negligible. Until larger prospective observational studies have been conducted,
        balancing these risks remains challenging.
      
    
  

  
    
      
        
        Lassa fever, a viral hemorrhagic fever caused by the Lassa virus and commonly
        transmitted by its rodent host, is endemic in certain areas of West Africa, where several
        hundred thousand people are estimated to be infected each year. The disease is asymptomatic
        or mild in approximately 80% of infected patients, but the remaining 20% have severe
        multisystem disease. Estimated overall mortality is 1%–2%.
        Death rates are particularly high for women in the third trimester of pregnancy, and for
        fetuses, about 95% of which die in the uterus of infected pregnant mothers. The most common
        complication of Lassa fever is deafness. Various degrees of deafness occur in approximately
        one-third of cases, and in many cases hearing loss is permanent. Disease severity does not
        seem to affect this complication: deafness may develop in mild as well as in severe
        cases.
        Lassa fever remains a serious challenge to public health in West Africa, threatening
        both local residents in rural areas and those who serve them, particularly medical care
        providers. Ribavirin, an antiviral drug, has been used successfully in Lassa fever
        patients, but it needs to be given early and is not readily available in the infected
        areas. Given the ecology of the rodent host and conditions in the endemic area, a vaccine
        is mandatory for control. Lassa vaccine initiatives have suffered from a lack of funding in
        the past, but bioterrorism and recent importation of the disease to the United States and
        Europe have brought new resources to Lassa virus science.
        Early attempts to develop a Lassa fever vaccine in the 1980s focused on killed
        pathogens, which caused a strong humoral response but failed to protect nonhuman primate
        test animals. Subsequently, recombinant vaccines used vaccinia vectors carrying different
        combinations of structural Lassa proteins. Some of these protected 90% of nonhuman primates
        from a lethal challenge in the absence of a strong humoral response, suggesting that
        cellular responses are important for protection.
        Use of vaccinia vectors in humans is problematic, especially in areas where HIV
        infection is common—immune-suppressed individuals can develop serious skin lesions—and
        several alternative vaccines based on other vectors as well as harmless vaccinia ones are
        under development. Thomas Geisbert and colleagues now report promising results with a
        replication-competent vaccine based on attenuated recombinant vesicular stomatitis virus
        vectors expressing the Lassa viral glycoprotein. A single intramuscular vaccination
        protected all four vaccinated cynomolgus macaques against a lethal challenge of a
        particular Lassa strain, while two control monkeys that had received empty vector died
        after injection with the same dose of virus.
        These are encouraging results, but future larger studies will need to assess the
        duration of protection and demonstrate the safety of this replication-competent vaccine.
        Another crucial question is how quickly vaccinated individuals acquire protection, and thus
        whether the vaccine would be suitable for creating a ring of vaccination around an outbreak
        zone, the most likely early application of a promising candidate vaccine. In addition,
        there are at least four different strains of the Lassa virus, and an ideal vaccine should
        provide protection across all strains. Finally, conducting trials in endemic areas, many of
        which lack political stability, remains a serious challenge.
      
    
  

  
    
      
        PRESENTATION of CASE
        A 25-y-old woman presented with pulmonary embolism. She had been taking, without
        apparent complication, norgestimate/ethynil estradiol (Ortho Tri-Cyclen; 0.180, 0.215, and
        0.250 mg norgestimate cycles and 35 μg ethinyl estradiol) for 2 y, followed by the same
        drug combination but with a lower dose of ethinyl estradiol (Ortho Tri-Cyclen Lo; 25 μg
        ethinyl estradiol) for 14 mo. A nonsmoker, she lacked a relevant family history and was
        vigorously athletic.
        One month prior to presentation, she developed neck pain; disc protrusion at C5-C6 was
        detected by magnetic resonance imaging. The patient was prescribed valdecoxib, 20 mg twice
        a day (b.i.d.), for 2 wk. Her neck pain resolved. However, towards the end of this
        treatment period, she developed left-sided pleuritic chest and shoulder pain after a 6-h
        car ride. She was started on cyclobenzaprine, 10 mg b.i.d., and continued on valdecoxib.
        Her left-sided pain abated gradually. However, 18 d later, she developed right-sided chest
        and shoulder pain. A diagnosis of left iliac vein thrombosis and bilateral pulmonary emboli
        was based on a computed tomography scan. She was heparinized and continued on therapy with
        warfarin, 5 mg b.i.d., and enoxaparin, 60 mg b.i.d. Despite this, a ventilation-perfusion
        scan performed 13 d later showed multiple pulmonary emboli (Figure 1).
        Currently, the patient is on warfarin, 2.5 mg daily, while completing a 6-mo warfarin
        regimen. Her warfarin is well tolerated and otherwise the patient is in good health.
      
      
        DISCUSSION
        Coxibs, selective inhibitors of cyclooxygenase-2 (COX-2), increase the risk of
        myocardial infarction and stroke [1–3], prompting concern for patients with established
        cardiovascular disease. Caution may also extend to individuals predisposed to thrombosis by
        genetic or environmental factors.
        Risk factors for spontaneous thrombosis include oral contraceptives, genetic
        predisposition to a hypercoaguable state, and prolonged periods of stasis [4–6]. At least
        two risk factors pertained to this patient.
        The patient had been taking oral contraceptives for 3 y prior to the index event, albeit
        without recognized thrombotic complication. It is possible that despite this extended
        period of apparent tolerance, the embolic events were solely related to use of the oral
        contraceptives [4].
        The relatively small risk of venous thromboembolism attributable to oral contraceptive
        use may interact geometrically with the similarly small absolute risk of a procoagulant
        mutation, such as Factor V Leiden [5]. However, documented genetic risk factors, such as
        abnormalities in lupus anticoagulant, anti-thrombin III, proteins C and S, plasma
        homocysteine, anticardiolipin, β2 glycoprotein antibody, and prothrombotic mutations in
        Factor V and prothrombin were excluded. It remains possible that the patient was
        genetically predisposed to thrombosis by a mutation in an undetermined factor.
        Finally, prolonged stasis, such as that which occurred during the 6-h car trip, may
        account for the clinical event [6]. However, the absolute risk is small, and the patient
        had made this trip on multiple occasions devoid of apparent clinical complications during
        the prior 3 y.
        The risk of thrombosis from valdecoxib has now been established [3,7], and this,
        together with its propensity rarely to cause Stevens-Johnson syndrome without a mitigating
        benefit over traditional nonsteroidal anti-inflammatory drugs, has led to its withdrawal
        from the market
        . The cardiovascular hazard of coxibs appears likely to be attributable
        to the suppression of COX-2-derived prostacyclin [1,8]. Deletion of the prostacyclin
        receptor in mice does not cause spontaneous thrombosis, but rather enhances the response to
        thrombotic stimuli [9]. This is consistent with the fact that a cardiovascular signal from
        a coxib is most easily detected in patients with hemostatic activation, such as was
        observed under placebo-controlled conditions in two trials in patients undergoing
        cardiopulmonary bypass grafting [7] and anecdotally in patients with connective tissue
        disease [10].
        Although this case does not establish a causative linkage with valdecoxib, the clinical
        event was not manifest until three potential risk factors were combined (oral
        contraceptives, prolonged stasis, and coxib treatment). Given the multiplicative
        interactions of risk factors for thromboembolic disease and the apparently untoward
        concurrence of two of them—the contraceptive pill and frequent road trips for the preceding
        3 y—the rapid occurrence of the clinical event following initiation of the coxib suggests a
        causative link to the COX-2 inhibitor. However, it also remains formally possible that this
        temporal relationship was a coincidence.
        This case report has been reported to the regulatory authorities and to the manufacturer
        of valdecoxib.
        Just as the small absolute risks of thrombosis attributable to oral contraceptives and
        prothrombotic mutations may interact dramatically [5], selective inhibitors of COX-2 may
        also interact with genetic and environmental factors that predispose to the risk of
        thrombosis.
      
    
  

  
    
      
        
        Smoking is the single largest preventable cause of disease and premature death,
        according to the World Health Organization. Smoking-related diseases kill one in ten adults
        globally, i.e., 4 million deaths annually; by 2030, if current trends continue, smoking
        will kill one in six people. Smoking is a prime factor in heart disease, stroke, and
        chronic lung disease, which cost the United States more than $150 billion a year. The
        relationship between smoking and cardiovascular disease is well documented, as is the
        association of smoking with increased levels of inflammatory markers and accelerated
        atherosclerosis. It is also well known that when smokers quit, their risk of mortality and
        future cardiac events declines, but there is little data quantifying the rate of this risk
        reduction.
        Smoking triggers an immunologic response to vascular injury, which is associated with
        increased levels of inflammatory markers, such as C-reactive protein and white blood cell
        count. Several studies have shown that such markers predict future cardiovascular events.
        Markers such as C-reactive protein are also increasingly implicated in the pathogenesis of
        atherosclerosis. There are, however, still some gaps in our knowledge of cardiovascular
        disease, smoking, and the predictive use of such markers. For example, few studies have
        examined the impact of smoking cessation on levels of inflammatory markers or on
        cardiovascular risk reduction; the level and rate at which the inflammatory response
        subsides following smoking cessation is also uncertain. Furthermore, whether traditional
        risk factors can explain the decline in cardiovascular risk following smoking cessation is
        also unclear.
        In this month's 
        PLoS Medicine , Arvind Bakhru and Thomas Erlinger investigate the
        association between smoking and smoking cessation and levels of inflammatory markers and
        cardiovascular risk factors. Data were gathered on 15,489 US adults between 1988 and 1994
        in the Third National Health and Nutrition Examination Survey. Of these, 7,665 were
        classified as never smokers, 3,459 were former smokers, and 4,365 were current smokers.
        The investigators focused on changes in C-reactive protein, white blood cell count,
        albumin, and fibrinogen, and the traditional risk factors—total cholesterol, high-density
        lipoprotein cholesterol, triglycerides, systolic blood pressure, and diabetes—that occurred
        with decreased smoking intensity and increased time since smoking cessation. They found
        that inflammatory markers had a dose-dependent and temporal relationship to smoking and
        smoking cessation. They noted that both inflammatory and traditional risk factors improved
        with less smoking, but as the time since smokers quit increased, inflammatory markers
        resolved more slowly than traditional cardiovascular risk factors. Still, the
        smoking-associated inflammatory response returned to normal within five years after smokers
        quit, suggesting that the vascular effects were reversible and that cardiovascular risk
        subsides gradually with reduced exposure.
        The authors conclude that these findings support the hypothesis that cardiovascular risk
        falls as inflammatory response falls, and that inflammatory markers are good indicators of
        this risk reduction. Despite limitations of the study, including possible errors from
        self-reporting and lack of data on second-hand smoke and newer measures such as
        interleukin-6 and high-sensitivity C-reactive protein, the inflammatory markers studied
        here demonstrated a much clearer trend and longer-lasting effect after smoking cessation
        than traditional risk factors, and hence were more useful and accurate markers of
        disease.
        As with related studies, these results suggest that smoking cessation should be a more
        prominent goal of public policy, and the authors conclude that policymakers must pursue
        smoking cessation plans as an opportunity to make savings on health care through
        cardiovascular risk reduction. Further research should explore the acute phase response in
        the months after smoking cessation, which this and other studies have not been able to
        study adequately.
      
    
  

  
    
      
        
        If you are overweight, then losing weight is good for your health, surely?
        Unfortunately, the evidence on which an answer to this seemingly simple question might be
        based is at best equivocal, and at worst very controversial. Previous work has shown that
        weight loss in obese people improves risk factors associated with cardiovascular diseases
        and diabetes, but studies are conflicting on the long-term effects of weight loss on
        mortality. A study in this month's 
        PLoS Medicine by Jaakko Kaprio and colleagues on a Finnish dataset adds
        more evidence to this debate, but experts are divided on what can be concluded from it.
        The major difficulty in getting clear results on this question is that it is virtually
        impossible to do a controlled trial to answer it. Hence, the evidence accumulated has come
        mostly from epidemiological studies, but it is notoriously difficult to remove all
        confounding factors from these studies. Kaprio and colleagues' study is another
        epidemiological study, but we should not simply dismiss the data as unreliable just because
        of the problems inherent to such a study design. Instead, we should consider their study in
        the light of all the other evidence available.
        Starting from a group of 19,993 twins from Finland who have been studied since 1975, the
        authors gathered data from the 2,957 overweight participants who remained after they had
        excluded people with pre-existing disease, and those with missing data. These twins had
        been asked in 1975 if they intended to lose weight, and then had information on weight
        collected in 1981. Information on mortality was then collected over the next 18 years; the
        authors then analyzed mortality in relation to intention to lose weight and actual weight
        change.
        In total, 268 people died. When the results were analyzed, the surprising finding was
        that people who intended to lose weight, and who did so, had a somewhat higher mortality
        than those who intended to lose weight but whose weight remained stable, or went up. People
        who intended to lose weight, and who did so, also had a slightly higher mortality than
        those who did not intend to lose weight and whose weight was stable.
        The problems with such a study are outlined in an accompanying Perspective (DOI:
        10.1371/journal.pmed.002018) by Meir Stampfer from Harvard School of Public Health, and
        there is no doubt that these results seem counterintuitive. Some readers may take away the
        idea from this paper that overweight people should not be advised to lose weight, but
        Stampfer cautions against that interpretation. Perhaps the safest interpretation of these
        results is that by the time adults are overweight, the health benefits of losing weight are
        not clear-cut. If there is one message therefore that should be taken from the paper it is
        this: in order to prevent the associated health effects of obesity, preventing obesity,
        especially in childhood, should be an overriding public health priority.
        The study leaves us with the question of how intentional weight loss could lead to
        excess mortality. The authors suggest that this could be due to the unavoidable loss of
        lean body mass, which according to several other studies may increase mortality, and which
        may outweigh the beneficial effects of losing fat mass in healthy individuals. The authors
        therefore conclude that “the long-term effects of weight loss are complex, and they may be
        composed of oppositely operating effects with net results reflecting the balance between
        these effects.”
      
    
  

  
    
      
        
        As cells specialize during development they pass through different levels of
        differentiation, from the earliest stem cells through to the highly specialized types that
        make up the body's organs. Hence, a number of different tissues may derive from common
        precursors. For example, muscle, fat, cartilage, and bone are all derived from a group of
        mesenchymal precursor cells that originate in the paraxial mesoderm. So pluripotent (i.e.,
        able to differentiate into any cell type) human embryonic stems cells are potentially a
        starting point for the regeneration of all types of diseased or damaged organs (and already
        researchers have shown that it is possible to stimulate human embryonic stem cells to
        differentiate into specific cell types such as neural or hematopoietic cells). The
        isolation of intermediate multipotent stem cells (which can differentiate into a limited
        number of cell types) may also be valuable. For example, the production of an unlimited
        supply of mesenchymal precursors would be very useful, not only for the understanding of
        how cells differentiate, but also for eventual practical application.
        In this month's 
        PLoS Medicine , Lorenz Studer and colleagues from the Sloan-Kettering
        Institute in New York describe a protocol for deriving mesenchymal precursors, which they
        then show are capable of differentiating into specialized cell types.
        They used two undifferentiated stem cell lines—from the 22 lines that were approved in
        2001 by President Bush for use in federally funded research in the United States. The
        specifications for approval for these lines are clear—see the guidelines at
        http://stemcells.nih.gov/research/registry/eligibilityCriteria.asp. The number of human
        embryonic stem cell lines available for researchers are strictly limited, making it
        necessary to develop protocols that expand these cells along various lineages.
        In order to differentiate the cells into mesenchymal precursors, the stem cell lines
        were cocultured with mouse feeder cells to produce five different polyclonal lines. The
        authors then cultured these polyclonal precursors with appropriate tissue-specific
        stimulation in attempt to produce fat, bone, cartilage, or muscle cells. The evidence that
        the authors provide for these cells being differentiated includes analysis of gene
        expression, surface antigens, and immunocytochemistry typical of the mature tissues. For
        example, the authors were able to show the presence of fat granules in adipocytes, calcium
        in the matrix of osteogenic cells, and collagen in chondrocytes. It was harder to produce
        muscle cells, but even these types of cells could eventually be induced by specific culture
        conditions.
        What are the possible concerns about these types of studies? One obvious one is the
        potential for residual undifferentiated cells to turn into tumors, but the authors tested
        the differentiated cell cultures for cell surface markers characteristic of
        undifferentiated cells and found no evidence of them. Another worry for the use of these
        cells directly in humans is the need, at least at the beginning, to culture the cells with
        mouse feeder cells—obviously no human treatment could contain cells contaminated with mouse
        cells. Further development of protocols will be needed to address this issue. However, as
        the authors comment, “the high purity, unlimited availability, and multipotentiality of
        hESMPCs [human embryonic stem cell–derived mesenchymal precursor cells] will provide the
        basis for future therapeutic efforts using these cells in preclinical animal models of
        disease.” In addition, the techniques described here will provide a very useful resource
        for studying mesenchymal cell development.
      
    
  

  
    
      
        
        
        PLoS Medicine is a sufficiently new journal that we are often doing
        something for the first time. This issue's “first” is the publication of a research article
        that reports data exclusively from animals, more precisely from six cynomolgus macaques
        used to test the efficacy of a new Lassa fever vaccine. There is no question that animal
        studies are an important part of medical research, but which ones, if any, belong in a
        medical journal? More to the point, which ones belong in 
        PLoS Medicine ? Although our journal's focus is on human studies, we have
        decided, on occasion, to publish animal studies that have important and proximal
        implications for clinical research, and maybe even practice.
        Lassa fever causes serious morbidity and mortality in West Africa. The virus's natural
        hosts are rodents, and as there is little chance for effective rodent control in the
        endemic areas, a vaccine is the most feasible way to gain control of the disease. Several
        research groups around the world have worked on vaccine development—and their efforts have
        been boosted by the classification of Lassa virus as a Category A bioweapons agent—but to
        date no vaccine is available for either general or high-risk application in humans.
        Thomas Geisbert and colleagues have developed and now report tests of a recombinant
        vaccine based on a replication-competent vesicular stomatitis virus (DOI:
        10.1371/journal.pmed.0020183). One shot of this vaccine protected four out of four
        vaccinated monkeys against a lethal virus challenge, whereas the two control animals died,
        making the vaccine a serious candidate for future application in humans. Clearly many
        issues about this vaccine still need to be resolved, such as vector safety, duration of
        protection, and breadth of protection (there are at least four distinct Lassa virus
        strains). Nevertheless, we accepted this paper because we and our advisers felt that the
        research was at a stage where clinical questions, such as patient safety and design of
        early human trials, should inform any additional studies in animals. The proper place for
        such a study is, we believe, a clinical journal.
        There are several other types of animal studies we consider appropriate for publication
        in 
        PLoS Medicine . These include studies that explore off-label uses of
        approved medical interventions in validated animal disease models, again based on the
        studies' direct relevance to potential treatment of human patients. More often, we would
        publish human studies that also include experimental animal data, which typically explore
        molecular mechanisms suggested by the human data.
        Animal studies that are submitted to medical journals can be broadly divided into two
        groups—“animal clinical trials” and exploratory studies. We will assess the former in a
        similar way to how we look at human trials. This assessment will include not only the
        ethical conduct of the study and approval by the respective regulatory authority, but also
        the rigor of the methodology. Too often, animal clinical trials, i.e., prospective,
        hypothesis-testing studies that evaluate the effects of a health-related intervention in
        animals, are not performed with the same rigor that has been developed over past decades
        and widely adopted by the clinical research community. In particular, animal studies often
        have inappropriate controls, are underpowered, involve researchers monitoring outcomes who
        are not blinded to treatment allocation, or lack proper statistical analysis.
        Exploratory studies, designed to yield insight into disease etiology, pathology, or the
        mechanisms by which a particular treatment affects a disease state, are a crucial early
        part of the translation of basic research findings into clinical practice. However, by and
        large, they will not be appropriate for 
        PLoS Medicine . Instead, we encourage submission of important advances
        from this early translational stage to 
        PLoS Biology , our flagship open-access biology journal
        (www.plosbiology.org).
        It would be foolish to deny the existence of a sizeable “grey area” between these types
        of study. Between our journals, we will try to provide open-access publication for any
        important study at this interface between biology and medicine, and will be happy to talk
        with authors on a case-by-case basis. In addition, we currently cross-reference studies
        between the two journals—and will do so even more when our new journals come on line. For
        example, 
        PLoS Medicine has published a number of Perspectives on research articles
        published in 
        PLoS Biology , and 
        PLoS Biology regularly highlights papers from 
        PLoS Medicine on its home page. In this way, because all our journals are
        open access, the difference to the reader between a paper published in 
        PLoS Medicine and any other PLoS journal, is just a rodent click.
      
    
  

  
    
      
        
        As a neurologist subspecializing in epilepsy at a respected academic institution, I (DH)
        assumed that I knew everything I needed to know about epilepsy and patients with epilepsy.
        I was wrong.
        In September of 1994, John Lester, my colleague in the Department of Neurology at
        Massachusetts General Hospital, showed me an online bulletin board for neurology patients
        that he had created [1]. In reading through the online messages, I observed hundreds of
        patients with neurological diseases sharing their experiences and discussing their problems
        with one another.
        I knew that many patients with chronic diseases had been making use of online medical
        information [2]. Nonetheless, I was shocked, fascinated, and more than a bit confused by
        what I saw. I'd been trained in the old medical school style: my instructors had insisted
        that patients could not be trusted to understand or manage complex medical matters.
        Thinking back through my years of training and practice, I realized that there had always
        been an unspoken prohibition against groups of patients getting together. I had the
        uncomfortable sense that by promoting interactions between patients and de-emphasizing the
        central role of the physician, I might be violating some deep taboo.
      
      
        Remarkably Complex Stories
        My initial doubts notwithstanding, I found dozens of well-informed, medically competent
        patients sharing information on a variety of topics. I was especially struck by the many
        stories recounting the development of a particular patient's illness, the patient's efforts
        to manage it, and the resulting interactions with health professionals. By telling their
        stories in such elaborate detail, experienced group members could offer a great deal of
        useful advice and guidance to those newly diagnosed, based on what they had learned in
        their own online research, what they had been told by their clinicians, and what they had
        deduced from personal experiences with the disease.
        These “patient stories” often included a number of empowering elements that set them
        apart from the advice patients typically receive from their clinicians: role modeling by an
        active, critical, well-informed “expert patient” ([1]; http://patientweb.net), comparative
        reviews and recommendations of clinicians and treatment facilities [2–5], and advice about
        how to handle the practical details of living with a chronic illness [6] (such as how to
        organize a home medical record, manage treatment side effects, find the best drug prices,
        and deal with less-than-perfect health professionals and health-care provider systems, and
        a wide variety of other topics relating to effective medical self-management). These
        extended patient narratives—no two alike—thus gave rise to an accumulated body of what my
        colleagues and I began to think of as an expert patient knowledge base. We concluded that
        these patient narratives could be invaluable resources for clinicians and researchers,
        interested in taking an in-depth look at the changing roles of patients and clinicians in
        the Internet age.
        The constant outpouring of sympathy and support that we observed in interactions among
        community members surpassed anything a patient might conceivably expect to receive at a
        doctor's office. As Richard Rockefeller, President of the Health Commons Institute, has
        suggested, disease-specific online patient networks provide their members with an
        invaluable type of around-the-clock support that he has called the “chicken soup of the
        Internet” [7].
        Working with several colleagues, I initiated an observational study to analyze the ways
        in which E-patients were using this new medium. Since I am an epilepsy specialist, we
        decided to focus on an epilepsy support group at the site Lester had created, BrainTalk
        Communities (http://www.braintalk.org) (Figure 1) [8]. The BrainTalk Communities currently
        host more than 300 free online groups for neurological conditions (such as Alzheimer
        disease, multiple sclerosis, Parkinson disease, chronic pain, epilepsy, and Huntington
        disease) for patients across the globe. More than 200,000 individuals visit the BrainTalk
        Communities' Web site on a regular basis. This site is now owned and operated by an
        independent nonprofit group, BrainTalk Communities, and is no longer formally associated
        with Massachusetts General Hospital.
        What we found surprised us. We assumed that most interactions would be support related,
        with some members describing their medical experiences and others offering active
        listening, sympathy, and understanding. But while such interactions were an important part
        of the group process, they were observed in only about 30% of the postings. In the
        remaining 70% of the postings, group members provided each other with what amounted to a
        crash course in their shared disease, discussing topics such as the anatomy, physiology,
        and natural history of the disorder; treatment options and management guidelines for each
        form of treatment; and treatment side effects, medical self-management, the day-to-day
        practicalities of living with the disease, and the effects of their condition on family and
        friends (Table 1).
      
      
        A Source of Information
        Much of the information that the group provided to members was similar to what I
        routinely provided to my own clinic patients. So I was surprised to learn that many of the
        clinicians caring for group members provided considerably less information, guidance, and
        support. And some, apparently, provided none at all. Statements such as “My provider is too
        busy,” “My provider doesn't care,” or “My provider doesn't seem to know about
        such-and-such” were alarmingly common. About 10% of the members' posts spontaneously
        mentioned that they had been unable to get the medical information that they needed from
        their own clinicians. When we surveyed members directly, more than 30% said that they had
        been unable to obtain all the medical information they would have liked from their
        physicians (Table 2). This was a primary reason for many members' participation in the
        group.
        Some other types of information, especially practical tips for living with epilepsy and
        the social aspects of the disease, went far beyond what I had been providing for my own
        patients. I am a board-certified epilepsy specialist at one of the most highly respected
        medical centers in the United States, yet I learned a great deal about these topics from
        the support group. I now share many of the things I learned from group members with my
        clinic patients.
        The BrainTalk Communities epilepsy support group that we observed was facilitated by
        volunteer patient moderators, with little or no professional input. About 6% of the
        postings contained information that some of our medical reviewers considered at least
        partly mistaken, misinterpreted, outdated, or incomplete. We observed that other group
        members frequently corrected such misinformation. And group participants appeared to
        understand that they should not take uncorroborated statements as hard facts. They seemed
        well aware that some postings were erroneous, and in fact seemed to substantially
        overestimate the incidence of questionable materials.
        We observed no serious problems as a result of these questionable postings, and saw many
        reports by patients who had obtained better care, prevented medical mistakes, or averted
        serious injury because of the information and advice they received from fellow group
        members. We concluded that, as Ferguson and Frydman have suggested, many professionals have
        seriously overestimated the risks and underestimated the benefits of online support groups
        and other online health resources for patients, probably because they do not operate within
        our familiar professionally centered constructs [9].
      
      
        What I've Learned
        In retrospect, the most important thing I (DH) have learned from our online group was
        that patients want to know about, and in most cases are perfectly capable of understanding
        and dealing with, everything their physician knows about their disease and its treatments.
        After observing the group, I realized that I had been providing my patients with a very
        limited subset of what I knew about their condition. Today, there is nothing that I know
        about epilepsy that I would hesitate to share with a patient. For example, I now offer my
        patients an open and frank discussion of the very rare sudden unexpected death in epilepsy
        syndrome. I had previously not mentioned this rare but alarming complication, fearing that
        some patients might become overly concerned with it. But once I discovered that BrainTalk
        Communities group members discussed this topic quite openly and freely online, reviewing
        the scientific data in a sophisticated way, I began to share my knowledge on this topic
        with my clinic patients. My newfound frankness has been much appreciated. And none of my
        patients have become unduly troubled by these discussions.
        I have also learned that an online group like the BrainTalk Communities epilepsy group
        is not only much smarter than any single patient, but is also smarter, or at least more
        comprehensive, than many physicians—even many medical specialists. While some postings do
        contain erroneous material, online groups of patients who share an illness engage in a
        continuous process of self-correction, challenging questionable statements and addressing
        misperceptions as they occur. And while no single resource, including physicians, should be
        considered the last word in medical knowledge, the consensus opinion arrived at by patient
        groups is usually quite excellent. And if more expert clinicians offered to consult
        informally with the online support groups devoted to their medical specialties—as I now
        do—we could help group members make information and opinion shared in these groups even
        better.
        I had been taught to believe that patients could only be “empowered” by their
        clinicians. And while I do believe that clinicians can help in this regard by sharing their
        knowledge openly and by encouraging patient self-reliance, it now seems quite clear that
        growing numbers of patients are perfectly capable of empowering themselves, with or without
        their clinician's blessing. Physicians and other health professionals should do all they
        can to support them in this worthy effort.
        As a result of what we've learned from these online patient networks, our research group
        has developed a password-protected Web site, PatientWeb (https://fisher.mgh.harvard.edu/),
        for the patients that we see in the clinic—all those patients with epilepsy who receive
        medical care at the Massachusetts General Hospital and Brigham and Women's Hospital. Thanks
        to what we have learned from these online groups, we plan to pilot new ways for private,
        local online groups made up of patients with the same disease and receiving care from the
        same clinicians to collaborate with each other, and with their clinicians, more
        effectively.
      
      
        Conclusions
        Clinicians have overestimated the downsides, while seriously underestimating the
        benefits, of condition-specific online patient support communities. These free online
        resources now provide invaluable services 24 hours a day, seven days a week, for patients
        across the country and around the world. It would be unfortunate indeed if medical
        professionals let their uneasiness at this emerging trend toward patient empowerment and
        autonomy cloud their ability to assess the impressive benefits these groups provide.
        Many patients are now ready, willing, and able to take a more active role in their own
        care, and the care of others with related diseases. By encouraging patients to do more for
        themselves and for each other, clinicians can help mitigate many of the negative effects of
        contemporary time-pressured medical practice. Thus, even though there may now be less time
        for the counseling, storytelling, support, information sharing, and empowerment-based
        training that was once a routine part of the typical office visit, we can now help our
        patients obtain such services by referring them to online patient networks.
        The distributed expertise of online support groups is by no means limited to the
        emotional aspects of the illness and to the practical logistics of living with the
        disorder. It can also include current reviews of the literature, reports from the latest
        medical meetings, accounts of behind-the-scenes activities at the best treatment centers,
        sophisticated guidance on dealing with medical professionals, and excellent advice on
        dealing with complex aspects of medical management.
        Finally, I have concluded that few, if any, physicians could have created a system like
        BrainTalk Communities. As a tech-savvy non-physician intimately familiar with both the
        inner workings of medical care and the power of information technology systems to create
        effective online communities, John Lester was less proprietary than most physicians are
        about medicine's proper professional “turf.” He was also less inhibited by professional
        biases regarding the potential value of the medical contributions that “unqualified”
        individuals might make. This is not an isolated occurrence. We suspect that the intensely
        professionally centered enculturation most physicians receive in their training and
        practice environments may render them, in the words of John Seely Brown and Paul Draguld,
        “blinkered if not blind” to the emergence of many promising new technocultural changes,
        which currently present new opportunities for health-care innovation [10]. Thus, physicians
        who seek to innovate in these areas might benefit greatly—as I have—from joining forces
        with Web developers, Net-savvy social scientists, experienced E-patients, and other
        colleagues unencumbered by the limiting belief systems that may result from our traditional
        medical training.
        In light of their empowering social dynamics and volunteer economics, we suspect that
        patient-led online groups may prove to be a considerably more promising and sustainable
        health-care resource than professionally moderated therapy groups. And we are convinced
        that networked work teams linking patients, caregivers, and medical professionals will be
        an important model for future health-care innovation.
      
    
  

  
    
      
        
        On May 15, 2005, the Public Library of Science and the Government Accountability
        Project, a public interest legal group that advises and supports whistleblowers
        (www.whistleblower.org), co-sponsored a private meeting near the Capitol Building in
        Washington, D. C. In the room were four of the most high profile medical whistleblowers of
        recent times. All four have gone public with information about practices in medicine and
        medical research that they believe are risking the public's health or safety [1–5]. One of
        them was David Graham, Associate Director in the United States Food and Drug
        Administration's (FDA's) Office of Drug Safety, whose research on rofecoxib (Vioxx) pointed
        to the serious cardiovascular risks of the drug [1]. Graham was speaking in his own
        capacity and was not representing the FDA. An anonymous fifth whistleblower—a research
        scientist at a major drug company—participated by phone.
        The whistleblowers took turns to share their stories, including their accounts of
        retaliations they said they had faced from their employers on raising their concerns, which
        led to lawsuits by at least two of the whistleblowers [2,3]. The picture that emerged from
        these accounts—a picture of American medicine's inappropriate ties with the pharmaceutical
        industry—was deeply troubling.
        As the investigative medical journalist Jeanne Lenzer reports in her Essay for 
        PLoS Medicine [6], the whistleblowers spoke of public regulatory agencies
        that are putting the interests of drug companies ahead of the safety of patients, and of
        pharmaceutical companies that allow their marketing departments to knowingly downplay
        serious side effects when promoting their drugs. And they spoke of the woefully inadequate
        protection offered to those in the medical community who feel morally compelled to blow the
        whistle.
        It was Lenzer who conceived the idea for the meeting. She believed that important
        lessons would emerge from having these medical whistleblowers, who come from very different
        professional backgrounds, together in one room to share their experiences. It took her many
        months of planning. In particular, she needed to gain the trust of the industry research
        scientist, so that the scientist could feel sure that anonymity would be preserved. But all
        of her planning nearly came to nothing. At the last moment the original journal sponsor
        pulled out on the advice of its lawyers.
        
          
            
              The picture that emerged from these accounts was deeply troubling.
            
          
        
        Lenzer's phone call to PLoS, enquiring whether we might step in, came just ten days
        before the event was due to happen. We took our own legal advice and then agreed to sponsor
        the roundtable. PLoS was eager to support this event, and willing to accept any small legal
        risks, because we believe that the issues raised will be of huge interest to the medical
        community, to the press, to patients, and to the broader public. Further, the event fits
        well with our own public service mission of making all scientific and medical research
        results freely and publicly available, and with our belief that transparency in the conduct
        and publication of research is important for public trust. And the topic of the roundtable
        was in line with other articles we have published highlighting the many ways in which
        medicine has become tightly entangled with industry, to the great detriment of patients
        [7–9].
        The risks to a journal in sponsoring such an event are, of course, much smaller than the
        risks that the whistleblowers at the roundtable faced in going public with their stories.
        Studies have shown that whistleblowers in both public service and private industry almost
        always experience retaliation from their employers, with those employed longer experiencing
        greater retaliation [10,11]. They risk loss of earnings, intimidation, harassment,
        victimization, and personal abuse, and they traditionally receive little help from
        statutory authorities [12,13].
        The Washington whistleblowers' stories illustrate these issues. Psychiatrist Stefan
        Kruszewski described how he was fired from his job at the Pennsylvania Department of Public
        Welfare (DPW) after alerting his seniors to prescribing practices across the state that he
        considered to be alarming and dangerous [2]. “I was fired in a demeaning manner,” said
        Kruszewski, who has sued DPW over his firing. “My two offices were emptied and the contents
        of these offices were put in the gutter.” David Graham—who testified at a US Senate Finance
        Committee hearing on rofexocib (Vioxx), the FDA, and Merck [1]—said that there was a
        conspiracy by senior management at the FDA “to intimidate me ahead of the Senate
        testimony.” Both of these individuals contend that pharmaceutical industry influence over
        their employers (a state and a federal regulatory agency, respectively) played a part in
        the difficulties these individuals faced in getting their concerns heard [1,2].
        Lenzer's report will, we hope, spark discussion and debate about how American
        medicine—clinicians, researchers, regulatory agencies, and medical journals—can disentangle
        itself from the influence of the pharmaceutical industry. In the past, medical journals and
        their editors have played an important part in exposing the complex relationships between
        the pharmaceutical industry and medicine [14–16], including between industry and the
        medical journals themselves [9]. 
        PLoS Medicine will continue to look critically at these relationships. A
        common theme at the roundtable was that, armed with information, the public too could have
        an important role in unpicking these ties. “The pharma–FDA complex has to be dismantled,”
        said Graham, “and the American people have to insist on that, otherwise we're going to have
        disasters like Vioxx that happen in the future.” Patients, health professionals, and even
        the industry itself all surely stand to gain from disentanglement.
      
    
  

  
    
      
        
        A year ago, I received an E-mail from a research scientist at a major pharmaceutical
        company. The scientist had read my articles on whistleblowers who had raised concerns about
        the undue influence of the pharmaceutical industry on American medicine My industry source
        had information for me about drug company practices, but—out of fear of career ruin—would
        only talk on the condition that I would conceal the scientist's identity.
        For the next year or so, I had repeated contacts with the scientist. As I listened to
        this researcher—and to the other medical whistleblowers that I continued to interview—it
        occurred to me that each whistleblower was like the proverbial blind man with a hand on the
        elephant. Each could describe one piece of the puzzle, but the full picture could only
        emerge by bringing these whistleblowers together.
        With an eye to focusing on the systemic problems that have allowed American medicine to
        be unduly influenced by industry, on May 15, 2005, I brought together five whistleblowers
        in Washington, D. C. I asked them each to tell their story and to suggest ways to restore
        objectivity to medicine and medical research.
      
      
        The Whistleblowers
        Four whistleblowers attended in person, and the anonymous industry scientist
        participated via speakerphone. The whistleblowers came from an extraordinary variety of
        different professional backgrounds.
        
          
          
            David Graham
            This Food and Drug Administration (FDA) safety officer raised concerns about the
            cardiovascular side effects of rofecoxib (Vioxx) and other Cox-2 inhibitors. He
            testified at a United States Senate Finance Committee hearing on rofexocib, the FDA,
            and Merck [1,2]. Graham attended the roundtable in his own personal capacity and was
            not representing the FDA.
            
              Each whistleblower was like the proverbial blind man with a hand on the
              elephant.
            
          
        
        
          
          
            Allen Jones
            This investigator at the Pennsylvania Office of the Inspector General led an
            investigation into an off-the-books account, funded in part by drug companies, from
            which payments were made to state employees to develop a medication treatment
            algorithm. He filed a civil rights lawsuit against the Pennsylvania Office of the
            Inspector General to protect his right to publicly discuss his findings, and was later
            fired from his job for talking to the press [3–6].
          
        
        
          
          
            Stefan Kruszewski
            This Harvard-trained psychiatrist was hired by the Bureau of Program Integrity in
            the Pennsylvania Department of Public Welfare to oversee the state's mental health and
            substance misuse programs. He filed a law suit in a federal court in the Middle
            District of Pennsylvania, charging that he was fired after uncovering widespread abuse
            and fraud in the bureau [7,8].
          
        
        
          
          
            Kathleen Slattery-Moschkau
            This former drug representative left the pharmaceutical industry after witnessing
            marketing practices that she found disturbing. She wrote and directed the movie 
            Side Effects , a fictionalized account of her experiences [9,10].
          
        
        
          
          
            The anonymous research scientist
            This is an industry insider who said to me, ahead of the roundtable, that the
            culture of secrecy at drug companies too often results in claims that are closer to
            “propaganda” than science.
          
        
      
      
        Lessons Learned from the Roundtable
        
          
          
            Ties between drug regulators and industry may influence new drug approval
            David Graham described the frustrations he had felt in his almost 20 years of
            experience as an FDA drug safety officer. Although he was instrumental, he said, in
            getting ten drugs off the market because of safety concerns, his experience was like a
            salmon swimming upstream—“a single individual…against the tide.” The tide, he said, “is
            an entire institution whose mission is to approve drugs and make industry happy.”
            The FDA, said Graham, is in a “collaborative relationship” with industry. The FDA
            gets money from drug companies through the Prescription Drug User Fee Act of 1992 (see
            http://www.fda.gov/cber/pdufa.htm) “to approve new drugs and approve them more
            quickly.” The mindset at the FDA, he said, is that “we will find a reason to approve a
            drug no matter how small the indication for the drug.” Graham explained that a senior
            official at the FDA had told him: “industry is our client.“
            When the FDA knows there is a serious problem with a new drug, he said, the FDA
            deals with this by saying, “well, we'll handle it in labeling” even though, said
            Graham, “FDA knows labeling doesn't work.”
            “There is no independent voice for drug safety in the United States,” he said. The
            upper-level managers in the FDA's Office of Drug Safety are appointed from the FDA's
            Office of New Drugs, which approves new medicines. This makes the Office of Drug Safety
            “captive,” he said, to the Office of New Drugs.
            The anonymous scientist said that in order to speed up drug approval, companies
            “don't measure things like whether we are really curing the disease, or prolonging
            life, or preventing hospitalization, or whether a patient is truly more functional.
            Oftentimes, we're measuring intermediate, lesser things, markers, predictors—we 
            hope —of these clinical endpoints, but they may or may not be
            accurate.”
            And the FDA, said the scientist, requires just two positive studies to grant
            approval to a new drug, but there is no limitation on how many negative studies can be
            done before one or two positive studies are produced. This can lead to approval of a
            drug even when most studies are negative or show no effect.
            Both Graham and the anonymous scientist suggested putting an end to the Prescription
            Drug User Fee Act, and Graham argued that there needs to be independent authority for
            those in charge of drug safety. They indicated that two bills in Congress, introduced
            by Senator Grassley and by Congressman Hinchey, at least partly address these
            concerns.
            “The pharma–FDA complex has to be dismantled,” said Graham, “and the American people
            have to insist on that, otherwise we're going to have disasters like Vioxx that happen
            in the future.”
          
        
        
          
          
            The race to approve new drugs without proper safety testing may be compromising
            the public's health
            “Drug companies assiduously avoid acquiring information about side effects,” said
            the industry scientist. “Drug companies will not conduct safety studies unless they
            have to—meaning basically that they're required by a regulator—and that rarely
            happens.” High-risk patients who might have a bad reaction to a drug, said the
            scientist, “are excluded from studies deliberately, even though, when the drug is
            approved, these patients will be targeted for sales.” When a safety study is proposed
            within the industry, said the scientist, “a typical response will be that if we
            conducted a study to find out if there was a safety problem, people would learn about
            it and think we 
            had a problem [which] would destroy the image of safety that has been
            so carefully constructed.”
            
              “There is no independent voice for drug safety in the United States.”
            
            Studies are too small and are conducted over too brief a period to properly assess
            safety: “The largest studies—the phase three studies, [which] might be several thousand
            people—last for a few months. If drugs kill one in several thousand per year, this
            would be a public health catastrophe. A blockbuster drug with that kind of hazard
            associated with it could be associated with tens of thousands of deaths a year, and it
            would never be detected in studies of the kind that we routinely submit and are the
            basis for approval.” These drugs, said the scientist, and these kinds of risks, are
            “essentially out there now, unlabeled, unnoticed, all beneath the radar.”
            The scientist said that, “to ensure that safety problems will go unnoticed, we
            compound the problem of conducting small studies by setting a statistical threshold for
            acknowledging the safety problem that is so high that you know in advance it could
            never be reached for any serious side effect, like myocardial infarction.” This
            practice, said the scientist, “virtually ensures that if a bad side effect happens to
            show up, it's not going to reach the arbitrary level that we call statistically
            significant, and the company can maintain that it's just bad luck.” And if a bad result
            does happen, “typically a company is not going to publish the study at all. If they do
            publish it, the bad result can be omitted as ‘not statistically important.’”
          
        
        
          
          
            The funding of state officials by industry may be affecting prescribing
            patterns
            Allen Jones described how he believed that drug companies were acting at the state
            level to influence the prescribing of psychiatric medications.
            “I began to investigate an account into which pharmaceutical companies were paying
            money that was being accessed by state employees,” he said. “Additionally, I found that
            various pharmaceutical companies were paying state employees directly—also giving them
            trips, perks, lavish meals, transportation, honorariums up to $2,000 for speaking in
            their official capacities at drug company events. They were given unrestricted
            educational grants that were deposited into an off-the-books account—unregistered,
            unmonitored, literally operated out of a drawer.”
            These same state officials, he said, were responsible for dictating clinical policy
            and writing guidelines for the treatment of patients in the state system. These
            officials were, he said, receiving money from companies with a stake in the guidelines.
            “The protocol they [the officials] were developing was called the Texas Medication
            Algorithm Project, TMAP, which began in Texas in the mid-90s. It outlined detailed
            medication guidelines for schizophrenia, depression, and bipolar disorder. It
            recommends almost exclusive usage of newer, patented, very expensive atypical
            antipsychotics, SSRIs [selective serotonin uptake inhibitors], and mood stabilizers.”
            The Texas Medication Algorithm Project, said Jones, was based on “expert consensus”
            from industry-supported meetings.
            Jones said that when he wanted to investigate these findings, he was shut down. “I
            was told point black, ‘Look, drug companies write checks to politicians, they write
            checks to politicians on both sides of the aisle—back off.’” He was told, he said, to
            “quit being a salmon, quit swimming against a stream.” He wouldn't back down from his
            investigation, he said, and was demoted. On November 22, 2002, he filed a civil rights
            lawsuit “to preserve my job and my right to speak out.” His employer, he said, took him
            off investigative duties altogether.
            Stefan Kruszewski, who has filed a law suit in a federal court in Pennsylvania,
            raised concerns to his seniors in the Pennsylvania Department of Public Welfare about
            prescribing practices in the state that he did not feel were evidence based, and said
            he lost his job for raising his concerns. For example, he alerted his seniors to the
            off-label prescribing of the anticonvulsant gabapentin (Neurontin) for mood disorders
            and addictive disorders.
            “The pharmaceutical industry is the single most powerful lobbying group on Capitol
            Hill—outspending even the oil and banking industries,” said Jones. “It should come as
            no surprise that the ties go far beyond just the mental health officials who wrote the
            guidelines, but extend to many of the politicians who, in the end, allowed an
            investigation into pharma corruption to be dropped, and the investigator—me—to be
            fired.”
            Efforts to detect and deter fraud and abuse due to these conflicts, he said, “will
            be likely to be undermined as long as those charged with detecting fraud and abuse,
            like the [Pennsylvania] Inspector General, are appointed by politicians who are
            themselves beholden to the drug industry. Such positions should instead be filled by
            career civil servants and not political appointees.”
          
        
        
          
          
            Regulatory agencies are not being held accountable
            In comments that echoed his testimony to the US Senate Finance Committee, Graham
            said that, “FDA was the single greatest obstacle to doing anything effective with
            Vioxx. As a result, nearly 60,000 people probably died from that drug. That's as many
            of our soldiers that were killed in the Vietnam war [who] died as a result of Vioxx
            use. And FDA had the opportunity, the responsibility, to stop that and didn't. In fact,
            FDA allowed it to continue. In my book, FDA shares in the responsibility for those
            deaths and yet it's not being held accountable by Congress.” Congress itself, added
            Graham, is deeply beholden to the drug industry since many politicians receive “often
            quite a bit of campaign contributions” from the industry.
            Kruszewski reflected upon the problems he said he had encountered in Pennsylvania,
            saying that “there is no accountability in the system for oversight [agencies].” He has
            become “a stronger advocate than ever for a federal patient bill of rights.”
          
        
        
          
          
            Marketing departments can influence doctors’ prescribing habits
            The research scientist said that the job was attractive because of the “many
            excellent drugs” developed, such as drugs to treat HIV, but the scientist “also saw
            drugs marketed in a way that will exaggerate the benefits and conceal the risks.”
            Kathleen Slattery-Moschkau gave an insider's view of drug marketing practices, from
            her former experiences as a drug rep. She clutched her head in disbelief as she told
            the roundtable that doctors would come up to her with patients' charts asking her for
            advice on treating patients. Slattery-Moschkau, like most of the drug representatives
            she came to know over the years, had no science background at all.
            
              “Drug companies assiduously avoid acquiring information about side effects.”
            
            The various techniques drug representatives were trained in to “educate doctors”
            eventually proved to be not just “comical” but “also scary,” she said. “Whether it was
            hiring, training, what we were told to say about drugs and what we were told not to
            say,” it was marketing, not science, that dominated. One of the techniques used by drug
            companies was to buy doctors' prescribing records so drug representatives knew “to the
            dime” what drugs doctors were prescribing and could tailor their marketing to them.
            Drug representatives developed “personality profiles” on doctors and were taught to
            pitch their sales to specific personality types. Representatives were compensated, she
            said, by “how many prescriptions we could encourage.”
            Both Slattery-Moschkau and the industry scientist described tensions within drug
            companies between marketing departments and industry scientists. “The marketing spin on
            things,” said the scientist, “carries the day.”
          
        
        
          
          
            The published medical literature contains many biases
            “When studies are published,” said the scientist, “they are frequently written not
            by the trained research scientist, who might have designed and analyzed the study, but
            by a designated medical writer with little if any background in research, but who is
            trained instead to craft the findings of the study in the best possible way for the
            company.”
            The body of literature available to the public, said the scientist, “is a biased
            sample of what companies want people to see.” The research scientist described “a
            culture of secrecy,” which makes it hard even for industry scientists tasked with
            ensuring drug safety to obtain the full datasets needed to genuinely understand a
            drug's risk–benefit profile.
          
        
      
      
        Conclusion
        Whistleblowers have been compared to bees—they have just one sting to use and using it
        may lead to career suicide [11]. Many of the whistleblowers at the roundtable said they had
        experienced retaliation from their employers for raising concerns, but all had felt
        obligated to speak out about practices in medicine and medical research that they believe
        are risking the public's health or safety. Graham said he felt “trapped by the truth” and
        had to act. “There are bigger issues here,” said Kruszewski. “I felt right from the start
        [that] if I wallowed in self-pity about being fired and having my belongings piled in the
        gutter that I would never understand why all these things were happening. The bigger issue
        is that we've got people in the pharmaceutical industry and the health-care industry all
        acting in synchrony.”
        Each of these whistleblowers, in very different ways—from making a satiric film to
        speaking out in Congress—has shone light on how this “synchrony” may be compromising the
        integrity of American medicine. We should not have to rely on medical whistleblowers to
        alert us to these fault lines. If we are to restore objectivity to drug development,
        prescribing, and safety monitoring, we must be willing to examine and change all of the
        institutions that allow this synchrony to occur.
      
    
  

  
    
      
        
        The term “parasite” derives from a classical Greek word that was used to refer to “a
        guest who comes to dinner and doesn't leave” or “a class of priests who had meals at public
        expense” (Clitodemus, in 
        Athenaeus Grammaticus , ca. 378 B.C.). In modern usage, it
        refers to eukaryotic organisms that range from single-cell protozoa to complex
        multicellular worms. The diseases caused by these organisms represent some of the world's
        greatest health problems.
        Malaria, for example, currently affects about 500 million people and causes about 3,000
        deaths a day—mostly in sub-Saharan Africa [1]. Schistosomiasis, caused by 
        Schistosoma spp. (blood flukes), affects over 250 million people
        in the tropical world [2], and a recent meta-analysis showed that the disease is
        significantly associated with anemia, chronic pain, diarrhea, exercise intolerance, and
        undernutrition [3]. Almost a billion people are infected with the nematode parasites: 
        Ascaris (roundworm), 
        Ancylostoma (hookworm), and 
        Trichuris (whipworm) [2]. Why then have so few effective drugs
        been produced against these diseases?
        The answer lies primarily in the fact that these are diseases affecting people who are
        poor and living in poor regions of the world. They represent little or no viable market for
        the pharmaceutical industry, especially given the market requirements of the recently
        merged pharmaceutical giants. Given this reality, how can imaginative and effective
        strategies be developed to meet this challenge?
      
      
        New Strategies for Antiparasitic Drug Design
        At last year's meeting of the American Society of Tropical Medicine and Hygiene, the
        organization that hosts one of the largest international gatherings of basic scientists and
        tropical medicine specialists, several symposia highlighted efforts in antiparasitic drug
        design. What was five years ago a fairly dark vision of the future, now appears
        brighter.
        Several nonprofit organizations are now operating that are dedicated to this unmet
        medical need, some collaborative interest exists in industry, and philanthropies have
        backed new initiatives. There are also a number of new academic consortia with novel
        strategies to address issues of target discovery and preclinical development.
        
        Strategy one: Developing a drug that has both a commercial market in the west
        and an application against a neglected parasitic disease. This strategy is exemplified
        by the international consortium of researchers organized by Dr. Richard Tidwell of the
        University of North Carolina. With funding from the Bill and Melinda Gates Foundation, more
        than a dozen faculty and scientists from six research institutions (see Box 1) are working
        in collaboration with Immtech International to manage the “cross-over” development of
        antifungals produced by Immtech as potential drugs for African trypanosomiasis, also known
        as African sleeping sickness. This approach takes advantage of drug development of a class
        of compounds targeting a viable commercial market for Immtech (fungal diseases of the
        developed world). At the same time, academic- and nonprofit-institute scientists are
        targeting these compounds in parallel toward an “unprofitable” major health problem—African
        sleeping sickness. Specifically, an analog of the antifungal pentamidine has been optimized
        as a trypanocidal agent and is in clinical trials in Africa.
        
        Strategy two: An academic consortium gets federal support for pharmacokinetic
        and toxicology studies. This second strategy is exemplified by work under the
        leadership of Dr. Donald Krogstad and colleagues at Tulane University. Here, without an
        industrial partner, Krogstad and colleagues are using in-house chemistry and computational
        assistance to modify chloroquine analogs in an effort to overcome the problem of
        chloroquine resistance in malaria treatment. Their main partner in this effort is the
        National Institute of Allergy and Infectious Diseases, which has provided support for
        pharmacokinetic and toxicology studies by virtue of pre-existing contracts with SRI
        International (an independent nonprofit research-and-development organization) in Menlo
        Park, California, United States. At least one compound from this series is entering
        clinical trials.
        
        Strategy three: An academic center uses a philanthropic gift to woo expertise
        from industry and build infrastructure for preclinical drug development. This third
        strategy is represented by the center that I direct, the Sandler Center for Basic Research
        in Parasitic Diseases at the University of California, San Francisco (UCSF). Here,
        philanthropic support was used to build infrastructure for a consortium of laboratories
        that mimics what might be found in a small- to medium-sized pharmaceutical company
        organized to carry out preclinical drug discovery and development. These core laboratories
        include computational support for drug discovery and chemical library selection, X-ray
        crystallography, drug metabolism and toxicology facilities, animal models of parasitic
        diseases, high-throughput screening, and synthetic chemistry.
        Initial work at this center focused on a drug lead for Chagas disease and was supported
        by a Tropical Disease Research Unit Program project grant from the National Institute of
        Allergy and Infectious Diseases. This compound, now a drug candidate, was originally
        synthesized by Dr. Jim Palmer of Khepri Pharmaceuticals (now Celera) as part of an industry
        search for anti-inflammatory and anticancer drugs focusing on cysteine proteases.
        Validation of this compound as an antiparasitic was carried out by the UCSF team through
        rodent models of disease and, next, supported, as in the case of the Tulane consortium, by
        Dr. Chuck Litterst, Director of the National Institute of Allergy and Infectious Diseases
        Drug Development and Surveillance Group. The UCSF group initially handed over further
        development of a drug candidate to the Institute for OneWorld Health (iOWH), one of the
        nonprofit organizations that has sprung up to meet “downstream” drug development needs for
        neglected diseases (see Sidebar). iOWH facilitated tests of drug safety, minimum effective
        dose, and a formulation for large-scale Good Manufacturing Practice manufacture. The Drugs
        for Neglected Diseases Initiative (www.dndi.org) has now become the main partner for
        further development [4]. The Sandler Center consortium itself is now focusing on completing
        preclinical development of similar drug candidates targeting homologous enzymes in several
        other major global pathogens.
        There is another way in which this third strategy represents a radical departure from
        the usual research paradigm of academic centers. A High-Throughput Screening Center has
        been established by the Sandler Center consortium, building upon the expertise of Drs. Kip
        Guy (now director of the center) and Janice Williams (now manager of the center), who
        brought an industrial perspective to the university. A recently completed screen used an
        assay that allowed robotic screening of a library of thousands of FDA-approved drugs and
        natural products for activity against 
        Trypanosoma brucei , the parasite that causes African sleeping
        sickness. The “hit list” from this screen is being made available on a Web site
        (http://itsa.ucsf.edu/~schisto/fruit.html) for those organizations (such as the Drugs for
        Neglected Diseases Initiative [4], iOWH, and the World Health Organization) whose missions
        include licensing, manufacture, and distribution. Similar screens are being set up for
        leishmaniasis, Chagas disease, schistosomiasis, and malaria.
      
      
        A New Drug Pipeline
        The success of these three unusual strategies, all originating within academic centers,
        provides a new drug pipeline for parasitic diseases with no market value for the
        pharmaceutical industry (Figure 1). The future is likely to see a number of academic or
        academic–industrial collaborations supporting preclinical development in consortia like
        those described above.
        This prediction was already borne out at the April 2005 meeting, “Drug Development for
        Diseases of Protozoa,” sponsored by the Keystone Symposia. In the short time since the
        American Society of Tropical Medicine and Hygiene meeting, it has become clear that more
        academic laboratories are encouraged to pursue drug discovery and development avenues
        beyond research traditionally thought of as academic. New consortia modeled after the three
        described above are now functional or in late planning stages at the University of
        Washington, Johns Hopkins University, Harvard University, and the University of Dundee. In
        addition, significant help from some large pharmaceutical companies, notably the Trés
        Cantos laboratory of GlaxoSmithKline, has fueled focused academic- and
        nonprofit-organization drug development efforts. Three nonprofit organizations (the Drugs
        for Neglected Diseases Initiative, iOWH, and the Medicines for Malaria Venture) are
        actively working to support various stages of the drug-development pipeline. Their efforts
        are particularly key to the “downstream” elements of drug manufacture and clinical
        trials.
        Finally, philanthropic organizations such as the Bill and Melinda Gates Foundation, the
        Sandler Family Supporting Foundation, and the Ellison Foundation have contributed to these
        endeavors. Their continued support, and the recruitment of other interested parties, will
        be crucial to maintaining the momentum engendered by the three projects outlined here.
        
          Editorial Note: PloS has received financial support in the form of a grant
          from the Sandler Family Supporting Foundation.
        
      
    
  

  
    
      
        
        Research into the etiology of schizophrenia has never been as interesting or as
        provocative as in the past three years. There has been progress on several fronts, but
        particularly regarding the molecular genetics of this complex disorder of mind and brain.
        At the same time, a number of critically important and unresolved issues remain that
        qualify the ultimate clinical and scientific validity of the results. However, the recent
        progress in this historically difficult area of inquiry does not seem to be widely
        appreciated. The purpose of this article is to provide a high-level review of progress, its
        limitations, and the implications for clinical research and clinical practice.
        The public health importance of schizophrenia is clear. The median lifetime prevalence
        of schizophrenia is 0.7–0.8% [1], with onset typically ranging from adolescence to early
        adulthood and a course of illness typified by exacerbations, remissions, and substantial
        residual symptoms and functional impairment [2]. Morbidity is substantial, and
        schizophrenia ranks ninth in global burden of illness [3]. In addition, schizophrenia is
        often comorbid with drug dependence (principally alcohol, nicotine, cannabis, and cocaine)
        and important medical conditions (obesity, Type 2 diabetes mellitus) [4]. Mortality due to
        natural and unnatural causes is considerable, and the projected lifespan for individuals
        with schizophrenia is some 15 years less than the general population [5]. The personal,
        familial, and societal costs of schizophrenia are enormous.
      
      
        Etiological Clues
        A substantial body of epidemiological research has established a set of risk factors for
        schizophrenia. A subset of this work is summarized in Figure 1. Of a large set of pre- and
        antenatal risk factors [6], having a first-degree relative with schizophrenia is associated
        with an odds ratio of almost ten. The general impact of some of the risk factors in Figure
        1 remains uncertain, and, additionally, migrant status, urban residence, cannabis use, and
        biological sex are supported as risk factors for schizophrenia. Although the attributable
        risk of some of these risk factors may be greater (e.g., place and season of birth) [7],
        the size of the odds ratio for family history suggests that searching for the familial
        determinants of schizophrenia is rational for etiological research.
      
      
        Unpacking the Family History Risk Factor
        Studies of families, adoptees, and twins have been widely used to attempt to understand
        the relative contributions of genetic and environmental effects upon risk for
        schizophrenia. These “old genetics” approaches use phenotypic resemblance of relatives as
        an indirect means by which to infer the roles of genes and environment. There are many
        important assumptions and methodological issues with these studies [8]; however, genetic
        epidemiological studies of schizophrenia have yielded a remarkably consistent set of
        findings, as summarized in Table 1 [9, 10].
        To summarize this literature briefly, schizophrenia is familial, or “runs” in families.
        Both adoption and twin studies indicate that the familiality of schizophrenia is due mainly
        to genetic effects. Twin studies suggest the relevance of small but significant shared
        environmental influences that are likely prenatal in origin. Thus, schizophrenia is best
        viewed as a complex trait resulting from both genetic and environmental etiological
        influences. These results are only broadly informative, as they provide no information
        about the location of the genes or the identity of the environmental factors that
        predispose or protect against schizophrenia. Searching for genetic influences that mediate
        vulnerability to schizophrenia is rational, given the larger overall effect size and lesser
        error of measurement in comparison to typical assessments of environmental effects. Note
        that high heritability is no guarantee of success in efforts to identify candidate
        genes.
      
      
        Genomewide Linkage Studies of Schizophrenia
        Modern genotyping technologies and statistical analyses have enabled the discovery of
        genetic loci related to the etiology of many complex traits [11], such as Type 2 diabetes
        mellitus, obesity, and Alzheimer's disease. These “discovery science” approaches have been
        applied to schizophrenia, and are summarized in Figure 2. The 27 samples shown here
        included from one to 294 multiplex pedigrees (see Glossary) (median 34) containing 32 to
        669 (median 101) individuals affected with a narrow definition of schizophrenia. There were
        310 to 950 (median 392) genetic markers in the first-stage genome scans.
        “Hard” replication—implication of the same markers, alleles, and haplotypes in the
        majority of samples—is elusive. It is evident from Figure 2 that these studies are
        inconsistent, and no genomic region was implicated in more than four of the 27 samples. The
        Lewis et al. meta-analysis [12] included most of the studies in Figure 2 and found that one
        region on Chromosome 2 was stringently significant and several additional regions neared
        significance. Our focus on first-stage genome scans does not adequately capture the
        evidence supporting replication for certain regions (e.g., 6p) [13–18]. However, there
        appears to be “soft” replication across studies.
        It is unlikely that all of these linkage findings are true. The regions suggested by the
        Lewis et al. meta-analysis implicate more than 3,000 genes (18% of all known genes). For
        the 27 samples in Figure 2, the percentages of all known genes implicated by 0, 1, 2, 3,
        and 4 linkage studies were 42%, 35%, 14%, 6%, and 3%, respectively. This crude summation
        suggests that linkage analysis is an imprecise tool—implausibly large numbers of genes are
        implicated and few genes are consistently identified in more than a small subset of
        studies.
        There are several potential reasons why clear-cut or “hard” replication was not found.
        With respect to the teams that conducted these enormously effortful studies, it is possible
        that no study possessed sufficient statistical power to detect the subtle genetic effects
        suspected for schizophrenia. For example, it would require 4,900 pedigrees to have 80%
        power to detect a locus accounting for 5% of variance in liability to schizophrenia at α =
        0.001. These calculations make highly optimistic assumptions, and less favorable
        assumptions can lead to sample size requirements above 50,000 sibling pairs. For
        comparison, the total number of pedigrees in Figure 2 is less than 2,000.
        In addition, it is possible that etiological heterogeneity (different combinations of
        genetic and environmental causes between samples) and technical differences (different
        ascertainment, assessment, genotyping, and statistical analysis between samples)
        contributed; however, their impact is uncertain, whereas insufficient power is clear. If
        correct, the implication is that Figure 2 contains a mix of true and false positive
        findings.
      
      
        Association Studies of Schizophrenia
        Schizophrenia—like most other complex traits in biomedicine—has had a large number of
        genetic case-control association studies [19]. Although research practice is changing,
        interpretation of many studies is hindered by small sample sizes and a tendency to genotype
        a single genetic marker of the hundreds that might be available in a gene. For example, a
        widely studied functional genetic marker in 
        COMT (rs4680) is probably not associated with schizophrenia [20], but
        nearby genetic markers assessed in a minority of studies may be [21].
        However, as discussed in the next section, a number of methodologically adequate
        association studies of schizophrenia appear to support the role of several candidate genes
        in the etiology of schizophrenia. Similar to the linkage study data, “hard” replication
        remains elusive.
      
      
        Synthesis
        Despite the limitations of the accumulated linkage and association studies, there are
        good suggestions that these studies have identified plausible candidate genes for
        schizophrenia. Table 2 summarizes the evidence in support of a set of possible candidate
        genes for schizophrenia. Reports supporting the role of many of these genes have appeared
        in top-tier international journals known for rigorous peer review. The evidence for several
        genes is encouraging but currently insufficient to declare any a clear-cut cause of
        schizophrenia.
        The accumulated data provide particular support for 
        DISC1 , 
        DTNBP1 , 
        NRG1 , and 
        RGS4 . Each of these genes has received support from multiple lines of
        evidence with imperfect consistency: 1) The case for each of these as a candidate gene for
        schizophrenia is supported by linkage studies; 2) The preponderance of association study
        findings provides further support; 3) mRNA from each gene is expressed in the prefrontal
        cortex as well as in other areas of the brain; and 4) Additional neurobiological data link
        the functions of these genes to biological processes thought to be related to
        schizophrenia. For example, 
        DISC1 modulates neurite outgrowth, there is an extensive literature on
        the involvement of 
        NRG1 in the development of the CNS, and 
        RGS4 may modulate intracellular signaling for many G-protein-coupled
        receptors. Moreover, 
        DTNBP1 and 
        RGS4 have been reported to be differentially expressed in postmortem
        brain samples of individuals with schizophrenia.
        This encouraging summation of work in progress masks a critical issue—the lack or
        consistent replication for the same markers and haplotypes across studies. The literature
        supports the contention that genetic variation in these genes is associated with
        schizophrenia, but it lacks impressive consistency in the precise genetic regions and
        alleles implicated. In contrast, association studies of other complex human genetic
        diseases have produced unambiguous, consistent, and clear-cut (“hard”) replication. For
        example, 1) in Type 1 diabetes mellitus, the bulk of both the linkage and association data
        implicate the 
        HLA region and 
        INS [22]; 2) for Type 2 diabetes mellitus, there are a number of findings
        in the literature where the association evidence appears to be consistent and compelling (
        CAPN10 , 
        KCNJ11 , and 
        PPARG )—the data indicate that the same marker allele is significantly
        associated and has an effect size of similar direction and magnitude [22] (the linkage data
        are less congruent, probably due to power considerations); and 3) for age-related macular
        degeneration, at least five studies show highly significant association with the same 
        CFH Y402H polymorphism [23–27] in a region strongly implicated by
        multiple linkage studies. For these findings, the data are highly compelling and consistent
        and provide a solid foundation for the next generation of studies to investigate the
        mechanisms of the gene–phenotype connection. Power/type 2 error appears to be a major
        factor—if the genetic effect is sufficiently large (
        HLA in Type 1 diabetes mellitus or 
        CFH in age-related macular degeneration)—or, if the sample size is large,
        then there appears to be a greater chance of “hard” replication.
        At present, the data for schizophrenia are confusing, and there are two broad
        possibilities. The first possibility is that the current findings for some of the best
        current genes are true. This implies that the genetics of schizophrenia are different from
        other complex traits in the existence of very high degrees of etiological heterogeneity:
        schizophrenia is hyper-complex, and we need to invoke more complicated genetic models than
        other biomedical disorders. The alternative possibility is that the current findings are
        clouded by Type 1 and Type 2 error. Schizophrenia is similar to other complex traits: it is
        possible that there are kernels of wheat, but it is highly likely that there is a lot of
        chaff. At present, the second and more parsimonious possibility has not been rigorously
        excluded. The impact of Type 1/Type 2 error is likely, and it is not clear why
        schizophrenia should be inherently more complex. At present, we cannot resolve these
        possibilities.
      
      
        Public Health Implications
        The public health importance of schizophrenia is clear, and the rationale for the search
        for genetic causes is strong. Schizophrenia research has never been easy: the current epoch
        of investigation into the genetics of schizophrenia provides a set of tantalizing clues,
        but definitive answers are not yet fully established. Findings from the accumulated
        literature appear to be more than chance yet sufficiently variable as to render “hard”
        replication elusive. The currently murky view of this literature may result from the
        competing filters of Type 1 and Type 2 error. The current literature could be a mix of true
        and false positive findings; however, it would be a momentous advance for the field if even
        one of the genes in Table 2 were a true positive result.
        This body of work is not yet ready for wholesale translation into clinical practice.
        However, it is not premature to inform patients that this work is advancing and that it
        holds promise for new insights into etiology, pathophysiology, and treatment on the five-
        to ten-year horizon. On a larger scale, the treatment of the mentally ill mirrors the
        humanity of a society; in many societies, the return image is not flattering. If a specific
        genetic variation were proven to be causal to schizophrenia, this poor reflection might
        improve [28].
      
    
  

  
    
      
        
        HIV/AIDS has reached alarming proportions in Southeast Asia [1]. The magnitude of the
        epidemic is projected to exceed that of sub-Saharan Africa in the 21st century [2]. More
        than 7 million South Asians are currently infected with HIV [3], nearly 5 million of whom
        are in India [3].
        Nepal has had a comparatively lower prevalence of HIV/AIDS compared to other countries
        in Southeast Asia. Seasonal migration and sexual trafficking across a porous Indian border
        [4], fuelled by a bloody Maoist conflict (see Sidebar), has raised Nepal's HIV prevalence
        to the second highest in the region after India.
        In this essay, we characterize the current HIV epidemic in Nepal, look at the ways in
        which the conflict may be fuelling the infection rate, and discuss the current local and
        international response from the health and development community. We argue that there is
        currently a short window of opportunity to take action to control the HIV epidemic in
        Nepal—and that inaction will lead to HIV becoming the biggest killer of the young and
        middle-aged in the next decade.
      
      
        The Current HIV Epidemic in Nepal
        The first case of AIDS was reported in Nepal in 1998 [5]. Most cases of HIV infection in
        Nepal are HIV-1, although HIV-2 was also recently reported [6]. As of February 2005, the
        National Center for AIDS and STD Control in Nepal reported that there were 4,755
        HIV-positive people and 856 confirmed cases of AIDS in Nepal [7]. However, because of the
        poor surveillance systems and the lack of access to quality voluntary counselling and
        testing services coupled with antiretroviral treatment, these prevalence figures are likely
        to be a gross under-estimate [2].
        A dramatically higher estimate comes from UNAIDS, which estimated that 62,000 out of a
        population of 24.1 million in Nepal were living with HIV/AIDS in 2003 [8]. One in 200
        (0.5%) people aged 15–49 years are living with HIV/AIDS in Nepal [8]. About 30% of those
        infected are female [8]. WHO/UNAIDS estimates that 940 children are living with HIV and
        that nearly 13,000 children were orphaned in Nepal due to AIDS at the end of 2003 [5]. The
        prevalence in the general population may still be low, but it masks an increasing
        prevalence in several risk groups—the prevalence of HIV/AIDS consistently exceeds 5% in
        injecting drug users, commercial sex workers, and migrant workers [5].
      
      
        High-Risk Groups
        Among injecting drug users—estimated to be about 30,000 in Nepal—about 40% are
        HIV-positive [9]. Needle sharing and risky sexual behaviour is common in this group. The
        figures are particularly alarming in Kathmandu, the capital city, where nearly 68% of
        injecting drug users are HIV-positive [10]. A subtype C virus with restricted genetic
        diversity is thought to have caused this epidemic in Kathmandu [11]. Concomitant hepatitis
        C infection is a contributing factor to the rapidity and severity of disease progression in
        injecting drug users, and 94% of users in Kathmandu have tested positive for hepatitis C
        [12].
        The predominant mode of transmission of HIV in Nepal is heterosexual contact with
        commercial sex workers. HIV prevalence rates are about 4% among sex workers in the Terai
        regions of Nepal and about 1.5% in their clients (which is more than five times the
        national average prevalence) [13]. In Kathmandu, nearly 17% of sex workers are HIV-positive
        [8]. There are about 25,000 sex workers in Kathmandu [4] and an estimated 200,000 Nepalese
        women working in Indian brothels. About 5,000 to 10,000 Nepalese sex workers are trafficked
        every year [4], numbers that are likely to increase as a result of the conflict. One
        striking estimate is that nearly 70% of sex workers returning from India are HIV-positive
        [8].
        As in India, a major contributor to the spread of HIV in Nepal has been Nepal's mobile
        population, including truckers and migrant workers. About half of the estimated cases of
        HIV are in 29 highway districts of Nepal [14]. A 1999 survey indicated that 75% of the
        truckers surveyed had had sex with a sex worker and that only 70% of the truckers had used
        a condom during their last sexual encounter [15].
        Men who have sex with men still account for only a small proportion of those affected
        with HIV in Nepal. But the recent detention and subsequent release of 39 male transvestite
        members of the Blue Diamond Society, a local organization that provides sexual health,
        HIV/AIDS, and advocacy services to sexual minorities, highlights the challenges faced by
        sexual minorities in Nepal [16].
      
      
        The Role of Conflict in Fuelling the Epidemic
        Several factors contribute to the propagation of HIV in times of conflict (Box 1) [17].
        While accurate numbers are hard to come by, the recent conflict may have contributed to the
        propagation of HIV/AIDS by fuelling displacement [18]. As the insurgency drags on, seasonal
        and long-term migration of labourers to neighbouring countries, such as India, is becoming
        critical to the economic survival of many households. Young men have left the country “en
        masse” for fear of execution and migrated to the high-prevalence areas in India. UNAIDS
        estimates at least 10% of the 2 million to 3 million Nepalese migrant workers in India are
        HIV-positive [19]. These men are now infecting spouses and others in many parts of the
        country. By pushing rural residents from war-torn areas to the capital, Kathmandu, the
        conflict may have helped spread HIV/AIDS.
        Some 200,000 to 400,000 people may have been displaced in Nepal since the beginning of
        the conflict [20]. Migrations within the country and internationally, coupled with a
        general lack of awareness and knowledge about risk factors, is likely to have contributed
        to the propagation of the epidemic. The far western regions of the country, which are less
        economically developed and which are the hotbed of the current insurgency, are more prone
        to migration. These regions have one of the fastest growth rates of HIV in South Asia.
        Migrants are more likely to practice high-risk sexual behaviour [21]. About 8% of
        migrant male workers returning from Mumbai and examined in western Nepal were infected with
        HIV [22]. The epidemiological impact on women in migrant communities has yet to be
        realized, as only 0.5% of 900 women in the Kailali district in western Nepal were
        HIV-positive, although more than 30% had some form of STD [23].
        Sex traffickers have shifted their trade from Sindhupalchowk and Nuwakot in central
        Nepal to Rukum and Rolpa (the hotbeds of the insurgency) in the mid-west, taking undue
        advantage of the socio-economic conditions borne by conflict and violence. The low
        socio-economic status of women along with the current conflict makes the women in this
        region particularly vulnerable [24]. Save the Children Norway's recent study, 
        Impacts of Armed Conflict Pushing Women and Girls Into Sexual Abuse and Sex
        Trade , revealed that about 19% of female sex workers stated they had entered the sex
        trade directly because of the conflict [25].
        While the number of people infected has risen, HIV prevention and awareness work has
        declined in Nepal as a result of the conflict. Save the Children's work in the Accham
        district of western Nepal has been hindered by fighting between the Maoist rebels and
        government forces since early 2002 [26]. Offices of nongovernmental organizations (NGOs)
        have been burned, and volunteers are afraid to work [26]. Médecins Sans Frontières was
        forced to curtail its activities last year in Jumla, one of the poorest districts in the
        Midwest, due to the conflict. Local activists fear that the situation will continue to
        deteriorate if NGOs are not allowed to work in Jumla. People living with HIV/AIDS in rural
        Nepal are desperate for care and support.
      
      
        Condom Use
        In a study of young men in Nepal in 2001, only 42% of men used a condom consistently
        [27]. The vast majority of young men, about 80%, who had sex with non-regular partners felt
        they were not at risk of contracting any form of sexually transmitted disease or HIV [27].
        The reasons for non-use included fear of losing sexual pleasure, embarrassment over buying
        condoms, and a belief that careful selection of partners offers sufficient protection [27].
        Monitoring of behavioural trends has shown that condom use during the last sexual encounter
        as reported by female sex workers has markedly increased from 63% in 1998 to 90% in 2002
        [28].
        However, consistent use of condoms as reported by female sex workers (54%) still needs
        improvement [28]. Many female sex workers are young children and lack the power to
        negotiate safe sex [29]. Many of the young girls are considered free of disease by clients
        who see no need to use condoms. These girls command a high price as “virgins” for brothel
        owners who do not want to disturb the situation by requiring condom use [29]. Further, the
        weakening of traditional socio-cultural constraints in times of conflict makes the girls
        more prone to sexual abuse and exploitation and more likely to engage in high-risk sexual
        behaviour.
      
      
        The Local and International Response to the Epidemic
        Nepal's National Center for AIDS and STD Control has received support from WHO, UNAIDS,
        UNDP, and USAID. But it lacks substantive funding to complete the necessary studies and
        interventions that are the key to HIV control. National efforts aimed at awareness of HIV
        are complicated by the ethnic diversity of Nepal, where some 75 different ethnic groups
        exist, speaking more than 50 languages.
        A network of about 1,600 NGOs is now working on HIV/AIDS in Nepal. Local organisations
        are doing their best to reach out to those living with HIV/AIDS, but due to the
        urban-centric nature of most funding, funds available for the rural areas are scarce. Most
        donor representatives lack direct knowledge of the rural areas and rely on the instructions
        provided by NGOs in Kathmandu to dispense funds.
        Foreign aid, which accounts for nearly 60% of Nepal's development budget, may have
        paradoxically contributed to lopsided development in Nepal. While aid money has favoured
        urban development and centralized power, the rural–urban gap has widened over the years. In
        Nepal, weak linkages between urban and rural areas, and lack of roads, communications,
        infrastructure, and appropriate skills among the rural poor mean that this urban bias has
        led to centralization of effective power on the one hand and maintenance of the economic,
        social, and political status quo on the other [30]. Urban biases inevitably play a
        deterrent role, discouraging poor patients from seeking help. The poor see very little of
        the aid money, since most of it is used for prevention, information, and awareness in urban
        centres rather than for care and support in rural areas.
        In our experience, some donors consider HIV care and support to be too expensive to
        fund, arguing that Nepal lacks the kind of infrastructure—clinics, district hospitals, and
        distribution units—needed to provide effective antiretroviral treatment, and that
        antiretrovirals are a priority only in countries with a high prevalence of HIV/AIDS (1% or
        more) in the general population [31].
        Harm-reduction interventions have been shown to slow the course of HIV among intravenous
        drug users (IDUs) in many developed countries [32], but in Nepal the concept of harm
        reduction is still new. Few harm-reduction programs are government-supported or integrated
        into mainstream service delivery. Organisations such as the Lifesaving and Lifegiving
        Society, a street-based NGO established in Nepal in 1991, have been providing education,
        counselling, and primary health care—as well as bleach, sterile water, condoms, and new
        needles and syringes—to IDUs to lower their risk of acquiring blood-borne diseases [33].
        The prevalence of HIV infection among IDUs who were in regular contact with the program
        from 1991 to 1994 was low, at 1.6% [33].
        However successful, these programs have not reached the border zones with India where
        HIV infection has risen dramatically among IDUs. This dramatic rise is not surprising since
        research has shown that cross-border drug-use patterns in areas of Nepal bordering India
        are particularly conducive to risky needle sharing [34]. Unlike IDUs in other Nepalese
        towns, very few of the IDUs in border towns belong to stable “injecting groups.” Sharing of
        contaminated injecting equipment in border towns is widespread, in part because of the
        makeshift arrangements in which the cross-border injecting takes place. Users often share
        their small amounts of money to buy drugs. Sexual intercourse with casual partners occurs,
        with inconsistent condom use [34]. Effective intervention would therefore require complex
        cross-border collaborative efforts [34].
        People living with HIV/AIDS are stigmatized and face discrimination at all societal
        levels—in the community, at health facilities, and, most importantly, within the family
        [35]. In a recent survey by CARE-Nepal, almost half of those who came to the voluntary
        counselling and testing centre at the Doti District Hospital in Silgadhi, a
        conflict-affected area, during June–July 2004 tested positive for HIV, and almost all of
        those tested positive were widows in their twenties and thirties [35]. About 60% of them
        were breast-feeding their infants. These young widows faced rejection from their families,
        discrimination at work, and difficulty in coping with their life circumstances [35].
        One important development in recent years has been within the area of condom promotion
        for HIV/AIDS prevention. A national research study found that 76.6% of retail outlets
        surveyed had never sold a condom in 2002 [36]. Population Services International began a
        national condom promotion program in early 2002 [37] using a three-pronged approach: (1) a
        national media campaign promoting condoms (Figure 1); (2) increased widespread condom
        availability within the private sector to destigmatize condom use; and (3) targeted condom
        promotion to high-risk groups such as sex workers [38]. The impact of such efforts on
        condom sales has been dramatic. Total sale of condoms jumped from 11.9 million units in
        2002 to 23.1 million units in 2004, and sales continue to climb [39]. Creating long-term
        behaviour change and making condoms accessible in the private sector and affordable through
        subsidy to high-risk groups such as female sex workers appears to be increasing the uptake
        of condoms in Nepal.
      
      
        A Window of Opportunity
        There is a window of opportunity to combat HIV/AIDS in Nepal. If trends continue, AIDS
        will be the leading cause of death among 15–49-year-olds in the next ten years [8]. 100,000
        to 200,000 young adults could be infected, and 10,000 to 15,000 annual AIDS deaths could
        occur in the next ten years [8]. What steps must be taken to prevent Nepal from being
        devastated by the infection?
        First, we must gather better data on HIV seroprevalence, on sexual behaviour in the
        general population, and on sexual networking [40]. We have argued in this essay that the
        extent of the AIDS epidemic in Nepal will depend upon rates at which sexual partners are
        exchanged by commercial sex workers and the men who regularly visit them, as well as the
        proportion of the general population that has multiple and concurrent sexual partners.
        There are extensive migration patterns both within the country and internationally, fuelled
        by the recent conflict, which provide the potential for considerable sexual networking.
        Better knowledge of this networking is crucial for our HIV control efforts.
        Second, the government must recognize and acknowledge the needs of high-risk groups—drug
        users, commercial sex workers, migrant workers, and men who have sex with men. Plans to
        create more behaviour changes are needed within these groups. The 2002–2006 HIV/AIDS
        strategy proposed by the government [41], which adopts a multisectoral approach focusing on
        prevention among vulnerable groups, on control, care and support, and on voluntary
        counselling, hopes to address some of these issues.
        Third, information campaigns should focus on changing attitudes that create barriers to
        regular use of condoms. Condoms should be readily available, and research should focus on
        the impact of mass media on perceptions of risk, negative attitudes toward condoms, and
        risky behaviour [27]. A culturally specific approach to HIV prevention is needed that
        includes education of clients and brothel owners about condoms.
        Fourth, the various players involved in addressing HIV and who are working in closely
        adjacent fields need to interact more and partner with each other, and there needs to be
        greater inclusion of civil society in campaigns to raise HIV/AIDS awareness and reduce
        stigma.
        Finally, successful national programs that have the scale to alter the course of the
        epidemic in Nepal should be expanded.
      
      
        Conclusion
        HIV/AIDS is no longer only a health issue, it is also a development issue. Tackling the
        epidemic will require not only prevention and control of HIV infection among vulnerable and
        risk groups, but a multi-sectoral approach addressing the lack of access by risk groups to
        health care and education and recognition of the populations at risk [42]. People living
        with HIV and AIDS should be brought to the forefront in the fight against HIV/AIDS [35].
        Family members, local communities, civil society organizations, donors, and the government
        all have their own important role to play [35]. The status of women must change so that
        they are considered autonomous individuals who can make their own decisions [29].
        As with all international declarations on HIV/AIDS, there is an absolute need to take a
        strong human rights approach to combating the epidemic. This approach includes recognizing
        fundamental rights such as access to health care and information, addressing gender equity,
        and a concerted effort to reduce sex trafficking. It further requires addressing the root
        causes of poverty and inequality, which give rise to the phenomenon of migration and
        trafficking as well as propagate violent uprisings. Only such efforts will prevail in
        mitigating the effects of both HIV and conflict in Nepal.
      
    
  

  
    
      
        
        Richard Smith's key suggestion [1] is that medical journals “should stop publishing
        trials” and concentrate on “critically evaluating them.” This bold and radical suggestion
        deserves wide debate. It's obvious that many medical journals are losing relevance as
        vehicles for scientific information, but it's unclear what will save them. Even as journals
        strive to better enforce their conflicts-of-interest disclosure rules, drug companies will
        strive to find or create other publication outlets that can communicate to physicians
        precisely what advertisers wish to communicate. In sum, an unanticipated effect of purging
        clinical trial reports from medical journals might be an even larger proliferation of frank
        advertising outlets and messages that might more effectively catch doctors' attentions.
      
    
  

  
    
      
        
        Evidence that calorie restriction (CR) retards aging and extends median and maximal life
        span was first described in the 1930s by McCay et al. [1]. Since then, similar observations
        have been made in a variety of species including rodents, fish, fruit flies, worms, and
        yeast [2]—and although they are not yet definitive, results from ongoing longevity studies
        in monkeys suggest that CR will also extend life span in longer-lived species [3,4].
        There are many theories explaining the mechanisms by which CR extends life span. An
        early hypothesis was that delayed sexual maturation might be a mechanism. However, it has
        since been shown that CR initiated in older animals also increases life span [5]. Reduced
        metabolic rate—with consequent reduction in free radical production—was another early,
        leading hypothesis to explain the anti-aging effects of CR. But there are many other
        metabolic effects that have been associated with CR, including altered insulin sensitivity
        and signaling, stress resistance, altered neuroendocrine function, and changes in nutrient
        signaling. Any, or a combination, of these biological changes may retard aging. However,
        recent studies seem to favor a highly conserved stress response that evolved early in most
        species to increase an organism's chance of surviving adversity (such as CR) by triggering
        concerted physiological responses [6].
      
      
        Nutrient Composition of Calorie Restricted Diets
        Previous studies in rodents have shown that the effects of CR on extending life span are
        dose dependent, with a 20% reduction in calorie intake producing a smaller increment in
        life span as compared to a 40% reduction in food intake [7]. From this work and others, the
        reduction in calories was recognized to be of paramount importance in the longevity
        response, and alterations in the nutrient content of diets were considered irrelevant.
        However, a recent study in 
        PLoS Biology by Mair et al. challenges this long-held concept of CR
        [8].
      
      
        From Fruit Flies to Rodents
        In the study by Mair et al., the authors examined life span in fruit flies (
        Drosophila melanogaster ) fed one of four different diets: (1) a
        combination of yeast and sugar (control), (2) restricted in yeast only, (3) restricted in
        sugar only, and (4) restricted in yeast and sugar. The authors observed that in the
        restricted sugar group, as compared to the controls, maximal life span was unchanged and
        median life span was increased by only 12%. On the other hand, both maximal and median life
        spans were increased substantially in the restricted yeast group and in the restricted
        yeast and sugar group (Figure 1). Importantly, the authors claim that total calorie
        contents of the restricted sugar and restricted yeast diets were similar. Thus, from this
        study it can be implied that restricting carbohydrate is less advantageous than restricting
        protein/lipid for mediating the effects of dietary restriction (DR) on life span.
        It is of concern that the authors did not directly measure the flies' total food intake
        but only estimated intake by examining their feeding behavior. This method may not take
        into account possible differences in the rate of food uptake of restricted flies, which
        could affect the results.
        
          Investigators have undertaken studies of prolonged calorie restriction in humans
        
        Only a handful of studies have investigated the role of altering nutrient composition on
        longevity in rodents. The results from these studies have been contradictory, with some
        studies showing no life-span extension following restriction of fat only [9] and others
        showing increased life span following replacement of casein-protein for soy-protein [10].
        However, the concept that it is not just reduced calorie intake that drives the life-span
        extension effect is not new, and the timing of food intake has also been proposed to be of
        importance. For example, when rodents are fed every other day, improvements in biomarkers
        of aging and increased life span are observed, even though measured calorie intake and body
        weight were not statistically different from ad-libitum or pair-fed animals [11,12]. These
        responses were dependent on genotype and the age at which the protocol was implemented.
        Furthermore, animals fed every other day had a better response to neurotoxic stressors as
        compared to animals maintained on prolonged CR.
        The mechanisms behind the differences in restricting carbohydrate vs. protein/lipid on
        life-span extension were not examined in the Mair et al. study [8]. Such mechanisms
        obviously imply the existence of molecular systems in cells that sense
        macronutrients—systems that may respond not only to nutrient availability but also to the
        hormonal response elicited by these dietary nutrients [13]. For example, restricting
        dietary carbohydrates increases the plasma concentration of B-hydroxybutyrate (that is,
        ketogenesis), a shift that may counteract life-span extension in mammals. However, rat
        models of Alzheimer and Parkinson diseases fed a ketogenic diet exhibit increased
        resistance to seizures and have increased protection of neurons [14]. Ketogenic diets are
        also prescribed to patients with epilepsy, and although there have been no randomized
        controlled trials, large observational studies (some prospective) suggest that this diet
        does have a beneficial effect on seizures [15].
        However, it is likely that carbohydrate and protein may differentially alter
        nutrient-sensing pathways such as Sir2 and mammalian target of rapomyocin (mTOR), which are
        gaining acceptance as mediators of the life-span extension effects of CR [16]. Sir2 (the
        mammalian homolog is SIRT1) is a nicotinamide-adenine-dinucleotide-dependent histone
        deacetylase that interacts with numerous transcription factors to silence gene
        transcription. Sir2 is upregulated by CR and is required for life-span extension effects of
        CR in 
        Caenorhabditis elegans (reviewed in [6]). mTOR is a
        serine/threonine kinase that is activated by insulin, nutrients, and growth factors and is
        a central regulator of ribosome biogenesis, protein synthesis, and cell growth. Inhibition
        of mTOR increases life span in 
        Drosophila and 
        C. elegans (reviewed in [16]).
      
      
        Are There Implications for Human Life Span?
        Obviously, invertebrate organisms cannot serve as reliable models for human longevity,
        and the results by Mair et al. [8] should not be extrapolated to mammals in general. But if
        this result could be replicated in humans, then the prospect of DR to increase life span
        would be eminently more attractive than overall CR. This would mean that a change in food
        patterns could have a similar effect to the dramatic reduction of total food intake.
        However, the life-span extension effects of CR have not been proven in humans, and the jury
        is still out on whether nutrient composition will even affect life span in non-human
        primates.
        In close collaboration with the National Institute on Aging, investigators in Baton
        Rouge, Boston, and Saint Louis (all in the United States) have undertaken studies of
        prolonged CR in humans. These studies aim to test the feasibility and safety of different
        types of calorie restricted diets in non-obese people and to determine the effects of CR on
        risk factors for age-related diseases, psychological factors, immune function, oxidative
        stress, and molecular pathways identified in lower species [17]. These kinds of studies
        will further help identify the mechanisms underpinning the effect of CR or DR on
        longevity.
      
    
  

  
    
      
        
        Like SARS, Ebola, and other emerging infectious diseases, antibiotic resistance in
        bacteria may have a zoonotic origin [1]. Evidence suggests that antibiotic use in
        agriculture has contributed to antibiotic resistance in the pathogenic bacteria of humans,
        but the chain from cause to effect is long and complicated.
        Antibiotic use clearly selects for antibiotic resistance, but how far do these effects
        extend beyond the population where antibiotics are used? Antibiotics and
        antibiotic-resistant bacteria (ARB) are found in the air and soil around farms, in surface
        and ground water, in wild animal populations, and on retail meat and poultry [2–9]. ARB are
        carried into the kitchen on contaminated meat and poultry, where other foods are
        cross-contaminated because of common unsafe handling practices [10,11]. Following
        ingestion, bacteria occasionally survive the formidable but imperfect gastric barrier, and
        colonize the gut [12].
        Patterns of colonization (asymptomatic carriage) and infection (symptomatic carriage) in
        human populations provide additional evidence that ARB occasionally move from animals to
        humans [13,14]. The strongest evidence comes from the history of the use of antibiotics for
        growth promotion in Europe. After first Denmark and then the European Union banned the use
        of antibiotics for growth promotion, prevalence of resistant bacteria declined in farm
        animals, in retail meat and poultry, and within the general human population [8,15].
        Despite the evidence linking bacterial antibiotic resistance on farms to resistance in
        humans, the impact of agricultural antibiotic use remains controversial [16–19] and poorly
        quantified. This is partly because of the complex of population-level processes underlying
        the between-species (“heterospecific”) and within-species, host-to-host (“horizontal”)
        spread of ARB. To emerge as human pathogens, new strains of ARB must (1) evolve,
        originating from mutations or gene transfer; (2) spread, usually horizontally among humans
        or animals, but occasionally heterospecifically; and (3) cause disease.
        All three of these steps are complex and imperfectly understood. The emergence of a new
        type of resistance is a highly random event, which can't be predicted accurately, and may
        involve multiple steps that preclude perfect understanding even after the fact. Spread is
        equally complicated and may obscure the origins of resistance. In some cases, emergence of
        resistance in one bacterial species is a consequence of the emergence and spread in another
        species, followed by the transfer of resistance genes from one bacterial species to
        another. Because of the underlying complexity, mathematical models are necessary to develop
        theory—a qualitative understanding of the underlying epidemiological processes [20–25].
        Theory helps researchers organize facts, identify missing information, design surveillance,
        and analyze data [26].
      
      
        Horizontal Transmission
        Theory clearly shows that the impact of agricultural antibiotic use depends on whether
        resistant bacteria have high, low, or intermediate horizontal transmission rates in human
        populations [23,24]. The rate of horizontal transmission among humans is determined by the
        underlying biology of the pathogen, medical antibiotic use, and hospital infection control,
        but not by agricultural antibiotic use [22]. On the other hand, a farm where multiple
        antibiotics are used routinely, universally, and in low quantities for growth promotion is
        likely to be an excellent environment for the evolution of multiple resistance factors,
        including some variants that might never have evolved in humans. Thus, even very rare
        transmission resulting from agricultural antibiotics may have a medical impact by
        introducing new resistant variants to the human population. The epidemiology of spread in
        the human population dictates how the impact of agricultural antibiotic use should be
        assessed.
        Zoonotic pathogens, such as 
        Campylobacter and 
        Salmonella , are generally regarded as having low horizontal
        transmission rates in human populations. While resistance in zoonotic infections should be
        directly attributable to resistance in the zoonotic reservoir, the impact of agricultural
        antibiotic use remains controversial [18,27–29]. Zoonotic species could acquire resistance
        genes from human commensal bacteria during the infection process, but this hypothesis is
        difficult to test.
        For pathogens with high horizontal transmission rates, resistant bacteria will spread
        rapidly once they have emerged, and prevalence will be maintained at a steady state by
        horizontal transmission. Thus, the impact of subsequent heterospecific transmission is
        limited (Figure 1). Nevertheless, one or two heterospecific transmission events could be
        sufficient to cause the appearance of a highly successful ARB genotype in humans, affecting
        the timing, nature, and extent of spread within the human population [22]. Not only are
        such events difficult to trace, but their impact is impossible to measure, since there is
        no way to know what type of resistance would have appeared and with what temporal pattern,
        if transfers from animals had been prevented.
        The case where horizontal human transmission rates are intermediate is particularly
        interesting. If each case in a population generates approximately one new case (a situation
        we call “quasi-epidemic” transmission), each instance of heterospecific transmission will
        initiate a long chain of horizontal transmission that eventually burns out. Quasi-epidemic
        transmission can amplify a relatively low amount of heterospecific transmission and
        substantially increase prevalence [23–25]. The effect is sustained as long as
        heterospecific transmission continues. A corollary is that banning agricultural antibiotic
        use would have maximal benefits if horizontal transmission is quasi-epidemic [24].
        Moreover, the effects are most difficult to estimate because both heterospecific and
        horizontal transmission must be accounted for.
        These principles apply to bacteria associated with outpatient antibiotic use and
        community-acquired infections as well as those that are primarily hospital-acquired.
        Although quasi-epidemic transmission would seem to be a special case, it may in fact be the
        rule for many hospital-acquired bacteria because it is the natural endpoint of the
        interplay between economics and ecology [30]. By spending money on hospital infection
        control, hospital administrators can reduce nosocomial transmission rates for resistant
        bacteria. For example, hospitals may screen and isolate patients who are likely to be
        carriers (i.e., active surveillance) and implement infection-control measures, but this
        comes at the cost of isolating patients [31]. Total costs are minimized by spending just
        enough to eliminate (or nearly eliminate) the pathogen; thus, quasi-epidemic transmission
        is the economic optimum [30].
      
      
        The Community as a Reservoir for Resistance
        Horizontal transmission is further complicated by population structure, such as the
        movement of humans through hospitals and long-term care facilities. Medical antibiotic use
        and horizontal transmission rates are high in hospitals, but this is counterbalanced by
        short hospital stays. An emerging view for hospital-acquired bacterial infections is that
        persistent asymptomatic carriage plays a key role in the epidemic of resistance. ARB can
        asymptomatically colonize a person for years: even if the number of other people infected
        during a single hospital visit is less than one, this number will exceed one when summed
        over several hospital visits [25,32,33]. Thus, the ecological reservoir of resistance in
        the community plays an important role in the increasing frequency of resistance in
        hospital-acquired infections.
        Short hospital visits and long persistence times of ARB in people guarantee that some of
        the costs associated with failed infection control are passed on to other hospitals—new
        carriers are frequently discharged from one hospital only to be admitted to another
        hospital later [30]. Thus, the harm done by these ARB is borne by the whole human
        population, particularly all of the health-care institutions that serve a single catchment
        population. In economic terms, the damage caused by the carriage of ARB is a kind of
        pollution.
        By comparing the total number of new carriers generated in the community, the impacts of
        agricultural antibiotic use on hospitals can be compared directly to the impact hospitals
        have on each other (Figure 2). The rate of heterospecific transmission is intrinsically
        difficult to measure directly because the risk of exposure and colonization per meal is
        very small. Nevertheless, agricultural antibiotic use may generate as many carriers as
        hospitals for the simple reason that the population experiences many more meals than
        hospital discharges [34]. When agricultural and nosocomial transmission are equally rare in
        the population, the latter will be much easier to identify and quantify.
      
      
        A Natural Experiment: Glycopeptides and Vancomycin-Resistant Enterococci
        Is the impact of agricultural antibiotic use on the emergence and spread of ARB in
        humans large or small relative to medical antibiotic use? Put another way, are farms or
        hospitals bigger polluters? A large-scale natural experiment was conducted in the United
        States and several European countries when each adopted different policies on glycopeptide
        use in animals (avoparcin) and humans (vancomycin) [16,17,35–37]. Many European countries
        approved avoparcin for animal growth promotion in the 1970s, but the US did not.
        In the early 1980s, demand for vancomycin in US hospitals surged because of increasing
        aminoglycoside resistance among enterococci and methicillin resistance in 
        Staphylococcus aureus . Physicians in US hospitals also used
        oral vancomycin for some 
        Clostridium difficile infections [37–39]. In the late 1980s and
        early 1990s, vancomycin-resistant enterococci (VRE) emerged and spread through US
        health-care systems. In Europe, hospitals used less vancomycin because most enterococci
        were sensitive to aminoglycosides, and oral vancomycin was seldom used. VRE still emerged
        and spread through European hospitals, but the problem has been less severe than in the US
        [40].
        A different pattern emerges for community prevalence of VRE. VRE are rarely found
        outside of hospitals in the US, except for patients who have a prior history of
        hospitalization. Community prevalence of VRE in the US is typically less than 1%. In
        contrast, community prevalence of VRE was estimated at 2%–12% in Europe during the late
        1990s, including carriage by people with no history of hospitalization [17,41–48]. In other
        words, the European community reservoir generated by vancomycin use in hospitals and
        avoparcin use in agriculture was apparently much larger than the US community reservoir
        generated only by vancomycin use in hospitals.
        The prevalence of VRE in the community declined after the EU banned avoparcin [15].
        Thus, avoparcin is at least partly responsible for the reservoir of VRE in the European
        community, but how much of that reservoir came from avoparcin and how much came from
        hospitals? To weigh the impact, we subtract the community prevalence of VRE in the US
        (<1%) from the community prevalence of VRE in Europe (>2%). The remainder (>1%) is
        attributed to avoparcin. This analysis probably underestimates the real impact because
        vancomycin was used less in European than in US hospitals. Thus, avoparcin use in Europe
        would appear to be responsible for generating a larger reservoir of VRE in the community
        than US hospitals. Put another way, the impact of avoparcin use on European hospitals was
        larger than the impact of US hospitals on one another.
      
      
        Conclusion
        Despite the evidence that avoparcin use has had a large impact on the emergence and
        spread of VRE by increasing the reservoir of VRE in the EU, some uncertainty continues to
        surround the clinical significance of VRE strains of animal origin and of the zoonotic
        origins of resistance in general. Bacterial strains circulating in hospitalized populations
        may be genetically distinct from those circulating in the general human population
        [13,17,49]. Thus, bacterial populations are some combination of zoonotic, quasi-epidemic,
        and epidemic strains. The complexity of bacterial population biology and genetics makes it
        practically impossible to trace bacteria (or resistance factors) from the farm to the
        hospital, or to directly attribute some fraction of new infections to agricultural
        antibiotic use. Asymptomatic carriage of resistance factors by nonfocal commensal bacteria
        adds to a general risk of resistance, but transfer of resistance among bacterial species is
        unpredictable and difficult to quantify. Until more evidence is available, it is prudent
        and reasonable to consider bacteria with resistance genes a general threat [50–52].
        Some part of the controversy over agricultural antibiotic use has been a disagreement
        about how to weigh evidence and make decisions when the underlying biological processes are
        complex. In this case, the effects of agricultural antibiotic use on human health remain
        uncertain, despite extensive investigation, and the effects may be unknowable, unprovable,
        or immeasurable by the empirical standards of experimental biology. What should be done
        when complexity makes an important public-health effect intrinsically difficult to measure?
        What is an appropriate “null hypothesis” or its equivalent? Should the same standards of
        proof be used in science and science-based policy? Where should the burden of proof
        fall?
        Scientific assessments for policy should summarize the best state of the science,
        recognizing that the burdens and standards of proof are necessarily softer because of the
        uncertainty that is introduced by biological complexity. The best decisions weigh the
        evidence in light of the inherent uncertainty. The EU banned the use of antibiotics for
        growth promotion, based on the precautionary principle. The use of the precautionary
        principle was criticized by some as unscientific in this context. In fact, the intrinsic
        problem of knowability, posed by the biological complexity of the problem, makes the use of
        precautionary decision making particularly suitable in this arena. The assumption that
        plausible dangers are negligible, even when it is known that such dangers are
        constitutively very difficult to measure, may be more unscientific than the use of
        precaution.
      
    
  

  
    
      
        
        Twins' research is a favorite tool of the human geneticist, but it has a controversial
        history. Nazi doctor Josef Mengele, infamous for his work at Auschwitz, was fascinated by
        twins. He sought them out at the extermination camp and used them in violent experiments.
        Later, British psychologist Cyril Burt worked on the heredity of intelligence, producing
        findings that some suspected were “too good”—and which later were shown to be based on
        fraudulent data involving invented twins.
        In this month's 
        PLoS Medicine , Nancy Krieger and colleagues examine the health of female
        twins from a very different perspective. To understand the impact of lifetime socioeconomic
        position on health, they studied twins who were raised together but who had different
        socioeconomic position after adolescence. Many studies have compared the health of twins
        raised separately since birth or early childhood, but few have investigated the adult
        health of twins raised together but who had different post-adolescent socioeconomic
        position. Such a study could clarify the impact of adult experiences on adult health in a
        population matched on early life experiences, the authors say.
        Krieger's team employed data from a cohort of 434 twins who lived in the San Francisco
        Bay area in 1978–1979; the average age at recruitment was 41 years old. The cohort was
        given a health and sociodemographic questionnaire. Data on anthropometric and biological
        characteristics were obtained by physical examination and laboratory analyses; the process
        was repeated at a second examination around 1989–1990. At the second phase of the study, 72
        women (8.3%) did not return, of whom 36 had died, which left 352 twin pairs (58%
        monozygotic, 42% dizygotic), representing 81.1% of the original cohort.
        The sociodemographic and health characteristics of the full cohort (352 pairs) and the
        cohort analyzed (those pairs where it was known both that they had lived together until at
        least age 14 and their joint socioeconomic was available—i.e., 308 pairs) were quite
        similar: 40% grew up in working-class households and 80% in households where the father had
        fewer than four years of college education.
        At the second examination, 32% of the twin pairs in the analytic cohort had a difference
        in their adult household occupational class, and 20% had different levels of college
        education. The team found that health outcomes among monozygotic adult female twins who
        lived together through childhood varied by their subsequent socioeconomic position. Twins
        with differing occupational class differed in health status compared with twins with
        similar occupational class. The working-class twin had significantly higher systolic blood
        pressure, diastolic blood pressure, and LDL cholesterol than her professional,
        non-working-class twin. Twins discordant on educational level, however, had similar health
        status, likely reflecting the fact that current occupational class is a better measure for
        investigating the impact of lifetime socioeconomic position than is education (which is
        completed usually in early adulthood). Dizgyotic twins with differing adult socioeconomic
        position, either occupational class or educational level, did not notably differ in their
        adult health status.
        These novel findings lend support to the hypothesis that health is shaped not only by
        early life experiences but also by cumulative experiences across the lifecourse, the
        authors say.
        As with many twin studies, the numbers of individuals studied is relatively limited. The
        lack of some biological and personal data such as detailed occupational class position over
        time, lack of data on income, poverty, wealth, debt, gestational age, birthweight, and
        birth order also limits the conclusions that can be drawn. These limitations are countered,
        however, by the tight matching of the twins on early life and childhood exposures, as well
        as by the matching for genetic endowment among the monozygotic twins. Together, the
        findings have important implications for health policy, since they suggest that adult
        socioeconomic position does have an impact on adult health above and beyond early life
        exposures, and they also add to our understanding of how societal conditions shape
        population patterns of health, disease, and well-being.
      
    
  

  
    
      
        
        There are at least 300 million acute cases of malaria each year globally, resulting in
        more than a million deaths. Ninety percent of deaths due to malaria occur in Africa, south
        of the Sahara, mostly in young children. The number of deaths is increasing, and one key
        factor linked to this has been widespread drug resistance of 
        Plasmodium falciparum to conventional antimalarials, such as
        sulfadoxine-pyrimethamine (SP); such resistance is widespread in southeast Asia, South
        America, and Africa. The inappropriate use of antimalarials during the past century has
        contributed to this increase in resistance. For example, there has been overreliance on
        quinolines (such as chloroquine) and antifolates (such as pyrimethamine) resulting in
        cross-resistance among these drug classes. However, in the past decade, a new group of
        antimalarials—the artemisinin compounds, such as artesunate, artemether, and
        dihydroartemisinin—have been deployed on an increasingly large scale.
        These compounds produce a very rapid therapeutic response, are active against parasites
        resistant to multiple drugs, are well tolerated, and reduce gametocyte carriage. To date,
        no parasite resistance to these compounds has been detected.
        If used alone, the artemisinins will cure falciparum malaria in seven days, but studies
        in southeast Asia have shown that combinations of artemisinin compounds with certain
        synthetic drugs produce high cure rates after just three days of treatment. There is also
        some evidence that combinations of therapies could greatly retard development of resistance
        to the partner drug. Although combinations including artemisinins have been widely
        advocated, they are expensive and relatively untested in highly endemic areas.
        In this month's 
        PLoS Medicine , Adoke Yeka and colleagues compared artemisinin-based
        compounds and other combination therapies in four districts with varying transmission
        intensity in Uganda in 2,160 patients aged six months or greater with uncomplicated
        falciparum malaria. The team tested the combination of chloroquine and SP, currently the
        first-line therapy in Uganda, the combination of amodiaquine and SP, a cheap regimen proven
        to be efficacious in previous trials, and the combination of amodiaquine and
        artesunate.
        During the 28-day study they collected data on the efficacy of the different regimens
        and examined the effect on recrudescence and new infections after therapy. Combined
        amodiaquine and artesunate was the most efficacious regimen for preventing recrudescence,
        but this benefit was outweighed by an increased risk of new infection. This result was
        probably due to artesunate being rapidly eliminated, leaving only amodiaquine to provide
        post-treatment prophylaxis. Considering all recurrent infections, the combination of
        amodiaquine and SP was at least as efficacious as the other combinations at all sites and
        superior at the highest transmission sites.
        In all, 72% of all recurrent infections were due to new infections, and with the two
        most efficacious regimens (amodiaquine and SP, and amodiaquine and artesunate) this
        proportion was 80%. The identification of new infections stressed the need for other
        malaria control measures, such as bed nets, said the authors.
        They also suggested that antimalarials should be judged not just on their impact on
        recrudescence but also on their impact on the risk of new infections after therapy.
        Previous studies have suggested that patients who suffer recrudescence have a higher risk
        of complicated malaria and death. Artemisinins are highly attractive antimalarials, but
        when used as monotherapy, they have a high risk of recrudescence and hence must be combined
        with other antimalarials to achieve maximum efficacy. But whether the partner drug should
        be long or short acting remains unclear, said the authors.
        Altogether, artemisinin combinations offer great hope for Africa, the authors say,
        although the ideal combination regimen remains uncertain and cost is a problem. To compare
        the efficacy of the different therapies, bigger and longer controlled trials are needed in
        conditions of varied transmission intensity. Nevertheless, based on the results of this
        study and others, Uganda has chosen a combination of artemether and lumefantrine as its
        first-line therapy against malaria.
      
    
  

  
    
      
        
        Hepatitis B is a serious global public health problem but is preventable with safe and
        effective vaccines that have been available since 1982. Despite these vaccines, about 2
        billion people have been infected with hepatitis B virus (HBV), and more than 350 million
        have lifelong infections. These chronically infected people are at high risk of death from
        cirrhosis of the liver and liver cancer, which both kill about 1 million people each
        year.
        Suppression of viral replication in chronic carriers of HBV is an effective approach to
        controlling disease progression. Current antiviral therapies include lamivudine and
        alpha-interferon, but long-term resolution of the disease is disappointing because of low
        seroconversion rates and the development of drug-resistant viral mutants.
        In this month's 
        PLoS Medicine , Lisa F. P. Ng and colleagues describe the identification
        of a host factor that has a significant effect on viral replication efficiency. The team
        began by examining the serum viral load of a group of carriers of hepatitis B in relation
        to the HBV genome carried. They found a significant association between high serum viral
        load and a natural sequence variant within the HBV enhancer II regulatory region at
        position 1752. Upon testing all four possible 1752 variants, the 1752A variant had the
        highest transcriptional activity.
        Further investigation of this enhanced transcriptional activity revealed evidence of
        possible interaction with host DNA binding proteins. The team found that a protein present
        in the human host—hnRNPK—could be isolated by direct binding to a viral fragment derived
        from the HBV variant of these infected patients.
        hnRNPK has previously been shown to be involved in several cellular functions—for
        example, as a regulator of signal transduction and of gene expression. On further
        examination of the role of hnRNPK in HBV replication, they established that hnRNPK is
        capable of acting on the full length of HBV, rather than just a partial fragment. They
        compared four full-length replicative HBV clones, identical except for a single base change
        at position 1752, that were transfected with two different hnRNPK expression constructs and
        showed that 1752A was more efficient at promoting replication than the other three
        variants.
        To further show the role of hnRNPK in HBV replication, the team tested the effect of
        over-expression and down-regulation of the cellular protein. Using siRNA, designed to
        reduce endogenous hnRNPK, they showed suppression of both hnRNPK mRNA and HBV viral load,
        whereas a control siRNA had no effect on HBV viral load.
        Despite these findings, the mechanism behind hnRNPK on HBV replication needs further
        exploration, the authors say, concluding that viral replication efficiency was determined
        by a combination of viral sequence and interaction with specific host proteins. However,
        they suggest that these results indicate that although drug development of antivirals is an
        established research avenue, targeting the host is an untapped opportunity.
        They describe parallels with anti-EGFR antibody treatment of breast cancer cells, which
        produced a decrease in cell replication rate and corresponding reduction in hnRNPK
        expression levels; this result suggested that hnRNPK levels could be modulated by anti-EGFR
        treatment, thus highlighting new treatment options for altering the HBV viral load in
        chronic carriers.
        The authors conclude that the future of long-term viral clearance will require
        combination therapy of targeting the virus directly, blocking host support proteins, and
        using immuno-modulating agents.
      
    
  

  
    
      
        
        Postherpetic neuralgia (PHN) is a chronically painful condition that is a complication
        of shingles (acute herpes zoster), a recurrence of the varicella-zoster virus, which
        initially causes chickenpox. Although shingles usually resolves within a month, some people
        continue to feel the pain of PHN long after the rash and blisters heal, because of nerve
        damage (neuropathic pain) caused by the shingles. Not everyone who has had shingles
        develops PHN, although it is a common complication of shingles in older adults.
        Despite advances in antiviral therapy during acute herpes zoster and the more recent
        introduction of vaccination against varicella zoster, PHN continues to be a significant
        clinical problem, with 10–20% of patients developing persistent neuropathic pain after
        acute herpes zoster reactivation. The nature of PHN pain is variable, which implies that a
        variety of mechanisms might be operating. This variability has led to the hypothesis that
        treatment plans could be optimised for individual patients on the basis of the individual
        pattern of their symptoms or the underlying mechanism of the pain.
        However, the current evidence base for therapies in PHN is based on clinical trials of
        analgesics, which have examined PHN as a single disease entity. Furthermore, there is
        little evidence for the efficacy of drugs for specific sets of symptoms and no simple way
        to determine which pain mechanisms might be operating in an individual patient.
        In this month's 
        PLoS Medicine , Andrew Rice and colleagues reassessed the evidence base
        by doing a systematic review and meta-analysis of analgesic therapy for PHN, which has
        fundamentally changed in the wake of several major new trials. The authors searched the
        literature for trials of PHN and retrieved 62 articles, of which 35 were kept for final
        analysis.
        Their analysis confirmed several previous research findings, although they cautioned
        that the meta-analytic study design of collecting data from a range of trials had several
        inherent pitfalls, and it is difficult to directly compare treatments across different
        trials.
        However, they found evidence for analgesic efficacy in established PHN for orally
        administered therapies, such as tricyclic antidepressants, some opioids, gabapentin,
        tramadol, and pregabalin. Some topically administered therapies, such as lidocaine and
        capsaicin, were associated with analgesic efficacy in selected patients. However, it
        appeared that therapies such as oral administration of certain NMDA receptor antagonists,
        codeine, ibuprofen, lorazepam, 5HT1 receptor agonists, and acyclovir were not efficacious
        in PHN.
        Altogether, the authors conclude that the evidence base supports the first-line use of a
        tricyclic antidepressant for orally administered treatment of PHN, reserving the
        gabapentinoids for second-line use. Topical treatments, such as lidocaine or capsaicin,
        should be considered as first-line treatment if a patient falls into the “sensitised
        nociceptor” as opposed to “deafferentation” sub-group of PHN patients.
        The role of intrathecal steroid is still not clear: one trial indicated that intrathecal
        steroids were associated with benefits in patients with PHN, but this therapy might be
        hazardous, and the authors and other researchers have concluded that further high-quality
        trials of this therapy are needed. The authors found little evidence regarding possible
        synergistic effects of the various treatments to support or refute the concomitant use of
        combinations of drugs.
        They stressed that any treatment plan must recognise the importance of the
        biopsychosocial model of chronic pain and that any pharmacologically based management of
        PHN should be combined with advice on and management of psychological and social
        aspects.
        Finally, as there is no single pathophysiology that underlies PHN, they propose that
        future studies should use quantitative sensory evaluation to clearly categorise subsets of
        participants for better interpretation of treatment effects.
      
    
  

  
    
      
        
        When the SARS epidemic showed the first signs of waning, the World Health Organization
        proclaimed that the turnaround was a testament to the efficient response of health systems
        worldwide and justified its decisive action in issuing a global alert.
        That swift response was partly due to infectious disease experts being able to use
        models of disease spread, even though SARS was a newly emerging disease, to help plan their
        next move. In fact, epidemiologists have used mathematical models to predict and understand
        the dynamics of infectious diseases for more than 200 years. The emergence of diseases such
        as Ebola, SARS, and West Nile virus, and multi-drug-resistant malaria—as well as the
        potential for diseases to be introduced by bioterrorism—has attached even greater
        importance to this management tool.
        Models are used to provide information on such infections and predict the effect of
        alternative courses of action. In this month's 
        PLoS Medicine , Helen Wearing and colleagues suggest, however, that many
        off-the-shelf models are inappropriate for making quantitative predictions because
        substantial biases have been introduced by two important, yet largely ignored, assumptions.
        The authors warn that if such biases are not corrected, health authorities risk making
        overly optimistic health policy decisions.
        They begin with the “SEIR” class of models, in which the host population is classified
        according to infectious status, i.e., individuals are susceptible, exposed, infectious, or
        recovered. This model assumes that the rate of leaving the exposed or infectious class is
        constant, irrespective of the time already spent in that class. Although mathematically
        convenient, this assumption gives rise to exponentially distributed latent (incubation) and
        infectious periods, which is epidemiologically unrealistic for most infections, say the
        authors, who suggest instead that it would more sensible to specify the probability of
        leaving a class as a function of the time spent within the class. Hence, initially, the
        chance of leaving the class is small but then increases as the mean infectious/incubation
        period is reached. This assumption would give a more realistic distribution of incubation
        and infectious periods.
        The authors also note another issue that has received surprisingly little attention in
        infectious disease models, namely, the influence of incubation and infectious period
        distributions on the invasion dynamics of an infection into a largely susceptible
        population—despite its obvious application to emerging infections and possible “deliberate
        exposure.”
        The impact of these differences on models could translate into potentially important
        public health concerns, say the authors. They tested their theory by using analytical
        methods to show that, first, ignoring the incubation period or, second, assuming
        exponentially distributed incubation and infectious periods (when including the incubation
        period) always resulted in underestimating the basic reproductive ratio of an infection
        from outbreak data. They then illustrated these points by fitting epidemic models to data
        from an influenza outbreak. Their results suggested that within a strict management
        setting, epidemiological details could make a crucial difference.
        Although previous studies have shown the importance of using realistic distributions of
        incubation and infectious periods in endemic disease models, few studies have considered
        the effects associated with making predictions for an emerging disease. Discrepancies
        between estimates of reproductive ratio from exponentially distributed and
        gamma-distributed fits confirm the need to have precise distributions of incubation and
        infectious periods. Although such data are available from post hoc analyses of epidemics,
        they are lacking for novel emerging infections. The key point is that uncertainty about
        these distributions should be incorporated into models when making quantitative
        predictions.
        The take home message is that when developing models for public health use, policy
        makers need to pay attention to the intrinsic assumptions within classical models. The
        authors note that while some practitioners are using their approach, most applied
        epidemiological studies still use models that incorporate exponentially distributed
        incubation and infectious periods; the authors hope their work will point to the next steps
        in delivering quantitatively accurate epidemiological models.
      
    
  

  
    
      
        
        It doesn't take a trained physician to know that a disease needs to be diagnosed to be
        treated. And it doesn't take an economist to know that a disease cannot be diagnosed if the
        required tools are unaffordable or impractical. The absence of early diagnosis and
        treatment is particularly problematic for infectious disease, where the lack of early
        treatment or isolation can result in an epidemic.
        Given the technological requirements, diagnosis and monitoring of HIV infection is
        problematic in resource-poor areas. The advent of rapid tests for diagnosing HIV infection
        represents one part of the solution. Less clear is how patients diagnosed with HIV
        infection will be monitored, given the importance of CD4 cell counts. A decrease in CD4
        + T lymphocytes—a critical immune cell infected by HIV—is one of the
        hallmarks of HIV disease, and CD4
        + cell number is a key factor in determining disease progression and
        monitoring treatment. The methods for determining CD4
        + cell numbers are technically complex, expensive, and not easily
        transportable. These factors severely limit the ability to monitor HIV disease in locations
        where resources, training, and mobility are limited.
        Lymphocytes are characterized by cell surface markers; thus, CD4
        + lymphocytes express the CD4 marker on their surface. Antibody probes
        that specifically recognize this and other cell surface markers (such as CD8, which
        distinguishes that lymphocyte population from CD4
        + lymphocytes, and CD3, which is a marker for all T lymphocytes) are
        used to count and differentiate various cell populations. By labeling cells with
        fluorescently tagged antibodies that recognize one or more cell surface molecules, the
        relative and absolute numbers of specific cells can be determined by a technique called
        flow cytometry. The labeled cells are passed through the flow cytometer, where the
        fluorescent probes are activated by lasers in a manner that can be read by specific
        detectors. The CD4
        + cell number is directly correlated with the resultant fluorescent
        intensity and other light scatter properties. The problem is that flow cytometry requires
        costly reagents and substantial technical expertise—factors that limit its use in less
        developed areas.
        Taking advantage of advances in microfluidics, digital imaging, and cell analysis,
        William Rodriguez and colleagues now report on a way to count CD4
        + cells in a relatively quick, easy, and affordable manner. Small
        volumes of blood (an amount that could be obtained by a finger prick as opposed to drawing
        blood from a vein) are labeled as in flow cytometry, but with far less of the expensive
        reagents. Microfiltration allows the labeled CD4
        + cells to be captured and separated from red blood cells, another
        simplification relative to flow cytometry. Digital images of the labeled cells, obtained by
        digital fluorescence microscopy, are then analyzed by newly developed software that can
        distinguish the CD4, CD8, and CD3 labels, thus allowing determination of absolute CD4
        + counts, CD4
        + percentages, and CD4
        + :CD8
        + lymphocyte ratios.
        Rodriguez et al. found that this new method was less accurate than flow cytometry for
        determining absolute CD4
        + lymphocyte counts above 500 cells/mm3 (levels that are typically not
        relevant for monitoring HIV-infected individuals). But the method was as accurate as flow
        cytometry at clinically relevant levels of CD4+ cells for HIV-infected adult individuals.
        Although only a small number of pediatric patients were examined (and thus statistical
        significance could not be ascertained), the method appears to be also effective in
        determining CD4
        + lymphocyte percentages in children.
        The detection system used in the present report is a tabletop instrument that serves as
        a prototype for a fully portable handheld model, which is now under development. After some
        modest training, such a tool should allow a variety of health-care workers in remote areas
        to accurately analyze the CD4
        + status of HIV-infected patients (the basis for treatment decisions)
        locally. In an accompanying Perspective discussing this new tool (DOI:
        10.1371/journal.pmed.0020214), Zvi Bentwich argues that before it is ready for widespread
        use, several issues still need to be resolved, such as its final cost and its applicability
        to pediatric patients. “Despite these reservations,” he says, “the authors of this study
        should be commended for addressing an extremely important issue and developing this novel
        approach for counting CD4 in patients with HIV.”
      
    
  

  
    
      
        Introduction
        With an estimated 3.5 million people with HIV/AIDS, Nigeria is home to one of every 11
        of the 40 million people with HIV/AIDS worldwide [1]. The HIV prevalence among adults in
        Nigeria has increased from 1.8% in 1991 to an estimated 5.8% in 2001 [1]. Prevalence ranges
        from 2% to 14.9% in the country's 36 states and Federal Capital Territory [2]. According to
        official estimates, Nigeria has an estimated 3.6 million people with HIV/AIDS and
        approximately 310,000 AIDS deaths this year alone [3], and these numbers are projected to
        increase each year. In 1999, with the election of President Olusegun Obasanjo, Nigeria
        emerged from approximately 20 years of military dictatorship in which little governmental
        attention or funding was directed at addressing HIV/AIDS [4].
        People living with HIV/AIDS (PLWA) in Nigeria have been found to be subject to
        discrimination and stigmatization in the work place, and by family and communities [5,6].
        PLWA may also face discrimination from those employed in the health-care sector. [5].
        Discriminatory or unethical behavior by health-care professionals against PLWA, as
        documented in other countries [7–11], may create an atmosphere that interferes with
        effective prevention and treatment by discouraging individuals from being tested or seeking
        information on how to protect themselves and others from HIV/AIDS [12–14]. Furthermore,
        discriminatory practices and violations of international principles of medical ethics may
        serve to legitimize other forms of discrimination against people living with HIV/AIDS.
        Anecdotal information suggests that health-care professionals in Nigeria may engage in
        discrimination against and stigmatization of PLWA [6,15]. The prevalence, character of, and
        factors contributing to these practices are, however, largely undocumented. To address
        this, Physicians for Human Rights (PHR), Policy Project Nigeria, and the Center for the
        Right to Health conducted a survey of health professionals in four sites in Nigeria. The
        study was designed to answer three research questions. (1) Are there discriminatory
        practices in the health sector that affect the health and well-being of people with
        HIV/AIDS in Nigeria? (2) How receptive are health workers and institutions to treating
        people with HIV/AIDS? And (3) what underlying factors may contribute to any discriminatory
        practices? The study was intended to inform ongoing policy discussions and development of
        effective interventions.
      
      
        Methods
        
          Sampling
          At the time of the study, approximately 120,000,000 people were living in the 36
          states and Federal Capital Territory of Nigeria [16]. We conducted the study in four
          states: Abia, Gombe, Kano, and Oyo. These sites were selected by dividing the country's
          six geopolitical zones into two sections—north and south—in order to capture geographical
          and other differences and then randomly selecting two of three zones from each section.
          Within the four selected zones, using health-care facility lists compiled by Nigeria's
          Federal Ministry of Health [17], we identified states that have a tertiary care
          institution and randomly selected one of these states from each zone. To obtain a
          representative sample of health-care professionals, we proportionally sampled doctors,
          nurses, and midwives from the tertiary facility and systematically selected public and
          private secondary and primary health-care facilities in the four states. Fifty-four
          percent of the health-care professionals were sampled from tertiary care facilities. We
          determined the sample size based on local activists' estimates that 10% of clinicians
          have discriminatory behavior and attitudes, a margin of error of ± 0.01% and a 90%
          confidence (10% significance) level. The sample size required given these constraints was
          301 health-care professionals. However, our sample design included several levels of
          clustering, and we therefore assumed a design effect of three and thus the sample size
          needed was calculated to be approximately 1,000. Eligible facilities were medical
          facilities included in the published federal government database, which indicated that
          there were 2,585 health facilities in the four states [17]. In each health-care facility,
          we systematically sampled from all doctors, nurses, and midwives to acquire information
          about their knowledge, attitudes, and behavior. Eligible professionals were physicians or
          certified nurses or midwives working in positions with direct patient contact. Data on
          the number of health-care professionals were derived from Federal Ministry of Health
          data, which indicated that these four states have a total of nearly 4,500 health-care
          professionals who serve a population of approximately 17.8 million people [18].
        
        
          Survey Questionnaire
          The 104-item health-care professional survey included questions on respondent
          demographics; practices regarding informed consent, testing, and disclosure; treatment
          and care of patients with HIV/AIDS; and attitudes and beliefs about treatment and care of
          patients with HIV/AIDS including informed consent, testing, and disclosure.
          Treatment and care practices of patients with HIV/AIDS were assessed using Likert-type
          scales (e.g., the possible answers were “always,” “most of the time,” “sometimes,”
          “rarely,” and “never”). Attitudes and beliefs were assessed by a response of “agree” or
          “disagree” with statements regarding testing, treatment, and care of patients with
          HIV/AIDS.
          Using a separate 103-item survey instrument, we obtained information about each
          facility's capacity, resources, and policies from the person in charge of the facility.
          The questionnaires were written and interviews conducted in English. Seven regional,
          human rights, and medical experts reviewed the questionnaires for content validity. The
          instruments were pilot tested among 20 participants in Lagos and suggestions regarding
          clarity and cultural appropriateness were incorporated.
        
        
          Interviewers
          After completing an intensive training program, 24 Nigerian surveyors conducted the
          survey interviews. Interviewer training consisted of 5 d of classroom teaching and
          role-playing followed by several days of field observation and ongoing supervision by PHR
          and Nigerian researchers.
          All interviews were conducted over 5 wk in October and November 2002. Interviews
          lasted approximately 20–30 min and were conducted in the most private setting possible
          within each health-care facility. All questionnaires were reviewed for completeness and
          for correctness of recording after the interview by the interviewers themselves, by the
          Nigerian research team leaders, and by PHR field supervisors at the end of each day.
        
        
          Definitions
          In the surveys, informed consent was defined as ensuring that a patient who is
          competent to make decisions is informed and consulted about his or her care. Respondents
          were informed that this included the responsibility of the clinician to let the patient
          know about any procedure or medical decision, reasonable alternatives to it, and the
          risks, benefits, uncertainties, and possible consequences related to each alternative.
          The clinician must carry out the discussion in layperson's terms, assess the patient's
          understanding along the way, and ensure that the patient understands the information and
          consents to it voluntarily [19]. Universal precautions were defined as the use of
          protective barriers such as gloves, gowns, aprons, masks, or protective eyewear, which
          can reduce the risk of exposure to potentially infective materials at all times
          regardless of a patient's HIV or other status [20].
        
        
          Human Subjects Protection
          This study was reviewed and approved by an independent ethics review board of
          individuals with expertise in clinical medicine, public health, bioethics, and
          international HIV/AIDS and human rights research developed for this research project by
          PHR. In reviewing the research, the review board was guided by the relevant provisions of
          Title 45 of the US Code of Federal Regulations [21], and complied with the Declaration of
          Helsinki, as revised in 2000 [22]. The study was also reviewed for ethical and cultural
          appropriateness by a panel convened in Nigeria by Policy Project Nigeria. In addition,
          permission for the study and access to facilities was granted by the Nigerian Federal
          Ministry of Health, state and local government authorities, and facility directors. There
          were no limitations placed on movement or surveying. Verbal informed consent was obtained
          from all participants, their names were not recorded, and only minimal identifying
          information was taken in order to preserve the anonymity of their responses. Participants
          did not receive any compensation.
        
        
          Statistical Analysis
          The data were analyzed using Stata 7 [23]. To control for clustering and design
          effect, the sample was weighted by the number of states selected with a tertiary facility
          from each of six selected geopolitical zones, the number of local government areas per
          location, the number of facilities selected from each local government area, and the
          response rate in each location. The study's principal objective was to describe
          health-care professional practices and attitudes towards people with HIV/AIDS, rather
          than to conduct comparisons between professionals or explore associations between
          professional characteristics and different outcomes. However, we conducted bivariate
          analyses using chi-square analyses and simple logistic regression to compare negative
          practices and attitudes among the three health specialties surveyed (doctors, nurses, and
          midwives) and to test for associations between reported facility resources and providers'
          reported adequacy of AIDS training and reported negative practices and attitudes about
          HIV/AIDS.
        
      
      
        Results
        
          Characteristics of Facilities
          Of the 163 facilities sampled, 20 were no longer operational; for ten, contact could
          not be established after two attempts at the time of sampling; and 15 were not eligible.
          Of the 118 eligible facilities where contact was established, 111 participated in the
          study (78% of operational facilities). Over half of the facilities were general hospitals
          (54%) and 23% were primary health centers.
          Eighty-four percent of facility directors reported not having antiretroviral
          medications in their facility. Moreover, the availability of other medications and
          dietary supplements was limited, and protective materials and other supplies and
          utilities were not always available (Table 1).
        
        
          Characteristics of Respondents
          Of the 1,103 professionals sampled, 23 were not eligible, five were not available
          after two attempts at the time of sampling, eight were interrupted during the course of
          the interview, and 46 refused to participate. Consequently, 1,021 professionals
          participated in the study (93% response rate). Although we did not gather information on
          nonrespondents, most of the 46 nonrespondents who refused to participate cited lack of
          time as the reason they were unable to participate (36), with other nonrespondents citing
          other obligations (four), fear of reprisal (one), and opposition to the study
          (three).
          
            Sociodemographic characteristics
            Professionals were predominantly female (67%) with a mean age of 36 y. Fifty-six
            percent were nurses, 31% were physicians, and 12% were certified midwives (Table
            2).
          
          
            HIV/AIDS training
            Most professionals reported having some training on HIV/AIDS (Table 2). Current
            literature (69%), conferences (56%), and courses as a student (52%) were most
            frequently reported by professionals as the sources of this training. Seven percent
            reported having no training on HIV/AIDS at all.
          
        
        
          Testing and Consent
          
            Practices
            Seventeen percent of surveyed health-care professionals reported that their facility
            had a written HIV testing policy (Table 3). Respondents indicated that the policies
            included requirements for informed consent (58%), pre-test counseling (53%), post-test
            counseling (52%), and post-test referral (29%).
            Over 50% of professionals reported obtaining informed consent of patients for HIV
            tests half of the time or less, including 14% who reported never obtaining consent for
            HIV tests (Table 3). Fifty-four percent of respondents reported that, regardless of
            consent, routine HIV testing of all patients scheduled for surgery always took place at
            their facilities, and 50% reported such routine HIV testing of all women attending
            antenatal care clinics. Providers who reported that they lacked adequate training in
            HIV/AIDS treatment and ethics had 50% higher odds of reporting that they failed to
            obtain informed consent for HIV tests (more than 50% of the time) compared to providers
            reporting adequate training in these areas (odds ratio (OR) 1.53, 95% confidence
            interval [CI] 1.17–2.01).
          
          
            Attitudes
            Ninety-one percent of professionals agreed that staff and health-care professionals
            should be informed when a patient is HIV-positive so they can protect themselves (Table
            4). Over three-quarters of respondents (78%) agreed that there are circumstances when
            it is appropriate to test a patient without his or her knowledge or permission.
            Fifty-seven percent of participants believed that relatives and sexual partners of
            patients with HIV/AIDS should be notified of the patient's status even without the
            patient's consent. Forty-six percent of professionals thought that the charts or beds
            of patients with HIV should be marked so that health facility workers know the
            patient's status.
            Forty percent believed that health-care professionals with HIV/AIDS should not be
            working in any area of the health professions that requires patient contact. Twenty
            percent of respondents agreed that many of those who have HIV/AIDS behaved immorally
            and deserve the disease (Table 4).
            Providers working in facilities that did not always practice universal precautions
            (65% citing lack of sufficient materials as the reason) were significantly more likely
            than those working in facilities that always observed universal precautions to agree
            that people with HIV/AIDS should not be employed in the health field (OR 1.43, 95% CI
            1.09–1.74) and should not work in areas that require patient contact. They also had
            higher odds of agreeing that under certain circumstances, patients could be tested for
            HIV without their knowledge or permission (OR 1.63, 95% CI 1.14–2.33) Working in a
            facility that did not always practice universal precautions, being a nurse or midwife,
            and reporting inadequate training in HIV/AIDS treatment were all associated with
            agreeing that patients with HIV/AIDS should be on a separate ward in a hospital or
            clinic. Nurses and midwives both had more than five times the odds of agreeing that
            people with HIV/AIDS should not be employed in the health field than doctors and that
            the charts or beds of HIV patients should be marked. Nurses and midwives also had
            almost twice the odds of physicians of agreeing that under certain circumstances it is
            acceptable to test patients for HIV without their consent or knowledge.
          
        
        
          Treatment and Care
          
            Practices
            Among health-care professionals, the three most important concerns about treating
            patients with HIV/AIDS were fear of becoming contaminated (81%), contamination of
            facility, materials, or instruments (17%), and not having materials needed to treat
            them (10%) (Table 5). Seventy-two percent of respondents reported that universal
            precautions were always practiced in the facilities in which they worked. Lack of
            materials—reported by 65% of professionals—was cited as the main reason for
            non-practice of universal precautions (Table 5).
            Nine percent of professionals reported refusing to care for a patient with HIV/AIDS,
            and 9% indicated that they had refused a patient with HIV/AIDS admission to a hospital
            (Table 6). Sixty-six percent had observed other health-care professionals refusing to
            care for a patient with HIV/AIDS, and 43% had observed others refusing a patient with
            HIV/AIDS admission to a hospital. While less than one percent of professionals reported
            verbally mistreating a patient with HIV/AIDS, 27% of respondents reported seeing others
            verbally mistreat patients with HIV/AIDS.
            Thirty-eight percent of professionals reported giving confidential information to a
            patient's family member without the patient's consent, and 53% had observed this
            behavior. Twelve percent of professionals reported giving confidential information to a
            person not related to a patient without consent, and 22% had observed this behavior
            (Table 6).
            Providers who reported inadequate training in HIV/AIDS treatment and in ethics were
            significantly more likely to have refused to treat a patient with HIV/AIDS than those
            reporting adequate training in those two areas (OR 2.06, 95% CI 1.31–3.22). Providers
            working in facilities that did not always practice universal precautions were not more
            likely to have refused care to a patient themselves but were significantly more likely
            to report having observed other providers refuse to care for a patient with HIV/AIDS
            (OR 1.09, 95% CI 1.01–1.45). There were no differences among specialties in reporting
            having refused to care for a patient with HIV/AIDS.
          
          
            Attitudes
            To prevent discrimination by health-care professionals against patients with
            HIV/AIDS, most participants (87%) indicated that health-care professionals who engage
            in discriminatory practices should be educated and counseled. Health facility policies
            against discrimination were cited as solutions by 19% of professionals, and stronger
            laws against discrimination were suggested by 11% (Table 6).
            Ninety-four percent indicated that medications to treat opportunistic infections may
            prolong the life of a patient who is HIV-positive (Table 7). Over half (59%) of
            professionals agreed that people with HIV/AIDS should be on a separate ward in a
            hospital or clinic. Forty-eight percent of participants expressed their belief that a
            person with HIV/AIDS cannot be treated effectively in their facility. Forty percent of
            health-care professionals reported that it is possible to determine a person's HIV
            status by looking at him or her, and 21% agreed that they could refuse to treat a
            patient with HIV/AIDS to protect themselves and their family. Twelve percent expressed
            agreement with the statement that treatment of opportunistic infections in patients
            with HIV/AIDS wastes resources, and 8% agreed that treating someone with HIV/AIDS is a
            waste of precious resources.
            Nurses had higher odds than physicians of agreeing that treating opportunistic
            infections in patients with HIV/AIDS is a waste of resources (OR 2.14, 95% CI
            1.35–3.40), but physicians were 50% more likely than nurses to agree that they could
            refuse to treat a patient with HIV/AIDS to protect themselves and their family.
            Respondents who reported inadequate training in HIV/AIDS treatment also were
            significantly more likely to agree it was acceptable to refuse to treat a patient for
            these reasons (OR 1.34, 95% CI 1.31–3.22). Physicians had significantly higher odds
            than either nurses or midwives of agreeing that there were circumstances under which it
            was appropriate to reveal a person's HIV status to others without the patient's
            knowledge or permission.
          
        
      
      
        Discussion
        Most health-care professionals in the four states where the study was conducted appeared
        to be providing care to patients who were HIV-positive and complying with their ethical
        responsibilities despite their lack of training on HIV/AIDS and their having insufficient
        supplies of materials needed for treatment and prevention in the facilities where they
        work. A significant number, however, reported engaging in discriminatory and/or unethical
        behavior. These practices are corrosive to the health professions as they taint all health
        professionals and erode trust in them. They also represent missed opportunities for
        prevention, positive living education, and treatment, thereby undermining Nigeria's
        concerted national efforts to address the HIV/AIDS epidemic. Our study findings suggest
        that there are several factors that may contribute to such discriminatory and/or unethical
        behavior by health-care professionals against people with HIV/AIDS in Nigeria.
        The vast majority of professionals expressed an interest in additional information and
        suggested education as a way to address discriminatory behaviors by their colleagues. An
        immediate investment to ensure the education of all existing clinical staff about HIV/AIDS,
        including modes of transmission, universal precautions, and the rights of PLWA would likely
        reduce the number of discriminatory practices towards PLWA and may improve these patients'
        care and access to health services. This assertion is supported by previous studies that
        demonstrate the effect of HIV/AIDS education of nurses and other health workers on their
        attitudes and behavior towards patients who are HIV-positive in Nigeria and elsewhere
        [24–26]. These studies also suggest that education about scientific matters is not likely
        to be sufficient to achieve change in practice and that educational programs may also need
        to address attitudes and cultural beliefs.
        This study further suggests that the lack of protective and other materials needed to
        treat and prevent the spread of HIV and related conditions contributes to discriminatory
        behavior. While the issue of access to affordable antiretroviral treatment is the subject
        of much debate in Nigeria [13,27], many of the facilities in this study did not even have
        sufficient stocks of basic antibiotics to treat opportunistic infections. The lack of
        protective materials, documented in the health facility survey and cited also by
        professionals as the main reason for not applying universal precautions, contributes to
        discriminatory behavior in two ways. First, professionals lacking adequate protection may
        come to fear PLWA and fear may lead to discrimination [28–30]. Second, lack of resources
        also results in differential treatment practices that may contribute to stigmatization of
        PLWA.
        In order to do their jobs safely and effectively, health professionals must be provided
        with adequate supplies of essential protective materials. Further, the lack of basic
        medications hampers the ability of health professionals to provide appropriate treatment.
        Without these materials, it is unlikely that education of health professionals and
        implementation of anti-discrimination policies alone will have the desired impact on
        practice.
        It is likely that in other low-resource contexts, the absence of medications needed to
        treat HIV/AIDS-related illnesses, a lack of materials needed for protection of health
        personnel, and insufficient knowledge of health personnel about HIV/AIDS may contribute to
        discriminatory behavior towards people with HIV/AIDS. The role of these factors should be
        investigated. While addressing these factors may not eliminate all discriminatory behavior,
        these basic investments in the health-care sector are likely to result in improvements.
        HIV infection is both a product of and a factor contributing to human rights violations
        [12]. The documented marginalization of certain groups, and their increased risk for
        infection with HIV in Nigeria [1], must be considered in light of this study.
        Misconceptions must be taken into account when developing education and training programs
        for professionals and the public. Nigerian health professionals are members of their
        society, one in which stigma and moral judgment appear to be attached to HIV/AIDS [6].
        Twenty percent of respondents agreed that many of those who have HIV/AIDS behaved immorally
        and deserve the disease. As such, it is likely that governmental and facility policies and
        monitoring to reduce discriminatory practices in the health-care sector will be an
        important aspect of addressing these practices.
        Numerous international and regional human rights instruments, to which Nigeria is a
        party [31], protect the rights of PLWA. These include [32–37] the African Charter on Human
        and People's rights [36], the Convention on Elimination of All Forms of Discrimination
        against Women [34], the Convention on the Rights of the Child [35], the International
        Convention on Elimination of All Forms of Racial Discrimination [37], the International
        Covenant on Economic, Social, and Cultural Rights [32], and the International Covenant on
        Civil and Political Rights [33]. Of these, only the African Charter and the Convention on
        the Rights of the Child have been incorporated into the domestic law of Nigeria [4,38].
        Nigeria is also a signatory to the Universal Declaration of Human Rights [39].
        The above instruments set out Nigeria's obligations to protect the rights of PLWA
        including the right to life [33], the right to education [36], the right to marry and found
        a family [33], the right to nondiscrimination [36], the right to share in the benefits of
        scientific advancements [32], the right to privacy [33], and the right to freedom of
        association [36].
        Several of the instruments to which Nigeria is a party include the right to health
        [34–37,40]. The right to health, was first elaborated in the International Covenant on
        Economic, Social, and Cultural Rights, Article 12 [32], which states:
        
          
          
            1. The States Parties to the present Covenant recognize the right of everyone to
            the enjoyment of the highest attainable standard of physical and mental health.
            2. The steps to be taken by the States Parties to the present Covenant to achieve
            the full realization of this right shall include those necessary for:
          
        
        
          
          
            (a) The provision for the reduction of the stillbirth-rate and of infant
            mortality and for the healthy development of the child;
            (b) The improvement of all aspects of environmental and industrial
            hygiene;
            (c) The prevention, treatment and control of epidemic, endemic, occupational and
            other diseases;
            (d) The creation of conditions which would assure to all medical service and
            medical attention in the event of sickness.
          
        
        In 2000, the Committee on Economic Social and Cultural Rights (ESCR Committee),
        responsible for interpretation and monitoring of the International Covenant on Economic,
        Social, and Cultural Rights, published General Comment 14 on the Right to the Highest
        Attainable Standard of Health [40]. The ESCR Committee determined that fulfillment of the
        right to health means that access to health services must not be limited based on
        discrimination on a prohibited ground, including HIV status .
        In General Comment 14, the ESCR Committee also set out the core obligations of a state
        party to protect the right to health, which include ensuring “the right of access to health
        facilities, goods and services on a non-discriminatory basis, especially for vulnerable or
        marginalized groups,” the provision of essential drugs “as from time to time defined by
        WHO's Action Programme on Essential Drugs,” and ensuring “equitable distribution of all
        health facilities, goods and services.” In addition to these and other core obligations,
        the ESCR Committee also set out “obligations of comparable priority” , including a state
        party's obligation “to take measures to prevent, treat and control epidemic and endemic
        diseases,” “to provide education and access to information concerning the main health
        problems in the community, including methods of preventing and controlling them,” and “to
        provide appropriate training for health personnel, including education on health and human
        rights.”
        The ESCR Committee also stated in General Comment 14 that “any person or group who is a
        victim of a violation of the right to health should have access to effective judicial or
        other appropriate remedies at both national and international levels.”As a state party,
        Nigeria is bound by the provisions of the International Covenant on Economic, Social, and
        Cultural Rights and the authoritative interpretations of the ESCR Committee. This study
        finds that some PLWA have been excluded from access to health care because of their HIV
        status and that, at this time, PLWA have no access to judicial or other remedial processes
        to address this. The data further suggest that inadequate education of health personnel
        about HIV/AIDS along with a lack of protective and treatment materials likely contribute to
        these behaviors by health professionals. It is therefore likely that Nigeria has not met
        its core obligations to fulfill and protect the right to health. The findings of this study
        suggest that, in order to fulfill its obligations, the government of Nigeria should
        continue to address gaps in policy and legislation and work together with the international
        community to ensure that health professionals receive the training, protective materials,
        and medications they need to treat PLWA.
        International principles of medical ethics and Nigerian codes of conduct clearly provide
        for patient autonomy, i.e., the right to informed consent and confidentiality of patient
        information. In addition to representing violations of human rights, the denial of
        treatment and breaches of informed consent and confidentiality detailed in this paper
        contravene international principles of medical ethics and Nigerian health professional
        codes of conduct. The 
        Rules of Professional Conduct for Medical and Dental Practitioners in
        Nigeria [41] states that “a doctor shall preserve absolute secrecy on all he knows
        about his patient even after the patient has died, because of the confidence entrusted to
        him.” The binding rules also state that “practitioners…must always obtain consent of the
        patient or the competent relatives...before embarking on any special treatment procedures
        with determinable risks.”
        Nigerian medical practitioners also have a duty under these rules to report any
        unethical conduct by their peers to the Medical and Dental Council of Nigeria. According to
        the rules, “every doctor or dentist must be his brother's keeper, with regard to the
        observance and indeed the enforcement of the rules and regulations which guide the
        profession. Doctors and dentists should expose without fear or favour, before the Medical
        and Dental Council of Nigeria either directly or through the Nigerian Medical Association,
        any corrupt, dishonest, unprofessional or criminal act or omission on the part of any
        doctor or dentist.” There is no indication that this may have happened in the case of the
        breaches documented in this study. At the time of publication, no specific medical ethics
        principles on HIV and AIDS have been articulated by the Medical and Dental Council of
        Nigeria.
        
          Limitations
          The study was conducted in four states in Nigeria with a total population of 17
          million [2]. It is possible that these sites, though chosen at random from states with
          tertiary care facilities, may differ significantly from others in terms of resources and
          training provided to health-care providers. Although sampled systematically, it is
          possible that sampled facilities and health-care professionals may differ significantly
          from those that were not sampled in the four study states.
          Although the findings of this study can not be generalized to Nigeria as a whole, it
          is likely that, depending on resources and training available to the health-care sector,
          the level of discriminatory behavior may differ in other parts of the country.
          The apparent discrepancy between reported and observed behavior may indicate under- or
          overreporting of discriminatory behavior or may result from health-care professionals
          within the same institution having observed the same incidents.
          While this study focused on HIV/AIDS, it is possible that health-care professionals
          also engage in inappropriate behavior toward or breach the confidentiality of people with
          other conditions. The health-care system in Nigeria is underfunded and suffers from
          fundamental problems including material scarcity and inadequacies in infrastructure,
          which may contribute to this behavior overall [4,42,43]. We did not specifically ask
          clinicians to compare their treatment of patients positive for HIV with that of other
          patients. Even if health-care professionals engage in breaches of confidentiality and
          other inappropriate behavior toward patients with other conditions, however, it is likely
          that the consequences of such actions may be worse for patients positive for HIV than for
          patients with other conditions.
          Despite efforts to ensure privacy during interviews, the lack of privacy, or concern
          about job status, may have resulted in an underreporting of discriminatory behavior
          and/or an overreporting of “correct” practices or attitudes. Although interviewers were
          careful to explain that there would be no material gain or penalty to the respondent or
          his or her facility from participation in the study, the responses may have been
          inaccurate if respondents judged it in their material or political interest to exaggerate
          or conceal certain behaviors.
        
        
          Conclusion
          Despite these limitations, the study documents a significant proportion of health
          professionals in four states in Nigeria as reporting discriminatory attitudes and
          engaging in discriminatory and unethical behavior toward patients with HIV or AIDS,
          including denial of care, breach of confidentiality, and non-consented HIV testing. The
          breaches of confidentiality and testing for HIV without informed consent reported by
          participants are in contravention of international principles of medical ethics [44], and
          are also breaches of the Nigerian physician code of conduct [41]. The study identifies
          four factors that may contribute to this behavior: lack of correct information and
          education about HIV/AIDS and prevention of infection, lack of protective materials needed
          for the practice of universal precautions, lack of materials needed to care for and treat
          patients with HIV/AIDS, and prevailing attitudes about PLWA. This study suggests that
          adequately addressing these discriminatory practices and attitudes requires targeted
          education of health professionals and provision of adequate resources to health-care
          facilities combined with instituting and enforcing anti-discrimination policies.
        
      
    
  

  
    
      
        
        Jonathan Mann, founder of the World Health Organization's Global Program on AIDS and
        untiring advocate for justice for people with HIV/AIDS, addressed the United Nations
        General Assembly in 1987 [1]. His speech characterized the three major phases of an
        HIV/AIDS epidemic. After the initial silent spread of virus came the outbreak of ill
        health. The final stage, he said—the stage of social impact—is marked by stigma, grinding
        down its victims with shame and isolation.
        Mann's tragically short life was devoted to protecting all who stood to be diminished by
        illness-related stigma and the erosion of elemental human rights [2]. Why were stigma and
        human rights so essential to the work of a medical doctor fighting an infectious
        disease?
      
      
        Fear of Stigma Fuels the HIV Epidemic
        Stigma is of utmost concern because it is both the cause and effect of secrecy and
        denial, which are both catalysts for HIV transmission. Fear of stigma limits the efficacy
        of HIV-testing programs across sub-Saharan Africa, because in most villages everyone
        knows—sooner or later—who visits test sites [3,4]. While in some places the advent of free
        and accessible antiretroviral therapy has offered hope and encouraged people to go for
        testing, [5] stigma remains a barrier to testing even where treatment is available [6].
        Without HIV testing, an essential first step to treatment, years may go by while people who
        are infected transmit the virus to others. When individuals finally become ill and seek
        care, treatment as a prevention strategy has lost much of its potential effectiveness.
        Fear of stigma can cause pregnant women to avoid HIV testing, the first step in reducing
        mother-to-child transmission [7–9]. It may force mothers to expose babies to HIV infection
        through breast-feeding because the mothers do not want to arouse suspicion of their HIV
        status by using alternative feeding methods [10,11]. Fear of stigma, and the resulting
        denial, may even inhibit condom use in HIV discordant couples. Further evidence of how
        stigma leads to denial is the way in which newspaper obituaries avoid mentioning HIV/AIDS
        as a cause of death.
        HIV-related stigma directly hurts people, who lose community support due to their real
        or supposed HIV infection. Individuals may be isolated within their family, hidden away
        from visitors, or made to eat alone [3]. These repercussions may or may not be simple acts
        of heartlessness. They may be a well-intentioned but ignorant attempt to preserve the
        family. In the community, the entire family may be sanctioned because one member is ill; in
        an impoverished society with no safety net of public services, this can be ominous for
        everyone [3].
        In many African villages, an individual's, and a family's, life is closely intertwined
        with others. The same people have lived closely together for several generations, and there
        are few secrets. Inside families, caregivers may be largely concerned about contracting HIV
        through casual contact, and outside they fear the gossip that can greatly affect everyone's
        social standing. Neighbors and other customers, for instance, may refuse to purchase
        vegetables or poultry from someone associated with HIV [12]. In impoverished areas, this
        can devastate a family's chances of economic survival.
        The language used to describe people living with HIV (such as “she is an HIV,” “he is a
        walking corpse”) clearly conveys stigmatizing attitudes. A particularly powerful example of
        stigmatizing language is found in parts of Tanzania, where an HIV-positive person is called
        
        nyambizi , or submarine, implying that the HIV-positive person is
        stealthy, menacing, and deadly [3,13].
        People living with HIV can also experience a form of internalized stigma (Figure 1).
        Even without the burden of externally imposed social opprobrium, those living with a
        serious illness can face an enormous and painful inner struggle. They may eventually cease
        to be who they were, instead becoming a unitary “person with an illness” or—more damning—an
        “ill person,” a thing in which personhood and illness have completely fused. The
        philosopher Simone Weil characterized this assault of illness upon the self with the
        classical Greek notion of the soul—
        Malheur (affliction) stamps the soul to its very depths with scorn and
        disgust [14].
        The combination of external stigma and internal oppression of the self may impose a
        heavy burden. In our experience of working with people with HIV in Africa, the result of
        this burden is often a downward spiral marked by fatalism, self-loathing, and isolation
        from others. And by shaming and silencing the very people who could credibly speak for HIV
        prevention and provide care for HIV-positive others, stigma fuels the HIV epidemic,
        consigning more people to suffering and death.
      
      
        Stigma in Society
        Stigma is part of the attitudes and social structures that set people against each
        other. It impedes any countervailing forces for social equality. Certainly since Erving
        Goffman's seminal work on stigma in the early 1960s, stigma (plural stigmata) has been
        recognized as “an attribute that is significantly discrediting,” and it is known as a
        potent and painful force in individual lives [15]. Fueled by prejudice and appealing to it,
        stigma functions to diminish the person or group being targeted. Some commentators since
        Goffman have particularly examined stigma's broader social functioning. They have noted
        that while subordinating individuals or groups in society, the stigmatizing process also
        reinforces hierarchical patterns of privilege, where those at the top of a stratified
        society are pre-eminent over, and sometimes predatory upon, others at lower levels
        [16].
        To see this perhaps more clearly, think of certain religious settings where punishment
        theories of illness causation are in force [17–19]. One such outlook presumes an aroused
        deity or ancestor bringing illness upon a person in retribution for an offense. This notion
        stigmatizes people struggling with their illness. It blames their sickness upon
        misbehaviors, while at the same time it rationalizes privileging the well over the ill.
        Punishment theories authorize communities to isolate or purge the “impure”—people whose
        illness or imagined “sinfulness” would contaminate the whole—while reassuring that virtue
        and social status will protect the righteous.
        Clergy and other religious leaders are as susceptible as any to the temptation to
        exercise power over others. This imbalance of power is facilitated by such structured
        inequalities within churches as the preeminence of clergy over laity, of men over women,
        and even by the presumed superiority of the more “spiritual” over the less so. Under the
        influence of western missionaries, many African Christian organizations still promote
        evangelical formulae in which, it is taught, creation was originally good, but then the
        “fall” of humankind occurred, which is bad, and finally, redemption is available only for
        the chosen. This theological approach warrants valorizing or stigmatizing people as “saved”
        or “sinner,” “pure” or “impure,” “us” or “them,” and it strengthens the broader social
        stratifications within which stigma flourishes. What is weakened is the opportunity to
        apply healing insights from the rich Christian legacy of compassion, liberation, and hope
        [20].
      
      
        Gender and HIV
        In much of sub-Saharan Africa, women are a subordinate group who are expected to become
        pregnant, bear children, and fulfill the sexual desires of their husbands without
        hesitation [20]. Such traditional assumptions, sometimes reinforced by the missionary
        religions, greatly benefit men while predisposing women to HIV infection. Often husbands
        carry HIV, while barrier methods of disease prevention, such as condoms, are proscribed,
        perhaps most vigorously by male-dominant religious organizations.
        In addition to women's subordinate status in many societies, they are also frequently
        stigmatized as the vectors of HIV transmission, despite overwhelming evidence to the
        contrary. In Malawi, the term for a sexually transmitted disease, regardless of its origin,
        is “woman's disease.” [21] Husbands have beaten and/or abandoned wives thought to be
        HIV-positive, despite the fact that many women contract the virus from their husbands. Some
        women are subject to violence if they refuse a sexual overture, ask their husband to use a
        condom, or request an HIV test. If a husband should die, the wife's in-laws may seize her
        inheritance [22]. A woman exhibiting the independence needed to protect her health and
        self-esteem risks the disapprobation of her family and of the community.
        Men are the clear winners of this arrangement in both social and economic terms, and
        many widows and their children, dispossessed or not, struggle against enormous odds simply
        to survive. Public attitudes, stigma among them, help to sustain the entire unjust
        system.
      
      
        Stigma and Human Rights
        We marvel at the prescience and lucidity of Mann, a doctor who was dedicated to treating
        the whole person—both the physical ills and the emotional distress attendant upon these
        ills, including the stigma inherited from or imposed by societies where the oppression of
        some fortifies the privilege of others. Mann respected the healing potential of social
        justice in general, and of human rights in particular. He knew that a society in which
        multiple injustices routinely occur is itself not well, and he knew that widespread respect
        of human rights made less room for stigma and its harmful consequences.
        
          Respect of human rights makes less room for stigma
        
        The way to tackle social oppression of any kind is to introduce strategies that address
        underlying conditions of poverty, racism, and sexism that support such oppression [5]. This
        approach should be bolstered by sufficient legal and policy mechanisms to protect people
        subject to stigma and the erosion of human rights in general [5,23]. The same mechanisms
        should be functional and accessible to all.
        To be effective, all HIV interventions should include an analysis of how stigma
        functions, how it enhances dominance and subordination in society, how it is that some win
        and others lose in the pernicious struggle for pre-eminence, and why it is that such a
        social scheme perversely flourishes in the first place [16].
        Enlightened HIV prevention and care interventions (Figure 2) will empower the
        stigmatized through health education that lifts self-blame and shifts opprobrium to
        external, self-serving forces. While teaching respect for all through a more just society,
        these interventions will help people who are stigmatized to critique unjust societal
        dynamics and challenge assumptions and warrants of privilege.
        A tall order? Maybe, but Mann asked all of us—those struggling with illness and the
        presumably healthy—to make societies as healthy as their individual members.
      
    
  

  
    
      
        Introduction
        Worldwide approximately 40 million people are infected with HIV, and more than 3 million
        people died of AIDS last year alone [1]. Unfortunately, numerous obstacles to providing
        effective antiretroviral treatment to the majority of infected individuals in resource-poor
        countries exist. The development of a vaccine or other preventive biomedical intervention
        therefore bears the greatest hope to curb the rampant HIV epidemic [2].
        Research on HIV vaccines and prevention relies strongly on preclinical studies in
        macaque models for the identification and evaluation of potential vaccines or prophylactic
        treatment strategies [3]. Initially, the goal was to use animal trials to screen for
        preventive interventions that induce sterilizing immunity (i.e., protection against
        infection) since this would clearly be the most effective way to contain the AIDS pandemic.
        Unfortunately, most of the vaccine approaches assessed to date in animal studies have
        failed to induce sterilizing immunity [4–7], although some prophylactic approaches were
        found to reduce susceptibility to infection [8–12]. As a result of this shortcoming,
        vaccine candidates are at present primarily examined with regard to their effects on set
        point viremia, disease progression, and their general immunogenicity, rather than with
        regard to the degree of protection against infection they confer. However, the inference as
        to the degree of sterilizing immunity from the level of immunogenicity is limited by our
        lack of knowledge about the mechanisms of protection against infection as such [13].
        The inability of most vaccine candidates to induce protection against infection in
        animal studies may be due, at least in part, to unintended consequences of the design of
        the animal trials, rather than to problems inherent in the vaccination approaches
        themselves. In most animal studies that seek to test the efficacy of a given preventive
        intervention, very high challenge doses are used, typically of approximately 10–100 times
        the infectious dose at which 50% of the animals become infected (
        ID
        50 ). The motivation for using such high challenge doses is mostly
        practical: the experimenter wants to ascertain infection success in unvaccinated/untreated
        animals, which can then be compared to the hopefully lower infection success in
        vaccinated/treated animals. There are, however, concerns with using high challenge doses.
        Firstly, the extremely high probability of infection in high-dose challenge studies
        conflicts with the low transmission rate of HIV per sex act [14–17]. Although it has been
        argued that transmission rates may be higher under some circumstances (such as during acute
        infection or when other infections of the genital tract are present) than the estimates
        obtained from discordant couple studies suggest (e.g., the recent study by Pilcher et al.
        [18]), transmission of HIV during one sex act surely does not occur with certainty.
        Secondly, protection against high-dose virus challenges may be more difficult to achieve
        because the use of high challenge doses makes stochastic extinctions that can play an
        important role in early control of the infection [19] very unlikely. Thus, standard
        high-dose challenge studies may rule out preventive intervention strategies that could
        protect against infections following “real life” exposures.
        The problems of using high virus doses in animal studies can be illustrated by the
        discrepancy between the protection zidovudine (AZT) confers in animals and humans. Whereas
        macaques [20,21] and mice [22] were not protected from infection with high challenge doses
        by zidovudine (a relatively weak antiretroviral drug when used in monotherapy), clinical
        studies surprisingly showed that two-thirds of perinatal infections (i.e., mother-to-child
        transmissions during birth) can be prevented by zidovudine administration [23]. It is
        important to note that the use of zidovudine to prevent perinatal HIV infection is a
        biomedical intervention aiming to protect from infection, whereas zidovudine is most
        commonly used as a therapeutic agent after infection. This example suggests that there is a
        need for experimental designs that allow the assessment of the protection against infection
        with lower, and thus more realistic, challenge doses.
        The belief that experiments involving realistically low challenge doses require
        unfeasibly large numbers of animals has prevented the development of low-dose challenge
        models. In this theoretical study, we show that, contrary to this widely held belief,
        low-dose challenge experiments can be designed such that they do not require large numbers
        of animals. Using statistical power analysis, we compare two experimental designs (see
        Figure 1): (i) a single low-dose challenge design in which each animal is challenged only
        once, and (ii) a repeated low-dose challenge design in which each animal is challenged
        until it is infected or a predetermined maximum number of challenges is reached. We find
        that the repeated low-dose challenge design does not require unfeasibly large numbers of
        animals.
        In the following, we are going to discuss the case of assessing whether a vaccine
        candidate induces sterilizing immunity. All the considerations in this article, however,
        apply equally to other preventive interventions, such as microbicides.
      
      
        Methods
        To assess the quality of the single and the repeated low-dose challenge designs, we
        conducted a statistical power analysis. The statistical power of an experimental design is
        defined as the probability that an effective vaccine or treatment is correctly determined
        to be effective. This analysis consists of simulating the experiments, evaluating them, and
        then repeating this procedure thousands of times to estimate the statistical power of a
        given experimental design.
        
          Simulation of Single Low-Dose Challenge Experiments
          In our simulations of the single low-dose challenge experiments, we assume that we
          have 
          n unvaccinated control animals and 
          n vaccinated animals.
          In the control group, we simulate single challenges of each animal with the 
          ID
          50 by performing 
          n Bernoulli trials with a probability of success of 
          p
          c = 0.5. The probability of success corresponds to the
          probability with which an animal becomes infected after a single challenge. (By assuming
          the same probability 
          p
          c for each animal, we ignore potential between-animal variation
          of the susceptibility to infection. This assumption will be relaxed below.) The results
          of these trials can be written as a vector 
          
            x
          
          
          c , the entries of which were either zero (uninfected) or one
          (infected):
          
          By summing over the elements of 
          
            x
          
          
          c , we obtain the number of infected animals in the control group,
          
          ι
          c :
          
          In the vaccinated group, we simulate single challenges with the 
          ID
          50 similarly to the control group by performing Bernoulli trials.
          However, we assume that, because of vaccination, the probability of infection (or
          success) in the vaccinated group, 
          p
          v , is lower than that in the control animals, 
          p
          c . The relation of 
          p
          v to the effect of the vaccine on the susceptibility of the host,
          
          VE
          S , is given by:
          
          The results of these Bernoulli trials can again be written as a vector 
          
            x
          
          
          v , and summing the elements of 
          
            x
          
          
            v
          we obtain the number of infected animals in the vaccinated group, 
          ι
          v .
          The outcome of the simulated experiment can then be summarized in a contingency table
          as shown in Table 1. On this contingency table, we perform a standard one-tailed Fisher's
          exact test [24] to assess whether the fraction of infected animals in the vaccinated
          group is significantly lower than that in the control group.
        
        
          Simulation of Repeated Low-Dose Challenge Experiments
          In our simulations of the repeated low-dose challenge experiments, we once more assume
          that we have 
          n unvaccinated control animals and 
          n vaccinated animals.
          We again simulate challenges of each control animal with the 
          ID
          50 by performing Bernoulli trials with a probability of success of 
          p
          c = 0.5. Unlike in the simulations of the single low-dose
          challenge experiments, however, we now repeatedly challenge each animal until it is
          infected or until a maximum number of challenges, 
          C
          max , has been performed. We assume that the probability of
          infection 
          p
          c is independent of how often an animal has been challenged
          before. The results of these repeated Bernoulli trials can be written as two vectors, 
          
            y
          
          
          c , which contains the number of challenges that have been
          performed on each animal:
          
          and 
          
            s
          
          
          c , which contains information on whether a given animal is
          uninfected (zero) or infected (one):
          
          By summing over 
          
            y
          
          
          c , we obtain the total number of challenges performed in the
          control group, η
          
          c :
          
          And, by summing over 
          
            s
          
          
          c , we obtain the number of infected animals in the control group,
          
          ι
          c :
          
          To simulate repeated low-dose challenges in the vaccinated group, we perform repeated
          Bernoulli trials with a probability of success 
          p
          v . For a given vaccine efficacy 
          VE
          S ,
          p
          v is determined by equation 3. Analogously to the control group,
          the results of these repeated Bernoulli trials can be written as two vectors, 
          
            y
          
          
            v
          and 
          
            s
          
          
          v , and summing the elements of these two vectors yields the total
          number of challenges performed in the vaccinated group, η
          
          v , and, the number of infected animals in the vaccinated group, 
          ι
          v .
          As in the case of the single low-dose challenge design, the outcome of the simulated
          experiment can be summarized in a contingency table (Table 2). To assess whether the
          fraction of infected animals in the vaccinated group is significantly lower than that in
          the control group, we again perform a one-tailed Fisher's exact test [24]. In general,
          the number of challenges, η
          
            c
          and η
          
          v , is larger than the number of animals per group, 
          n . This increase of numbers in the contingency table leads to
          increased statistical power of the repeated low-dose challenge design. To analyze the
          outcome of the simulated repeated low-dose challenge experiments, we chose to use
          Fisher's exact test rather than a more obvious Cox proportional hazards model because the
          latter depends on large sample asymptotics while we were interested in cases of small
          numbers of experimental animals.
        
        
          Heterogeneity in Infection Probabilities
          In our mml:mathematical description of challenge experiments, we have assumed that
          animals within each group have equal infection probabilities—
          p
          c and 
          p
          v , for the control and vaccinated groups, respectively. To
          simulate potential animal-to-animal variation in susceptibility to infection, we relaxed
          this assumption and assigned individual infection probabilities to each animal.
          The individual infection probabilities are drawn from a β-distribution, which is often
          used as a prior distribution for binomial proportions. The β-distribution has two shape
          parameters, α and β. Its probability density is given by
          
          and its mean and variance are
          
          
          We assume that μ = 
          p
          c in the control group and μ = 
          p
          v = (1 − 
          VE
          S )
          p
          c in the vaccinated group. Further, we assume that the
          coefficients of variation, 
          CV = σ/μ, of the distributions in the two groups are equal. With these
          assumptions, we can rewrite the two shape parameters of the β-distribution, α and β, in
          terms of the infection probability, 
          p, and the coefficient of variation, 
          CV:
          
          
          Hereby, 
          p = p
          c for the control group and 
          p = p
          v = (1 − 
          VE
          S )
          p
          c for the vaccinated group.
          To incorporate potential heterogeneity in susceptibility into the virtual low-dose
          challenge experiments, we replaced the probability of success in the Bernoulli trials
          (see above) with the individual infection probabilities.
        
        
          Power Analysis
          To calculate the statistical power of the single and the repeated low-dose challenge
          designs, we performed 100,000 such simulated experiments for a given number, 
          n, of animals per group. The statistical power can be estimated as the
          fraction of simulated experiments in which the vaccine is found to be significantly
          efficacious (significance level α = 0.05). We estimated the statistical power for the
          number of animals per group, 
          n, ranging from one to 20, and for vaccine efficacies 
          VE
          S = 0.67, 0.8, and 0.9. The power analysis outlined above was
          implemented in the R Language of Statistical Computing [25]. An R-script that performs
          the power analysis presented here is provided as Protocol S1.
          For large numbers of animals per group, 
          n, the statistical power can be approximated using asymptotic theory.
          For the single low-dose challenge design the power is approximately (e.g., [26], p.
          240):
          
          Hereby, Φ denotes the cumulative normal distribution,
          
          and 
          z
          α is the standard normal deviate associated with the one-tailed
          probability α (the significance level). Furthermore, 
          p
          c and 
          p
          v denote the infection probabilities of animals in the control
          and vaccinated groups, respectively, and 
          n the number of animals per group. The term 1/
          n in the numerator is the continuity correction [27,28].
          For the repeated low-dose challenge design, the number of challenges is not the same
          as the number of animals, 
          n, but is a random variable. The number of challenges for each
          individual is geometrically distributed with a maximum of 
          C
          max . The expected number of challenges in the control group, ,
          and the vaccinated group, are
          
          and
          
          Substituting the expected number of challenges for the actual number, we can
          approximate the statistical power of the repeated low-dose challenge design as
          
          Hereby, γ = (1/〈η
          c 〉 + 1/〈η
          v 〉)/2 is the continuity correction. For 
          C
          max = 1, equation 17 reduces to equation 13. Because the
          approximation in equation 17 involves the substitution of a random variable with its
          expectation, it is less accurate than the approximation for the power of the single
          low-dose challenge design in equation 13. The R-script provided as Protocol S1 also
          contains a function that calculates the statistical power using equation 17.
        
      
      
        Results
        
          Single Low-Dose Challenge Design Requires Large Numbers of Animals
          How would we measure protection against infection in a low-dose challenge model? The
          most straight-forward design would involve a large number of hosts, some vaccinated and
          some unvaccinated. After challenge with a low dose, one would determine the fraction of
          infected hosts in vaccinated and unvaccinated groups, and assess whether there is a
          statistically significant difference in the fractions (see Figure 1A).
          To assess how many animals would be required in a single low-dose challenge
          experiment, we performed a statistical power analysis (see Methods). The statistical
          power of an experimental design is defined as the probability that, in an experiment with
          an effective vaccine, the vaccine is correctly determined to be effective. Obviously the
          power depends on the efficacy of the vaccine (which is called the “effect size” in the
          context of power analysis) and the number of host animals used in the experiment. In the
          power analysis we performed, we assumed that we had equal numbers of unvaccinated and
          vaccinated animals, and that all animals within a group were equally susceptible to
          infection. Lastly, we assumed that the vaccine was “leaky” [29,30], i.e., that the
          susceptibility of vaccinated animals was by a constant factor lower than the
          susceptibility of the unvaccinated control animals.
          In virtual experiments, we then challenged each (virtual) animal once with a challenge
          dose of one 
          ID
          50 , the dose at which on average 50% of the unvaccinated animals
          become infected after a single challenge. Using a one-sided Fisher's exact test, we
          tested whether the fraction of infected animals in the vaccinated group was significantly
          lower than in the control group. Performing 100,000 such virtual experiments for a given
          number 
          n animals per group, we estimated the statistical power as the fraction
          of virtual experiments that yielded significant results (significance level α =
          0.05).
          The result of this power analysis is shown by the green curves in Figure 2. We
          calculated the power for vaccine efficacies of 67%, 80%, and 90%. We found that, even for
          the highest vaccine efficacy of 90%, the single low-dose challenge design required more
          than 20 animals per group to reach a statistical power of 95%. Thus, the single low-dose
          challenge design is not feasible, or at least not practical, to assess the efficacy of a
          vaccine or other preventive interventions in animals.
        
        
          Repeated Low-Dose Challenge Design Does Not Require Large Numbers of Animals
          We propose an alternative design involving repeated challenges of individual animals
          with low doses, which circumvents the disadvantage of the single low-dose challenge
          design that large numbers of host individuals are required. Repeated challenges
          effectively “recycle” host animals, thus increasing the statistical power of the
          experiment. In addition to increasing the statistical power of the experimental design,
          repeated challenges recapitulate much more realistically the circumstances of human
          exposure than single challenges. In this alternative design, the efficacy of a vaccine
          can be estimated by measuring the difference in the number of challenges needed to infect
          vaccinated versus unvaccinated hosts (see Figure 1B).
          To show that this alternative design does not require unfeasibly large numbers of
          animals, we performed a statistical power analysis (see Methods). As for the single
          low-dose design, we assumed that we had equal numbers of unvaccinated and vaccinated
          animals, and that all animals within a group were equally susceptible to infection. We
          further made the important assumption that the susceptibility of an individual animal was
          independent of how often the animal was unsuccessfully challenged previously. This
          assumption is commonly adopted in statistical models that are used to estimate the
          transmission rate of HIV [14–17]. By making this assumption, we ignored that an
          unsuccessful challenge may induce some degree of immunity against subsequent challenges.
          We would like to emphasize, however, that this assumption is not crucial for our
          argument, unless the degree of induced immunity is very high. Lastly, we again assumed
          that the vaccine was leaky [29,30].
          In virtual experiments, we then challenged the (virtual) animals repeatedly with a
          challenge dose of one 
          ID
          50 . We allowed for a maximum number of 20 challenges of each
          individual animal. Table 3 shows the outcome of one such virtual experiment. We analyzed
          the outcome of the virtual experiments with a one-tailed Fisher's exact test (see
          Methods). We again estimated the statistical power by performing 100,000 such virtual
          experiments for a given number 
          n animals per group.
          Figure 2 shows the statistical power of the repeated low-dose challenge design as a
          function of the number of animals per group for varying vaccine efficacies (black lines),
          and compares it to the statistical power of the single low-dose challenge design (green
          lines). The statistical power achieved with the repeated low-dose challenge design is
          generally higher than that achieved with the single low-dose challenge design. If the
          vaccine is 90% effective (
          VE
          S = 0.9), i.e., it reduces the susceptibility by a factor of ten,
          the number of animals per group could be as low as five to achieve more than 95%
          statistical power. In contrast, in single low-dose challenge experiments with the same
          number of animals per group the statistical power is only 15%. Thus, repeated low-dose
          challenge experiments are expected to require far fewer animals than single low-dose
          challenge experiments.
        
        
          How Often Should Virus Challenges Be Repeated?
          To investigate how the maximum number of challenges affected the statistical power, we
          plotted the power against 
          C
          max for trials involving six and 12 animals per group (Figure 3).
          We found that the power increases with 
          C
          max , but for high 
          C
          max the returns diminished considerably. The lower the number of
          animals per group, 
          n, the higher the maximum number of challenges, 
          C
          max , for which the power effectively saturated. Even for low
          numbers of animals per group, 
          n, however, the maximum number of challenges, 
          C
          max , needed to unfold the full potential of the repeated low-dose
          challenge design was in a feasible range.
        
        
          Impact of Animal-to-Animal Variation in Susceptibility
          To study how potential heterogeneity in susceptibility affected the power of low-dose
          challenge trials, we simulated experiments in which each animal was assigned an
          individual infection probability (see Methods). In these simulations, the degree of
          heterogeneity was measured by the coefficient of variation, 
          CV, of the susceptibility distributions. Figure 4A shows susceptibility
          distributions for three different values of 
          CV .
          We extended our power analysis by considering the impact of the heterogeneity
          parameter 
          CV on the statistical power (Figure 4B). We found that the statistical
          power of the single low-dose challenge design was almost unaffected by animal-to-animal
          variation in infection probability, whereas, for the repeated low-dose challenge design,
          the power decreased with increasing heterogeneity. Importantly, however, the power did
          not decrease linearly with heterogeneity: it was sufficiently stable in the range 0 < 
          CV < 0.3 and dropped mainly for 
          CV > 0.3. Thus, over a wide range of potential animal-to-animal
          variation in susceptibility, low-dose challenge designs are sufficiently powered, and the
          power of the repeated low-dose experiments is superior to that of single low-dose
          challenge experiments.
        
      
      
        Discussion
        Preclinical studies assessing the efficacy of potential vaccines, microbicides, or
        systemic chemoprophylaxis are usually conducted with very high virus challenge doses, which
        result in infection with certainty. Since these high challenge doses do not reflect the low
        probability of HIV transmission in humans, vaccines or prophylactic treatment strategies
        that are effective against “real life” exposures may go undetected in high-dose challenge
        experiments. For example, zidovudine was found to prevent a large fraction of perinatal HIV
        infections [23], even though studies in animal models, conducted with high challenge doses,
        could not establish any protection against infection by zidovudine [20–22].
        In this paper, we investigated how efficacy trials of vaccines and preventive treatment
        could be conducted with low challenge doses in animal models. We showed that the repeated
        low-dose challenge design is expected to require far fewer experimental animals than
        commonly believed. It may therefore be feasible to conduct trials with low challenge doses,
        which more realistically simulate exposures of humans to HIV, allowing us to more directly
        and sensitively assess vaccine or treatment efficacy than with high-dose challenge
        experiments.
        Owing to the concerns with high challenge doses, several research groups, including our
        own, have started to develop low-dose challenge models [31–34]. In these preliminary
        studies, infection could be achieved by challenging macaques intra-rectally [31],
        intra-vaginally [32,34], or orally [33].
        Since adopting low-dose challenge approaches has far-reaching consequences for the
        design of efficacy trials of vaccines or preventive treatment in animal models, we would
        like to discuss how some important aspects of trial design, such as transient infections,
        the challenge schedule, the route of infection, and the phenotype and dose of the challenge
        strain, should be dealt with and could be optimized.
        Using virus challenge doses that do not give rise to infection with certainty, one has
        to carefully define what one means by successful infection. This question is of particular
        importance in the repeated low-dose challenge design, because the efficacy of a preventive
        intervention is estimated on the basis of the number of challenges needed to infect an
        individual animal. Low-dose challenges have been observed to give rise to transiently
        detectable viremia [32–34]. Since transient infection is much more likely to lead to
        immunization [35], thus leading to lower probabilities of infection in subsequent
        challenges, we suggest considering transient viremia as successful infection and not to
        re-challenge animals that were transiently infected.
        The time interval between challenges is also an essential parameter in the design of
        repeated low-dose challenge experiments. In the four ongoing repeated low-dose challenge
        studies [31–34], different approaches have been taken, with time intervals ranging from
        hours to a week. There may be logistical reasons for choosing short time intervals between
        challenges, but from a statistical standpoint, the time intervals should be large enough to
        allow the identification of the challenge that gives rise to infection. Otherwise, the
        statistical power of the experimental design will be suboptimal and a beneficial effect of
        the vaccine candidate may be missed.
        In parallel to using more realistic, lower challenge doses, other crucial parameters of
        the experimental infection process, such as the route of transmission and the coreceptor
        usage of the challenge virus, should also be chosen to be as realistic as possible. Thus,
        we propose infecting intra-vaginally or intra-rectally in experiments that aim to assess a
        vaccine or prophylactic treatment against sexual transmission of HIV. Further, we suggest
        using challenge viruses that utilize CCR5 as coreceptor, such as for example SHIV-SF162P
        [36], rather than the standard strain SHIV89.6P, which has been found to use mainly CXCR4
        [37,38]. These more realistic choices of the route of infection and coreceptor usage will
        permit the assessment of the efficacy of the preventive intervention in a setting that more
        accurately reflects HIV exposures of humans, and will enable us to carefully investigate
        the processes that give rise to infection.
        The challenge dose in a low-dose challenge study is another parameter of crucial
        importance. Although the most realistic choice would be a challenge dose that gives rise to
        infection with a probability of approximately 0.0005–0.10 [14–17], such extremely low doses
        would require unfeasibly large numbers of repeated challenges per animal. Moreover, there
        is substantial variation in transmission rates due to differences in factors such as virus
        load or the presence of other infections of the genital tract [15–18], and theoretical
        studies suggest that preventing the transmission events that occur with higher probability
        would have a disproportionately large effect on controlling the epidemic [39]. To maximize
        their epidemiological relevance, low-dose challenge experiments should therefore involve
        challenge doses that reflect transmission probabilities at the upper end of the spectrum.
        As a compromise between the practicality of high doses and the sensitivity associated with
        realistically low doses, we propose the 
        ID
        50 . The 
        ID
        50 can be estimated using well-established nonparametric methods like
        Spearman-Kärber [40] or single-parameter methods [41], and there is software available,
        such as a freely distributed package called ID50 developed by John Spouge
        (http://www.ncbi.nlm.nih.gov/CBBresearch/Spouge/Virology/, which allows an automated
        estimation of the 
        ID
        50 from data generated in titration experiments.
        The inability to detect sterilizing immunity in high-dose challenge experiments led to a
        shift of focus towards indirect effects of vaccine candidates on the pathogenicity of the
        infection and the infectiousness of the vaccinee. This shift of focus required the
        development of novel statistical models that allowed the estimation of these indirect
        effects [42,43]. Will the estimation of vaccine efficacy in repeated low-dose challenge
        studies also require the development of novel statistical techniques? The answer to this
        question depends on how much the realities of the infection process deviate from our
        idealized model. There are three potential deviations. First, we assumed in large parts of
        this study that the susceptibilities to infection were equal for all animals within each
        group. This is almost certainly not the case. Although we have shown that low-dose
        challenge experiments are sufficiently powered even if there is substantial
        animal-to-animal variation in susceptibility, we did not develop the statistical techniques
        that would allow the estimation of this variation. The extent of animal-to-animal variation
        in susceptibility can, in principle, be estimated, but this will probably require larger
        numbers of animals than the estimation of vaccine efficacy. Second, the vaccine may affect
        the susceptibility of individual animals differently. While we assumed in the present study
        that the vaccine is leaky, i.e., that the susceptibility is reduced by a constant factor in
        each animal, other modes of action of a vaccine are possible. In particular, some animals
        could be completely protected by vaccination, while others may remain completely
        susceptible. This mode of action is referred to as all-or-none [29,30]. Statistical methods
        based on maximum likelihood approaches exist that allow the determination of the mode of
        action of a given vaccine. However, these methods are based on large sample asymptotics,
        and exact methods will have to be developed to analyze the outcome of low-dose challenge
        experiments that involve small numbers of animals. Last, it will have to be determined
        whether the probability of infection changes with the number of challenges performed in a
        given animal, or, to put it differently, whether the animal has a “memory” of previous
        challenges. In our analysis, we assumed that the susceptibility of an animal did not change
        from challenge to challenge. If the probability of infection changes significantly with the
        number of challenges, however, the development of novel statistical models that take such
        changes into account will be necessary to adequately estimate vaccine efficacy.
        In addition to the potential to assess the vaccine or microbicide efficacy more
        sensitively and in a more realistic setting, a low-dose challenge approach may enable us to
        answer questions that cannot even be asked in high-dose challenge models. Some of the most
        relevant of these questions relate to the effect of challenges that do not lead to
        infection. If a low-dose challenge does not give rise to infection, where was the virus
        blocked? Did the virus fail to establish an infection at all? Or did it replicate
        transiently, but was cleared by the host's immunity? And, very importantly, is an
        unsuccessfully challenged animal partially immunized against further challenges, or,
        alternatively, do unsuccessful challenges facilitate future infection by “seeding” animals
        with defective proviruses that may recombine with complementing viruses upon subsequent
        exposures [44]?
        The answers to these questions would greatly enhance our understanding of HIV
        transmission and pathogenesis, and thus would provide further guidance toward an effective
        vaccine or microbicide. Furthermore, by assessing the protection against infection
        directly, we may be able to discern the specific types and levels of vaccine-induced
        cellular and humoral immune responses associated with sterilizing immunity [13]. This would
        provide important benchmarks by which to judge new vaccine candidates, and could also allow
        retrospective analysis of vaccine candidates evaluated earlier in high-dose challenge
        studies.
        In conclusion, the repeated low-dose challenge approach may enable us to assess the
        potential efficacy of vaccines and prophylactic treatment strategies more realistically,
        and more sensitively than the standard high-dose challenge approach. The increased
        sensitivity may allow us to more rapidly identify interventions that significantly reduce
        the transmission of low-dose infections that characterize the natural spread of HIV.
      
      
        Supporting Information
      
    
  

  
    
      
        
        People living with HIV/AIDS (PLWA) face many forms of stigma and discrimination. This is
        the case in whichever country they may live, as has been shown in a number of previous
        research studies. In addition to experiencing unfair treatment in their families,
        communities, and places of work, PLWA may encounter discrimination from health-care
        professionals. This can interfere with effective prevention and treatment. Discriminatory
        practices in the health-care sector may also appear to legitimize other forms of
        discrimination against PLWA.
        Vincent Iacopino and colleagues from the organization Physicians for Human Rights, in
        collaboration with researchers from Policy Project–Nigeria and the Center for the Right to
        Health (also in Nigeria) investigated the problem in Nigeria. With a population of roughly
        130 million, Nigeria is home to one in 11 of the 40 million PLWA worldwide. Around 6% of
        adult Nigerians are thought to be HIV-positive, and there will be an estimated 310,000 AIDS
        deaths this year. The indications are that infection rates will increase. Until now, little
        has been known about the nature and extent of discrimination against patients with HIV/AIDS
        in Nigeria.
        Trained interviewers conducted a cross-sectional questionnaire survey of 1,021 Nigerian
        health-care professionals in 111 health-care facilities in four of Nigeria's 36 states.
        Those sampled were 324 physicians, 541 nurses, and 133 midwives, and 23 health-care workers
        of unknown profession. Fifty-four percent of them worked in public tertiary care
        facilities. Many of the survey's results are worrying. Nine percent of professionals
        reported refusing to care for a patient with HIV/AIDS, and 9% said they had refused a
        patient with HIV/AIDS admission to a hospital. Fifty-nine percent agreed that PLWA should
        be on a separate ward, and 40% believed a person's HIV status could be determined by their
        appearance. Ninety-one percent agreed that staff should be informed when a patient was
        HIV-positive in order to protect themselves. Forty percent believed health-care
        professionals with HIV/AIDS should not be allowed to work in any area of health-care
        requiring patient contact. Twenty percent agreed that many with HIV/AIDS had behaved
        immorally and deserved their infection. Eight percent felt that treating someone with
        HIV/AIDS was a waste of resources.
        Providers who reported working in facilities that did not always practice universal
        precautions against HIV transmission were more likely to favor restrictive policies towards
        PLWA. In general, basic materials needed for treatment and prevention of HIV infection were
        not sufficiently available. Providers who reported less adequate training in HIV/AIDS
        treatment and in ethics were more likely to report negative attitudes towards patients with
        HIV/AIDS. There was no consistent pattern of differences in negative attitudes and
        practices across the different professions surveyed.
        The researchers concluded that, while most health-care professionals surveyed reported
        being in compliance with their ethical obligations, discriminatory behavior and attitudes
        towards patients with HIV/AIDS existed among a significant proportion. Inadequate education
        about HIV/AIDS and a lack of protective and treatment materials appear to favor these
        practices and attitudes. The findings of the study, in just four states, cannot be
        generalized to Nigeria as a whole and, although sampled systematically, it is possible that
        sampled facilities and health-care professionals may differ significantly from those that
        were not sampled in the study states. Concerns over a perceived lack of privacy in the
        interviews or about job status may have resulted in an underreporting of discriminatory
        behavior and/or an overreporting of “correct” practices or attitudes. The authors note that
        the health-care system in Nigeria is underfunded and suffers from fundamental problems,
        including material scarcity and inadequacies in infrastructure, both of which may
        contribute to discriminatory behavior. They call for targeted education of health-care
        professionals and provision of adequate resources to health-care facilities, and for the
        introduction and enforcement of anti-discrimination policies.
      
    
  

  
    
      
        
        Animal models can play an essential role in guiding preclinical vaccine development,
        including in studies of preclinical vaccine safety, vaccine toxicity, and vaccine
        immunogenicity. Appropriate pathogen challenge models can also provide the opportunity to
        perform preclinical tests of vaccine efficacy. Preclinical tests of HIV vaccine efficacy
        are usually performed by exposing macaques to simian immunodeficiency virus (SIV), a virus
        that is closely related to HIV. However, the viral inoculum sizes used to infect macaques
        with SIV vastly exceed the amounts of HIV that humans are exposed to during a given
        exposure. Typically animals are exposed to 10–100 times the infectious dose at which 50% of
        the animals become infected (ID50). These excessive doses may not provide realistic
        preclinical tests of vaccine efficacy. Indeed, no vaccine has been shown to be effective in
        preventing infection by SIV (so-called sterilizing immunity) in such high viral inoculum
        trials. Now, in a paper published in 
        PLoS Medicine , Roland Regoes and colleagues speculate that an
        alternative approach to trials in animals not only can mimic the human patterns of repeated
        low-dose exposure, but also can remove one concern for animal researchers—the need to use
        very large numbers of animals in experiments.
        What the researchers did was use statistical power analysis to compare a single low-dose
        challenge design, in which each animal is challenged only once, and a repeated low-dose
        challenge design, in which each animal is challenged until it is infected or a
        predetermined maximum number of challenges is reached. The statistical power of an
        experimental design—a measure of the statistical quality—was assessed by simulating
        experiments, evaluating them, and then repeating the procedure thousands of times.
        What they found was that the experimental design using a single low dose of virus in
        each animal required unfeasibly large numbers of animals; even for the highest modeled
        vaccine efficacy of 90% the single low-dose challenge design required more than 20 animals
        per group to reach a statistical power of 95%. However, when the researchers modeled a
        protocol of repeatedly challenging the (virtual) animals with a challenge dose of one 
        ID
        50 , and allowing for a maximum number of 20 challenges of each
        individual animal, as few as five animals were required to achieve more than 95% of
        statistical power.
        Where do these results leave the design of HIV trials? To begin with, the results should
        encourage researchers to develop animal models that reflect, to the fullest extent
        possible, what is known about the natural history and pathogenesis of the disease in
        humans, rather than designing trials to fit the animal models that are available. The
        authors have made available the programming script of their analysis so anyone can repeat
        it; it would be interesting to know whether preclinical trials assessing vaccines or
        treatments against infections by other pathogens could be usefully modeled in this way as
        well.
      
    
  

  
    
      
        
        Approximately 40% of the world's population, mostly living in the world's poorest
        countries, is at risk of malaria. In the tropical and subtropical regions of the world,
        malaria causes 300 million acute illnesses and at least 1 million deaths annually. Ninety
        percent of these deaths occur in Africa, south of the Sahara, mostly among young
        children.
        To assess disease severity, peripheral blood parasitemia is measured, but this is only a
        weak predictor of mortality in falciparum malaria. In addition, a microscopist is only able
        to count the less pathogenic circulating stages of the parasite, whereas the more
        pathogenic parasitized erythrocytes, sequestered in the capillaries and containing mature
        parasites, are not seen and therefore not counted. However, sequestered 
        Plasmodium falciparum parasites secrete Histidine-rich protein 2
        (PfHRP2), which is liberated into the plasma at schizont rupture.
        In this month's 
        PLoS Medicine , Arjen Dondorp and colleagues suggest that the plasma
        concentration of this protein might provide a better estimate for the patient's total
        parasite biomass and therefore be a more accurate prognostic indicator than circulating
        parasite load. There is evidence to support this hypothesis. A recent study by the same
        team measured PfHRP2 in 
        P. falciparum cultures, and showed that approximately 89% of
        PfHRP2 is liberated at schizont rupture and that the variation in the amount released is
        limited.
        In the current study the researchers measured plasma PfHRP2 concentrations in 337
        patients with varying severity of falciparum malaria and, using a simple mathematical
        model, estimated the total body parasite biomass. This value was compared with measures of
        disease severity and outcome. The developmental stage distribution of circulating
        parasites, which also provides information on the sequestered parasites, was also evaluated
        in relation to plasma PfHRP2 levels in these patients.
        The researchers found that the estimated geometric mean parasite burden was more than
        six times higher in patients with severe malaria than in patients hospitalized without
        signs of severe disease, and was highest in patients who died. Statistical analysis
        revealed that the estimated total parasite biomass was clearly associated with disease
        severity and outcome. By contrast, peripheral blood parasitemia and the number of
        circulating parasites were not associated with disease outcome, nor with other measures of
        severity such as admission plasma lactate concentrations.
        The finding that sequestered parasite biomass is associated with disease severity fits
        with current thinking that sequestration of erythrocytes containing the mature forms of the
        parasite is the central pathological process in falciparum malaria.
        However, the team noted there were several factors that might contribute to inaccuracies
        in the model. For example, the amount of PfHRP2 secreted per parasite varies between
        different parasite strains. Also, in high transmission areas, where partial immunity
        against the disease develops, clearance of PfHRP2 might be increased in the presence of
        antibodies against the protein; in these areas—such as countries in sub-Saharan Africa—the
        model would thus underestimate the parasite burden and might need to be adapted further for
        use.
        Despite these issues, estimates of plasma PfHRP2 concentrations may be useful as a
        research tool to stratify patients' parasite loads, say the authors. They conclude that
        quantitative measurements of plasma PfHRP2 in patients with falciparum malaria could be
        used to estimate the total parasite biomass, a parameter pivotal in the pathophysiology of
        the disease, and that this total parasite biomass is associated with clinical measures of
        the severity of the disease.
      
    
  

  
    
      
        
        Scientific truth is a moving target. In the process of peer review, authors, reviewers,
        and editors work together to minimize the reporting of false results. However, even if one
        assumes no bias, wrongdoing, or ignorance on the part of any of the individuals
        involved—which is unrealistic, no doubt—chances are that some findings will turn out to be
        false. But is it inevitable, as John Ioannidis argues in an Essay in this issue of 
        PLoS Medicine (DOI: 10.1371/journal.pmed.0020124), that the majority of
        findings are actually false?
        Although his calculations are based on assumptions about complex scenarios that we do
        not fully understand—as is true for most research projects—Ioannidis argues convincingly
        that many published findings will turn out to be false.
        Ioannidis is not the first to raise some of these concerns. Indeed, there are already
        initiatives under way that seek to address them. Increasingly, researchers design
        individual studies, systematic reviews, and meta-analyses using Bayesian statistics, in
        which the issue of pre-study odds is taken into account. And issues such as reducing
        sources of bias when assessing evidence are addressed in the methodology used by the
        Cochrane Collaboration in the production of its systematic reviews.
        Ioannidis doesn't define “findings,” but it seems important to attempt to separate data
        (“in this study 5% of people examined who lived in San Francisco from 1965–1970 developed
        lung cancer compared with 20% of people studied who lived in Anchorage”) from conclusions
        (“lung cancer rates are higher in Anchorage than San Francisco”) and hypotheses (“cold
        weather exacerbates the consequences of smoking”).
        Hypotheses will inevitably change, as they depend not only on the study but also on the
        context of other relevant research and knowledge. Conclusions are also often based on
        current knowledge and assumptions, and, thus, subject to change. The data should be more
        robust; for instance, other researchers applying the same methods to study the same group
        of patients at the same time should be able to generate the same data. However, research
        progress depends on conclusions being tested elsewhere. The major issue about the truth of
        research findings would therefore seem to concern the conclusions, and Ioannidis's claim
        that most conclusions are false is probably correct. Is that a problem? Can it be
        avoided?
        The possibility that most conclusions are false might be an inevitable part of the
        research endeavor. That said, researchers and those involved in publication of research
        must do what they can to reduce false conclusions.
        One way to do this is to delay publication until such a time when the chances that a
        conclusion is true are sufficiently high. If many published conclusions are false, we
        (editors and reviewers) need to ask ourselves whether we are setting the bar too low. But
        what is the consequence of setting it higher?
        Research progress depends on dissemination of results, and journal articles are the most
        effective tool we currently have to share them. The answer, therefore, cannot be that we
        wait until conclusions are proven beyond a doubt before we publish them. Publication of
        preliminary findings, negative studies, confirmations, and refutations is an essential part
        of the process of getting closer to the truth. Everyone involved in the generation and
        publication of research results needs to be open-minded, rigorous, and honest in designing
        experiments, analyzing results, reporting findings, peer-reviewing manuscripts, providing
        comments, and accepting that uncertainty exists in research.
        Ioannidis suggests how studies could be designed from the outset to increase their
        chances of producing true results. He also gives some corollaries that allow readers to get
        a sense of the extent of uncertainty for a particular study. He stresses that reliable
        evidence generally comes from several studies and from several teams of researchers, and
        that what matters is the totality of the evidence.
        What can editors do? At high-impact journals such as 
        PLoS Medicine , we see it as our job to select important articles. This
        means the conclusions reported should be more rather than less likely to be true. But
        better measures of importance are that a study should address a substantial clinical or
        public- health question, in as rigorous a way as possible, and the findings should be
        likely to have an effect on how other researchers think about the question. In reporting
        studies, we ask that data are clearly delineated from conclusions, and conclusions from
        hypotheses. In addition to individual studies, editors should (and at 
        PLoS Medicine we do) ensure there is a place for articles that synthesize
        evidence from different sources.
        Too often editors and reviewers reward only the cleanest results and the most
        straightforward conclusions. At 
        PLoS Medicine , we seek to create a publication environment that is
        comfortable with uncertainty. We encourage authors to discuss biases, study limitations,
        and potential confounding factors. We acknowledge that most studies published should be
        viewed as hypothesis-generating, rather than conclusive. And we publish high-quality
        negative and confirmatory studies.
        We also accept some responsibility for educating consumers of research about the
        research process. Consumers also need to become comfortable with uncertainty, and
        understand the strengths and weaknesses intrinsic to every study conducted and published.
        Besides selecting papers and influencing how results are reported, we use the synopses and
        patient summaries to highlight uncertainties in research papers. We also encourage
        contributions such as the essay by Ioannidis to our magazine section that will help
        research producers and consumers to understand research findings in context.
      
    
  

  
    
      
        
        Inflammatory bowel disease (IBD) is the term that encompasses chronic relapsing diseases
        characterized by inflammation of the bowel, specifically ulcerative colitis (UC) and Crohn
        disease (CD). They are common disorders; in the UK, for example, together they affect about
        one person in every 400, amounting to a total of 120,000 cases of UC and 60,000 of CD, with
        6,000 new cases of UC and 3,000 new cases of CD every year. Current estimates total at more
        than 1 million cases in the US and Europe. The symptoms for both these conditions—which
        suggest an abnormal immune response at the intestinal mucosa—include abdominal pain,
        diarrhea, fever, severe fatigue, and weight loss. Current understanding of disease
        pathogenesis suggests a complex action of multiple environmental factors that trigger
        disease in individuals with a susceptible genetic background.
        Today, finding genes that have a role in diseases has been made easier by the sequencing
        of the human genome and creation of an expressed sequence tag clone database. Previous
        positional cloning studies have revealed three genes associated with IBD that carry
        variants with a causative role. Another of the tools for analyzing genes is microarray
        technology, in which the expression of transcripts of thousands of genes can be
        investigated simultaneously. This approach offers an insight into disease
        pathophysiology.
        In a research article published in 
        PLoS Medicine , Christine Costello and colleagues have gone further with
        cDNA microarrays to attempt to decipher gene regulatory events and identify genes that
        might be involved in the pathophysiology of these IBD. They found 650 genes that were
        differentially regulated between normal control individuals and the individuals with one of
        the two IBD subtypes. In fact, 500 and 272 differentially regulated transcripts were
        identified between control individuals and patients with CD and UC, respectively. There was
        an imbalance between over- and underexpressed genes in the IBD subtypes. In CD,
        approximately 84% of differentially expressed genes were found to be down-regulated
        compared with 42% of genes in UC. However, the authors caution that this finding was highly
        influenced by the types and numbers of genes present on any microarray system; in addition,
        none of the 122 differentially expressed genes in CD and UC was overexpressed in one
        disease and underexpressed in the other. These observations support the notion of a shared
        general inflammatory profile underlying each form of IBD, with more specific events in the
        pathophysiological cascade being disease-specific.
        In a second part of the study, the team tried to interpret the functional consequences
        of changes in gene expression observed in the microarray analysis. They used an
        annotation-based pathway database, which classified differentially expressed genes into
        three major groups: immune and inflammatory response; oncogenesis, cell proliferation, and
        growth; or structure and permeability. For immune and inflammatory response, the team
        identified many genes associated with aberrant immune response; it is not surprising,
        perhaps, to find a general up-regulation of immune response and antigen presentation in
        IBD. Several genes associated with cell growth and proliferation were up-regulated in UC—a
        finding similar to previous microarray studies that had reported involvement of
        cancer-related genes in IBD (although the altered genes were different). There was also an
        enrichment of genes associated with structure and permeability; in this class several genes
        were ubiquitously altered in both IBD and non-IBD samples, reflecting dysregulation of
        genes for paracellular permeability, degradation of extracellular matrix, and barrier
        protection against bacterial invasion of the epithelial surface.
        Ultimately this study highlights the complex pathogenesis of UC and CD, and indicates
        some possible future avenues for research of mucosal diseases in general. It demonstrates
        that genomic technologies are suitable to directly dissect human pathophysiology.
      
    
  

  
    
      
        
        The World Health Organization estimates that about 20% of all deaths in children younger
        than five years old are due to acute lower respiratory tract infections (LRTIs), with 90%
        of these deaths due to pneumonia. But despite LRTIs being among the most frequent diseases
        in the first years of life, the viral causes of these illnesses are not always clear.
        Several viruses are known to be involved, e.g., respiratory syncytial virus (RSV),
        influenza viruses, parainfluenza viruses, and human metapneumovirus, but none of these
        pathogens is detected in a substantial number of cases.
        In a new article published in 
        PLoS Medicine , Lia van der Hoek and colleagues investigate the
        association of acute LRTIs with human coronavirus HCoV-NL63, a virus they recently
        described. They suggest that HCoV-NL63, a new member of the Coronaviridae family, is one of
        the most frequently detected viruses in children less than three years old with LRTIs and
        that this virus is strongly associated with croup.
        The team analyzed samples from the PRI.DE study, a prospective population-based study of
        LRTIs in children younger than three years old in Germany. They assessed by PCR 949 samples
        of nasopharyngeal secretions from children with LRTIs.
        In all, 49 samples (5.2%) were positive for HCoV-NL63 RNA. Viral RNA was more prevalent
        in samples from outpatients (7.9%) than hospitalized patients (3.2%), and co-infection with
        either RSV or parainfluenza virus 3 was observed frequently. With an overall occurrence of
        5.2%, HCoV-NL63 was the third most frequently detected pathogen in this patient group (RSV
        was the highest, found in 31.4% of samples). The researchers focused on HCoV-NL63 in cases
        of respiratory disease where no other viral pathogen could be detected in order to identify
        clinical symptoms associated with HCoV-NL63 infection. Samples in which only HCoV-NL63 RNA
        could be detected had a significantly higher viral load than samples containing additional
        respiratory viruses. A strong association with croup was apparent: 43% of the
        HCoV-NL63-positive patients with high HCoV-NL63 load and absence of co-infection had croup,
        compared with 6% in the HCoV-NL63-negative group. Indeed, a significantly higher fraction
        (17.4%) of samples from croup patients than non-croup patients (4.2%) contained HCoV-NL63
        RNA.
        This study strengthens the evidence for the role of HCoV-NL63 in croup. Previous studies
        have shown croup to occur mostly in boys, with peak occurrence in the second year of life
        and predominantly in the late autumn or early winter season. And HCoV-NL63 infection seems
        to follow these trends, said the authors.
        However, the authors warned that the high percentage of HCoV-NL63-positive samples could
        be due to a strong viral activity in the study year, and long-term studies are needed to
        determine whether HCoV-NL63 infections occur in cycles peaking every two to three years, as
        observed for other respiratory viruses. But they noted that HCoV-NL63 has spread worldwide,
        with the virus found in Australia, Canada, Japan, Belgium, and the US. Thus, health
        authorities should add HCoV-NL63 to the list of pathogens that can cause numerous LRTIs in
        young children.
      
    
  

  
    
      
        
        Dementia remains an incurable condition and its increasing prevalence is a deeply
        worrying aspect of the “graying” of the population. An important question for researchers
        is to establish whether dementia incidence, prevalence, and national history vary from one
        location to another. Incidence studies are particularly valuable for less biased comparison
        of disease occurrence, as well as being essential for policy makers. Many biases can,
        however, be introduced in such studies. Dropout and mortality are particular reasons for
        concern.
        Most of the numerous studies of dementia incidence have been restricted to single sites.
        Authors have frequently attempted to assess whether rates in a given study are similar to
        those obtained elsewhere. However, variations between studies in the methodology employed
        make such comparisons unreliable. Where within-country variations in incidence have been
        noted, as has happened in the US, they have often been ascribed to methodological
        differences, but one cannot be certain whether this is the case.
        Risk factors for other chronic disorders common in old age (notably cardiovascular
        disease and cancers) do vary in their prevalence between and within countries. In the UK,
        for example, the incidence of stroke is known to vary considerably across the country. A
        high proportion of dementia patients are thought to have a vascular component to their
        dementia, and it has been assumed that dementia incidence could be reduced if vascular risk
        were better controlled. One way to test this hypothesis is to compare sites with known
        variation in vascular risk to assess whether there is also variation in the incidence of
        dementia.
        The Medical Research Council Cognitive Function and Ageing Study (MRC CFAS) is a
        multi-site, population-based study in the UK of individuals aged 65 years and over living
        in the community, including institutions. Diverse sites have been chosen, with varying
        exposures of potential importance in dementia. A two-phase two-wave design has been
        employed, with the waves two years apart. A standard set of instruments for the diagnosis
        of dementia is used throughout. CFAS now publishes incidence estimates from five sites,
        using likelihood-based methods to compare the first two waves of interviews.
        Predictably, incidence rates of dementia, for both sexes, were found to rise with age,
        from 6.7 per 1,000 person years at age 65–69 years to 68.5 per 1,000 person years at age 85
        years and above. The authors estimate that around 163,000 new cases of dementia occur in
        England and Wales each year. However, there was no convincing evidence of variation across
        sites, and the incidence rates do not reflect the variations in the prevalence of possible
        risk factors in these sites. We therefore cannot assume that action to reduce vascular risk
        will have a significant impact on dementia incidence.
        Another issue addressed by the study is previous suggestions in the literature that
        dementia incidence rates might be lower in the oldest age groups. The limited number of
        respondents in these age groups in previous studies made it impossible to test this
        hypothesis. The CFAS, however, found no evidence of any such tailing off in incidence,
        which also has implications for policy and planning.
        The CFAS is important because it provides the first multi-site comparison of incidence
        rates in ethnically homogeneous populations within a country, and within Europe, using
        identical methodology across sites. The methodological approach developed for the study
        will also be of value for researchers undertaking other studies of dementia incidence, and
        in other chronic disease studies involving a two-phase selection process.
      
    
  

  
    
      
        
        The only people who don't know in 2005 that animal research is irrelevant for human
        disease are those who don't understand it or those who benefit from it. As a physician,
        clinical researcher, and former animal researcher, I know that though they are our closest
        genetic relatives, primates have failed as research models virtually whenever they have
        been used.
        As a partial list of failures, allow me to submit the notorious forced smoking
        experiments, which allowed cigarettes to be promoted widely for decades; the abject failure
        of a quarter-century of primate research on AIDS to provide any useful insights; the false
        leads and dangerous vaccines produced during polio research (verified by Albert Sabin,
        himself); the failure of primate studies to improve risks for birth defects and premature
        births; and the failure of monkey studies to identify nonsteroidal anti-inflammatory drug
        cardiovascular risk [1].
        The 
        PLoS Medicine editors state in hopeful language that the Lassa fever
        vaccine was successful in four monkeys, and, thus, is a suitable agent for human study [2].
        Recall that VaxGen's AIDS vaccine (AIDSVAX) showed great success in primate studies, but
        was an abject failure in two human clinical trials, including a trial of over 2,500
        injection drug users in Thailand [3] and a multinational trial of over 5,000 high-risk
        individuals [4].
        Consider the fruitless decades-long effort to produce an AIDS vaccine in primates, the
        failure to produce even a single case of human AIDS in any primate studied, or the failure
        to identify even one useful AIDS drug from primate studies. Genetic and physiological
        imperatives dictate that no animal model, even higher primates, gives information
        applicable to humans. The Human Genome Project [5] tells us that there is sufficient
        genetic diversity among humans that pharmacogenetic and pharmacogenomic techniques will
        have an increasing role in overcoming problems related to polymorphisms and other
        variations. We can't even apply scientific findings uniformly to humans, and 
        PLoS Medicine is now promoting monkey research?
        I am very disappointed that 
        PLoS Medicine has regressed to reporting animal research. It is
        discouraging that in this era of rapid biomedical advancement, and appropriate relegation
        of animal research to the historical dustbin, PLoS has chosen to re-introduce an
        anachronistic, medically discredited, and unethical research tool to its reporting.
      
    
  

  
    
      
        
        Whistleblowers serve no function if they cannot tell their stories. The present story of
        whistleblowing—as discussed, in part, in 
        PLoS Medicine —that involves the pharmaceutical industry, pharmaceutical
        benefit management corporations, the managed care industry, and the political and lobbying
        forces that zealously guard their secrets could not have been told without the help of
        courageous men and women [1, 2] For that reason, those of us who congregated in Washington,
        D.C., on May 15th, 2005, at the invitation and support of the Public Library of Science and
        the Government Accountability Project feel particularly humbled and grateful to these two
        sponsors. Our convictions could not have been aired were it not for the essential First
        Amendment work of responsible journalists, who exemplify the best in investigatory
        research.
        For me, whistleblowing is not a theoretical exercise. It has a human face and tangible
        features. It is the face of children and adults who have been injured or killed by
        misrepresented pharmaceuticals; clinical research trial results that have been sequestered
        from the scientific community and whose incomplete findings cause injury; and
        pharmaceuticals that are detailed to physicians, not to save lives or necessarily improve
        the health or welfare of the recipients, but to make money.
        In the lonely and, at times, discouraging world of whistleblowing, we whistleblowers are
        passionate, and often successful, because our efforts have a different goal than the
        corporations and political interests whose operations we occasionally challenge. Our goal
        is to tell the truth. That honest effort is the source of any ethical difference we can or
        might make. Truth is the basis for the power of a whistleblower, one that can withstand the
        assault of unprecedented odds against being heard put forth by that sum of political power,
        expediency, and money.
        A whistleblower's success depends upon competent and articulate media. The debate to
        improve the status quo—be it in pharmaceutical marketing or managed-care decision
        making—cannot proceed or flourish without it.
        Ralph Waldo Emerson, American essayist and philosopher (1803–1882), commented about
        success (I have adapted his comments for all of us who gathered in Washington in mid-May
        2005): “To leave the world a bit better, whether by a healthy child, a garden patch or a
        redeemed social condition; to know even one life breathed easier because you have lived;
        this is to have succeeded [as a whistleblower].”
      
    
  
